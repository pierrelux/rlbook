
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Projection Methods for Functional Equations &#8212; Practical Reinforcement Learning: From Algorithms to Applications</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=b4b7a797" />
    <link rel="stylesheet" type="text/css" href="_static/graphviz.css?v=fd3f3429" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"bm": ["{\\boldsymbol #1}", 1]}, "processEscapes": true}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'projdp';</script>
    <script src="_static/iframe-modal.js?v=f72a1242"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Simulation-Based Approximate Dynamic Programming" href="simadp.html" />
    <link rel="prev" title="Smooth Bellman Optimality Equations" href="regmdp.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Practical Reinforcement Learning: From Algorithms to Applications</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Why This Book?
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Modeling</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="modeling.html">Why Build a Model? For Whom?</a></li>

<li class="toctree-l1"><a class="reference internal" href="ssm.html">Dynamics Models for Decision Making</a></li>




<li class="toctree-l1"><a class="reference internal" href="simulation.html">Programs as Models</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Numerical Trajectory Optimization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ocp.html">Discrete-Time Trajectory Optimization</a></li>


<li class="toctree-l1"><a class="reference internal" href="cocp.html">Trajectory Optimization in Continuous Time</a></li>




</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">From Trajectories to Policies</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="mpc.html">Model Predictive Control</a></li>




<li class="toctree-l1"><a class="reference internal" href="dp.html">Dynamic Programming</a></li>



</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Approximate Dynamic Programming</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="regmdp.html">Smooth Bellman Optimality Equations</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Projection Methods for Functional Equations</a></li>
<li class="toctree-l1"><a class="reference internal" href="simadp.html">Simulation-Based Approximate Dynamic Programming</a></li>



<li class="toctree-l1"><a class="reference internal" href="cadp.html">Policy Parametrization Methods</a></li>







</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="appendix_examples.html">Example COCPs</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix_ivps.html">Solving Initial Value Problems</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix_nlp.html">Nonlinear Programming</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/pierrelux/rlbook" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pierrelux/rlbook/edit/main/projdp.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pierrelux/rlbook/issues/new?title=Issue%20on%20page%20%2Fprojdp.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/projdp.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Projection Methods for Functional Equations</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-general-framework">The General Framework</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-choose-a-finite-dimensional-approximation-space">Step 1: Choose a Finite-Dimensional Approximation Space</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-define-the-residual-function">Step 2: Define the Residual Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-choose-projection-conditions">Step 3: Choose Projection Conditions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#galerkin-method-test-against-the-basis">Galerkin Method: Test Against the Basis</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#method-of-moments-test-against-monomials">Method of Moments: Test Against Monomials</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#collocation-method-test-against-delta-functions">Collocation Method: Test Against Delta Functions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#subdomain-method-test-against-indicator-functions">Subdomain Method: Test Against Indicator Functions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#least-squares-an-alternative-framework">Least Squares: An Alternative Framework</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-solve-the-finite-dimensional-problem">Step 4: Solve the Finite-Dimensional Problem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-verify-the-solution">Step 5: Verify the Solution</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#application-to-the-bellman-equation">Application to the Bellman Equation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#collocation-for-the-bellman-equation">Collocation for the Bellman Equation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-the-collocation-system">Solving the Collocation System</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iterative-solution-successive-approximation">Iterative Solution: Successive Approximation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-of-solution-methods">Comparison of Solution Methods</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shape-preserving-considerations">Shape-Preserving Considerations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#galerkin-projection-and-least-squares-temporal-difference">Galerkin Projection and Least Squares Temporal Difference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-projected-bellman-equations">The Projected Bellman Equations</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="projection-methods-for-functional-equations">
<h1>Projection Methods for Functional Equations<a class="headerlink" href="#projection-methods-for-functional-equations" title="Link to this heading">#</a></h1>
<p>The Bellman optimality equation <span class="math notranslate nohighlight">\(\mathrm{L}v = v\)</span> is a functional equation: an equation where the unknown is an entire function rather than a finite-dimensional vector. When the state space is continuous or very large, we cannot represent the value function exactly on a computer. We must instead work with finite-dimensional approximations. This motivates projection methods, a general framework for transforming infinite-dimensional problems into tractable finite-dimensional ones.</p>
<p>Even if we restrict our search to a finite-dimensional subspace of functions, we still need to verify that our candidate solution <span class="math notranslate nohighlight">\(\hat{v}\)</span> satisfies the equation. For a true solution, the <strong>residual function</strong> <span class="math notranslate nohighlight">\(R(s) = \mathrm{L}\hat{v}(s) - \hat{v}(s)\)</span> should equal zero at every state <span class="math notranslate nohighlight">\(s\)</span> in our (potentially infinite) state space. But we cannot check infinitely many conditions.</p>
<p>Projection methods replace “the residual is zero everywhere” with a feasible requirement that we can verify computationally. We restrict our search to functions <span class="math notranslate nohighlight">\(\hat{v}(s) = \sum_{i=1}^n a_i \varphi_i(s)\)</span> for some basis <span class="math notranslate nohighlight">\(\{\varphi_1, \ldots, \varphi_n\}\)</span>, and find coefficients <span class="math notranslate nohighlight">\(a\)</span> that make the residual “small” according to a chosen criterion. The criterion determines the projection method.</p>
<section id="the-general-framework">
<h2>The General Framework<a class="headerlink" href="#the-general-framework" title="Link to this heading">#</a></h2>
<p>Consider an operator equation of the form</p>
<div class="math notranslate nohighlight">
\[
\mathscr{N}(f) = 0,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathscr{N}: B_1 \to B_2\)</span> is a continuous operator between complete normed vector spaces <span class="math notranslate nohighlight">\(B_1\)</span> and <span class="math notranslate nohighlight">\(B_2\)</span>. For the Bellman equation, we have <span class="math notranslate nohighlight">\(\mathscr{N}(v) = \mathrm{L}v - v\)</span>, so that solving <span class="math notranslate nohighlight">\(\mathscr{N}(v) = 0\)</span> is equivalent to finding the fixed point <span class="math notranslate nohighlight">\(v = \mathrm{L}v\)</span>.</p>
<p>The projection method approach consists of several conceptual steps that transform this infinite-dimensional problem into a finite-dimensional one.</p>
<section id="step-1-choose-a-finite-dimensional-approximation-space">
<h3>Step 1: Choose a Finite-Dimensional Approximation Space<a class="headerlink" href="#step-1-choose-a-finite-dimensional-approximation-space" title="Link to this heading">#</a></h3>
<p>We begin by selecting a basis <span class="math notranslate nohighlight">\(\Phi = \{\varphi_1, \varphi_2, \ldots, \varphi_n\}\)</span> and approximating the unknown function as a linear combination:</p>
<div class="math notranslate nohighlight">
\[
\hat{f}(x) = \sum_{i=1}^n a_i \varphi_i(x).
\]</div>
<p>The choice of basis functions <span class="math notranslate nohighlight">\(\varphi_i\)</span> is problem-dependent. Common choices include:</p>
<ul class="simple">
<li><p><strong>Polynomials</strong>: For smooth problems, we might use Chebyshev polynomials or other orthogonal polynomial families</p></li>
<li><p><strong>Splines</strong>: For problems where we expect the solution to have regions of different smoothness</p></li>
<li><p><strong>Radial basis functions</strong>: For high-dimensional problems where tensor product methods become intractable</p></li>
</ul>
<p>The number of basis functions <span class="math notranslate nohighlight">\(n\)</span> determines the flexibility of our approximation. In practice, we start with small <span class="math notranslate nohighlight">\(n\)</span> and increase it until the approximation quality is satisfactory. The only unknowns now are the coefficients <span class="math notranslate nohighlight">\(a = (a_1, \ldots, a_n)\)</span>.</p>
<p>While the classical presentation of projection methods emphasizes polynomial bases, the framework applies equally well to other function classes. Neural networks, for instance, can be viewed through this lens: a neural network <span class="math notranslate nohighlight">\(\hat{f}(x; \theta)\)</span> with parameters <span class="math notranslate nohighlight">\(\theta\)</span> defines a flexible function class, and many training procedures can be interpreted as projection methods with specific choices of test functions or residual norms. The distinction is that classical methods typically use predetermined basis functions with linear coefficients, while neural networks use adaptive nonlinear features. Throughout this chapter, we focus on the classical setting to develop the core concepts, but the principles extend naturally to modern function approximators.</p>
</section>
<section id="step-2-define-the-residual-function">
<h3>Step 2: Define the Residual Function<a class="headerlink" href="#step-2-define-the-residual-function" title="Link to this heading">#</a></h3>
<p>Since we are approximating <span class="math notranslate nohighlight">\(f\)</span> with <span class="math notranslate nohighlight">\(\hat{f}\)</span>, the operator <span class="math notranslate nohighlight">\(\mathscr{N}\)</span> will generally not vanish exactly. Instead, we obtain a <strong>residual function</strong>:</p>
<div class="math notranslate nohighlight">
\[
R(x; a) = \mathscr{N}(\hat{f}(\cdot; a))(x).
\]</div>
<p>For a true solution, this residual would be identically zero everywhere: <span class="math notranslate nohighlight">\(R(x; a) = 0\)</span> for all <span class="math notranslate nohighlight">\(x\)</span> in the domain. For our approximation, we aim to make it as close to the zero function as possible. <strong>The residual measures how far our candidate solution is from satisfying the equation at each point <span class="math notranslate nohighlight">\(x\)</span>.</strong></p>
<p>Think of <span class="math notranslate nohighlight">\(R(\cdot; a)\)</span> as living in an infinite-dimensional function space. You can imagine it as a “vector” with one component <span class="math notranslate nohighlight">\(R(x; a)\)</span> for each point <span class="math notranslate nohighlight">\(x\)</span> in the domain. In finite dimensions, we would check if a vector is zero by verifying that each component equals zero. Here, we face an impossible task: we cannot verify that <span class="math notranslate nohighlight">\(R(x; a) = 0\)</span> at every single point. The projection conditions we introduce next provide different feasible ways to measure “how zero” this function is.</p>
</section>
<section id="step-3-choose-projection-conditions">
<h3>Step 3: Choose Projection Conditions<a class="headerlink" href="#step-3-choose-projection-conditions" title="Link to this heading">#</a></h3>
<p>The heart of the projection method is choosing how to verify that <span class="math notranslate nohighlight">\(R(\cdot; a)\)</span> is “close to zero.” Since we cannot check <span class="math notranslate nohighlight">\(R(x; a) = 0\)</span> at every point, we must select a feasible criterion. Nearly all projection methods work by choosing <span class="math notranslate nohighlight">\(n\)</span> <strong>test functions</strong> <span class="math notranslate nohighlight">\(\{p_1, \ldots, p_n\}\)</span> and requiring:</p>
<div class="math notranslate nohighlight">
\[
\langle R(\cdot; a), p_i \rangle = \int_{\mathcal{S}} R(x; a) p_i(x) w(x) dx = 0, \quad i = 1, \ldots, n,
\]</div>
<p>for some weight function <span class="math notranslate nohighlight">\(w(x)\)</span>. This yields <span class="math notranslate nohighlight">\(n\)</span> equations to determine the <span class="math notranslate nohighlight">\(n\)</span> coefficients in <span class="math notranslate nohighlight">\(a\)</span>. The different projection methods are distinguished entirely by their choice of test functions <span class="math notranslate nohighlight">\(p_i\)</span>. If the residual has zero projection against all our test functions, we declare it “close enough” to the zero function.</p>
<p>In <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>, to verify a vector <span class="math notranslate nohighlight">\(\mathbf{r}\)</span> is zero, we could check if <span class="math notranslate nohighlight">\(\langle \mathbf{r}, \mathbf{e}_i \rangle = 0\)</span> for each standard basis vector <span class="math notranslate nohighlight">\(\mathbf{e}_i\)</span>. If <span class="math notranslate nohighlight">\(\mathbf{r}\)</span> is orthogonal to all coordinate directions, it must be the zero vector. For functions in infinite dimensions, we cannot test against all directions, so we choose <span class="math notranslate nohighlight">\(n\)</span> representative test functions and verify orthogonality against them.</p>
<p>Let us examine the standard choices of test functions and what they reveal about the residual:</p>
<section id="galerkin-method-test-against-the-basis">
<h4>Galerkin Method: Test Against the Basis<a class="headerlink" href="#galerkin-method-test-against-the-basis" title="Link to this heading">#</a></h4>
<p>The Galerkin method chooses test functions <span class="math notranslate nohighlight">\(p_i = \varphi_i\)</span>, the same basis functions used to approximate <span class="math notranslate nohighlight">\(\hat{f}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\langle R(\cdot; a), \varphi_i \rangle = 0, \quad i = 1, \ldots, n.
\]</div>
<p>To understand what this means, recall that in finite dimensions, two vectors are orthogonal when their inner product is zero. For functions, <span class="math notranslate nohighlight">\(\langle R, \varphi_i \rangle = \int R(x) \varphi_i(x) w(x) dx = 0\)</span> expresses the same concept: <span class="math notranslate nohighlight">\(R\)</span> and <span class="math notranslate nohighlight">\(\varphi_i\)</span> are orthogonal as functions. But there’s more to this than just testing against individual basis functions.</p>
<p>Consider our approximation space <span class="math notranslate nohighlight">\(\text{span}\{\varphi_1, \ldots, \varphi_n\}\)</span> as an <span class="math notranslate nohighlight">\(n\)</span>-dimensional subspace within the infinite-dimensional space of all functions. Any function <span class="math notranslate nohighlight">\(g\)</span> in this space can be written as <span class="math notranslate nohighlight">\(g = \sum_{i=1}^n c_i \varphi_i\)</span> for some coefficients <span class="math notranslate nohighlight">\(c_i\)</span>. If the residual <span class="math notranslate nohighlight">\(R\)</span> is orthogonal to all basis functions <span class="math notranslate nohighlight">\(\varphi_i\)</span>, then by linearity of the inner product, for any such function <span class="math notranslate nohighlight">\(g\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\langle R, g \rangle = \left\langle R, \sum_{i=1}^n c_i \varphi_i \right\rangle = \sum_{i=1}^n c_i \langle R, \varphi_i \rangle = 0.
\]</div>
<p>This shows that <span class="math notranslate nohighlight">\(R\)</span> is orthogonal to every function we can represent with our basis. The residual has “zero overlap” with our approximation space: we cannot express any part of it using our basis functions. In this sense, the residual is as “invisible” to our approximation as possible.</p>
<p>This condition is the defining property of optimality. By choosing our approximation <span class="math notranslate nohighlight">\(\hat{f}\)</span> so that the residual <span class="math notranslate nohighlight">\(R = \mathscr{N}(\hat{f})\)</span> is orthogonal to the entire approximation space, we ensure that <span class="math notranslate nohighlight">\(\hat{f}\)</span> is the orthogonal projection of the true solution onto <span class="math notranslate nohighlight">\(\text{span}{\varphi_1, \ldots, \varphi_n}\)</span>. Within this <span class="math notranslate nohighlight">\(n\)</span>-dimensional space, no better choice is possible: any other coefficients would yield a residual with a nonzero component inside the space, and therefore a larger norm.</p>
<p>The finite-dimensional analogy makes this concrete. Suppose you want to approximate a vector <span class="math notranslate nohighlight">\(\mathbf{v} \in \mathbb{R}^3\)</span> using only the <span class="math notranslate nohighlight">\(xy\)</span>-plane (a 2D subspace). The best approximation is to project <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> onto the plane, giving <span class="math notranslate nohighlight">\(\hat{\mathbf{v}} = (v_1, v_2, 0)\)</span>. The error is <span class="math notranslate nohighlight">\(\mathbf{r} = \mathbf{v} - \hat{\mathbf{v}} = (0, 0, v_3)\)</span>, which points purely in the <span class="math notranslate nohighlight">\(z\)</span>-direction, orthogonal to the entire <span class="math notranslate nohighlight">\(xy\)</span>-plane. This is precisely the Galerkin condition in action: the error is orthogonal to the approximation space.</p>
</section>
<section id="method-of-moments-test-against-monomials">
<h4>Method of Moments: Test Against Monomials<a class="headerlink" href="#method-of-moments-test-against-monomials" title="Link to this heading">#</a></h4>
<p>The method of moments, for problems on <span class="math notranslate nohighlight">\(D \subset \mathbb{R}\)</span>, chooses test functions <span class="math notranslate nohighlight">\(p_i(x) = x^{i-1}\)</span> for <span class="math notranslate nohighlight">\(i = 1, \ldots, n\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\langle R(\cdot; a), x^{i-1} \rangle = 0, \quad i = 1, \ldots, n.
\]</div>
<p>This requires the first <span class="math notranslate nohighlight">\(n\)</span> moments of the residual function to vanish, ensuring the residual is “balanced” in the sense that it has no systematic trend captured by low-order polynomials. The moments <span class="math notranslate nohighlight">\(\int x^k R(x; a) w(x) dx\)</span> measure weighted averages of the residual, with increasing powers of <span class="math notranslate nohighlight">\(x\)</span> giving more weight to larger values. Setting these to zero ensures the residual doesn’t grow systematically with <span class="math notranslate nohighlight">\(x\)</span>. This approach is particularly useful when <span class="math notranslate nohighlight">\(w(x)\)</span> is chosen as a probability measure, making the conditions natural moment restrictions familiar from statistics and econometrics.</p>
</section>
<section id="collocation-method-test-against-delta-functions">
<h4>Collocation Method: Test Against Delta Functions<a class="headerlink" href="#collocation-method-test-against-delta-functions" title="Link to this heading">#</a></h4>
<p>The collocation method chooses test functions <span class="math notranslate nohighlight">\(p_i(x) = \delta(x - x_i)\)</span>, the Dirac delta functions at points <span class="math notranslate nohighlight">\(\{x_1, \ldots, x_n\}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\langle R(\cdot; a), \delta(\cdot - x_i) \rangle = R(x_i; a) = 0, \quad i = 1, \ldots, n.
\]</div>
<p>This is projection against the most localized test functions possible: delta functions that “sample” the residual at specific points, requiring the residual to vanish exactly where we test it. When using orthogonal polynomials with collocation points at the zeros of the <span class="math notranslate nohighlight">\(n\)</span>-th polynomial, the Chebyshev interpolation theorem guarantees that forcing <span class="math notranslate nohighlight">\(R(x_i; a) = 0\)</span> at these specific points makes <span class="math notranslate nohighlight">\(R(x; a)\)</span> small everywhere. Using the zeros of orthogonal polynomials as collocation points produces well-conditioned systems and near-optimal interpolation error. The computational advantage is significant. Collocation avoids numerical integration entirely, requiring only pointwise evaluation of <span class="math notranslate nohighlight">\(R\)</span>.</p>
</section>
<section id="subdomain-method-test-against-indicator-functions">
<h4>Subdomain Method: Test Against Indicator Functions<a class="headerlink" href="#subdomain-method-test-against-indicator-functions" title="Link to this heading">#</a></h4>
<p>The subdomain method partitions the domain into <span class="math notranslate nohighlight">\(n\)</span> subregions <span class="math notranslate nohighlight">\(\{D_1, \ldots, D_n\}\)</span> and chooses test functions <span class="math notranslate nohighlight">\(p_i = I_{D_i}\)</span>, the indicator functions:</p>
<div class="math notranslate nohighlight">
\[
\langle R(\cdot; a), I_{D_i} \rangle = \int_{D_i} R(x; a) w(x) dx = 0, \quad i = 1, \ldots, n.
\]</div>
<p>This requires the residual to have zero average over each subdomain, ensuring the approximation is good “on average” over each piece of the domain. This approach is particularly natural for finite element methods where the domain is divided into elements, ensuring local balance of the residual within each element.</p>
</section>
<section id="least-squares-an-alternative-framework">
<h4>Least Squares: An Alternative Framework<a class="headerlink" href="#least-squares-an-alternative-framework" title="Link to this heading">#</a></h4>
<p>The least squares approach doesn’t fit the test function framework directly. Instead, we minimize:</p>
<div class="math notranslate nohighlight">
\[
\min_a \int_{\mathcal{S}} R(x; a)^2 w(x) dx = \min_a \langle R(\cdot; a), R(\cdot; a) \rangle.
\]</div>
<p>The first-order conditions for this minimization problem are:</p>
<div class="math notranslate nohighlight">
\[
\left\langle R(\cdot; a), \frac{\partial R(\cdot; a)}{\partial a_i} \right\rangle = 0, \quad i = 1, \ldots, n.
\]</div>
<p>Thus least squares implicitly uses test functions <span class="math notranslate nohighlight">\(p_i = \partial R / \partial a_i\)</span>, the gradients of the residual with respect to parameters. Unlike other methods where test functions are chosen a priori, here they depend on the current guess for <span class="math notranslate nohighlight">\(a\)</span> and on the structure of our approximation.</p>
<p>We can now see the unifying structure: all projection methods (except least squares in its direct form) follow the same template of picking <span class="math notranslate nohighlight">\(n\)</span> test functions and requiring <span class="math notranslate nohighlight">\(\langle R, p_i \rangle = 0\)</span>. They differ only in their philosophy about which test functions best reveal whether the residual is “nearly zero.” Galerkin tests against the approximation basis itself (natural for orthogonal bases), the method of moments tests against monomials (ensuring polynomial balance), collocation tests against delta functions (pointwise satisfaction), subdomain tests against indicators (local average satisfaction), and least squares tests against residual gradients (global norm minimization). Each choice reflects different priorities: computational efficiency, theoretical optimality, ease of implementation, or sensitivity to errors in different regions of the domain.</p>
</section>
</section>
<section id="step-4-solve-the-finite-dimensional-problem">
<h3>Step 4: Solve the Finite-Dimensional Problem<a class="headerlink" href="#step-4-solve-the-finite-dimensional-problem" title="Link to this heading">#</a></h3>
<p>The projection conditions give us a system to solve for the coefficients <span class="math notranslate nohighlight">\(a\)</span>. For test function methods (Galerkin, collocation, moments, subdomain), we solve:</p>
<div class="math notranslate nohighlight">
\[
P_i(a) \equiv \langle R(\cdot; a), p_i \rangle = 0, \quad i = 1, \ldots, n.
\]</div>
<p>This is a system of <span class="math notranslate nohighlight">\(n\)</span> (generally nonlinear) equations in <span class="math notranslate nohighlight">\(n\)</span> unknowns. For least squares, we solve the optimization problem <span class="math notranslate nohighlight">\(\min_a \langle R(\cdot; a), R(\cdot; a) \rangle\)</span>.</p>
<p>The <strong>conditioning</strong> of the system depends on the choice of test functions. The Jacobian matrix has entries:</p>
<div class="math notranslate nohighlight">
\[
J_{ij} = \frac{\partial P_i}{\partial a_j} = \left\langle \frac{\partial R(\cdot; a)}{\partial a_j}, p_i \right\rangle.
\]</div>
<p>When test functions are orthogonal (or nearly so), the Jacobian tends to be well-conditioned. This is why orthogonal polynomial bases are preferred in Galerkin methods: they produce Jacobians with controlled condition numbers.</p>
<p>The <strong>computational cost per iteration</strong> varies significantly:</p>
<ul class="simple">
<li><p><strong>Collocation</strong>: Cheapest to evaluate since <span class="math notranslate nohighlight">\(P_i(a) = R(x_i; a)\)</span> requires only pointwise evaluation (no integration). The Jacobian is also cheap: <span class="math notranslate nohighlight">\(J_{ij} = \frac{\partial R(x_i; a)}{\partial a_j}\)</span>.</p></li>
<li><p><strong>Galerkin and moments</strong>: More expensive due to integration. Computing <span class="math notranslate nohighlight">\(P_i(a) = \int R(x; a) p_i(x) w(x) dx\)</span> requires numerical quadrature. Each Jacobian entry requires integrating <span class="math notranslate nohighlight">\(\frac{\partial R}{\partial a_j} p_i\)</span>.</p></li>
<li><p><strong>Least squares</strong>: Most expensive when done via the objective function, which requires integrating <span class="math notranslate nohighlight">\(R^2\)</span>. However, the first-order conditions reduce it to a system like Galerkin, with test functions <span class="math notranslate nohighlight">\(p_i = \partial R / \partial a_i\)</span>.</p></li>
</ul>
<p>For methods requiring integration, the choice of quadrature rule should match the basis. Gaussian quadrature with nodes at orthogonal polynomial zeros is efficient. When combined with collocation at those same points, the quadrature is exact for polynomials up to a certain degree. This coordination between quadrature and collocation makes <strong>orthogonal collocation</strong> effective.</p>
<p>The choice of solver depends on whether the finite-dimensional approximation preserves the structural properties of the original infinite-dimensional problem. This matters for the Bellman equation, where the original operator <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> is a contraction.</p>
<p><strong>Successive approximation</strong> (fixed-point iteration) is the natural choice when the original operator is a contraction, as it preserves the global convergence guarantees. However, the finite-dimensional approximation <span class="math notranslate nohighlight">\(\hat{\mathrm{L}}\)</span> may not inherit the contraction property of <span class="math notranslate nohighlight">\(\mathrm{L}\)</span>. The approximation can introduce spurious fixed points or destroy the contraction constant, leading to divergence or slow convergence. This is especially problematic when using high-order polynomial approximations, which can create artificial oscillations that destabilize the iteration.</p>
<p><strong>Newton’s method</strong> is often the default choice for projection methods because it doesn’t rely on the contraction property. Instead, it exploits the smoothness of the residual function. When the original problem is smooth and the approximation preserves this smoothness, Newton’s method provides quadratic convergence near the solution. However, Newton’s method requires good initial guesses and may converge to spurious solutions if the finite-dimensional problem has multiple fixed points that the original problem lacks.</p>
<p><strong>The choice of basis and projection method affects which algorithm is most appropriate</strong>. For example:</p>
<ul class="simple">
<li><p><strong>Linear interpolation</strong> often preserves contraction properties, making successive approximation reliable</p></li>
<li><p><strong>High-order polynomials</strong> may destroy contraction but provide smooth approximations suitable for Newton’s method</p></li>
<li><p><strong>Shape-preserving splines</strong> can maintain both smoothness and structural properties</p></li>
</ul>
<p><strong>In practice, which algorithm should we use?</strong> When the operator equation can be written as a fixed-point problem <span class="math notranslate nohighlight">\(f = \mathscr{T}f\)</span> and the operator <span class="math notranslate nohighlight">\(\mathscr{T}\)</span> is known to be a contraction, successive approximation is often the best starting point: it is computationally cheap and globally convergent. However, not all equations <span class="math notranslate nohighlight">\(\mathscr{N}(f) = 0\)</span> admit a natural fixed-point reformulation, and even when they do (e.g., <span class="math notranslate nohighlight">\(f = f - \alpha \mathscr{N}(f)\)</span> for some <span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span>), the resulting operator may not be a contraction in the finite-dimensional approximation space. In such cases, Newton’s method becomes the primary option despite its requirement for good initial guesses and higher computational cost per iteration. A hybrid approach often works well: use successive approximation when applicable to generate an initial guess, then switch to Newton’s method for refinement.</p>
<p>Another consideration is the conditioning of the resulting system. Poorly chosen basis functions or collocation points can lead to nearly singular Jacobians, causing numerical instability. Orthogonal bases and carefully chosen collocation points (like Chebyshev nodes) are preferred because they tend to produce well-conditioned systems.</p>
</section>
<section id="step-5-verify-the-solution">
<h3>Step 5: Verify the Solution<a class="headerlink" href="#step-5-verify-the-solution" title="Link to this heading">#</a></h3>
<p>Once we have computed a candidate solution <span class="math notranslate nohighlight">\(\hat{f}\)</span>, we must verify its quality. Projection methods optimize <span class="math notranslate nohighlight">\(\hat{f}\)</span> with respect to specific criteria (specific test functions or collocation points), but we should check that the residual is small everywhere, not just in the directions or at the points we optimized over.</p>
<p>Typical diagnostic checks include:</p>
<ul class="simple">
<li><p>Computing <span class="math notranslate nohighlight">\(\|R(\cdot; a)\|\)</span> using a more accurate quadrature rule than was used in the optimization</p></li>
<li><p>Evaluating <span class="math notranslate nohighlight">\(R(x; a)\)</span> at many points not used in the fitting process</p></li>
<li><p>If using Galerkin with the first <span class="math notranslate nohighlight">\(n\)</span> basis functions, checking orthogonality against higher-order basis functions</p></li>
</ul>
</section>
</section>
<section id="application-to-the-bellman-equation">
<h2>Application to the Bellman Equation<a class="headerlink" href="#application-to-the-bellman-equation" title="Link to this heading">#</a></h2>
<p>We now apply the projection method framework to the Bellman optimality equation. Recall that we seek a function <span class="math notranslate nohighlight">\(v\)</span> satisfying</p>
<div class="math notranslate nohighlight">
\[
v(s) = \mathrm{L}v(s) = \max_{a \in \mathcal{A}_s} \left\{ r(s,a) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a) v(j) \right\}.
\]</div>
<p>Writing this as an operator equation <span class="math notranslate nohighlight">\(\mathscr{N}(v) = 0\)</span> with <span class="math notranslate nohighlight">\(\mathscr{N}(v) = \mathrm{L}v - v\)</span>, the residual function for a candidate approximation <span class="math notranslate nohighlight">\(\hat{v}(s) = \sum_{i=1}^n a_i \varphi_i(s)\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
R(s; a) = \mathrm{L}\hat{v}(s) - \hat{v}(s) = \max_{a \in \mathcal{A}_s} \left\{ r(s,a) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a) \hat{v}(j) \right\} - \sum_{i=1}^n a_i \varphi_i(s).
\]</div>
<p>Any of the projection methods we discussed (Galerkin, method of moments, collocation, subdomain, or least squares) can be applied here. Each would give us <span class="math notranslate nohighlight">\(n\)</span> conditions to determine the <span class="math notranslate nohighlight">\(n\)</span> coefficients in our approximation. For instance:</p>
<ul class="simple">
<li><p><strong>Galerkin</strong> would require <span class="math notranslate nohighlight">\(\langle R(\cdot; a), \varphi_i \rangle = 0\)</span> for <span class="math notranslate nohighlight">\(i = 1, \ldots, n\)</span>, involving integration of the residual weighted by basis functions</p></li>
<li><p><strong>Method of moments</strong> would require <span class="math notranslate nohighlight">\(\langle R(\cdot; a), s^{i-1} \rangle = 0\)</span>, setting the first <span class="math notranslate nohighlight">\(n\)</span> moments of the residual to zero</p></li>
<li><p><strong>Collocation</strong> would require <span class="math notranslate nohighlight">\(R(s_i; a) = 0\)</span> at <span class="math notranslate nohighlight">\(n\)</span> chosen states, forcing the residual to vanish pointwise</p></li>
</ul>
<p>In practice, <strong>collocation is the most commonly used</strong> projection method for the Bellman equation. The reason is computational: collocation avoids the numerical integration required by Galerkin and method of moments. Since the Bellman operator already involves integration (or summation) over next states, adding another layer of integration for the projection conditions would be computationally expensive. Collocation sidesteps this by requiring the equation to hold exactly at specific points.</p>
<p>We focus on collocation in detail, though the principles extend to other projection methods.</p>
<section id="collocation-for-the-bellman-equation">
<h3>Collocation for the Bellman Equation<a class="headerlink" href="#collocation-for-the-bellman-equation" title="Link to this heading">#</a></h3>
<p>The collocation approach chooses <span class="math notranslate nohighlight">\(n\)</span> states <span class="math notranslate nohighlight">\(\{s_1, \ldots, s_n\}\)</span> (the collocation points) and requires:</p>
<div class="math notranslate nohighlight">
\[
R(s_i; a) = 0, \quad i = 1, \ldots, n.
\]</div>
<p>This gives us a system of <span class="math notranslate nohighlight">\(n\)</span> nonlinear equations in <span class="math notranslate nohighlight">\(n\)</span> unknowns:</p>
<div class="math notranslate nohighlight">
\[
\sum_{j=1}^n a_j \varphi_j(s_i) = \max_{a \in \mathcal{A}_{s_i}} \left\{ r(s_i,a) + \gamma \sum_{j \in \mathcal{S}} p(j|s_i,a) \hat{v}(j) \right\}, \quad i = 1, \ldots, n.
\]</div>
<p>The right-hand side requires evaluating the Bellman operator at the collocation points. For each collocation point <span class="math notranslate nohighlight">\(s_i\)</span>, we must:</p>
<ol class="arabic simple">
<li><p>For each action <span class="math notranslate nohighlight">\(a \in \mathcal{A}_{s_i}\)</span>, compute the expected continuation value <span class="math notranslate nohighlight">\(\sum_{j \in \mathcal{S}} p(j|s_i,a) \hat{v}(j)\)</span></p></li>
<li><p>Take the maximum over actions</p></li>
</ol>
<p>When the state space is continuous, the expectation involves integration, which typically requires numerical quadrature. When the state space is discrete but large, this is a straightforward (though potentially expensive) summation.</p>
<section id="solving-the-collocation-system">
<h4>Solving the Collocation System<a class="headerlink" href="#solving-the-collocation-system" title="Link to this heading">#</a></h4>
<p>The resulting system is nonlinear due to the max operator on the right-hand side. We can write this more compactly as finding <span class="math notranslate nohighlight">\(a \in \mathbb{R}^n\)</span> such that <span class="math notranslate nohighlight">\(F(a) = 0\)</span> where:</p>
<div class="math notranslate nohighlight">
\[
F_i(a) = \sum_{j=1}^n a_j \varphi_j(s_i) - \max_{u \in \mathcal{A}_{s_i}} \left\{ r(s_i,u) + \gamma \sum_{j \in \mathcal{S}} p(j|s_i,u) \hat{v}(j; a) \right\}.
\]</div>
<p><strong>Newton’s method</strong> is the standard approach for such systems. However, the max operator introduces a non-differentiability issue: the function <span class="math notranslate nohighlight">\(F(a)\)</span> is not everywhere differentiable because the optimal action can change discontinuously as <span class="math notranslate nohighlight">\(a\)</span> varies. Fortunately, the function is <strong>semismooth</strong>: it is locally Lipschitz continuous and directionally differentiable everywhere. This structure can be exploited by <strong>semi-smooth Newton methods</strong>, which generalize Newton’s method to semismooth equations by using any element of the generalized Jacobian (in the sense of Clarke’s generalized derivative) in place of the classical Jacobian.</p>
<p>In practice, implementing semi-smooth Newton for the Bellman equation is straightforward: at each iteration, we fix the optimal action <span class="math notranslate nohighlight">\(a^*(s_i; a^{(k)})\)</span> at the current guess <span class="math notranslate nohighlight">\(a^{(k)}\)</span>, compute the Jacobian assuming these actions remain optimal, and update:</p>
<div class="math notranslate nohighlight">
\[
a^{(k+1)} = a^{(k)} - J_F(a^{(k)})^{-1} F(a^{(k)}).
\]</div>
<p>As we approach the solution, the optimal actions typically stabilize, and the method achieves superlinear convergence despite the non-smoothness. The main practical requirement is a good initial guess, which can be obtained from the successive approximation method described next.</p>
</section>
</section>
<section id="iterative-solution-successive-approximation">
<h3>Iterative Solution: Successive Approximation<a class="headerlink" href="#iterative-solution-successive-approximation" title="Link to this heading">#</a></h3>
<p>Rather than solving the nonlinear system directly via Newton’s method, we can exploit the fixed-point structure of the problem. The collocation equations can be viewed as requiring that the approximation matches the Bellman operator at the collocation points. This suggests an iterative scheme that performs <strong>successive approximation</strong> (fixed-point iteration) in the finite-dimensional coefficient space. Starting with an initial guess <span class="math notranslate nohighlight">\(a^{(0)}\)</span>, we iterate:</p>
<ol class="arabic">
<li><p><strong>Maximization step</strong>: At each collocation point <span class="math notranslate nohighlight">\(s_i\)</span>, compute</p>
<div class="math notranslate nohighlight">
\[
   v_i^{(k+1)} = \max_{a \in \mathcal{A}_{s_i}} \left\{ r(s_i,a) + \gamma \sum_{j \in \mathcal{S}} p(j|s_i,a) \hat{v}(s_j; a^{(k)}) \right\}.
   \]</div>
</li>
<li><p><strong>Fitting step</strong>: Find coefficients <span class="math notranslate nohighlight">\(a^{(k+1)}\)</span> such that <span class="math notranslate nohighlight">\(\hat{v}(s_i; a^{(k+1)}) = v_i^{(k+1)}\)</span> for all <span class="math notranslate nohighlight">\(i = 1, \ldots, n\)</span>. This is a linear system if our approximation is linear in the coefficients.</p></li>
<li><p><strong>Check convergence</strong>: If <span class="math notranslate nohighlight">\(\|a^{(k+1)} - a^{(k)}\|\)</span> is sufficiently small, stop; otherwise return to step 1.</p></li>
</ol>
<p>This algorithm, which Judd calls <strong>parametric value function iteration</strong> or <strong>projection-based value iteration</strong>, separates the difficult nonlinear optimization (the max operator in the Bellman equation) from the approximation problem. Each iteration improves the approximation by ensuring it matches the Bellman operator at the collocation points. Mathematically, it performs successive approximation in the finite-dimensional coefficient space: we define an operator <span class="math notranslate nohighlight">\(\hat{\mathrm{L}}\)</span> that maps coefficients <span class="math notranslate nohighlight">\(a^{(k)}\)</span> to new coefficients <span class="math notranslate nohighlight">\(a^{(k+1)}\)</span> via the maximization and fitting steps, then iterate <span class="math notranslate nohighlight">\(a^{(k+1)} = \hat{\mathrm{L}}(a^{(k)})\)</span>.</p>
<section id="comparison-of-solution-methods">
<h4>Comparison of Solution Methods<a class="headerlink" href="#comparison-of-solution-methods" title="Link to this heading">#</a></h4>
<p>We now have two approaches to solving the collocation equations:</p>
<ol class="arabic simple">
<li><p><strong>Semi-smooth Newton</strong>: Solve the nonlinear system <span class="math notranslate nohighlight">\(F(a) = 0\)</span> directly using Newton’s method adapted for semismooth functions. This offers fast (superlinear) convergence near the solution but requires a good initial guess and may fail to converge from poor starting points.</p></li>
<li><p><strong>Successive approximation (parametric value iteration)</strong>: Iterate the map <span class="math notranslate nohighlight">\(a^{(k+1)} = \hat{\mathrm{L}}(a^{(k)})\)</span> that alternates between maximization and fitting steps. This is more robust to poor initial guesses and inherits global convergence properties when the finite-dimensional approximation preserves the contraction property of the Bellman operator.</p></li>
</ol>
<p><strong>Which should we use?</strong> Following Judd’s guidance: When the Bellman operator is known to be a contraction and the finite-dimensional approximation preserves this property (as with linear interpolation or carefully chosen low-order approximations), successive approximation is often the best choice. It is globally convergent and each iteration is relatively cheap. However, high-order polynomial approximations may destroy the contraction property or introduce numerical instabilities. In such cases, or when convergence is too slow, Newton’s method (or semi-smooth Newton) becomes necessary despite requiring good initial guesses.</p>
<p>A <strong>hybrid strategy</strong> works well in practice: use successive approximation to generate an initial approximation, then switch to semi-smooth Newton for rapid refinement once in the neighborhood of the solution. This combines the global convergence of successive approximation with the fast local convergence of Newton’s method.</p>
</section>
</section>
<section id="shape-preserving-considerations">
<h3>Shape-Preserving Considerations<a class="headerlink" href="#shape-preserving-considerations" title="Link to this heading">#</a></h3>
<p>In dynamic programming, the value function typically has specific structural properties that we want our approximation to preserve. For instance:</p>
<ul class="simple">
<li><p><strong>Monotonicity</strong>: If having more of a resource is better, the value function should be increasing</p></li>
<li><p><strong>Concavity</strong>: Diminishing returns often imply concave value functions</p></li>
<li><p><strong>Boundedness</strong>: The value function is bounded when rewards are bounded</p></li>
</ul>
<p>Standard polynomial approximation does not automatically preserve these properties. A polynomial fit to increasing, concave data points can produce a function with non-monotonic or convex regions between the data points. This can destabilize the iterative algorithm: artificially high values at non-collocation points can lead to poor decisions in the maximization step, which feeds back into even worse approximations.</p>
<p><strong>Shape-preserving approximation methods</strong> address this issue. For one-dimensional problems, Schumaker’s shape-preserving quadratic splines maintain monotonicity and concavity while providing continuously differentiable approximations. For multidimensional problems, linear interpolation on simplices preserves monotonicity and convex combinations (though not concavity or smoothness).</p>
<p>The trade-off is between smoothness and shape preservation. Smooth approximations (high-order polynomials or splines) enable efficient optimization in the maximization step through gradient-based methods, but risk introducing spurious features. Simple approximations (linear interpolation) guarantee shape preservation but introduce kinks that complicate optimization and may produce discontinuous policies when the true policy is continuous.</p>
</section>
</section>
<section id="galerkin-projection-and-least-squares-temporal-difference">
<h2>Galerkin Projection and Least Squares Temporal Difference<a class="headerlink" href="#galerkin-projection-and-least-squares-temporal-difference" title="Link to this heading">#</a></h2>
<p>An important special case emerges when we apply Galerkin projection to the <strong>policy evaluation</strong> problem rather than the optimality problem. For a fixed policy <span class="math notranslate nohighlight">\(\pi\)</span>, the policy evaluation Bellman equation is:</p>
<div class="math notranslate nohighlight">
\[
v^\pi(s) = \mathrm{L}_\pi v^\pi(s) = r(s,\pi(s)) + \gamma \sum_{s' \in \mathcal{S}} p(s'|s,\pi(s)) v^\pi(s').
\]</div>
<p>This is a linear operator (no max), making the projection problem significantly simpler. Consider a linear function approximation <span class="math notranslate nohighlight">\(\hat{v}(s) = \boldsymbol{\varphi}(s)^\top \mathbf{a}\)</span> where <span class="math notranslate nohighlight">\(\boldsymbol{\varphi}(s) = [\varphi_1(s), \ldots, \varphi_n(s)]^\top\)</span> are basis functions and <span class="math notranslate nohighlight">\(\mathbf{a} = [a_1, \ldots, a_n]^\top\)</span> are coefficients to determine. The residual is:</p>
<div class="math notranslate nohighlight">
\[
R(s; \mathbf{a}) = \mathrm{L}_\pi \hat{v}(s) - \hat{v}(s) = r(s,\pi(s)) + \gamma \sum_{s'} p(s'|s,\pi(s)) \boldsymbol{\varphi}(s')^\top \mathbf{a} - \boldsymbol{\varphi}(s)^\top \mathbf{a}.
\]</div>
<p>The Galerkin projection requires the residual to be orthogonal to all basis functions with respect to some weighting:</p>
<div class="math notranslate nohighlight">
\[
\sum_{s \in \mathcal{S}} \xi(s) R(s; \mathbf{a}) \varphi_j(s) = 0, \quad j = 1, \ldots, n,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\xi(s)\)</span> is a distribution over states (often the stationary distribution under policy <span class="math notranslate nohighlight">\(\pi\)</span>, or uniform over visited states). Substituting the residual:</p>
<div class="math notranslate nohighlight">
\[
\sum_s \xi(s) \left[ r(s,\pi(s)) + \gamma \sum_{s'} p(s'|s,\pi(s)) \boldsymbol{\varphi}(s')^\top \mathbf{a} - \boldsymbol{\varphi}(s)^\top \mathbf{a} \right] \varphi_j(s) = 0.
\]</div>
<p>Rearranging and writing in matrix form, let <span class="math notranslate nohighlight">\(\boldsymbol{\Xi}\)</span> be a diagonal matrix with <span class="math notranslate nohighlight">\(\Xi_{ss} = \xi(s)\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\Phi}\)</span> be the <span class="math notranslate nohighlight">\(|\mathcal{S}| \times n\)</span> matrix with rows <span class="math notranslate nohighlight">\(\boldsymbol{\varphi}(s)^\top\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{P}_\pi\)</span> be the transition matrix under policy <span class="math notranslate nohighlight">\(\pi\)</span>. The Galerkin conditions become:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\Phi}^\top \boldsymbol{\Xi} (\mathbf{r}_\pi + \gamma \mathbf{P}_\pi \boldsymbol{\Phi} \mathbf{a} - \boldsymbol{\Phi} \mathbf{a}) = \mathbf{0}.
\]</div>
<p>Solving for <span class="math notranslate nohighlight">\(\mathbf{a}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\Phi}^\top \boldsymbol{\Xi} (\boldsymbol{\Phi} - \gamma \mathbf{P}_\pi \boldsymbol{\Phi}) \mathbf{a} = \boldsymbol{\Phi}^\top \boldsymbol{\Xi} \mathbf{r}_\pi.
\]</div>
<p>This is precisely the <strong>Least Squares Temporal Difference (LSTD)</strong> solution for policy evaluation. The connection reveals that LSTD is Galerkin projection applied to the linear policy evaluation Bellman equation. The “least squares” name comes from the fact that this is the projection (in the weighted <span class="math notranslate nohighlight">\(\ell^2\)</span> sense) of the Bellman operator’s output onto the span of the basis functions.</p>
<p>The projection perspective clarifies a key aspect of approximate dynamic programming. The solution <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> does not satisfy the true Bellman equation <span class="math notranslate nohighlight">\(v = \mathrm{L}_\pi v\)</span> (which is typically impossible within our finite-dimensional approximation space). Instead, it satisfies <span class="math notranslate nohighlight">\(\hat{v} = \Pi \mathrm{L}_\pi \hat{v}\)</span>, where <span class="math notranslate nohighlight">\(\Pi\)</span> is the projection operator onto <span class="math notranslate nohighlight">\(\text{span}\{\varphi_1, \ldots, \varphi_n\}\)</span>. We find the fixed point of the <em>projected</em> Bellman operator, not the Bellman operator itself. This is why approximation error persists even at convergence: the best we can do is find the value function whose Bellman operator output projects back onto itself.</p>
<section id="the-projected-bellman-equations">
<h3>The Projected Bellman Equations<a class="headerlink" href="#the-projected-bellman-equations" title="Link to this heading">#</a></h3>
<p>The LSTD solution gives a closed-form expression and connects to iterative algorithms developed in the next chapter. Understanding convergence of these methods requires analyzing when the projected Bellman operator <span class="math notranslate nohighlight">\(\Pi \mathrm{L}_\pi\)</span> is a contraction.</p>
<p><strong>Norms and projections.</strong> Fix a feature matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Phi} \in \mathbb{R}^{|\mathcal{S}| \times n}\)</span> with full column rank and a probability distribution <span class="math notranslate nohighlight">\(\xi\)</span> over states. Define the <span class="math notranslate nohighlight">\(\xi\)</span>-weighted inner product and norm by</p>
<div class="math notranslate nohighlight">
\[
\langle u, v \rangle_\xi := \sum_s \xi(s) u(s) v(s) = u^\top \boldsymbol{\Xi} v, \qquad \|v\|_\xi := \sqrt{v^\top \boldsymbol{\Xi} v},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\Xi} = \text{diag}(\xi)\)</span>. The orthogonal projection onto <span class="math notranslate nohighlight">\(\text{span}(\boldsymbol{\Phi})\)</span> with respect to <span class="math notranslate nohighlight">\(\langle \cdot, \cdot \rangle_\xi\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\Pi = \boldsymbol{\Phi}(\boldsymbol{\Phi}^\top \boldsymbol{\Xi} \boldsymbol{\Phi})^{-1} \boldsymbol{\Phi}^\top \boldsymbol{\Xi}.
\]</div>
<p>An operator <span class="math notranslate nohighlight">\(\mathrm{T}\)</span> is a <strong><span class="math notranslate nohighlight">\(\beta\)</span>-contraction</strong> in norm <span class="math notranslate nohighlight">\(\|\cdot\|\)</span> if <span class="math notranslate nohighlight">\(\|\mathrm{T}v - \mathrm{T}w\| \leq \beta \|v - w\|\)</span> for all <span class="math notranslate nohighlight">\(v, w\)</span> and some <span class="math notranslate nohighlight">\(\beta &lt; 1\)</span>. It is a <strong>non-expansion</strong> if the same holds with <span class="math notranslate nohighlight">\(\beta = 1\)</span>.</p>
<p><strong>Why <span class="math notranslate nohighlight">\(\Pi\)</span> is a non-expansion.</strong> The key is the Pythagorean identity in weighted inner product spaces. For any <span class="math notranslate nohighlight">\(u \in \mathbb{R}^{|\mathcal{S}|}\)</span>, the projection <span class="math notranslate nohighlight">\(\Pi u\)</span> and the residual <span class="math notranslate nohighlight">\((I - \Pi)u\)</span> are <span class="math notranslate nohighlight">\(\xi\)</span>-orthogonal: <span class="math notranslate nohighlight">\(\langle \Pi u, (I-\Pi)u \rangle_\xi = 0\)</span>. Therefore,</p>
<div class="math notranslate nohighlight">
\[
\|u\|_\xi^2 = \|\Pi u\|_\xi^2 + \|(I-\Pi)u\|_\xi^2.
\]</div>
<p>Applying this to <span class="math notranslate nohighlight">\(u - v\)</span> gives</p>
<div class="math notranslate nohighlight">
\[
\|\Pi u - \Pi v\|_\xi^2 = \|\Pi(u-v)\|_\xi^2 \leq \|\Pi(u-v)\|_\xi^2 + \|(I-\Pi)(u-v)\|_\xi^2 = \|u - v\|_\xi^2,
\]</div>
<p>proving <span class="math notranslate nohighlight">\(\|\Pi u - \Pi v\|_\xi \leq \|u - v\|_\xi\)</span>.</p>
<p><strong>When is <span class="math notranslate nohighlight">\(\mathrm{L}_\pi\)</span> a contraction in <span class="math notranslate nohighlight">\(\|\cdot\|_\xi\)</span>?</strong> Write the policy evaluation operator as <span class="math notranslate nohighlight">\(\mathrm{L}_\pi v = r_\pi + \gamma \mathbf{P}_\pi v\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{P}_\pi\)</span> is the transition matrix under policy <span class="math notranslate nohighlight">\(\pi\)</span>. We know <span class="math notranslate nohighlight">\(\mathrm{L}_\pi\)</span> is a <span class="math notranslate nohighlight">\(\gamma\)</span>-contraction in <span class="math notranslate nohighlight">\(\|\cdot\|_\infty\)</span> from earlier chapters. However, whether it contracts in <span class="math notranslate nohighlight">\(\|\cdot\|_\xi\)</span> depends on the relationship between <span class="math notranslate nohighlight">\(\xi\)</span> and <span class="math notranslate nohighlight">\(\mathbf{P}_\pi\)</span>.</p>
<p>The key is to establish when the stochastic matrix <span class="math notranslate nohighlight">\(\mathbf{P}_\pi\)</span> is non-expansive in <span class="math notranslate nohighlight">\(\|\cdot\|_\xi\)</span>. Following Bertsekas (Lemma 6.3.1), suppose <span class="math notranslate nohighlight">\(\xi\)</span> is a <strong>steady-state probability vector</strong> for <span class="math notranslate nohighlight">\(\mathbf{P}_\pi\)</span> with positive components, meaning:</p>
<div class="math notranslate nohighlight">
\[
\xi^\top \mathbf{P}_\pi = \xi^\top, \qquad \text{or equivalently,} \qquad \xi(s') = \sum_s \xi(s) p(s'|s,\pi(s)) \text{ for all } s'.
\]</div>
<p>Then for any <span class="math notranslate nohighlight">\(z \in \mathbb{R}^{|\mathcal{S}|}\)</span>, using the convexity of the square function (Jensen’s inequality):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\|\mathbf{P}_\pi z\|_\xi^2 &amp;= \sum_s \xi(s) \left(\sum_{s'} p(s'|s,\pi(s)) z(s')\right)^2 \\
&amp;\leq \sum_s \xi(s) \sum_{s'} p(s'|s,\pi(s)) z(s')^2 \\
&amp;= \sum_{s'} \left(\sum_s \xi(s) p(s'|s,\pi(s))\right) z(s')^2
\end{align*}
\end{split}\]</div>
<p>Using the defining property of steady-state probabilities <span class="math notranslate nohighlight">\(\sum_s \xi(s) p(s'|s,\pi(s)) = \xi(s')\)</span>:</p>
<div class="math notranslate nohighlight">
\[
= \sum_{s'} \xi(s') z(s')^2 = \|z\|_\xi^2.
\]</div>
<p>Therefore <span class="math notranslate nohighlight">\(\|\mathbf{P}_\pi z\|_\xi \leq \|z\|_\xi\)</span>, showing that <span class="math notranslate nohighlight">\(\mathbf{P}_\pi\)</span> is non-expansive in <span class="math notranslate nohighlight">\(\|\cdot\|_\xi\)</span>. Since <span class="math notranslate nohighlight">\(\|\mathrm{L}_\pi v - \mathrm{L}_\pi w\|_\xi = \gamma \|\mathbf{P}_\pi(v-w)\|_\xi\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\|\mathrm{L}_\pi v - \mathrm{L}_\pi w\|_\xi \leq \gamma \|v - w\|_\xi.
\]</div>
<p>Thus <span class="math notranslate nohighlight">\(\mathrm{L}_\pi\)</span> is a <span class="math notranslate nohighlight">\(\gamma\)</span>-contraction in <span class="math notranslate nohighlight">\(\|\cdot\|_\xi\)</span> when <span class="math notranslate nohighlight">\(\xi\)</span> is the steady-state distribution of <span class="math notranslate nohighlight">\(\pi\)</span>.</p>
<p><strong>Contraction of the composition.</strong> Combining our two results: <span class="math notranslate nohighlight">\(\Pi\)</span> is a non-expansion and (under stationarity) <span class="math notranslate nohighlight">\(\mathrm{L}_\pi\)</span> is a <span class="math notranslate nohighlight">\(\gamma\)</span>-contraction in <span class="math notranslate nohighlight">\(\|\cdot\|_\xi\)</span>. Therefore,</p>
<div class="math notranslate nohighlight">
\[
\|\Pi \mathrm{L}_\pi v - \Pi \mathrm{L}_\pi w\|_\xi \leq \|\mathrm{L}_\pi v - \mathrm{L}_\pi w\|_\xi \leq \gamma \|v - w\|_\xi.
\]</div>
<p>By the Banach fixed-point theorem, <span class="math notranslate nohighlight">\(\Pi \mathrm{L}_\pi\)</span> has a unique fixed point in <span class="math notranslate nohighlight">\(\mathbb{R}^{|\mathcal{S}|}\)</span>, and iterates <span class="math notranslate nohighlight">\(v_{k+1} = \Pi \mathrm{L}_\pi v_k\)</span> converge to it from any <span class="math notranslate nohighlight">\(v_0\)</span>. This fixed point satisfies the <strong>projected Bellman equation</strong></p>
<div class="math notranslate nohighlight">
\[
v = \Pi(r_\pi + \gamma \mathbf{P}_\pi v), \qquad v \in \text{span}(\boldsymbol{\Phi}).
\]</div>
<p>Writing <span class="math notranslate nohighlight">\(v = \boldsymbol{\Phi} w\)</span> and left-multiplying by <span class="math notranslate nohighlight">\(\boldsymbol{\Phi}^\top \boldsymbol{\Xi}\)</span> yields the <strong>normal equations</strong></p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\Phi}^\top \boldsymbol{\Xi}(\boldsymbol{\Phi} - \gamma \mathbf{P}_\pi \boldsymbol{\Phi}) w = \boldsymbol{\Phi}^\top \boldsymbol{\Xi} r_\pi,
\]</div>
<p>which are precisely the LSTD equations we derived earlier. This result provides the theoretical foundation for temporal difference learning with linear function approximation: when learning on-policy (so <span class="math notranslate nohighlight">\(\xi\)</span> is stationary), convergence is guaranteed.</p>
<p><strong>Off-policy instability.</strong> When <span class="math notranslate nohighlight">\(\xi\)</span> is not stationary for <span class="math notranslate nohighlight">\(\mathbf{P}_\pi\)</span> (as occurs when data come from a different behavior policy), the Jensen argument breaks down. The transition operator <span class="math notranslate nohighlight">\(\mathbf{P}_\pi\)</span> need not be non-expansive in <span class="math notranslate nohighlight">\(\|\cdot\|_\xi\)</span>, so <span class="math notranslate nohighlight">\(\Pi \mathrm{L}_\pi\)</span> may fail to be a contraction. This is the root cause of off-policy divergence phenomena in linear TD learning (e.g., Baird’s counterexample). Importance weighting and other corrections are designed to restore stability in this regime.</p>
<p>The linearity of the policy evaluation operator <span class="math notranslate nohighlight">\(\mathrm{L}_\pi\)</span> is what gives us the closed-form solution. We could apply Galerkin projection to the Bellman optimality equation <span class="math notranslate nohighlight">\(v^* = \mathrm{L} v^*\)</span>, setting up orthogonality conditions <span class="math notranslate nohighlight">\(\sum_s \xi(s) R(s; \mathbf{a}) \varphi_j(s) = 0\)</span>. The max operator makes these conditions nonlinear in <span class="math notranslate nohighlight">\(\mathbf{a}\)</span>, eliminating the closed form and requiring iterative solution. This brings us back to the successive approximation methods discussed earlier for collocation.</p>
<p>This framework of projection methods (choosing test functions, defining residuals, and solving finite-dimensional systems) provides the conceptual foundation for approximate dynamic programming. One question remains: how do we evaluate the expectations in the Bellman operator when we lack explicit transition probabilities or when the state space is too large for exact computation? The next chapter introduces Monte Carlo integration methods, connecting classical projection methods to simulation-based approximate dynamic programming and reinforcement learning.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="regmdp.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Smooth Bellman Optimality Equations</p>
      </div>
    </a>
    <a class="right-next"
       href="simadp.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Simulation-Based Approximate Dynamic Programming</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-general-framework">The General Framework</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-choose-a-finite-dimensional-approximation-space">Step 1: Choose a Finite-Dimensional Approximation Space</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-define-the-residual-function">Step 2: Define the Residual Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-choose-projection-conditions">Step 3: Choose Projection Conditions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#galerkin-method-test-against-the-basis">Galerkin Method: Test Against the Basis</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#method-of-moments-test-against-monomials">Method of Moments: Test Against Monomials</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#collocation-method-test-against-delta-functions">Collocation Method: Test Against Delta Functions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#subdomain-method-test-against-indicator-functions">Subdomain Method: Test Against Indicator Functions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#least-squares-an-alternative-framework">Least Squares: An Alternative Framework</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-solve-the-finite-dimensional-problem">Step 4: Solve the Finite-Dimensional Problem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-verify-the-solution">Step 5: Verify the Solution</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#application-to-the-bellman-equation">Application to the Bellman Equation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#collocation-for-the-bellman-equation">Collocation for the Bellman Equation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-the-collocation-system">Solving the Collocation System</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iterative-solution-successive-approximation">Iterative Solution: Successive Approximation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-of-solution-methods">Comparison of Solution Methods</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shape-preserving-considerations">Shape-Preserving Considerations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#galerkin-projection-and-least-squares-temporal-difference">Galerkin Projection and Least Squares Temporal Difference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-projected-bellman-equations">The Projected Bellman Equations</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Pierre-Luc Bacon
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>