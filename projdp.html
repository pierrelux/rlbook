
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Projection Methods for Functional Equations &#8212; Practical Reinforcement Learning: From Algorithms to Applications</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=b4b7a797" />
    <link rel="stylesheet" type="text/css" href="_static/graphviz.css?v=fd3f3429" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"bm": ["{\\boldsymbol #1}", 1]}, "processEscapes": true}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'projdp';</script>
    <script src="_static/iframe-modal.js?v=f72a1242"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Simulation-Based Approximate Dynamic Programming" href="simadp.html" />
    <link rel="prev" title="Smooth Bellman Optimality Equations" href="regmdp.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Practical Reinforcement Learning: From Algorithms to Applications</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Why This Book?
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Modeling</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="modeling.html">Why Build a Model? For Whom?</a></li>

<li class="toctree-l1"><a class="reference internal" href="ssm.html">Dynamics Models for Decision Making</a></li>




<li class="toctree-l1"><a class="reference internal" href="simulation.html">Programs as Models</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Numerical Trajectory Optimization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ocp.html">Discrete-Time Trajectory Optimization</a></li>


<li class="toctree-l1"><a class="reference internal" href="cocp.html">Trajectory Optimization in Continuous Time</a></li>




</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">From Trajectories to Policies</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="mpc.html">Model Predictive Control</a></li>




<li class="toctree-l1"><a class="reference internal" href="dp.html">Dynamic Programming</a></li>



</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Approximate Dynamic Programming</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="regmdp.html">Smooth Bellman Optimality Equations</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Projection Methods for Functional Equations</a></li>
<li class="toctree-l1"><a class="reference internal" href="simadp.html">Simulation-Based Approximate Dynamic Programming</a></li>



<li class="toctree-l1"><a class="reference internal" href="cadp.html">Policy Parametrization Methods</a></li>







</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="appendix_examples.html">Example COCPs</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix_ivps.html">Solving Initial Value Problems</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix_nlp.html">Nonlinear Programming</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/pierrelux/rlbook" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pierrelux/rlbook/edit/main/projdp.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pierrelux/rlbook/issues/new?title=Issue%20on%20page%20%2Fprojdp.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/projdp.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Projection Methods for Functional Equations</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-does-it-mean-for-a-residual-to-be-zero">What Does It Mean for a Residual to Be Zero?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-special-role-of-spectral-methods">The Special Role of Spectral Methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#orthogonal-collocation-best-of-both-worlds">Orthogonal Collocation: Best of Both Worlds</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-general-framework">The General Framework</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-choose-a-finite-dimensional-approximation-space">Step 1: Choose a Finite-Dimensional Approximation Space</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-define-the-residual-function">Step 2: Define the Residual Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-choose-weighted-residual-conditions">Step 3: Choose Weighted Residual Conditions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#galerkin-method-test-against-the-basis">Galerkin Method: Test Against the Basis</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#method-of-moments-test-against-monomials">Method of Moments: Test Against Monomials</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#collocation-method-test-against-delta-functions">Collocation Method: Test Against Delta Functions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#subdomain-method-test-against-indicator-functions">Subdomain Method: Test Against Indicator Functions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#least-squares-an-alternative-framework">Least Squares: An Alternative Framework</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-solve-the-finite-dimensional-problem">Step 4: Solve the Finite-Dimensional Problem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-verify-the-solution">Step 5: Verify the Solution</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#application-to-the-bellman-equation">Application to the Bellman Equation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#collocation-for-the-bellman-equation">Collocation for the Bellman Equation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-the-collocation-system">Solving the Collocation System</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iterative-solution-successive-approximation">Iterative Solution: Successive Approximation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-of-solution-methods">Comparison of Solution Methods</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shape-preserving-considerations">Shape-Preserving Considerations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#galerkin-projection-and-least-squares-temporal-difference">Galerkin Projection and Least Squares Temporal Difference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-projected-bellman-equations">The Projected Bellman Equations</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="projection-methods-for-functional-equations">
<h1>Projection Methods for Functional Equations<a class="headerlink" href="#projection-methods-for-functional-equations" title="Link to this heading">#</a></h1>
<p>The Bellman optimality equation <span class="math notranslate nohighlight">\(\mathrm{L}v = v\)</span> is a functional equation: an equation where the unknown is an entire function rather than a finite-dimensional vector. When the state space is continuous or very large, we cannot represent the value function exactly on a computer. We must instead work with finite-dimensional approximations. This motivates projection methods, a general framework for transforming infinite-dimensional problems into tractable finite-dimensional ones.</p>
<section id="what-does-it-mean-for-a-residual-to-be-zero">
<h2>What Does It Mean for a Residual to Be Zero?<a class="headerlink" href="#what-does-it-mean-for-a-residual-to-be-zero" title="Link to this heading">#</a></h2>
<p>Suppose we have found a candidate approximate solution <span class="math notranslate nohighlight">\(\hat{v}\)</span> to the Bellman equation. To verify it satisfies <span class="math notranslate nohighlight">\(\mathrm{L}\hat{v} = \hat{v}\)</span>, we compute the <strong>residual function</strong> <span class="math notranslate nohighlight">\(R(s) = \mathrm{L}\hat{v}(s) - \hat{v}(s)\)</span>. For a true solution, this residual should be the <strong>zero function</strong>: <span class="math notranslate nohighlight">\(R(s) = 0\)</span> for every state <span class="math notranslate nohighlight">\(s\)</span>. But what does it really mean for a function to equal zero?</p>
<p>In finite dimensions, a vector <span class="math notranslate nohighlight">\(\mathbf{r} \in \mathbb{R}^n\)</span> equals zero if and only if <span class="math notranslate nohighlight">\(\langle \mathbf{r}, \mathbf{y} \rangle = 0\)</span> for every vector <span class="math notranslate nohighlight">\(\mathbf{y} \in \mathbb{R}^n\)</span>. This follows because if <span class="math notranslate nohighlight">\(\mathbf{r} \neq \mathbf{0}\)</span>, we can always choose <span class="math notranslate nohighlight">\(\mathbf{y} = \mathbf{r}\)</span>, giving <span class="math notranslate nohighlight">\(\langle \mathbf{r}, \mathbf{r} \rangle = \|\mathbf{r}\|^2 &gt; 0\)</span>. Conversely, if <span class="math notranslate nohighlight">\(\mathbf{r} = \mathbf{0}\)</span>, then <span class="math notranslate nohighlight">\(\langle \mathbf{r}, \mathbf{y} \rangle = 0\)</span> trivially for all <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>.</p>
<p>The key insight is that <strong>inner products can distinguish the zero vector from any nonzero vector</strong>: for any <span class="math notranslate nohighlight">\(\mathbf{r} \neq \mathbf{0}\)</span>, there exists some test vector <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> that “witnesses” the fact that <span class="math notranslate nohighlight">\(\mathbf{r}\)</span> is nonzero by producing <span class="math notranslate nohighlight">\(\langle \mathbf{r}, \mathbf{y} \rangle \neq 0\)</span>. This property—that we can tell apart (separate) different vectors by testing them with inner products—is what makes inner products so useful for verification.</p>
<p><strong>The same principle extends to functions.</strong> A function <span class="math notranslate nohighlight">\(R\)</span> equals the zero function if and only if its “inner product” with every “test function” <span class="math notranslate nohighlight">\(p\)</span> vanishes:</p>
<div class="math notranslate nohighlight">
\[
R = 0 \quad \text{if and only if} \quad \langle R, p \rangle = \int_{\mathcal{S}} R(s) p(s) w(s) ds = 0 \quad \text{for all test functions } p,
\]</div>
<p>where <span class="math notranslate nohighlight">\(w(s)\)</span> is a weight function (often chosen to emphasize certain regions of the state space). Why does this work? For the same reason as in finite dimensions: if <span class="math notranslate nohighlight">\(R\)</span> is not the zero function, there must be some region where <span class="math notranslate nohighlight">\(R(s) \neq 0\)</span>. We can then choose a test function <span class="math notranslate nohighlight">\(p\)</span> that is nonzero in that same region (for instance, <span class="math notranslate nohighlight">\(p(s) = R(s)\)</span> itself), which will produce <span class="math notranslate nohighlight">\(\langle R, p \rangle = \int R(s) p(s) w(s) ds &gt; 0\)</span>, witnessing that <span class="math notranslate nohighlight">\(R\)</span> is nonzero. Conversely, if <span class="math notranslate nohighlight">\(R\)</span> is the zero function, then <span class="math notranslate nohighlight">\(\langle R, p \rangle = 0\)</span> for any test function <span class="math notranslate nohighlight">\(p\)</span>.</p>
<p>This ability to <strong>distinguish between different functions using inner products</strong> is a fundamental principle from functional analysis. Just as we can test a vector by taking inner products with other vectors, we can test a function by taking inner products with other functions.</p>
<div class="dropdown admonition">
<p class="admonition-title">Connection to Functional Analysis</p>
<p>The principle that “a function equals zero if and only if it has zero inner product with all test functions” is a consequence of the <strong>Hahn-Banach theorem</strong>, one of the cornerstones of functional analysis. The theorem guarantees that for any nonzero function <span class="math notranslate nohighlight">\(R\)</span> in a suitable function space, there exists a continuous linear functional (which can be represented as an inner product with some test function <span class="math notranslate nohighlight">\(p\)</span>) that produces a nonzero value when applied to <span class="math notranslate nohighlight">\(R\)</span>. This is often phrased as “the dual space separates points.”</p>
<p>While you don’t need to know the Hahn-Banach theorem to use projection methods, it provides the rigorous mathematical foundation ensuring that our inner product tests are theoretically sound. The constructive argument we gave above (choosing <span class="math notranslate nohighlight">\(p = R\)</span>) works in simple cases with well-behaved functions, but the Hahn-Banach theorem extends this guarantee to much more general settings.</p>
</div>
<p><strong>Why is this useful?</strong> It transforms the pointwise condition “<span class="math notranslate nohighlight">\(R(s) = 0\)</span> for all <span class="math notranslate nohighlight">\(s\)</span>” (infinitely many conditions, one per state) into an equivalent condition about inner products. Of course, we still cannot test against <em>all</em> possible test functions—there are infinitely many of those too. But the inner product perspective suggests a natural computational strategy: <strong>choose a finite collection of test functions</strong> <span class="math notranslate nohighlight">\(\{p_1, \ldots, p_n\}\)</span> and require</p>
<div class="math notranslate nohighlight">
\[
\langle R, p_i \rangle = 0, \quad i = 1, \ldots, n.
\]</div>
<p>This gives us exactly <span class="math notranslate nohighlight">\(n\)</span> conditions that we can actually compute. This approach defines what are called <strong>weighted residual methods</strong>: we make the residual “small” by requiring it to satisfy certain weighted integral conditions.</p>
<p>Within weighted residual methods, there are two main families:</p>
<p><strong>Projection methods</strong> (also called orthogonal projection methods) directly require the residual to have zero inner product with chosen test functions:</p>
<div class="math notranslate nohighlight">
\[
\langle R, p_i \rangle = 0, \quad i = 1, \ldots, n.
\]</div>
<p>We are “projecting” the residual to be orthogonal to the span of the test functions. Different choices of test functions give different projection methods:</p>
<ul class="simple">
<li><p><strong>Galerkin</strong>: Test against the basis functions used to represent <span class="math notranslate nohighlight">\(\hat{v}\)</span>, so <span class="math notranslate nohighlight">\(p_i = \varphi_i\)</span></p></li>
<li><p><strong>Collocation</strong>: Test against delta functions <span class="math notranslate nohighlight">\(p_i = \delta(s - s_i)\)</span>, which reduces to pointwise evaluation <span class="math notranslate nohighlight">\(R(s_i) = 0\)</span></p></li>
<li><p><strong>Method of moments</strong>: Test against polynomials <span class="math notranslate nohighlight">\(p_i = s^{i-1}\)</span>, ensuring low-order moments of the residual vanish</p></li>
<li><p><strong>Subdomain method</strong>: Test against indicator functions <span class="math notranslate nohighlight">\(p_i = I_{D_i}\)</span> for subregions <span class="math notranslate nohighlight">\(D_i\)</span>, requiring zero average residual in each subdomain</p></li>
</ul>
<section id="the-special-role-of-spectral-methods">
<h3>The Special Role of Spectral Methods<a class="headerlink" href="#the-special-role-of-spectral-methods" title="Link to this heading">#</a></h3>
<p>You may encounter the term <strong>spectral methods</strong> in the literature. This doesn’t refer to a different choice of test functions, but rather to a choice of <strong>basis functions</strong> <span class="math notranslate nohighlight">\(\varphi_i\)</span>. Spectral methods use basis functions from families of orthogonal polynomials (like Chebyshev, Legendre, or Hermite polynomials) or trigonometric functions (Fourier series). The “spectral” name comes from the decomposition of the solution into these orthogonal components, analogous to decomposing a signal into frequency components.</p>
<p>What makes spectral bases special is their approximation properties: for smooth problems (functions with many continuous derivatives), spectral approximations achieve <strong>exponential convergence</strong>. As you add more basis functions, the approximation error decreases exponentially rather than polynomially. A function with <span class="math notranslate nohighlight">\(k\)</span> continuous derivatives approximated by piecewise polynomials of degree <span class="math notranslate nohighlight">\(p\)</span> has error <span class="math notranslate nohighlight">\(O(h^{p+1})\)</span> where <span class="math notranslate nohighlight">\(h\)</span> is the grid spacing. But the same function approximated by a spectral method with <span class="math notranslate nohighlight">\(n\)</span> terms has error that decreases like <span class="math notranslate nohighlight">\(O(e^{-cn})\)</span> for some constant <span class="math notranslate nohighlight">\(c &gt; 0\)</span>. This dramatic difference makes spectral methods extremely efficient for smooth problems.</p>
<p>Now, spectral bases can be combined with any projection method. When we use a spectral basis with <strong>Galerkin projection</strong> (testing against the basis functions themselves), we get a <strong>spectral Galerkin method</strong>. The orthogonality of the basis functions often simplifies the resulting linear systems. When we use a spectral basis with <strong>collocation</strong>, we get what’s often called a <strong>pseudospectral method</strong> or <strong>spectral collocation method</strong>.</p>
</section>
<section id="orthogonal-collocation-best-of-both-worlds">
<h3>Orthogonal Collocation: Best of Both Worlds<a class="headerlink" href="#orthogonal-collocation-best-of-both-worlds" title="Link to this heading">#</a></h3>
<p>A particularly elegant variant is <strong>orthogonal collocation</strong>, which exploits a beautiful connection between collocation and quadrature. The idea is to:</p>
<ol class="arabic simple">
<li><p>Choose basis functions from an orthogonal polynomial family (say, Chebyshev polynomials <span class="math notranslate nohighlight">\(T_0, T_1, \ldots, T_{n-1}\)</span>)</p></li>
<li><p>Choose collocation points at the <strong>zeros of the <span class="math notranslate nohighlight">\(n\)</span>-th polynomial</strong> in that family</p></li>
</ol>
<p>Why is this clever? Because these same points are also the optimal nodes for <strong>Gauss quadrature</strong> using the weight function associated with that polynomial family. For example, the zeros of the Chebyshev polynomial <span class="math notranslate nohighlight">\(T_n(s)\)</span> are also the Chebyshev-Gauss quadrature nodes. This means:</p>
<ul class="simple">
<li><p>We get the computational simplicity of collocation: the projection conditions are just <span class="math notranslate nohighlight">\(R(s_i) = 0\)</span> at the collocation points (no integrals to evaluate)</p></li>
<li><p>When we do need to compute integrals (say, inside the operator <span class="math notranslate nohighlight">\(\mathscr{N}\)</span> itself), we can use the collocation points as quadrature nodes and the resulting quadrature is <strong>exact</strong> for polynomials up to degree <span class="math notranslate nohighlight">\(2n-1\)</span></p></li>
<li><p>For smooth problems, we inherit the exponential convergence of spectral approximations</p></li>
</ul>
<p>This coordination between approximation and integration is why orthogonal collocation is so effective. You’ll sometimes see it called a “pseudospectral method,” though different authors use these terms with slight variations. The key point is that by carefully coordinating our choice of basis, test functions (collocation points), and quadrature nodes, we can achieve excellent accuracy with computational efficiency.</p>
<p>In summary, “spectral” describes the basis choice (orthogonal polynomials or Fourier), while “Galerkin,” “collocation,” etc. describe the projection choice (which test functions). Orthogonal collocation represents an optimal marriage of these choices for smooth problems.</p>
<p><strong>Least squares methods</strong> take a different approach: instead of requiring orthogonality to specific test functions, we minimize the overall size of the residual measured in a weighted norm:</p>
<div class="math notranslate nohighlight">
\[
\min_{a} \|R(\cdot; a)\|^2 = \min_{a} \int_{\mathcal{S}} R(s; a)^2 w(s) ds.
\]</div>
<p>This seeks the coefficients <span class="math notranslate nohighlight">\(a\)</span> that make the residual as small as possible in the least squares sense. Interestingly, the first-order optimality conditions for this minimization problem turn out to be equivalent to a projection method with test functions <span class="math notranslate nohighlight">\(p_i = \partial R / \partial a_i\)</span> (the derivatives of the residual with respect to the coefficients). So least squares can be viewed as a projection method with <em>data-dependent</em> test functions.</p>
<p>Both families aim to make the residual “close to zero,” but projection methods do this by requiring orthogonality to chosen directions, while least squares does this by directly minimizing the norm of the residual. The term “projection methods” as used in the approximate dynamic programming literature often refers to both families, since they share the same computational framework of restricting the search to a finite-dimensional subspace and solving for coefficients that satisfy certain residual conditions.</p>
<p>In summary, we have transformed the impossible task of verifying “<span class="math notranslate nohighlight">\(R(s) = 0\)</span> for all <span class="math notranslate nohighlight">\(s\)</span>” into a <strong>finite-dimensional</strong> problem: find coefficients <span class="math notranslate nohighlight">\(a = (a_1, \ldots, a_n)\)</span> in our approximation <span class="math notranslate nohighlight">\(\hat{v}(s) = \sum_{i=1}^n a_i \varphi_i(s)\)</span> such that either:</p>
<ul class="simple">
<li><p>The residual is orthogonal to <span class="math notranslate nohighlight">\(n\)</span> chosen test functions (projection methods), or</p></li>
<li><p>The residual has minimum norm (least squares methods)</p></li>
</ul>
<p>This is a major conceptual step forward: instead of infinitely many pointwise conditions, we have <span class="math notranslate nohighlight">\(n\)</span> conditions. However, these <span class="math notranslate nohighlight">\(n\)</span> conditions are not yet fully “feasible” computationally—each projection condition <span class="math notranslate nohighlight">\(\langle R, p_i \rangle = \int_{\mathcal{S}} R(s) p_i(s) w(s) ds = 0\)</span> still involves an integral that may need to be approximated numerically.</p>
<p><strong>The computational cost hierarchy.</strong> Different methods have different computational burdens:</p>
<ul class="simple">
<li><p><strong>Collocation</strong> is the cheapest: since <span class="math notranslate nohighlight">\(\langle R, \delta(\cdot - s_i) \rangle = R(s_i)\)</span>, we only evaluate the residual pointwise—no integration needed in the projection conditions themselves.</p></li>
<li><p><strong>Orthogonal collocation</strong> shares this advantage (projection conditions are just pointwise evaluations), but adds a bonus: if integrals appear elsewhere—say, inside the operator <span class="math notranslate nohighlight">\(\mathscr{N}\)</span>—the collocation points double as optimal quadrature nodes. This synergy between approximation and integration is particularly valuable for smooth problems.</p></li>
<li><p><strong>Galerkin methods</strong> require evaluating integrals <span class="math notranslate nohighlight">\(\int R(s) \varphi_i(s) w(s) ds\)</span> for each basis function. When using orthogonal polynomial bases (spectral Galerkin), these integrals can sometimes be simplified by orthogonality, but numerical quadrature is still typically needed.</p></li>
<li><p><strong>Method of moments and subdomain methods</strong> similarly require numerical quadrature to evaluate weighted integrals of the residual.</p></li>
<li><p><strong>Least squares</strong> requires computing <span class="math notranslate nohighlight">\(\int R(s)^2 w(s) ds\)</span>, which involves integrating the squared residual—potentially expensive, though the first-order conditions reduce this to a system similar to Galerkin.</p></li>
</ul>
<p>The general pattern: collocation methods avoid integration in the projection step by testing at points rather than against functions, while methods that test against smooth functions (Galerkin, moments, subdomain) must pay the computational cost of numerical integration.</p>
<p>The rest of this chapter develops this framework systematically, showing how to choose bases, select test functions, evaluate or approximate the necessary integrals, and solve the resulting finite-dimensional problems.</p>
</section>
</section>
<section id="the-general-framework">
<h2>The General Framework<a class="headerlink" href="#the-general-framework" title="Link to this heading">#</a></h2>
<p>Consider an operator equation of the form</p>
<div class="math notranslate nohighlight">
\[
\mathscr{N}(f) = 0,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathscr{N}: B_1 \to B_2\)</span> is a continuous operator between complete normed vector spaces <span class="math notranslate nohighlight">\(B_1\)</span> and <span class="math notranslate nohighlight">\(B_2\)</span>. For the Bellman equation, we have <span class="math notranslate nohighlight">\(\mathscr{N}(v) = \mathrm{L}v - v\)</span>, so that solving <span class="math notranslate nohighlight">\(\mathscr{N}(v) = 0\)</span> is equivalent to finding the fixed point <span class="math notranslate nohighlight">\(v = \mathrm{L}v\)</span>.</p>
<p>The projection method approach consists of several conceptual steps that transform this infinite-dimensional problem into a finite-dimensional one.</p>
<section id="step-1-choose-a-finite-dimensional-approximation-space">
<h3>Step 1: Choose a Finite-Dimensional Approximation Space<a class="headerlink" href="#step-1-choose-a-finite-dimensional-approximation-space" title="Link to this heading">#</a></h3>
<p>We begin by selecting a basis <span class="math notranslate nohighlight">\(\Phi = \{\varphi_1, \varphi_2, \ldots, \varphi_n\}\)</span> and approximating the unknown function as a linear combination:</p>
<div class="math notranslate nohighlight">
\[
\hat{f}(x) = \sum_{i=1}^n a_i \varphi_i(x).
\]</div>
<p>The choice of basis functions <span class="math notranslate nohighlight">\(\varphi_i\)</span> is problem-dependent. Common choices include:</p>
<ul class="simple">
<li><p><strong>Polynomials</strong>: For smooth problems, we might use Chebyshev polynomials or other orthogonal polynomial families</p></li>
<li><p><strong>Splines</strong>: For problems where we expect the solution to have regions of different smoothness</p></li>
<li><p><strong>Radial basis functions</strong>: For high-dimensional problems where tensor product methods become intractable</p></li>
</ul>
<p>The number of basis functions <span class="math notranslate nohighlight">\(n\)</span> determines the flexibility of our approximation. In practice, we start with small <span class="math notranslate nohighlight">\(n\)</span> and increase it until the approximation quality is satisfactory. The only unknowns now are the coefficients <span class="math notranslate nohighlight">\(a = (a_1, \ldots, a_n)\)</span>.</p>
<p>While the classical presentation of projection methods emphasizes polynomial bases, the framework applies equally well to other function classes. Neural networks, for instance, can be viewed through this lens: a neural network <span class="math notranslate nohighlight">\(\hat{f}(x; \theta)\)</span> with parameters <span class="math notranslate nohighlight">\(\theta\)</span> defines a flexible function class, and many training procedures can be interpreted as projection methods with specific choices of test functions or residual norms. The distinction is that classical methods typically use predetermined basis functions with linear coefficients, while neural networks use adaptive nonlinear features. Throughout this chapter, we focus on the classical setting to develop the core concepts, but the principles extend naturally to modern function approximators.</p>
</section>
<section id="step-2-define-the-residual-function">
<h3>Step 2: Define the Residual Function<a class="headerlink" href="#step-2-define-the-residual-function" title="Link to this heading">#</a></h3>
<p>Since we are approximating <span class="math notranslate nohighlight">\(f\)</span> with <span class="math notranslate nohighlight">\(\hat{f}\)</span>, the operator <span class="math notranslate nohighlight">\(\mathscr{N}\)</span> will generally not vanish exactly. Instead, we obtain a <strong>residual function</strong>:</p>
<div class="math notranslate nohighlight">
\[
R(x; a) = \mathscr{N}(\hat{f}(\cdot; a))(x).
\]</div>
<p>This residual measures how far our candidate solution is from satisfying the equation at each point <span class="math notranslate nohighlight">\(x\)</span>. As we discussed in the introduction, we will assess whether this residual is “close to zero” by testing its inner products against chosen test functions.</p>
</section>
<section id="step-3-choose-weighted-residual-conditions">
<h3>Step 3: Choose Weighted Residual Conditions<a class="headerlink" href="#step-3-choose-weighted-residual-conditions" title="Link to this heading">#</a></h3>
<p>Having chosen our basis and defined the residual, we must decide how to make the residual “close to zero.” As discussed in the introduction, we can either:</p>
<ol class="arabic simple">
<li><p><strong>Use projection conditions</strong>: Select <span class="math notranslate nohighlight">\(n\)</span> test functions <span class="math notranslate nohighlight">\(\{p_1, \ldots, p_n\}\)</span> and require:
$<span class="math notranslate nohighlight">\(
\langle R(\cdot; a), p_i \rangle = \int_{\mathcal{S}} R(x; a) p_i(x) w(x) dx = 0, \quad i = 1, \ldots, n,
\)</span><span class="math notranslate nohighlight">\(
for some weight function \)</span>w(x)<span class="math notranslate nohighlight">\(. This yields \)</span>n<span class="math notranslate nohighlight">\( equations to determine the \)</span>n<span class="math notranslate nohighlight">\( coefficients in \)</span>a$.</p></li>
<li><p><strong>Use a least squares condition</strong>: Minimize the norm of the residual directly:
$<span class="math notranslate nohighlight">\(
\min_a \int_{\mathcal{S}} R(x; a)^2 w(x) dx.
\)</span>$</p></li>
</ol>
<p>We begin by examining the main projection methods, distinguished entirely by their choice of test functions <span class="math notranslate nohighlight">\(p_i\)</span>, then discuss least squares as an alternative approach.</p>
<p>Let us examine the standard choices of test functions and what they reveal about the residual:</p>
<section id="galerkin-method-test-against-the-basis">
<h4>Galerkin Method: Test Against the Basis<a class="headerlink" href="#galerkin-method-test-against-the-basis" title="Link to this heading">#</a></h4>
<p>The Galerkin method chooses test functions <span class="math notranslate nohighlight">\(p_i = \varphi_i\)</span>, the same basis functions used to approximate <span class="math notranslate nohighlight">\(\hat{f}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\langle R(\cdot; a), \varphi_i \rangle = 0, \quad i = 1, \ldots, n.
\]</div>
<p>To understand what this means, recall that in finite dimensions, two vectors are orthogonal when their inner product is zero. For functions, <span class="math notranslate nohighlight">\(\langle R, \varphi_i \rangle = \int R(x) \varphi_i(x) w(x) dx = 0\)</span> expresses the same concept: <span class="math notranslate nohighlight">\(R\)</span> and <span class="math notranslate nohighlight">\(\varphi_i\)</span> are orthogonal as functions. But there’s more to this than just testing against individual basis functions.</p>
<p>Consider our approximation space <span class="math notranslate nohighlight">\(\text{span}\{\varphi_1, \ldots, \varphi_n\}\)</span> as an <span class="math notranslate nohighlight">\(n\)</span>-dimensional subspace within the infinite-dimensional space of all functions. Any function <span class="math notranslate nohighlight">\(g\)</span> in this space can be written as <span class="math notranslate nohighlight">\(g = \sum_{i=1}^n c_i \varphi_i\)</span> for some coefficients <span class="math notranslate nohighlight">\(c_i\)</span>. If the residual <span class="math notranslate nohighlight">\(R\)</span> is orthogonal to all basis functions <span class="math notranslate nohighlight">\(\varphi_i\)</span>, then by linearity of the inner product, for any such function <span class="math notranslate nohighlight">\(g\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\langle R, g \rangle = \left\langle R, \sum_{i=1}^n c_i \varphi_i \right\rangle = \sum_{i=1}^n c_i \langle R, \varphi_i \rangle = 0.
\]</div>
<p>This shows that <span class="math notranslate nohighlight">\(R\)</span> is orthogonal to every function we can represent with our basis. The residual has “zero overlap” with our approximation space: we cannot express any part of it using our basis functions. In this sense, the residual is as “invisible” to our approximation as possible.</p>
<p>This condition is the defining property of optimality. By choosing our approximation <span class="math notranslate nohighlight">\(\hat{f}\)</span> so that the residual <span class="math notranslate nohighlight">\(R = \mathscr{N}(\hat{f})\)</span> is orthogonal to the entire approximation space, we ensure that <span class="math notranslate nohighlight">\(\hat{f}\)</span> is the orthogonal projection of the true solution onto <span class="math notranslate nohighlight">\(\text{span}{\varphi_1, \ldots, \varphi_n}\)</span>. Within this <span class="math notranslate nohighlight">\(n\)</span>-dimensional space, no better choice is possible: any other coefficients would yield a residual with a nonzero component inside the space, and therefore a larger norm.</p>
<p>The finite-dimensional analogy makes this concrete. Suppose you want to approximate a vector <span class="math notranslate nohighlight">\(\mathbf{v} \in \mathbb{R}^3\)</span> using only the <span class="math notranslate nohighlight">\(xy\)</span>-plane (a 2D subspace). The best approximation is to project <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> onto the plane, giving <span class="math notranslate nohighlight">\(\hat{\mathbf{v}} = (v_1, v_2, 0)\)</span>. The error is <span class="math notranslate nohighlight">\(\mathbf{r} = \mathbf{v} - \hat{\mathbf{v}} = (0, 0, v_3)\)</span>, which points purely in the <span class="math notranslate nohighlight">\(z\)</span>-direction, orthogonal to the entire <span class="math notranslate nohighlight">\(xy\)</span>-plane. This is precisely the Galerkin condition in action: the error is orthogonal to the approximation space.</p>
</section>
<section id="method-of-moments-test-against-monomials">
<h4>Method of Moments: Test Against Monomials<a class="headerlink" href="#method-of-moments-test-against-monomials" title="Link to this heading">#</a></h4>
<p>The method of moments, for problems on <span class="math notranslate nohighlight">\(D \subset \mathbb{R}\)</span>, chooses test functions <span class="math notranslate nohighlight">\(p_i(x) = x^{i-1}\)</span> for <span class="math notranslate nohighlight">\(i = 1, \ldots, n\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\langle R(\cdot; a), x^{i-1} \rangle = 0, \quad i = 1, \ldots, n.
\]</div>
<p>This requires the first <span class="math notranslate nohighlight">\(n\)</span> moments of the residual function to vanish, ensuring the residual is “balanced” in the sense that it has no systematic trend captured by low-order polynomials. The moments <span class="math notranslate nohighlight">\(\int x^k R(x; a) w(x) dx\)</span> measure weighted averages of the residual, with increasing powers of <span class="math notranslate nohighlight">\(x\)</span> giving more weight to larger values. Setting these to zero ensures the residual doesn’t grow systematically with <span class="math notranslate nohighlight">\(x\)</span>. This approach is particularly useful when <span class="math notranslate nohighlight">\(w(x)\)</span> is chosen as a probability measure, making the conditions natural moment restrictions familiar from statistics and econometrics.</p>
</section>
<section id="collocation-method-test-against-delta-functions">
<h4>Collocation Method: Test Against Delta Functions<a class="headerlink" href="#collocation-method-test-against-delta-functions" title="Link to this heading">#</a></h4>
<p>The collocation method chooses test functions <span class="math notranslate nohighlight">\(p_i(x) = \delta(x - x_i)\)</span>, the Dirac delta functions at points <span class="math notranslate nohighlight">\(\{x_1, \ldots, x_n\}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\langle R(\cdot; a), \delta(\cdot - x_i) \rangle = R(x_i; a) = 0, \quad i = 1, \ldots, n.
\]</div>
<p>This is projection against the most localized test functions possible: delta functions that “sample” the residual at specific points, requiring the residual to vanish exactly where we test it. When using orthogonal polynomials with collocation points at the zeros of the <span class="math notranslate nohighlight">\(n\)</span>-th polynomial, the Chebyshev interpolation theorem guarantees that forcing <span class="math notranslate nohighlight">\(R(x_i; a) = 0\)</span> at these specific points makes <span class="math notranslate nohighlight">\(R(x; a)\)</span> small everywhere. Using the zeros of orthogonal polynomials as collocation points produces well-conditioned systems and near-optimal interpolation error. The computational advantage is significant. Collocation avoids numerical integration entirely, requiring only pointwise evaluation of <span class="math notranslate nohighlight">\(R\)</span>.</p>
</section>
<section id="subdomain-method-test-against-indicator-functions">
<h4>Subdomain Method: Test Against Indicator Functions<a class="headerlink" href="#subdomain-method-test-against-indicator-functions" title="Link to this heading">#</a></h4>
<p>The subdomain method partitions the domain into <span class="math notranslate nohighlight">\(n\)</span> subregions <span class="math notranslate nohighlight">\(\{D_1, \ldots, D_n\}\)</span> and chooses test functions <span class="math notranslate nohighlight">\(p_i = I_{D_i}\)</span>, the indicator functions:</p>
<div class="math notranslate nohighlight">
\[
\langle R(\cdot; a), I_{D_i} \rangle = \int_{D_i} R(x; a) w(x) dx = 0, \quad i = 1, \ldots, n.
\]</div>
<p>This requires the residual to have zero average over each subdomain, ensuring the approximation is good “on average” over each piece of the domain. This approach is particularly natural for finite element methods where the domain is divided into elements, ensuring local balance of the residual within each element.</p>
</section>
<section id="least-squares-an-alternative-framework">
<h4>Least Squares: An Alternative Framework<a class="headerlink" href="#least-squares-an-alternative-framework" title="Link to this heading">#</a></h4>
<p>The least squares approach doesn’t fit the test function framework directly. Instead, we minimize:</p>
<div class="math notranslate nohighlight">
\[
\min_a \int_{\mathcal{S}} R(x; a)^2 w(x) dx = \min_a \langle R(\cdot; a), R(\cdot; a) \rangle.
\]</div>
<p>The first-order conditions for this minimization problem are:</p>
<div class="math notranslate nohighlight">
\[
\left\langle R(\cdot; a), \frac{\partial R(\cdot; a)}{\partial a_i} \right\rangle = 0, \quad i = 1, \ldots, n.
\]</div>
<p>Thus least squares implicitly uses test functions <span class="math notranslate nohighlight">\(p_i = \partial R / \partial a_i\)</span>, the gradients of the residual with respect to parameters. Unlike other methods where test functions are chosen a priori, here they depend on the current guess for <span class="math notranslate nohighlight">\(a\)</span> and on the structure of our approximation.</p>
<p>We can now see the unifying structure of <strong>weighted residual methods</strong>: whether we use projection conditions or least squares minimization, all these methods follow the same template of restricting the search to an <span class="math notranslate nohighlight">\(n\)</span>-dimensional function space and imposing <span class="math notranslate nohighlight">\(n\)</span> conditions on the residual. For projection methods specifically, we pick <span class="math notranslate nohighlight">\(n\)</span> test functions and require <span class="math notranslate nohighlight">\(\langle R, p_i \rangle = 0\)</span>. They differ only in their philosophy about which test functions best reveal whether the residual is “nearly zero.” Galerkin tests against the approximation basis itself (natural for orthogonal bases), the method of moments tests against monomials (ensuring polynomial balance), collocation tests against delta functions (pointwise satisfaction), subdomain tests against indicators (local average satisfaction), and least squares tests against residual gradients (global norm minimization). Each choice reflects different priorities: computational efficiency, theoretical optimality, ease of implementation, or sensitivity to errors in different regions of the domain.</p>
</section>
</section>
<section id="step-4-solve-the-finite-dimensional-problem">
<h3>Step 4: Solve the Finite-Dimensional Problem<a class="headerlink" href="#step-4-solve-the-finite-dimensional-problem" title="Link to this heading">#</a></h3>
<p>The projection conditions give us a system to solve for the coefficients <span class="math notranslate nohighlight">\(a\)</span>. For test function methods (Galerkin, collocation, moments, subdomain), we solve:</p>
<div class="math notranslate nohighlight">
\[
P_i(a) \equiv \langle R(\cdot; a), p_i \rangle = 0, \quad i = 1, \ldots, n.
\]</div>
<p>This is a system of <span class="math notranslate nohighlight">\(n\)</span> (generally nonlinear) equations in <span class="math notranslate nohighlight">\(n\)</span> unknowns. For least squares, we solve the optimization problem <span class="math notranslate nohighlight">\(\min_a \langle R(\cdot; a), R(\cdot; a) \rangle\)</span>.</p>
<p>The <strong>conditioning</strong> of the system depends on the choice of test functions. The Jacobian matrix has entries:</p>
<div class="math notranslate nohighlight">
\[
J_{ij} = \frac{\partial P_i}{\partial a_j} = \left\langle \frac{\partial R(\cdot; a)}{\partial a_j}, p_i \right\rangle.
\]</div>
<p>When test functions are orthogonal (or nearly so), the Jacobian tends to be well-conditioned. This is why orthogonal polynomial bases are preferred in Galerkin methods: they produce Jacobians with controlled condition numbers.</p>
<p>The <strong>computational cost per iteration</strong> varies significantly:</p>
<ul class="simple">
<li><p><strong>Collocation</strong>: Cheapest to evaluate since <span class="math notranslate nohighlight">\(P_i(a) = R(x_i; a)\)</span> requires only pointwise evaluation (no integration). The Jacobian is also cheap: <span class="math notranslate nohighlight">\(J_{ij} = \frac{\partial R(x_i; a)}{\partial a_j}\)</span>.</p></li>
<li><p><strong>Galerkin and moments</strong>: More expensive due to integration. Computing <span class="math notranslate nohighlight">\(P_i(a) = \int R(x; a) p_i(x) w(x) dx\)</span> requires numerical quadrature. Each Jacobian entry requires integrating <span class="math notranslate nohighlight">\(\frac{\partial R}{\partial a_j} p_i\)</span>.</p></li>
<li><p><strong>Least squares</strong>: Most expensive when done via the objective function, which requires integrating <span class="math notranslate nohighlight">\(R^2\)</span>. However, the first-order conditions reduce it to a system like Galerkin, with test functions <span class="math notranslate nohighlight">\(p_i = \partial R / \partial a_i\)</span>.</p></li>
</ul>
<p>For methods requiring integration, the choice of quadrature rule should match the basis. Gaussian quadrature with nodes at orthogonal polynomial zeros is efficient. When combined with collocation at those same points, the quadrature is exact for polynomials up to a certain degree. This coordination between quadrature and collocation makes <strong>orthogonal collocation</strong> effective.</p>
<p>The choice of solver depends on whether the finite-dimensional approximation preserves the structural properties of the original infinite-dimensional problem. This matters for the Bellman equation, where the original operator <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> is a contraction.</p>
<p><strong>Successive approximation</strong> (fixed-point iteration) is the natural choice when the original operator is a contraction, as it preserves the global convergence guarantees. However, the finite-dimensional approximation <span class="math notranslate nohighlight">\(\hat{\mathrm{L}}\)</span> may not inherit the contraction property of <span class="math notranslate nohighlight">\(\mathrm{L}\)</span>. The approximation can introduce spurious fixed points or destroy the contraction constant, leading to divergence or slow convergence. This is especially problematic when using high-order polynomial approximations, which can create artificial oscillations that destabilize the iteration.</p>
<p><strong>Newton’s method</strong> is often the default choice for projection methods because it doesn’t rely on the contraction property. Instead, it exploits the smoothness of the residual function. When the original problem is smooth and the approximation preserves this smoothness, Newton’s method provides quadratic convergence near the solution. However, Newton’s method requires good initial guesses and may converge to spurious solutions if the finite-dimensional problem has multiple fixed points that the original problem lacks.</p>
<p><strong>The choice of basis and projection method affects which algorithm is most appropriate</strong>. For example:</p>
<ul class="simple">
<li><p><strong>Linear interpolation</strong> often preserves contraction properties, making successive approximation reliable</p></li>
<li><p><strong>High-order polynomials</strong> may destroy contraction but provide smooth approximations suitable for Newton’s method</p></li>
<li><p><strong>Shape-preserving splines</strong> can maintain both smoothness and structural properties</p></li>
</ul>
<p><strong>In practice, which algorithm should we use?</strong> When the operator equation can be written as a fixed-point problem <span class="math notranslate nohighlight">\(f = \mathscr{T}f\)</span> and the operator <span class="math notranslate nohighlight">\(\mathscr{T}\)</span> is known to be a contraction, successive approximation is often the best starting point: it is computationally cheap and globally convergent. However, not all equations <span class="math notranslate nohighlight">\(\mathscr{N}(f) = 0\)</span> admit a natural fixed-point reformulation, and even when they do (e.g., <span class="math notranslate nohighlight">\(f = f - \alpha \mathscr{N}(f)\)</span> for some <span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span>), the resulting operator may not be a contraction in the finite-dimensional approximation space. In such cases, Newton’s method becomes the primary option despite its requirement for good initial guesses and higher computational cost per iteration. A hybrid approach often works well: use successive approximation when applicable to generate an initial guess, then switch to Newton’s method for refinement.</p>
<p>Another consideration is the conditioning of the resulting system. Poorly chosen basis functions or collocation points can lead to nearly singular Jacobians, causing numerical instability. Orthogonal bases and carefully chosen collocation points (like Chebyshev nodes) are preferred because they tend to produce well-conditioned systems.</p>
</section>
<section id="step-5-verify-the-solution">
<h3>Step 5: Verify the Solution<a class="headerlink" href="#step-5-verify-the-solution" title="Link to this heading">#</a></h3>
<p>Once we have computed a candidate solution <span class="math notranslate nohighlight">\(\hat{f}\)</span>, we must verify its quality. Projection methods optimize <span class="math notranslate nohighlight">\(\hat{f}\)</span> with respect to specific criteria (specific test functions or collocation points), but we should check that the residual is small everywhere, not just in the directions or at the points we optimized over.</p>
<p>Typical diagnostic checks include:</p>
<ul class="simple">
<li><p>Computing <span class="math notranslate nohighlight">\(\|R(\cdot; a)\|\)</span> using a more accurate quadrature rule than was used in the optimization</p></li>
<li><p>Evaluating <span class="math notranslate nohighlight">\(R(x; a)\)</span> at many points not used in the fitting process</p></li>
<li><p>If using Galerkin with the first <span class="math notranslate nohighlight">\(n\)</span> basis functions, checking orthogonality against higher-order basis functions</p></li>
</ul>
</section>
</section>
<section id="application-to-the-bellman-equation">
<h2>Application to the Bellman Equation<a class="headerlink" href="#application-to-the-bellman-equation" title="Link to this heading">#</a></h2>
<p>We now apply the projection method framework to the Bellman optimality equation. Recall that we seek a function <span class="math notranslate nohighlight">\(v\)</span> satisfying</p>
<div class="math notranslate nohighlight">
\[
v(s) = \mathrm{L}v(s) = \max_{a \in \mathcal{A}_s} \left\{ r(s,a) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a) v(j) \right\}.
\]</div>
<p>Writing this as an operator equation <span class="math notranslate nohighlight">\(\mathscr{N}(v) = 0\)</span> with <span class="math notranslate nohighlight">\(\mathscr{N}(v) = \mathrm{L}v - v\)</span>, the residual function for a candidate approximation <span class="math notranslate nohighlight">\(\hat{v}(s) = \sum_{i=1}^n a_i \varphi_i(s)\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
R(s; a) = \mathrm{L}\hat{v}(s) - \hat{v}(s) = \max_{a \in \mathcal{A}_s} \left\{ r(s,a) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a) \hat{v}(j) \right\} - \sum_{i=1}^n a_i \varphi_i(s).
\]</div>
<p>Any of the projection methods we discussed (Galerkin, method of moments, collocation, subdomain, or least squares) can be applied here. Each would give us <span class="math notranslate nohighlight">\(n\)</span> conditions to determine the <span class="math notranslate nohighlight">\(n\)</span> coefficients in our approximation. For instance:</p>
<ul class="simple">
<li><p><strong>Galerkin</strong> would require <span class="math notranslate nohighlight">\(\langle R(\cdot; a), \varphi_i \rangle = 0\)</span> for <span class="math notranslate nohighlight">\(i = 1, \ldots, n\)</span>, involving integration of the residual weighted by basis functions</p></li>
<li><p><strong>Method of moments</strong> would require <span class="math notranslate nohighlight">\(\langle R(\cdot; a), s^{i-1} \rangle = 0\)</span>, setting the first <span class="math notranslate nohighlight">\(n\)</span> moments of the residual to zero</p></li>
<li><p><strong>Collocation</strong> would require <span class="math notranslate nohighlight">\(R(s_i; a) = 0\)</span> at <span class="math notranslate nohighlight">\(n\)</span> chosen states, forcing the residual to vanish pointwise</p></li>
</ul>
<p>In practice, <strong>collocation is the most commonly used</strong> projection method for the Bellman equation. The reason is computational: collocation avoids the numerical integration required by Galerkin and method of moments. Since the Bellman operator already involves integration (or summation) over next states, adding another layer of integration for the projection conditions would be computationally expensive. Collocation sidesteps this by requiring the equation to hold exactly at specific points.</p>
<p>We focus on collocation in detail, though the principles extend to other projection methods.</p>
<section id="collocation-for-the-bellman-equation">
<h3>Collocation for the Bellman Equation<a class="headerlink" href="#collocation-for-the-bellman-equation" title="Link to this heading">#</a></h3>
<p>The collocation approach chooses <span class="math notranslate nohighlight">\(n\)</span> states <span class="math notranslate nohighlight">\(\{s_1, \ldots, s_n\}\)</span> (the collocation points) and requires:</p>
<div class="math notranslate nohighlight">
\[
R(s_i; a) = 0, \quad i = 1, \ldots, n.
\]</div>
<p>This gives us a system of <span class="math notranslate nohighlight">\(n\)</span> nonlinear equations in <span class="math notranslate nohighlight">\(n\)</span> unknowns:</p>
<div class="math notranslate nohighlight">
\[
\sum_{j=1}^n a_j \varphi_j(s_i) = \max_{a \in \mathcal{A}_{s_i}} \left\{ r(s_i,a) + \gamma \sum_{j \in \mathcal{S}} p(j|s_i,a) \hat{v}(j) \right\}, \quad i = 1, \ldots, n.
\]</div>
<p>The right-hand side requires evaluating the Bellman operator at the collocation points. For each collocation point <span class="math notranslate nohighlight">\(s_i\)</span>, we must:</p>
<ol class="arabic simple">
<li><p>For each action <span class="math notranslate nohighlight">\(a \in \mathcal{A}_{s_i}\)</span>, compute the expected continuation value <span class="math notranslate nohighlight">\(\sum_{j \in \mathcal{S}} p(j|s_i,a) \hat{v}(j)\)</span></p></li>
<li><p>Take the maximum over actions</p></li>
</ol>
<p>When the state space is continuous, the expectation involves integration, which typically requires numerical quadrature. When the state space is discrete but large, this is a straightforward (though potentially expensive) summation.</p>
<section id="solving-the-collocation-system">
<h4>Solving the Collocation System<a class="headerlink" href="#solving-the-collocation-system" title="Link to this heading">#</a></h4>
<p>The resulting system is nonlinear due to the max operator on the right-hand side. We can write this more compactly as finding <span class="math notranslate nohighlight">\(a \in \mathbb{R}^n\)</span> such that <span class="math notranslate nohighlight">\(F(a) = 0\)</span> where:</p>
<div class="math notranslate nohighlight">
\[
F_i(a) = \sum_{j=1}^n a_j \varphi_j(s_i) - \max_{u \in \mathcal{A}_{s_i}} \left\{ r(s_i,u) + \gamma \sum_{j \in \mathcal{S}} p(j|s_i,u) \hat{v}(j; a) \right\}.
\]</div>
<p><strong>Newton’s method</strong> is the standard approach for such systems. However, the max operator introduces a non-differentiability issue: the function <span class="math notranslate nohighlight">\(F(a)\)</span> is not everywhere differentiable because the optimal action can change discontinuously as <span class="math notranslate nohighlight">\(a\)</span> varies. Fortunately, the function is <strong>semismooth</strong>: it is locally Lipschitz continuous and directionally differentiable everywhere. This structure can be exploited by <strong>semi-smooth Newton methods</strong>, which generalize Newton’s method to semismooth equations by using any element of the generalized Jacobian (in the sense of Clarke’s generalized derivative) in place of the classical Jacobian.</p>
<p>In practice, implementing semi-smooth Newton for the Bellman equation is straightforward: at each iteration, we fix the optimal action <span class="math notranslate nohighlight">\(a^*(s_i; a^{(k)})\)</span> at the current guess <span class="math notranslate nohighlight">\(a^{(k)}\)</span>, compute the Jacobian assuming these actions remain optimal, and update:</p>
<div class="math notranslate nohighlight">
\[
a^{(k+1)} = a^{(k)} - J_F(a^{(k)})^{-1} F(a^{(k)}).
\]</div>
<p>As we approach the solution, the optimal actions typically stabilize, and the method achieves superlinear convergence despite the non-smoothness. The main practical requirement is a good initial guess, which can be obtained from the successive approximation method described next.</p>
</section>
</section>
<section id="iterative-solution-successive-approximation">
<h3>Iterative Solution: Successive Approximation<a class="headerlink" href="#iterative-solution-successive-approximation" title="Link to this heading">#</a></h3>
<p>Rather than solving the nonlinear system directly via Newton’s method, we can exploit the fixed-point structure of the problem. The collocation equations can be viewed as requiring that the approximation matches the Bellman operator at the collocation points. This suggests an iterative scheme that performs <strong>successive approximation</strong> (fixed-point iteration) in the finite-dimensional coefficient space. Starting with an initial guess <span class="math notranslate nohighlight">\(a^{(0)}\)</span>, we iterate:</p>
<ol class="arabic">
<li><p><strong>Maximization step</strong>: At each collocation point <span class="math notranslate nohighlight">\(s_i\)</span>, compute</p>
<div class="math notranslate nohighlight">
\[
   v_i^{(k+1)} = \max_{a \in \mathcal{A}_{s_i}} \left\{ r(s_i,a) + \gamma \sum_{j \in \mathcal{S}} p(j|s_i,a) \hat{v}(s_j; a^{(k)}) \right\}.
   \]</div>
</li>
<li><p><strong>Fitting step</strong>: Find coefficients <span class="math notranslate nohighlight">\(a^{(k+1)}\)</span> such that <span class="math notranslate nohighlight">\(\hat{v}(s_i; a^{(k+1)}) = v_i^{(k+1)}\)</span> for all <span class="math notranslate nohighlight">\(i = 1, \ldots, n\)</span>. This is a linear system if our approximation is linear in the coefficients.</p></li>
<li><p><strong>Check convergence</strong>: If <span class="math notranslate nohighlight">\(\|a^{(k+1)} - a^{(k)}\|\)</span> is sufficiently small, stop; otherwise return to step 1.</p></li>
</ol>
<p>This algorithm, which Judd calls <strong>parametric value function iteration</strong> or <strong>projection-based value iteration</strong>, separates the difficult nonlinear optimization (the max operator in the Bellman equation) from the approximation problem. Each iteration improves the approximation by ensuring it matches the Bellman operator at the collocation points. Mathematically, it performs successive approximation in the finite-dimensional coefficient space: we define an operator <span class="math notranslate nohighlight">\(\hat{\mathrm{L}}\)</span> that maps coefficients <span class="math notranslate nohighlight">\(a^{(k)}\)</span> to new coefficients <span class="math notranslate nohighlight">\(a^{(k+1)}\)</span> via the maximization and fitting steps, then iterate <span class="math notranslate nohighlight">\(a^{(k+1)} = \hat{\mathrm{L}}(a^{(k)})\)</span>.</p>
<section id="comparison-of-solution-methods">
<h4>Comparison of Solution Methods<a class="headerlink" href="#comparison-of-solution-methods" title="Link to this heading">#</a></h4>
<p>We now have two approaches to solving the collocation equations:</p>
<ol class="arabic simple">
<li><p><strong>Semi-smooth Newton</strong>: Solve the nonlinear system <span class="math notranslate nohighlight">\(F(a) = 0\)</span> directly using Newton’s method adapted for semismooth functions. This offers fast (superlinear) convergence near the solution but requires a good initial guess and may fail to converge from poor starting points.</p></li>
<li><p><strong>Successive approximation (parametric value iteration)</strong>: Iterate the map <span class="math notranslate nohighlight">\(a^{(k+1)} = \hat{\mathrm{L}}(a^{(k)})\)</span> that alternates between maximization and fitting steps. This is more robust to poor initial guesses and inherits global convergence properties when the finite-dimensional approximation preserves the contraction property of the Bellman operator.</p></li>
</ol>
<p><strong>Which should we use?</strong> Following Judd’s guidance: When the Bellman operator is known to be a contraction and the finite-dimensional approximation preserves this property (as with linear interpolation or carefully chosen low-order approximations), successive approximation is often the best choice. It is globally convergent and each iteration is relatively cheap. However, high-order polynomial approximations may destroy the contraction property or introduce numerical instabilities. In such cases, or when convergence is too slow, Newton’s method (or semi-smooth Newton) becomes necessary despite requiring good initial guesses.</p>
<p>A <strong>hybrid strategy</strong> works well in practice: use successive approximation to generate an initial approximation, then switch to semi-smooth Newton for rapid refinement once in the neighborhood of the solution. This combines the global convergence of successive approximation with the fast local convergence of Newton’s method.</p>
</section>
</section>
<section id="shape-preserving-considerations">
<h3>Shape-Preserving Considerations<a class="headerlink" href="#shape-preserving-considerations" title="Link to this heading">#</a></h3>
<p>In dynamic programming, the value function typically has specific structural properties that we want our approximation to preserve. For instance:</p>
<ul class="simple">
<li><p><strong>Monotonicity</strong>: If having more of a resource is better, the value function should be increasing</p></li>
<li><p><strong>Concavity</strong>: Diminishing returns often imply concave value functions</p></li>
<li><p><strong>Boundedness</strong>: The value function is bounded when rewards are bounded</p></li>
</ul>
<p>Standard polynomial approximation does not automatically preserve these properties. A polynomial fit to increasing, concave data points can produce a function with non-monotonic or convex regions between the data points. This can destabilize the iterative algorithm: artificially high values at non-collocation points can lead to poor decisions in the maximization step, which feeds back into even worse approximations.</p>
<p><strong>Shape-preserving approximation methods</strong> address this issue. For one-dimensional problems, Schumaker’s shape-preserving quadratic splines maintain monotonicity and concavity while providing continuously differentiable approximations. For multidimensional problems, linear interpolation on simplices preserves monotonicity and convex combinations (though not concavity or smoothness).</p>
<p>The trade-off is between smoothness and shape preservation. Smooth approximations (high-order polynomials or splines) enable efficient optimization in the maximization step through gradient-based methods, but risk introducing spurious features. Simple approximations (linear interpolation) guarantee shape preservation but introduce kinks that complicate optimization and may produce discontinuous policies when the true policy is continuous.</p>
</section>
</section>
<section id="galerkin-projection-and-least-squares-temporal-difference">
<h2>Galerkin Projection and Least Squares Temporal Difference<a class="headerlink" href="#galerkin-projection-and-least-squares-temporal-difference" title="Link to this heading">#</a></h2>
<p>An important special case emerges when we apply Galerkin projection to the <strong>policy evaluation</strong> problem rather than the optimality problem. For a fixed policy <span class="math notranslate nohighlight">\(\pi\)</span>, the policy evaluation Bellman equation is:</p>
<div class="math notranslate nohighlight">
\[
v^\pi(s) = \mathrm{L}_\pi v^\pi(s) = r(s,\pi(s)) + \gamma \sum_{s' \in \mathcal{S}} p(s'|s,\pi(s)) v^\pi(s').
\]</div>
<p>This is a linear operator (no max), making the projection problem significantly simpler. Consider a linear function approximation <span class="math notranslate nohighlight">\(\hat{v}(s) = \boldsymbol{\varphi}(s)^\top \mathbf{a}\)</span> where <span class="math notranslate nohighlight">\(\boldsymbol{\varphi}(s) = [\varphi_1(s), \ldots, \varphi_n(s)]^\top\)</span> are basis functions and <span class="math notranslate nohighlight">\(\mathbf{a} = [a_1, \ldots, a_n]^\top\)</span> are coefficients to determine. The residual is:</p>
<div class="math notranslate nohighlight">
\[
R(s; \mathbf{a}) = \mathrm{L}_\pi \hat{v}(s) - \hat{v}(s) = r(s,\pi(s)) + \gamma \sum_{s'} p(s'|s,\pi(s)) \boldsymbol{\varphi}(s')^\top \mathbf{a} - \boldsymbol{\varphi}(s)^\top \mathbf{a}.
\]</div>
<p>The Galerkin projection requires the residual to be orthogonal to all basis functions with respect to some weighting:</p>
<div class="math notranslate nohighlight">
\[
\sum_{s \in \mathcal{S}} \xi(s) R(s; \mathbf{a}) \varphi_j(s) = 0, \quad j = 1, \ldots, n,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\xi(s)\)</span> is a distribution over states (often the stationary distribution under policy <span class="math notranslate nohighlight">\(\pi\)</span>, or uniform over visited states). Substituting the residual:</p>
<div class="math notranslate nohighlight">
\[
\sum_s \xi(s) \left[ r(s,\pi(s)) + \gamma \sum_{s'} p(s'|s,\pi(s)) \boldsymbol{\varphi}(s')^\top \mathbf{a} - \boldsymbol{\varphi}(s)^\top \mathbf{a} \right] \varphi_j(s) = 0.
\]</div>
<p>Rearranging and writing in matrix form, let <span class="math notranslate nohighlight">\(\boldsymbol{\Xi}\)</span> be a diagonal matrix with <span class="math notranslate nohighlight">\(\Xi_{ss} = \xi(s)\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\Phi}\)</span> be the <span class="math notranslate nohighlight">\(|\mathcal{S}| \times n\)</span> matrix with rows <span class="math notranslate nohighlight">\(\boldsymbol{\varphi}(s)^\top\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{P}_\pi\)</span> be the transition matrix under policy <span class="math notranslate nohighlight">\(\pi\)</span>. The Galerkin conditions become:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\Phi}^\top \boldsymbol{\Xi} (\mathbf{r}_\pi + \gamma \mathbf{P}_\pi \boldsymbol{\Phi} \mathbf{a} - \boldsymbol{\Phi} \mathbf{a}) = \mathbf{0}.
\]</div>
<p>Solving for <span class="math notranslate nohighlight">\(\mathbf{a}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\Phi}^\top \boldsymbol{\Xi} (\boldsymbol{\Phi} - \gamma \mathbf{P}_\pi \boldsymbol{\Phi}) \mathbf{a} = \boldsymbol{\Phi}^\top \boldsymbol{\Xi} \mathbf{r}_\pi.
\]</div>
<p>This is precisely the <strong>Least Squares Temporal Difference (LSTD)</strong> solution for policy evaluation. The connection reveals that LSTD is Galerkin projection applied to the linear policy evaluation Bellman equation. The “least squares” name comes from the fact that this is the projection (in the weighted <span class="math notranslate nohighlight">\(\ell^2\)</span> sense) of the Bellman operator’s output onto the span of the basis functions.</p>
<p>The projection perspective clarifies a key aspect of approximate dynamic programming. The solution <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> does not satisfy the true Bellman equation <span class="math notranslate nohighlight">\(v = \mathrm{L}_\pi v\)</span> (which is typically impossible within our finite-dimensional approximation space). Instead, it satisfies <span class="math notranslate nohighlight">\(\hat{v} = \Pi \mathrm{L}_\pi \hat{v}\)</span>, where <span class="math notranslate nohighlight">\(\Pi\)</span> is the projection operator onto <span class="math notranslate nohighlight">\(\text{span}\{\varphi_1, \ldots, \varphi_n\}\)</span>. We find the fixed point of the <em>projected</em> Bellman operator, not the Bellman operator itself. This is why approximation error persists even at convergence: the best we can do is find the value function whose Bellman operator output projects back onto itself.</p>
<section id="the-projected-bellman-equations">
<h3>The Projected Bellman Equations<a class="headerlink" href="#the-projected-bellman-equations" title="Link to this heading">#</a></h3>
<p>The LSTD solution gives a closed-form expression and connects to iterative algorithms developed in the next chapter. Understanding convergence of these methods requires analyzing when the projected Bellman operator <span class="math notranslate nohighlight">\(\Pi \mathrm{L}_\pi\)</span> is a contraction.</p>
<p><strong>Norms and projections.</strong> Fix a feature matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Phi} \in \mathbb{R}^{|\mathcal{S}| \times n}\)</span> with full column rank and a probability distribution <span class="math notranslate nohighlight">\(\xi\)</span> over states. Define the <span class="math notranslate nohighlight">\(\xi\)</span>-weighted inner product and norm by</p>
<div class="math notranslate nohighlight">
\[
\langle u, v \rangle_\xi := \sum_s \xi(s) u(s) v(s) = u^\top \boldsymbol{\Xi} v, \qquad \|v\|_\xi := \sqrt{v^\top \boldsymbol{\Xi} v},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\Xi} = \text{diag}(\xi)\)</span>. The orthogonal projection onto <span class="math notranslate nohighlight">\(\text{span}(\boldsymbol{\Phi})\)</span> with respect to <span class="math notranslate nohighlight">\(\langle \cdot, \cdot \rangle_\xi\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\Pi = \boldsymbol{\Phi}(\boldsymbol{\Phi}^\top \boldsymbol{\Xi} \boldsymbol{\Phi})^{-1} \boldsymbol{\Phi}^\top \boldsymbol{\Xi}.
\]</div>
<p>An operator <span class="math notranslate nohighlight">\(\mathrm{T}\)</span> is a <strong><span class="math notranslate nohighlight">\(\beta\)</span>-contraction</strong> in norm <span class="math notranslate nohighlight">\(\|\cdot\|\)</span> if <span class="math notranslate nohighlight">\(\|\mathrm{T}v - \mathrm{T}w\| \leq \beta \|v - w\|\)</span> for all <span class="math notranslate nohighlight">\(v, w\)</span> and some <span class="math notranslate nohighlight">\(\beta &lt; 1\)</span>. It is a <strong>non-expansion</strong> if the same holds with <span class="math notranslate nohighlight">\(\beta = 1\)</span>.</p>
<p><strong>Why <span class="math notranslate nohighlight">\(\Pi\)</span> is a non-expansion.</strong> The key is the Pythagorean identity in weighted inner product spaces. For any <span class="math notranslate nohighlight">\(u \in \mathbb{R}^{|\mathcal{S}|}\)</span>, the projection <span class="math notranslate nohighlight">\(\Pi u\)</span> and the residual <span class="math notranslate nohighlight">\((I - \Pi)u\)</span> are <span class="math notranslate nohighlight">\(\xi\)</span>-orthogonal: <span class="math notranslate nohighlight">\(\langle \Pi u, (I-\Pi)u \rangle_\xi = 0\)</span>. Therefore,</p>
<div class="math notranslate nohighlight">
\[
\|u\|_\xi^2 = \|\Pi u\|_\xi^2 + \|(I-\Pi)u\|_\xi^2.
\]</div>
<p>Applying this to <span class="math notranslate nohighlight">\(u - v\)</span> gives</p>
<div class="math notranslate nohighlight">
\[
\|\Pi u - \Pi v\|_\xi^2 = \|\Pi(u-v)\|_\xi^2 \leq \|\Pi(u-v)\|_\xi^2 + \|(I-\Pi)(u-v)\|_\xi^2 = \|u - v\|_\xi^2,
\]</div>
<p>proving <span class="math notranslate nohighlight">\(\|\Pi u - \Pi v\|_\xi \leq \|u - v\|_\xi\)</span>.</p>
<p><strong>When is <span class="math notranslate nohighlight">\(\mathrm{L}_\pi\)</span> a contraction in <span class="math notranslate nohighlight">\(\|\cdot\|_\xi\)</span>?</strong> Write the policy evaluation operator as <span class="math notranslate nohighlight">\(\mathrm{L}_\pi v = r_\pi + \gamma \mathbf{P}_\pi v\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{P}_\pi\)</span> is the transition matrix under policy <span class="math notranslate nohighlight">\(\pi\)</span>. We know <span class="math notranslate nohighlight">\(\mathrm{L}_\pi\)</span> is a <span class="math notranslate nohighlight">\(\gamma\)</span>-contraction in <span class="math notranslate nohighlight">\(\|\cdot\|_\infty\)</span> from earlier chapters. However, whether it contracts in <span class="math notranslate nohighlight">\(\|\cdot\|_\xi\)</span> depends on the relationship between <span class="math notranslate nohighlight">\(\xi\)</span> and <span class="math notranslate nohighlight">\(\mathbf{P}_\pi\)</span>.</p>
<p>The key is to establish when the stochastic matrix <span class="math notranslate nohighlight">\(\mathbf{P}_\pi\)</span> is non-expansive in <span class="math notranslate nohighlight">\(\|\cdot\|_\xi\)</span>. Following Bertsekas (Lemma 6.3.1), suppose <span class="math notranslate nohighlight">\(\xi\)</span> is a <strong>steady-state probability vector</strong> for <span class="math notranslate nohighlight">\(\mathbf{P}_\pi\)</span> with positive components, meaning:</p>
<div class="math notranslate nohighlight">
\[
\xi^\top \mathbf{P}_\pi = \xi^\top, \qquad \text{or equivalently,} \qquad \xi(s') = \sum_s \xi(s) p(s'|s,\pi(s)) \text{ for all } s'.
\]</div>
<p>Then for any <span class="math notranslate nohighlight">\(z \in \mathbb{R}^{|\mathcal{S}|}\)</span>, using the convexity of the square function (Jensen’s inequality):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\|\mathbf{P}_\pi z\|_\xi^2 &amp;= \sum_s \xi(s) \left(\sum_{s'} p(s'|s,\pi(s)) z(s')\right)^2 \\
&amp;\leq \sum_s \xi(s) \sum_{s'} p(s'|s,\pi(s)) z(s')^2 \\
&amp;= \sum_{s'} \left(\sum_s \xi(s) p(s'|s,\pi(s))\right) z(s')^2
\end{align*}
\end{split}\]</div>
<p>Using the defining property of steady-state probabilities <span class="math notranslate nohighlight">\(\sum_s \xi(s) p(s'|s,\pi(s)) = \xi(s')\)</span>:</p>
<div class="math notranslate nohighlight">
\[
= \sum_{s'} \xi(s') z(s')^2 = \|z\|_\xi^2.
\]</div>
<p>Therefore <span class="math notranslate nohighlight">\(\|\mathbf{P}_\pi z\|_\xi \leq \|z\|_\xi\)</span>, showing that <span class="math notranslate nohighlight">\(\mathbf{P}_\pi\)</span> is non-expansive in <span class="math notranslate nohighlight">\(\|\cdot\|_\xi\)</span>. Since <span class="math notranslate nohighlight">\(\|\mathrm{L}_\pi v - \mathrm{L}_\pi w\|_\xi = \gamma \|\mathbf{P}_\pi(v-w)\|_\xi\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\|\mathrm{L}_\pi v - \mathrm{L}_\pi w\|_\xi \leq \gamma \|v - w\|_\xi.
\]</div>
<p>Thus <span class="math notranslate nohighlight">\(\mathrm{L}_\pi\)</span> is a <span class="math notranslate nohighlight">\(\gamma\)</span>-contraction in <span class="math notranslate nohighlight">\(\|\cdot\|_\xi\)</span> when <span class="math notranslate nohighlight">\(\xi\)</span> is the steady-state distribution of <span class="math notranslate nohighlight">\(\pi\)</span>.</p>
<p><strong>Contraction of the composition.</strong> Combining our two results: <span class="math notranslate nohighlight">\(\Pi\)</span> is a non-expansion and (under stationarity) <span class="math notranslate nohighlight">\(\mathrm{L}_\pi\)</span> is a <span class="math notranslate nohighlight">\(\gamma\)</span>-contraction in <span class="math notranslate nohighlight">\(\|\cdot\|_\xi\)</span>. Therefore,</p>
<div class="math notranslate nohighlight">
\[
\|\Pi \mathrm{L}_\pi v - \Pi \mathrm{L}_\pi w\|_\xi \leq \|\mathrm{L}_\pi v - \mathrm{L}_\pi w\|_\xi \leq \gamma \|v - w\|_\xi.
\]</div>
<p>By the Banach fixed-point theorem, <span class="math notranslate nohighlight">\(\Pi \mathrm{L}_\pi\)</span> has a unique fixed point in <span class="math notranslate nohighlight">\(\mathbb{R}^{|\mathcal{S}|}\)</span>, and iterates <span class="math notranslate nohighlight">\(v_{k+1} = \Pi \mathrm{L}_\pi v_k\)</span> converge to it from any <span class="math notranslate nohighlight">\(v_0\)</span>. This fixed point satisfies the <strong>projected Bellman equation</strong></p>
<div class="math notranslate nohighlight">
\[
v = \Pi(r_\pi + \gamma \mathbf{P}_\pi v), \qquad v \in \text{span}(\boldsymbol{\Phi}).
\]</div>
<p>Writing <span class="math notranslate nohighlight">\(v = \boldsymbol{\Phi} w\)</span> and left-multiplying by <span class="math notranslate nohighlight">\(\boldsymbol{\Phi}^\top \boldsymbol{\Xi}\)</span> yields the <strong>normal equations</strong></p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\Phi}^\top \boldsymbol{\Xi}(\boldsymbol{\Phi} - \gamma \mathbf{P}_\pi \boldsymbol{\Phi}) w = \boldsymbol{\Phi}^\top \boldsymbol{\Xi} r_\pi,
\]</div>
<p>which are precisely the LSTD equations we derived earlier. This result provides the theoretical foundation for temporal difference learning with linear function approximation: when learning on-policy (so <span class="math notranslate nohighlight">\(\xi\)</span> is stationary), convergence is guaranteed.</p>
<p><strong>Off-policy instability.</strong> When <span class="math notranslate nohighlight">\(\xi\)</span> is not stationary for <span class="math notranslate nohighlight">\(\mathbf{P}_\pi\)</span> (as occurs when data come from a different behavior policy), the Jensen argument breaks down. The transition operator <span class="math notranslate nohighlight">\(\mathbf{P}_\pi\)</span> need not be non-expansive in <span class="math notranslate nohighlight">\(\|\cdot\|_\xi\)</span>, so <span class="math notranslate nohighlight">\(\Pi \mathrm{L}_\pi\)</span> may fail to be a contraction. This is the root cause of off-policy divergence phenomena in linear TD learning (e.g., Baird’s counterexample). Importance weighting and other corrections are designed to restore stability in this regime.</p>
<p>The linearity of the policy evaluation operator <span class="math notranslate nohighlight">\(\mathrm{L}_\pi\)</span> is what gives us the closed-form solution. We could apply Galerkin projection to the Bellman optimality equation <span class="math notranslate nohighlight">\(v^* = \mathrm{L} v^*\)</span>, setting up orthogonality conditions <span class="math notranslate nohighlight">\(\sum_s \xi(s) R(s; \mathbf{a}) \varphi_j(s) = 0\)</span>. The max operator makes these conditions nonlinear in <span class="math notranslate nohighlight">\(\mathbf{a}\)</span>, eliminating the closed form and requiring iterative solution. This brings us back to the successive approximation methods discussed earlier for collocation.</p>
<p>This framework of projection methods (choosing test functions, defining residuals, and solving finite-dimensional systems) provides the conceptual foundation for approximate dynamic programming. One question remains: how do we evaluate the expectations in the Bellman operator when we lack explicit transition probabilities or when the state space is too large for exact computation? The next chapter introduces Monte Carlo integration methods, connecting classical projection methods to simulation-based approximate dynamic programming and reinforcement learning.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="regmdp.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Smooth Bellman Optimality Equations</p>
      </div>
    </a>
    <a class="right-next"
       href="simadp.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Simulation-Based Approximate Dynamic Programming</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-does-it-mean-for-a-residual-to-be-zero">What Does It Mean for a Residual to Be Zero?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-special-role-of-spectral-methods">The Special Role of Spectral Methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#orthogonal-collocation-best-of-both-worlds">Orthogonal Collocation: Best of Both Worlds</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-general-framework">The General Framework</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-choose-a-finite-dimensional-approximation-space">Step 1: Choose a Finite-Dimensional Approximation Space</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-define-the-residual-function">Step 2: Define the Residual Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-choose-weighted-residual-conditions">Step 3: Choose Weighted Residual Conditions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#galerkin-method-test-against-the-basis">Galerkin Method: Test Against the Basis</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#method-of-moments-test-against-monomials">Method of Moments: Test Against Monomials</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#collocation-method-test-against-delta-functions">Collocation Method: Test Against Delta Functions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#subdomain-method-test-against-indicator-functions">Subdomain Method: Test Against Indicator Functions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#least-squares-an-alternative-framework">Least Squares: An Alternative Framework</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-solve-the-finite-dimensional-problem">Step 4: Solve the Finite-Dimensional Problem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-verify-the-solution">Step 5: Verify the Solution</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#application-to-the-bellman-equation">Application to the Bellman Equation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#collocation-for-the-bellman-equation">Collocation for the Bellman Equation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-the-collocation-system">Solving the Collocation System</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iterative-solution-successive-approximation">Iterative Solution: Successive Approximation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-of-solution-methods">Comparison of Solution Methods</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shape-preserving-considerations">Shape-Preserving Considerations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#galerkin-projection-and-least-squares-temporal-difference">Galerkin Projection and Least Squares Temporal Difference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-projected-bellman-equations">The Projected Bellman Equations</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Pierre-Luc Bacon
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>