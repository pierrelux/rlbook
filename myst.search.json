{"version":"1","records":[{"hierarchy":{"lvl1":"Example COCPs"},"type":"lvl1","url":"/appendix-examples","position":0},{"hierarchy":{"lvl1":"Example COCPs"},"content":"","type":"content","url":"/appendix-examples","position":1},{"hierarchy":{"lvl1":"Example COCPs","lvl2":"Inverted Pendulum"},"type":"lvl2","url":"/appendix-examples#inverted-pendulum","position":2},{"hierarchy":{"lvl1":"Example COCPs","lvl2":"Inverted Pendulum"},"content":"The inverted pendulum is a classic problem in control theory and robotics that demonstrates the challenge of stabilizing a dynamic system that is inherently unstable. The objective is to keep a pendulum balanced in the upright position by applying a control force, typically at its base. This setup is analogous to balancing a broomstick on your finger: any deviation from the vertical position will cause the system to tip over unless you actively counteract it with appropriate control actions.\n\nWe typically assume that the pendulum is mounted on a cart or movable base, which can move horizontally. The system’s state is then characterized by four variables:\n\nCart position:  x(t)  — the horizontal position of the base.\n\nCart velocity:  \\dot{x}(t)  — the speed of the cart.\n\nPendulum angle:  \\theta(t)  — the angle between the pendulum and the vertical upright position.\n\nAngular velocity:  \\dot{\\theta}(t)  — the rate at which the pendulum’s angle is changing.\n\nThis setup is more complex because the controller must deal with interactions between two different types of motion: linear (the cart) and rotational (the pendulum). This system is said to be “underactuated” because the number of control inputs (one) is less than the number of state variables (four). This makes the problem more challenging and interesting from a control perspective.\n\nWe can simplify the problem by assuming that the base of the pendulum is fixed.  This is akin to having the bottom of the stick attached to a fixed pivot on a table. You can’t move the base anymore; you can only apply small nudges at the pivot point to keep the stick balanced upright. In this case, you’re only focusing on adjusting the stick’s tilt without worrying about moving the base. This reduces the problem to stabilizing the pendulum’s upright orientation using only the rotational dynamics. The system’s state can now be described by just two variables:\n\nPendulum angle:  \\theta(t)  — the angle of the pendulum from the upright vertical position.\n\nAngular velocity:  \\dot{\\theta}(t)  — the rate at which the pendulum’s angle is changing.\n\nThe evolution of these two varibles is governed by the following ordinary differential equation:\\begin{bmatrix} \\dot{\\theta}(t) \\\\ \\ddot{\\theta}(t) \\end{bmatrix} = \\begin{bmatrix} \\dot{\\theta}(t) \\\\ \\frac{mgl}{J_t} \\sin{\\theta(t)} - \\frac{\\gamma}{J_t} \\dot{\\theta}(t) + \\frac{l}{J_t} u(t) \\cos{\\theta(t)} \\end{bmatrix}, \\quad y(t) = \\theta(t)\n\nwhere:\n\nm is the mass of the pendulum\n\ng is the acceleration due to gravity\n\nl is the length of the pendulum\n\n\\gamma is the coefficient of rotational friction\n\nJ_t = J + ml^2 is the total moment of inertia, with J being the pendulum’s moment of inertia about its center of mass\n\nu(t) is the control force applied at the base\n\ny(t) = \\theta(t) is the measured output (the pendulum’s angle)\n\nWe expect that when no control is applied to the system, the rod should be falling down when started from the upright position.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.integrate import odeint\nfrom IPython.display import HTML, display\nfrom matplotlib.animation import FuncAnimation\n\n# System parameters\nm = 1.0  # mass of the pendulum (kg)\ng = 9.81  # acceleration due to gravity (m/s^2)\nl = 1.0  # length of the pendulum (m)\ngamma = 0.1  # coefficient of rotational friction\nJ = 1/3 * m * l**2  # moment of inertia of a rod about its center of mass\nJ_t = J + m * l**2  # total moment of inertia\n\n# Define the ODE for the inverted pendulum\ndef pendulum_ode(state, t):\n    theta, omega = state\n    dtheta = omega\n    domega = (m*g*l/J_t) * np.sin(theta) - (gamma/J_t) * omega\n    return [dtheta, domega]\n\n# Initial conditions: slightly off vertical position\ntheta0 = 0.1  # initial angle (radians)\nomega0 = 0  # initial angular velocity (rad/s)\ny0 = [theta0, omega0]\n\n# Time array for integration\nt = np.linspace(0, 10, 500)  # Reduced number of points\n\n# Solve ODE\nsolution = odeint(pendulum_ode, y0, t)\n\n# Extract theta and omega from the solution\ntheta = solution[:, 0]\nomega = solution[:, 1]\n\n# Create two separate plots\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))\n\n# Plot for angle\nax1.plot(t, theta)\nax1.set_xlabel('Time (s)')\nax1.set_ylabel('Angle (rad)')\nax1.set_title('Pendulum Angle over Time')\nax1.grid(True)\n\n# Plot for angular velocity\nax2.plot(t, omega)\nax2.set_xlabel('Time (s)')\nax2.set_ylabel('Angular velocity (rad/s)')\nax2.set_title('Pendulum Angular Velocity over Time')\nax2.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n# Function to create animation frames\ndef get_pendulum_position(theta):\n    x = l * np.sin(theta)\n    y = l * np.cos(theta)\n    return x, y\n\n# Create animation\nfig, ax = plt.subplots(figsize=(8, 8))\nline, = ax.plot([], [], 'o-', lw=2)\ntime_text = ax.text(0.02, 0.95, '', transform=ax.transAxes)\n\ndef init():\n    ax.set_xlim(-1.2*l, 1.2*l)\n    ax.set_ylim(-1.2*l, 1.2*l)  # Adjusted to show full range of motion\n    ax.set_aspect('equal', adjustable='box')\n    return line, time_text\n\ndef animate(i):\n    x, y = get_pendulum_position(theta[i])\n    line.set_data([0, x], [0, y])\n    time_text.set_text(f'Time: {t[i]:.2f} s')\n    return line, time_text\n\nanim = FuncAnimation(fig, animate, init_func=init, frames=len(t), interval=40, blit=True)\nplt.title('Inverted Pendulum Animation')\nax.grid(True)\n\n# Convert animation to JavaScript\njs_anim = anim.to_jshtml()\n\n# Close the figure to prevent it from being displayed\nplt.close(fig)\n\n# Display only the JavaScript animation\ndisplay(HTML(js_anim))\n\n","type":"content","url":"/appendix-examples#inverted-pendulum","position":3},{"hierarchy":{"lvl1":"Example COCPs","lvl2":"Pendulum in the Gym Environment"},"type":"lvl2","url":"/appendix-examples#pendulum-in-the-gym-environment","position":4},{"hierarchy":{"lvl1":"Example COCPs","lvl2":"Pendulum in the Gym Environment"},"content":" Gym is a widely used abstraction layer for defining discrete-time reinforcement learning problems. In reinforcement learning research, there's often a desire to develop general-purpose algorithms that are problem-agnostic. This research mindset leads us to voluntarily avoid considering the implementation details of a given environment. While this approach is understandable from a research perspective, it may not be optimal from a pragmatic, solution-driven standpoint where we care about solving specific problems efficiently. If we genuinely wanted to solve this problem without prior knowledge, why not look under the hood and embrace its nature as a trajectory optimization problem? \n\nLet’s examine the code and reverse-engineer the original continuous-time problem hidden behind the abstraction layer. Although the pendulum problem may have limited practical relevance as a real-world application, it serves as an excellent example for our analysis. In the current version of \n\nPendulum, we find that the Gym implementation uses a simplified model. Like our implementation, it assumes a fixed base and doesn’t model cart movement. The state is also represented by the pendulum angle and angular velocity.\nHowever, the equations of motion implemented in the Gym environment are different and correspond to the following ODE:\\begin{align*}\n\\dot{\\theta} &= \\theta_{dot} \\\\\n\\dot{\\theta}_{dot} &= \\frac{3g}{2l} \\sin(\\theta) + \\frac{3}{ml^2} u\n\\end{align*}\n\nCompared to our simplified model, the Gym implementation makes the following additional assumptions:\n\nIt omits the term \\frac{\\gamma}{J_t} \\dot{\\theta}(t), which represents damping or air resistance. This means that it assumes an idealized pendulum that doesn’t naturally slow down over time.\n\nIt uses ml^2 instead of J_t = J + ml^2, which assumes that all mass is concentrated at the pendulum’s end (like a point mass on a massless rod), rather than accounting for mass distribution along the pendulum.\n\nThe control input u is applied directly, without a \\cos \\theta(t) term, which means that the applied torque has the same effect regardless of the pendulum’s position, rather than varying with angle. For example, imagine trying to push a door open. When the door is almost closed (pendulum near vertical), a small push perpendicular to the door (analogous to our control input) can easily start it moving. However, when the door is already wide open (pendulum horizontal), the same push has little effect on the door’s angle. In a more detailed model, this would be captured by the \\cos \\theta(t) term, which is maximum when the pendulum is vertical (\\cos 0° = 1) and zero when horizontal (\\cos 90° = 0).\n\nThe goal remains to stabilize the rod upright, but the way in which this encoded is through the following instantenous cost function:\\begin{align*}\nc(\\theta, \\dot{\\theta}, u) &= (\\text{normalize}(\\theta))^2 + 0.1\\dot{\\theta}^2 + 0.001u^2\\\\\n\\text{normalize}(\\theta) &= ((\\theta + \\pi) \\bmod 2\\pi) - \\pi\n\\end{align*}\n\nThis cost function penalizes deviations from the upright position (first term), discouraging rapid motion (second term), and limiting control effort (third term). The relative weights has been manually chosen to balance the primary goal of upright stabilization with the secondary aims of smooth motion and energy efficiency. The normalization ensures that the angle is always in the range [-\\pi, \\pi] so that the pendulum positions (e.g., 0 and 2\\pi) are treated identically, which could otherwise confuse learning algorithms.\n\nStudying the code further, we find that it imposes bound constraints on both the control input and the angular velocity through clipping operations:\\begin{align*}\nu &= \\max(\\min(u, u_{max}), -u_{max}) \\\\\n\\dot{\\theta} &= \\max(\\min(\\dot{\\theta}, \\dot{\\theta}_{max}), -\\dot{\\theta}_{max})\n\\end{align*}\n\nWhere u_{max} = 2.0 and \\dot{\\theta}_{max} = 8.0.  Finally, when inspecting the \n\nstep function, we find that the dynamics are discretized using forward Euler under a fixed step size of h=0.0.5. Overall, the discrete-time trajectory optimization problem implemented in Gym is the following:\\begin{align*}\n\\min_{u_k} \\quad & J = \\sum_{k=0}^{N-1} c(\\theta_k, \\dot{\\theta}_k, u_k) \\\\\n\\text{subject to:} \\quad & \\theta_{k+1} = \\theta_k + \\dot{\\theta}_k \\cdot h \\\\\n& \\dot{\\theta}_{k+1} = \\dot{\\theta}_k + \\left(\\frac{3g}{2l}\\sin(\\theta_k) + \\frac{3}{ml^2}u_k\\right) \\cdot h \\\\\n& -u_{\\max} \\leq u_k \\leq u_{\\max} \\\\\n& -\\dot{\\theta}_{\\max} \\leq \\dot{\\theta}_k \\leq \\dot{\\theta}_{\\max}, \\quad k = 0, 1, ..., N-1 \\\\\n\\text{given:} \\quad      & \\theta_0 = \\theta_{\\text{initial}}, \\quad \\dot{\\theta}_0 = \\dot{\\theta}_{\\text{initial}}, \\quad N = 200\n\\end{align*}\n\nwith g = 10.0, l = 1.0, m = 1.0, u_{max} = 2.0, and \\dot{\\theta}_{max} = 8.0. This discrete-time problem corresponds to the following continuous-time optimal control problem:\\begin{align*}\n\\min_{u(t)} \\quad & J = \\int_{0}^{T} c(\\theta(t), \\dot{\\theta}(t), u(t)) dt \\\\\n\\text{subject to:} \\quad & \\dot{\\theta}(t) = \\dot{\\theta}(t) \\\\\n& \\ddot{\\theta}(t) = \\frac{3g}{2l}\\sin(\\theta(t)) + \\frac{3}{ml^2}u(t) \\\\\n& -u_{\\max} \\leq u(t) \\leq u_{\\max} \\\\\n& -\\dot{\\theta}_{\\max} \\leq \\dot{\\theta}(t) \\leq \\dot{\\theta}_{\\max} \\\\\n\\text{given:} \\quad      & \\theta(0) = \\theta_0, \\quad \\dot{\\theta}(0) = \\dot{\\theta}_0, \\quad T = 10 \\text{ seconds}\n\\end{align*}","type":"content","url":"/appendix-examples#pendulum-in-the-gym-environment","position":5},{"hierarchy":{"lvl1":"Example COCPs","lvl2":"Heat Exchanger"},"type":"lvl2","url":"/appendix-examples#heat-exchanger","position":6},{"hierarchy":{"lvl1":"Example COCPs","lvl2":"Heat Exchanger"},"content":"\n\nWe are considering a system where fluid flows through a tube, and the goal is to control the temperature of the fluid by adjusting the temperature of the tube’s wall over time. The wall temperature, denoted as  T_w(t) , can be changed as a function of time, but it remains the same along the length of the tube. On the other hand, the temperature of the fluid inside the tube,  T(z, t) , depends both on its position along the tube  z  and on time  t . It evolves according to the following partial differential equation:\\frac{\\partial T}{\\partial t} = -v \\frac{\\partial T}{\\partial z} + \\frac{h}{\\rho C_p} (T_w(t) - T)\n\nwhere we have:\n\n v : the average speed of the fluid moving through the tube,\n\n h : how easily heat transfers from the wall to the fluid,\n\n \\rho  and  C_p : the fluid’s density and heat capacity.\n\nThis equation describes how the fluid’s temperature changes as it moves along the tube and interacts with the tube’s wall temperature. The fluid enters the tube with an initial temperature  T_0  at the inlet (where  z = 0 ). Our objective is to adjust the wall temperature  T_w(t)  so that by a specific final time  t_f , the fluid’s temperature reaches a desired distribution  T_s(z)  along the length of the tube. The relationship for  T_s(z)  under steady-state conditions (ie. when changes over time are no longer considered), is given by:\\frac{d T_s}{d z} = \\frac{h}{v \\rho C_p}[\\theta - T_s]\n\nwhere  \\theta  is a constant temperature we want to maintain at the wall. The objective is to control the wall temperature  T_w(t)  so that by the end of the time interval  t_f , the fluid temperature  T(z, t_f)  is as close as possible to the desired distribution  T_s(z) . This can be formalized by minimizing the following quantity:I = \\int_0^L \\left[T(z, t_f) - T_s(z)\\right]^2 dz\n\nwhere  L  is the length of the tube. Additionally, we require that the wall temperature cannot exceed a maximum allowable value  T_{\\max} :T_w(t) \\leq T_{\\max}","type":"content","url":"/appendix-examples#heat-exchanger","position":7},{"hierarchy":{"lvl1":"Example COCPs","lvl2":"Nuclear Reactor"},"type":"lvl2","url":"/appendix-examples#nuclear-reactor","position":8},{"hierarchy":{"lvl1":"Example COCPs","lvl2":"Nuclear Reactor"},"content":"\n\nIn a nuclear reactor, neutrons interact with fissile nuclei, causing nuclear fission. This process produces more neutrons and smaller fissile nuclei called precursors. The precursors subsequently absorb more neutrons, generating “delayed” neutrons. The kinetic energy of these products is converted into thermal energy through collisions with neighboring atoms. The reactor’s power output is determined by the concentration of neutrons available for nuclear fission.\n\nThe reaction kinetics can be modeled using a system of ordinary differential equations:\\begin{align*}\n\\dot{x}(t) &= \\frac{r(t)x(t) - \\alpha x^2(t) - \\beta x(t)}{\\tau} + \\mu y(t), & x(0) &= x_0 \\\\\n\\dot{y}(t) &= \\frac{\\beta x(t)}{\\tau} - \\mu y(t), & y(0) &= y_0\n\\end{align*}\n\nwhere:\n\nx(t): concentration of neutrons at time t\n\ny(t): concentration of precursors at time t\n\nt: time\n\nr(t) = r[u(t)]: degree of change in neutron multiplication at time t as a function of control rod displacement u(t)\n\n\\alpha: reactivity coefficient\n\n\\beta: fraction of delayed neutrons\n\n\\mu: decay constant for precursors\n\n\\tau: average time taken by a neutron to produce a neutron or precursor\n\nThe power output can be adjusted based on demand by inserting or retracting a neutron-absorbing control rod. Inserting the control rod absorbs neutrons, reducing the heat flux and power output, while retracting the rod has the opposite effect.\n\nThe objective is to change the neutron concentration x(t) from an initial value x_0 to a stable value x_\\mathrm{f} at time t_\\mathrm{f} while minimizing the displacement of the control rod. This can be formulated as an optimal control problem, where the goal is to find the control function u(t) that minimizes the objective functional:I = \\int_0^{t_\\mathrm{f}} u^2(t) \\, \\mathrm{d}t\n\nsubject to the final conditions:\\begin{align*}\nx(t_\\mathrm{f}) &= x_\\mathrm{f} \\\\\n\\dot{x}(t_\\mathrm{f}) &= 0\n\\end{align*}\n\nand the constraint |u(t)| \\leq u_\\mathrm{max}","type":"content","url":"/appendix-examples#nuclear-reactor","position":9},{"hierarchy":{"lvl1":"Example COCPs","lvl2":"Chemotherapy"},"type":"lvl2","url":"/appendix-examples#chemotherapy","position":10},{"hierarchy":{"lvl1":"Example COCPs","lvl2":"Chemotherapy"},"content":"Chemotherapy uses drugs to kill cancer cells. However, these drugs can also have toxic effects on healthy cells in the body. To optimize the effectiveness of chemotherapy while minimizing its side effects, we can formulate an optimal control problem.\n\nThe drug concentration y_1(t) and the number of immune cells y_2(t), healthy cells y_3(t), and cancer cells y_4(t) in an organ at any time t during chemotherapy can be modeled using a system of ordinary differential equations:\\begin{align*}\n\\dot{y}_1(t) &= u(t) - \\gamma_6 y_1(t) \\\\\n\\dot{y}_2(t) &= \\dot{y}_{2,\\text{in}} + r_2 \\frac{y_2(t) y_4(t)}{\\beta_2 + y_4(t)} - \\gamma_3 y_2(t) y_4(t) - \\gamma_4 y_2(t) - \\alpha_2 y_2(t) \\left(1 - e^{-y_1(t) \\lambda_2}\\right) \\\\\n\\dot{y}_3(t) &= r_3 y_3(t) \\left(1 - \\beta_3 y_3(t)\\right) - \\gamma_5 y_3(t) y_4(t) - \\alpha_3 y_3(t) \\left(1 - e^{-y_1(t) \\lambda_3}\\right) \\\\\n\\dot{y}_4(t) &= r_1 y_4(t) \\left(1 - \\beta_1 y_4(t)\\right) - \\gamma_1 y_3(t) y_4(t) - \\gamma_2 y_2(t) y_4(t) - \\alpha_1 y_4(t) \\left(1 - e^{-y_1(t) \\lambda_1}\\right)\n\\end{align*}\n\nwhere:\n\ny_1(t): drug concentration in the organ at time t\n\ny_2(t): number of immune cells in the organ at time t\n\ny_3(t): number of healthy cells in the organ at time t\n\ny_4(t): number of cancer cells in the organ at time t\n\n\\dot{y}_{2,\\text{in}}: constant rate of immune cells entering the organ to fight cancer cells\n\nu(t): rate of drug injection into the organ at time t\n\nr_i, \\beta_i: constants in the growth terms\n\n\\alpha_i, \\lambda_i: constants in the decay terms due to the action of the drug\n\n\\gamma_i: constants in the remaining decay terms\n\nThe objective is to minimize the number of cancer cells y_4(t) in a specified time t_\\mathrm{f} while using the minimum amount of drug to reduce its toxic effects. This can be formulated as an optimal control problem, where the goal is to find the control function u(t) that minimizes the objective functional:I = y_4(t_\\mathrm{f}) + \\int_0^{t_\\mathrm{f}} u(t) \\, \\mathrm{d}t\n\nsubject to the system dynamics, initial conditions, and the constraint u(t) \\geq 0.\n\nAdditional constraints may include:\n\nMaintaining a minimum number of healthy cells during treatment:y_3(t) \\geq y_{3,\\min}\n\nImposing an upper limit on the drug dosage:u(t) \\leq u_{\\max}","type":"content","url":"/appendix-examples#chemotherapy","position":11},{"hierarchy":{"lvl1":"Example COCPs","lvl2":"Government Corruption"},"type":"lvl2","url":"/appendix-examples#government-corruption","position":12},{"hierarchy":{"lvl1":"Example COCPs","lvl2":"Government Corruption"},"content":"In this model from Feichtinger and Wirl (1994), we aim to understand the incentives for politicians to engage in corrupt activities or to combat corruption. The model considers a politician’s popularity as a dynamic process that is influenced by the public’s memory of recent and past corruption. The objective is to find conditions under which self-interested politicians would choose to be honest or dishonest.\n\nThe model introduces the following notation:\n\nC(t): accumulated awareness (knowledge) of past corruption at time t\n\nu(t): extent of corruption (politician’s control variable) at time t\n\n\\delta: rate of forgetting past corruption\n\nP(t): politician’s popularity at time t\n\ng(P): growth function of popularity; g''(P) < 0\n\nf(C): function measuring the loss of popularity caused by C; f'(C) > 0, f''(C) \\geq 0\n\nU_1(P): benefits associated with being popular; U_1'(P) > 0, U_1''(P) \\leq 0\n\nU_2(u): benefits resulting from bribery and fraud; U_2'(u) > 0, U_2''(u) < 0\n\nr: discount rate\n\nThe dynamics of the public’s memory of recent and past corruption C(t) are modeled as:\\begin{align*}\n\\dot{C}(t) &= u(t) - \\delta C(t), \\quad C(0) = C_0\n\\end{align*}\n\nThe evolution of the politician’s popularity P(t) is governed by:\\begin{align*}\n\\dot{P}(t) &= g(P(t)) - f(C(t)), \\quad P(0) = P_0\n\\end{align*}\n\nThe politician’s objective is to maximize the following objective:\\int_0^{\\infty} e^{-rt} [U_1(P(t)) + U_2(u(t))] \\, \\mathrm{d}t\n\nsubject to the dynamics of corruption awareness and popularity.\n\nThe optimal control problem can be formulated as follows:\\begin{align*}\n\\max_{u(\\cdot)} \\quad & \\int_0^{\\infty} e^{-rt} [U_1(P(t)) + U_2(u(t))] \\, \\mathrm{d}t \\\\\n\\text{s.t.} \\quad & \\dot{C}(t) = u(t) - \\delta C(t), \\quad C(0) = C_0 \\\\\n& \\dot{P}(t) = g(P(t)) - f(C(t)), \\quad P(0) = P_0\n\\end{align*}\n\nThe state variables are the accumulated awareness of past corruption C(t) and the politician’s popularity P(t). The control variable is the extent of corruption u(t). The objective functional represents the discounted stream of benefits coming from being honest (popularity) and from being dishonest (corruption).","type":"content","url":"/appendix-examples#government-corruption","position":13},{"hierarchy":{"lvl1":"Solving Initial Value Problems"},"type":"lvl1","url":"/appendix-ivps","position":0},{"hierarchy":{"lvl1":"Solving Initial Value Problems"},"content":"An ODE is an implicit representation of a state-space trajectory: it tells us how the state changes in time but not precisely what the state is at any given time. To find out this information, we need to either solve the ODE analytically (for some special structure) or, as we’re going to do, solve them numerically. This numerical procedure is meant to solve what is called an IVP (initial value problem) of the form:\\text{Find } x(t) \\text{ given } \\dot{x}(t) = f(x(t), t) \\text{ and } x(t_0) = x_0","type":"content","url":"/appendix-ivps","position":1},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Euler’s Method"},"type":"lvl2","url":"/appendix-ivps#eulers-method","position":2},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Euler’s Method"},"content":"The algorithm to solve this problem is, in its simplest form, a for loop which closely resembles the updates encountered in gradient descent (in fact, gradient descent can be derived from the gradient flow ODE, but that’s another discussion). The so-called explicit Euler’s method can be implemented as follow:\n\nEuler’s method\n\nInput: f(x, t), x_0, t_0, t_{end}, h\n\nOutput: Approximate solution x(t) at discrete time points\n\nInitialize t = t_0, x = x_0\n\nWhile t < t_{end}:\n\nCompute x_{new} = x + h f(x, t)\n\nUpdate t = t + h\n\nUpdate x = x_{new}\n\nStore or output the pair (t, x)\n\nEnd While\n\nConsider the following simple dynamical system of a ballistic motion model, neglecting air resistance. The state of the system is described by two variables: y(t): vertical position at time t and v(t), the vertical velocity at time t. The corresponding ODE is:\\begin{aligned}\n\\frac{dy}{dt} &= v \\\\\n\\frac{dv}{dt} &= -g\n\\end{aligned}\n\nwhere g \\approx 9.81 \\text{ m/s}^2 is the acceleration due to gravity. In our code, we use the initial conditions\ny(0) = 0 \\text{ m} and v(0) = v_0 \\text{ m/s} where v_0 is the initial velocity (in this case, v_0 = 20 \\text{ m/s}).\nThe analytical solution to this system is:\\begin{aligned}\ny(t) &= v_0t - \\frac{1}{2}gt^2 \\\\\nv(t) &= v_0 - gt\n\\end{aligned}\n\nThis system models the vertical motion of an object launched upward, reaching a maximum height before falling back down due to gravity.\n\nEuler’s method can be obtained by taking the first-order Taylor expansion of x(t) at t:x(t + h) \\approx x(t) + h \\frac{dx}{dt}(t) = x(t) + h f(x(t), t)\n\nEach step of the algorithm therefore involves approximating the function with a linear function of slope f over the given interval h.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef f(y, t):\n    \"\"\"\n    Derivative function for vertical motion under gravity.\n    y[0] is position, y[1] is velocity.\n    \"\"\"\n    g = 9.81  # acceleration due to gravity (m/s^2)\n    return np.array([y[1], -g])\n\ndef euler_method(f, y0, t0, t_end, h):\n    \"\"\"\n    Implement Euler's method for the entire time range.\n    \"\"\"\n    t = np.arange(t0, t_end + h, h)\n    y = np.zeros((len(t), 2))\n    y[0] = y0\n    for i in range(1, len(t)):\n        y[i] = y[i-1] + h * f(y[i-1], t[i-1])\n    return t, y\n\ndef true_solution(t):\n    \"\"\"\n    Analytical solution for the ballistic trajectory.\n    \"\"\"\n    y0, v0 = 0, 20  # initial height and velocity\n    g = 9.81\n    return y0 + v0*t - 0.5*g*t**2, v0 - g*t\n\n# Set up the problem\nt0, t_end = 0, 4\ny0 = np.array([0, 20])  # initial height = 0, initial velocity = 20 m/s\n\n# Different step sizes\nstep_sizes = [1.0, 0.5, 0.1]\ncolors = ['r', 'g', 'b']\nmarkers = ['o', 's', '^']\n\n# True solution\nt_fine = np.linspace(t0, t_end, 1000)\ny_true, v_true = true_solution(t_fine)\n\n# Plotting\nplt.figure(figsize=(12, 8))\n\n# Plot Euler approximations\nfor h, color, marker in zip(step_sizes, colors, markers):\n    t, y = euler_method(f, y0, t0, t_end, h)\n    plt.plot(t, y[:, 0], color=color, marker=marker, linestyle='--', \n             label=f'Euler h = {h}', markersize=6, markerfacecolor='none')\n\n# Plot true solution last so it's on top\nplt.plot(t_fine, y_true, 'k-', label='True trajectory', linewidth=2, zorder=10)\n\nplt.xlabel('Time (s)', fontsize=12)\nplt.ylabel('Height (m)', fontsize=12)\nplt.title(\"Euler's Method: Effect of Step Size on Ballistic Trajectory Approximation\", fontsize=14)\nplt.legend(fontsize=10)\nplt.grid(True, linestyle=':', alpha=0.7)\n\n# Add text to explain the effect of step size\nplt.text(2.5, 15, \"Smaller step sizes\\nyield better approximations\", \n         bbox=dict(facecolor='white', edgecolor='black', alpha=0.7),\n         fontsize=10, ha='center', va='center')\n\nplt.tight_layout()\nplt.show()\n\nAnother way to understand Euler’s method is through the fundamental theorem of calculus:x(t + h) = x(t) + \\int_t^{t+h} f(x(\\tau), \\tau) d\\tau\n\nWe then approximate the integral term with a box of width h and height f, and therefore of area h f.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\n\ndef v(t):\n    \"\"\"\n    Velocity function for the ballistic trajectory.\n    \"\"\"\n    v0 = 20   # initial velocity (m/s)\n    g = 9.81  # acceleration due to gravity (m/s^2)\n    return v0 - g * t\n\ndef position(t):\n    \"\"\"\n    Position function (integral of velocity).\n    \"\"\"\n    v0 = 20\n    g = 9.81\n    return v0*t - 0.5*g*t**2\n\n# Set up the problem\nt0, t_end = 0, 2\nnum_points = 1000\nt = np.linspace(t0, t_end, num_points)\n\n# Calculate true velocity and position\nv_true = v(t)\nx_true = position(t)\n\n# Euler's method with a large step size for visualization\nh = 0.5\nt_euler = np.arange(t0, t_end + h, h)\nx_euler = np.zeros_like(t_euler)\n\nfor i in range(1, len(t_euler)):\n    x_euler[i] = x_euler[i-1] + h * v(t_euler[i-1])\n\n# Plotting\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12), sharex=True)\n\n# Plot velocity function and its approximation\nax1.plot(t, v_true, 'b-', label='True velocity')\nax1.fill_between(t, 0, v_true, alpha=0.3, label='True area (displacement)')\n\n# Add rectangles with hashed pattern, ruler-like annotations, and area values\nfor i in range(len(t_euler) - 1):\n    t_i = t_euler[i]\n    v_i = v(t_i)\n    rect = Rectangle((t_i, 0), h, v_i, \n                     fill=True, facecolor='red', edgecolor='r', \n                     alpha=0.15, hatch='///')\n    ax1.add_patch(rect)\n    \n    # Add ruler-like annotations\n    # Vertical ruler (height)\n    ax1.annotate('', xy=(t_i, 0), xytext=(t_i, v_i),\n                 arrowprops=dict(arrowstyle='<->', color='red'))\n    ax1.text(t_i - 0.05, v_i/2, f'v(t{i}) = {v_i:.2f}', rotation=90, \n             va='center', ha='right', color='red', fontweight='bold')\n    \n    # Horizontal ruler (width)\n    ax1.annotate('', xy=(t_i, -1), xytext=(t_i + h, -1),\n                 arrowprops=dict(arrowstyle='<->', color='red'))\n    ax1.text(t_i + h/2, -2, f'h = {h}', ha='center', va='top', \n             color='red', fontweight='bold')\n    \n    # Add area value in the middle of each rectangle\n    area = h * v_i\n    ax1.text(t_i + h/2, v_i/2, f'Area = {area:.2f}', ha='center', va='center', \n             color='white', fontweight='bold', bbox=dict(facecolor='red', edgecolor='none', alpha=0.7))\n\n# Plot only the points for Euler's method\nax1.plot(t_euler, v(t_euler), 'ro', markersize=6, label=\"Euler's points\")\nax1.set_ylabel('Velocity (m/s)', fontsize=12)\nax1.set_title(\"Velocity Function and Euler's Approximation\", fontsize=14)\nax1.legend(fontsize=10)\nax1.grid(True, linestyle=':', alpha=0.7)\nax1.set_ylim(bottom=-3)  # Extend y-axis to show horizontal rulers\n\n# Plot position function and its approximation\nax2.plot(t, x_true, 'b-', label='True position')\nax2.plot(t_euler, x_euler, 'ro--', label=\"Euler's approximation\", markersize=6, linewidth=2)\n\n# Add vertical arrows and horizontal lines to show displacement and time step\nfor i in range(1, len(t_euler)):\n    t_i = t_euler[i]\n    x_prev = x_euler[i-1]\n    x_curr = x_euler[i]\n    \n    # Vertical line for displacement\n    ax2.plot([t_i, t_i], [x_prev, x_curr], 'g:', linewidth=2)\n    \n    # Horizontal line for time step\n    ax2.plot([t_i - h, t_i], [x_prev, x_prev], 'g:', linewidth=2)\n    \n    # Add text to show the displacement value\n    displacement = x_curr - x_prev\n    ax2.text(t_i + 0.05, (x_prev + x_curr)/2, f'+{displacement:.2f}', \n             color='green', fontweight='bold', va='center')\n    \n    # Add text to show the time step\n    ax2.text(t_i - h/2, x_prev - 0.5, f'h = {h}', \n             color='green', fontweight='bold', ha='center', va='top')\n\nax2.set_xlabel('Time (s)', fontsize=12)\nax2.set_ylabel('Position (m)', fontsize=12)\nax2.set_title(\"Position: True vs Euler's Approximation\", fontsize=14)\nax2.legend(fontsize=10)\nax2.grid(True, linestyle=':', alpha=0.7)\n\n# Add explanatory text\nax1.text(1.845, 15, \"Red hashed areas show\\nEuler's approximation\\nof the area under the curve\", \n         bbox=dict(facecolor='white', edgecolor='black', alpha=0.7),\n         fontsize=10, ha='center', va='center')\n\nplt.tight_layout()\nplt.show()\n\n","type":"content","url":"/appendix-ivps#eulers-method","position":3},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Implicit Euler’s Method"},"type":"lvl2","url":"/appendix-ivps#implicit-eulers-method","position":4},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Implicit Euler’s Method"},"content":"An alternative approach is the Implicit Euler method, also known as the Backward Euler method. Instead of using the derivative at the current point to step forward, it uses the derivative at the end of the interval. This leads to the following update rule:x_{new} = x + h f(x_{new}, t_{new})\n\nNote that x_{new} appears on both sides of the equation, making this an implicit method. The algorithm for the Implicit Euler method can be described as follows:\n\nImplicit Euler’s Method\n\nInput: f(x, t), x_0, t_0, t_{end}, h\n\nOutput: Approximate solution x(t) at discrete time points\n\nInitialize t = t_0, x = x_0\n\nWhile t < t_{end}:\n\nSet t_{new} = t + h\n\nSolve for x_{new} in the equation: x_{new} = x + h f(x_{new}, t_{new})\n\nUpdate t = t_{new}\n\nUpdate x = x_{new}\n\nStore or output the pair (t, x)\n\nEnd While\n\nThe main difference in the Implicit Euler method is step 4, where we need to solve a (potentially nonlinear) equation to find x_{new}. This is typically done using iterative methods such as fixed-point iteration or Newton’s method.","type":"content","url":"/appendix-ivps#implicit-eulers-method","position":5},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl3":"Stiff ODEs","lvl2":"Implicit Euler’s Method"},"type":"lvl3","url":"/appendix-ivps#stiff-odes","position":6},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl3":"Stiff ODEs","lvl2":"Implicit Euler’s Method"},"content":"While the Implicit Euler method requires more computation per step, it often allows for larger step sizes and can provide better stability for certain types of problems, especially stiff ODEs.\n\nStiff ODEs are differential equations for which certain numerical methods for solving the equation are numerically unstable, unless the step size is taken to be extremely small. These ODEs typically involve multiple processes occurring at widely different rates. In a stiff problem, the fastest-changing component of the solution can make the numerical method unstable unless the step size is extremely small. However, such a small step size may lead to an impractical amount of computation to traverse the entire interval of interest.\n\nFor example, consider a chemical reaction where some reactions occur very quickly while others occur much more slowly. The fast reactions quickly approach their equilibrium, but small perturbations in the slower reactions can cause rapid changes in the fast reactions.\n\nA classic example of a stiff ODE is the Van der Pol oscillator with a large parameter. The Van der Pol equation is:\\frac{d^2x}{dt^2} - \\mu(1-x^2)\\frac{dx}{dt} + x = 0\n\nwhere \\mu is a scalar parameter. This second-order ODE can be transformed into a system of first-order ODEs by introducing a new variable y = \\frac{dx}{dt}:\\begin{aligned}\n\\frac{dx}{dt} &= y \\\\\n\\frac{dy}{dt} &= \\mu(1-x^2)y - x\n\\end{aligned}\n\nWhen \\mu is large (e.g., \\mu = 1000), this system becomes stiff. The large \\mu causes rapid changes in y when x is near ±1, but slower changes elsewhere. This leads to a solution with sharp transitions followed by periods of gradual change.","type":"content","url":"/appendix-ivps#stiff-odes","position":7},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Trapezoid Method"},"type":"lvl2","url":"/appendix-ivps#trapezoid-method","position":8},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Trapezoid Method"},"content":"The trapezoid method, also known as the trapezoidal rule, offers improved accuracy and stability compared to the simple Euler method. The name “trapezoid method” comes from the idea of using a trapezoid to approximate the integral term in the fundamental theorem of calculus. This leads to the following update rule:x_{new} = x + \\frac{h}{2}[f(x, t) + f(x_{new}, t_{new})]\n\nwhere  t_{new} = t + h . Note that this formula involves  x_{new}  on both sides of the equation, making it an implicit method, similar to the implicit Euler method discussed earlier.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle, Polygon\n\ndef v(t):\n    \"\"\"Velocity function for the ballistic trajectory.\"\"\"\n    v0 = 20   # initial velocity (m/s)\n    g = 9.81  # acceleration due to gravity (m/s^2)\n    return v0 - g * t\n\ndef position(t):\n    \"\"\"Position function (integral of velocity).\"\"\"\n    v0 = 20\n    g = 9.81\n    return v0*t - 0.5*g*t**2\n\n# Set up the problem\nt0, t_end = 0, 2\nnum_points = 1000\nt = np.linspace(t0, t_end, num_points)\n\n# Calculate true velocity and position\nv_true = v(t)\nx_true = position(t)\n\n# Euler's method and Trapezoid method with a large step size for visualization\nh = 0.5\nt_numeric = np.arange(t0, t_end + h, h)\nx_euler = np.zeros_like(t_numeric)\nx_trapezoid = np.zeros_like(t_numeric)\n\nfor i in range(1, len(t_numeric)):\n    # Euler's method\n    x_euler[i] = x_euler[i-1] + h * v(t_numeric[i-1])\n    \n    # Trapezoid method (implicit, so we use a simple fixed-point iteration)\n    x_trapezoid[i] = x_trapezoid[i-1]\n    for _ in range(5):  # 5 iterations should be enough for this simple problem\n        x_trapezoid[i] = x_trapezoid[i-1] + h/2 * (v(t_numeric[i-1]) + v(t_numeric[i]))\n\n# Plotting\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 16), sharex=True)\n\n# Plot velocity function and its approximations\nax1.plot(t, v_true, 'b-', label='True velocity')\nax1.fill_between(t, 0, v_true, alpha=0.3, label='True area (displacement)')\n\n# Add trapezoids and rectangles\nfor i in range(len(t_numeric) - 1):\n    t_i, t_next = t_numeric[i], t_numeric[i+1]\n    v_i, v_next = v(t_i), v(t_next)\n    \n    # Euler's rectangle (hashed pattern)\n    rect = Rectangle((t_i, 0), h, v_i, fill=True, facecolor='red', edgecolor='r', alpha=0.15, hatch='///')\n    ax1.add_patch(rect)\n    \n    # Trapezoid (dot pattern)\n    trapezoid = Polygon([(t_i, 0), (t_i, v_i), (t_next, v_next), (t_next, 0)], \n                        fill=True, facecolor='green', edgecolor='g', alpha=0.15, hatch='....')\n    ax1.add_patch(trapezoid)\n    \n    # Add area values\n    euler_area = h * v_i\n    trapezoid_area = h * (v_i + v_next) / 2\n    ax1.text(t_i + h/2, v_i/2, f'Euler: {euler_area:.2f}', ha='center', va='bottom', color='red', fontweight='bold')\n    ax1.text(t_i + h/2, (v_i + v_next)/4, f'Trapezoid: {trapezoid_area:.2f}', ha='center', va='top', color='green', fontweight='bold')\n\n# Plot points for Euler's and Trapezoid methods\nax1.plot(t_numeric, v(t_numeric), 'ro', markersize=6, label=\"Euler's points\")\nax1.plot(t_numeric, v(t_numeric), 'go', markersize=6, label=\"Trapezoid points\")\n\nax1.set_ylabel('Velocity (m/s)', fontsize=12)\nax1.set_title(\"Velocity Function: True vs Numerical Approximations\", fontsize=14)\nax1.legend(fontsize=10)\nax1.grid(True, linestyle=':', alpha=0.7)\n\n# Plot position function and its approximations\nax2.plot(t, x_true, 'b-', label='True position')\nax2.plot(t_numeric, x_euler, 'ro--', label=\"Euler's approximation\", markersize=6, linewidth=2)\nax2.plot(t_numeric, x_trapezoid, 'go--', label=\"Trapezoid approximation\", markersize=6, linewidth=2)\n\nax2.set_xlabel('Time (s)', fontsize=12)\nax2.set_ylabel('Position (m)', fontsize=12)\nax2.set_title(\"Position: True vs Numerical Approximations\", fontsize=14)\nax2.legend(fontsize=10)\nax2.grid(True, linestyle=':', alpha=0.7)\n\n# Add explanatory text\nax1.text(1.76, 17, \"Red hashed areas: Euler's approximation\\nGreen dotted areas: Trapezoid approximation\", \n         bbox=dict(facecolor='white', edgecolor='black', alpha=0.7),\n         fontsize=10, ha='center', va='center')\n\nplt.tight_layout()\nplt.show()\n\nAlgorithmically, the trapezoid method can be described as follows:\n\nTrapezoid Method\n\nInput:  f(x, t) ,  x_0 ,  t_0 ,  t_{end} ,  h \n\nOutput: Approximate solution  x(t)  at discrete time points\n\nInitialize  t = t_0 ,  x = x_0 \n\nWhile  t < t_{end} :\n\nSet  t_{new} = t + h \n\nSolve for  x_{new} in the equation:  x_{new} = x + \\frac{h}{2}[f(x, t) + f(x_{new}, t_{new})] \n\nUpdate  t = t_{new} \n\nUpdate  x = x_{new} \n\nStore or output the pair  (t, x) \n\nThe trapezoid method can also be derived by averaging the forward Euler and backward Euler methods. Recall that:\n\nForward Euler method:x_{n+1} = x_n + h f(x_n, t_n)\n\nBackward Euler method:x_{n+1} = x_n + h f(x_{n+1}, t_{n+1})\n\nTaking the average of these two methods yields:\\begin{aligned}\nx_{n+1} &= \\frac{1}{2} \\left( x_n + h f(x_n, t_n) \\right) + \\frac{1}{2} \\left( x_n + h f(x_{n+1}, t_{n+1}) \\right) \\\\\n&= x_n + \\frac{h}{2} \\left( f(x_n, t_n) + f(x_{n+1}, t_{n+1}) \\right)\n\\end{aligned}\n\nThis gives us the update rule for the trapezoid method. Recall that the forward Euler method approximates the solution by extrapolating linearly using the slope at the beginning of the interval [t_n, t_{n+1}] . In contrast, the backward Euler method extrapolates linearly using the slope at the end of the interval. The trapezoid method, on the other hand, averages these two slopes. This averaging provides better approximation properties than either of the methods alone, offering both stability and accuracy. Note finally that unlike the forward or backward Euler methods, the trapezoid method is also symmetric in time. This means that if you were to reverse time and apply the method backward, you would get the same results (up to numerical precision).","type":"content","url":"/appendix-ivps#trapezoid-method","position":9},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Trapezoidal Predictor-Corrector"},"type":"lvl2","url":"/appendix-ivps#trapezoidal-predictor-corrector","position":10},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Trapezoidal Predictor-Corrector"},"content":"The trapezoid method can also be implemented under the so-called predictor-corrector framework. This interpretation reformulates the implicit trapezoid rule into an explicit two-step process:\n\nPredictor Step:We make an initial guess for  x_{n+1}  using the forward Euler method:x_{n+1}^* = x_n + h f(x_n, t_n)\n\nThis is our “predictor” step, where  x_{n+1}^*  is the predicted value of  x_{n+1} .\n\nCorrector Step:We then use this predicted value to estimate  f(x_{n+1}^*, t_{n+1})  and apply the trapezoid formula:x_{n+1} = x_n + \\frac{h}{2} \\left[ f(x_n, t_n) + f(x_{n+1}^*, t_{n+1}) \\right]\n\nThis is our “corrector” step, where the initial guess  x_{n+1}^*  is corrected by taking into account the slope at  (x_{n+1}^*, t_{n+1}) .\n\nThis two-step process is similar to performing one iteration of Newton’s method to solve the implicit trapezoid equation, starting from the Euler prediction. However, to fully solve the implicit equation, multiple iterations would be necessary until convergence is achieved.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef f(y, t):\n    \"\"\"\n    Derivative function for vertical motion under gravity.\n    y[0] is position, y[1] is velocity.\n    \"\"\"\n    g = 9.81  # acceleration due to gravity (m/s^2)\n    return np.array([y[1], -g])\n\ndef true_solution(t):\n    \"\"\"\n    Analytical solution for the ballistic trajectory.\n    \"\"\"\n    y0, v0 = 0, 20  # initial height and velocity\n    g = 9.81\n    return y0 + v0*t - 0.5*g*t**2, v0 - g*t\n\ndef trapezoid_method_visual(f, y0, t0, t_end, h):\n    \"\"\"\n    Implement the trapezoid method for the entire time range.\n    Returns predictor and corrector steps for visualization.\n    \"\"\"\n    t = np.arange(t0, t_end + h, h)\n    y = np.zeros((len(t), 2))\n    y_predictor = np.zeros((len(t), 2))\n    y[0] = y_predictor[0] = y0\n    for i in range(1, len(t)):\n        # Predictor step (Euler forward)\n        slope_start = f(y[i-1], t[i-1])\n        y_predictor[i] = y[i-1] + h * slope_start\n        \n        # Corrector step\n        slope_end = f(y_predictor[i], t[i])\n        y[i] = y[i-1] + h * 0.5 * (slope_start + slope_end)\n    \n    return t, y, y_predictor\n\n# Set up the problem\nt0, t_end = 0, 2\ny0 = np.array([0, 20])  # initial height = 0, initial velocity = 20 m/s\nh = 0.5  # Step size\n\n# Compute trapezoid method steps\nt, y_corrector, y_predictor = trapezoid_method_visual(f, y0, t0, t_end, h)\n\n# Plotting\nplt.figure(figsize=(12, 8))\n\n# Plot the true solution for comparison\nt_fine = np.linspace(t0, t_end, 1000)\ny_true, v_true = true_solution(t_fine)\nplt.plot(t_fine, y_true, 'k-', label='True trajectory', linewidth=1.5)\n\n# Plot the predictor and corrector steps\nfor i in range(len(t)-1):\n    # Points for the predictor step\n    p0 = [t[i], y_corrector[i, 0]]\n    p1_predictor = [t[i+1], y_predictor[i+1, 0]]\n    \n    # Points for the corrector step\n    p1_corrector = [t[i+1], y_corrector[i+1, 0]]\n    \n    # Plot predictor step\n    plt.plot([p0[0], p1_predictor[0]], [p0[1], p1_predictor[1]], 'r--', linewidth=2)\n    plt.plot(p1_predictor[0], p1_predictor[1], 'ro', markersize=8)\n    \n    # Plot corrector step\n    plt.plot([p0[0], p1_corrector[0]], [p0[1], p1_corrector[1]], 'g--', linewidth=2)\n    plt.plot(p1_corrector[0], p1_corrector[1], 'go', markersize=8)\n    \n    # Add arrows to show the predictor and corrector adjustments\n    plt.arrow(p0[0], p0[1], h, y_predictor[i+1, 0] - p0[1], color='r', width=0.005, \n              head_width=0.02, head_length=0.02, length_includes_head=True, zorder=5)\n    plt.arrow(p1_predictor[0], p1_predictor[1], 0, y_corrector[i+1, 0] - y_predictor[i+1, 0], \n              color='g', width=0.005, head_width=0.02, head_length=0.02, length_includes_head=True, zorder=5)\n\n# Add legend entries for predictor and corrector steps\nplt.plot([], [], 'r--', label='Predictor step (Forward Euler)')\nplt.plot([], [], 'g-', label='Corrector step (Trapezoid)')\n\n# Labels and title\nplt.xlabel('Time (s)', fontsize=12)\nplt.ylabel('Height (m)', fontsize=12)\nplt.title(\"Trapezoid Method: Predictor-Corrector Structure\", fontsize=14)\nplt.legend(fontsize=10)\nplt.grid(True, linestyle=':', alpha=0.7)\n\nplt.tight_layout()\nplt.show()\n\n","type":"content","url":"/appendix-ivps#trapezoidal-predictor-corrector","position":11},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Collocation Methods"},"type":"lvl2","url":"/appendix-ivps#collocation-methods","position":12},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Collocation Methods"},"content":"The numerical integration methods we discussed earlier are inherently sequential: given an initial state, we step forward in time and approximate what happens over a short interval. The accuracy of this procedure depends on the chosen rule (Euler, trapezoid, Runge–Kutta) and on the information available locally. Each new state is obtained by evaluating a formula that approximates the derivative or integral over that small step.\n\nCollocation methods provide an alternative viewpoint. Instead of advancing one step at a time, they approximate the entire trajectory with a finite set of basis functions and require the dynamics to hold at selected points. This replaces the original differential equation with a system of algebraic equations: relations among the coefficients of the basis functions that must all be satisfied simultaneously. Solving these equations fixes the whole trajectory in one computation.\n\nSeen from this angle, integration rules, spline interpolation, quadrature, and collocation are all instances of the same principle: an infinite-dimensional problem is reduced to finitely many parameters linked by numerical rules. The difference is mainly in scope. Sequential integration advances the state forward one interval at a time, which makes it simple but prone to error accumulation. Collocation belongs to the class of simultaneous methods already introduced for DOCPs: the entire trajectory is represented at once, the dynamics are imposed everywhere in the discretization, and approximation error is spread across the horizon rather than accumulating step by step.\n\nThis global enforcement comes at a computational cost since the resulting algebraic system is larger and denser. However, the benefit is precisely the structural one we saw in simultaneous methods earlier: by exposing the coupling between states, controls, and dynamics explicitly, collocation allows solvers to exploit sparsity and to enforce path constraints directly at the collocation points. This is why collocation is especially effective for challenging continuous-time optimal control problems where robustness and constraint satisfaction are central.","type":"content","url":"/appendix-ivps#collocation-methods","position":13},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Quick Primer on Polynomials"},"type":"lvl2","url":"/appendix-ivps#quick-primer-on-polynomials","position":14},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Quick Primer on Polynomials"},"content":"Collocation methods are based on polynomial approximation theory. Therefore, the first step in developing collocation-based optimal control techniques is to review the fundamentals of polynomial functions.\n\nPolynomials are typically introduced through their standard form:p(t) = a_n t^n + a_{n-1} t^{n-1} + \\cdots + a_1 t + a_0\n\nIn this expression, the a_i are coefficients which linearly combine the powers of t to represent a function. The set of functions \\{ 1, t, t^2, t^3, \\ldots, t^n \\} used in the standard polynomial representation is called the monomial basis.\n\nIn linear algebra, a basis is a set of vectors in a vector space such that any vector in the space can be uniquely represented as a linear combination of these basis vectors. In the same way, a polynomial basis is such that any function  f(x)  (within the function space) to be expressed as:f(x) = \\sum_{k=0}^{\\infty} c_k p_k(x),\n\nwhere the coefficients  c_k  are generally determined by solving a system of equation.\n\nJust as vectors can be represented in different coordinate systems (bases), functions can also be expressed using various polynomial bases. However, the ability to apply a change of basis does not imply that all types of polynomials are equivalent from a practical standpoint. In practice, our choice of polynomial basis is dictated by considerations of efficiency, accuracy, and stability when approximating a function.\n\nFor instance, despite the monomial basis being easy to understand and implement, it often performs poorly in practice due to numerical instability. This instability arises as its coefficients take on large values: an ill-conditioning problem. The following kinds of polynomial often remedy this issues.","type":"content","url":"/appendix-ivps#quick-primer-on-polynomials","position":15},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl3":"Orthogonal Polynomials","lvl2":"Quick Primer on Polynomials"},"type":"lvl3","url":"/appendix-ivps#orthogonal-polynomials","position":16},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl3":"Orthogonal Polynomials","lvl2":"Quick Primer on Polynomials"},"content":"An orthogonal polynomial basis is a set of polynomials that are orthogonal to each other and form a complete basis for a certain space of functions. This means that any function within that space can be represented as a linear combination of these polynomials.\n\nMore precisely, let  \\{ p_0(x), p_1(x), p_2(x), \\dots \\}  be a sequence of polynomials where each  p_n(x)  is a polynomial of degree  n . We say that this set forms an orthogonal polynomial basis if any polynomial  q(x)  of degree  n  or less can be uniquely expressed as a linear combination of  \\{ p_0(x), p_1(x), \\dots, p_n(x) \\} . Furthermore, the orthogonality property means that for any  i \\neq j :\\langle p_i, p_j \\rangle = \\int_a^b p_i(x) p_j(x) w(x) \\, dx = 0.\n\nfor some weight function  w(x)  over a given interval of orthogonality  [a, b] .\n\nThe orthogonality property allows to simplify the computation of the coefficients involved in the polynomial representation of a function. At a high level, what happens is that when taking the inner product of  f(x)  with each basis polynomial,  p_k(x)  isolates the corresponding coefficient  c_k , which can be found to be:c_k = \\frac{\\langle f, p_k \\rangle}{\\langle p_k, p_k \\rangle} = \\frac{\\int_a^b f(x) p_k(x) w(x) \\, dx}{\\int_a^b p_k(x)^2 w(x) \\, dx}.\n\nHere are some examples of the most common orthogonal polynomials used in practice.","type":"content","url":"/appendix-ivps#orthogonal-polynomials","position":17},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl4":"Legendre Polynomials","lvl3":"Orthogonal Polynomials","lvl2":"Quick Primer on Polynomials"},"type":"lvl4","url":"/appendix-ivps#legendre-polynomials","position":18},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl4":"Legendre Polynomials","lvl3":"Orthogonal Polynomials","lvl2":"Quick Primer on Polynomials"},"content":"Legendre polynomials  \\{ P_n(x) \\}  are defined on the interval [-1, 1] and satisfy the orthogonality condition:\\int_{-1}^{1} P_n(x) P_m(x) \\, dx = \n\\begin{cases}\n0 & \\text{if } n \\neq m, \\\\\n\\frac{2}{2n + 1} & \\text{if } n = m.\n\\end{cases}\n\nThey can be generated using the recurrence relation:(n+1) P_{n+1}(x) = (2n + 1) x P_n(x) - n P_{n-1}(x),\n\nwith initial conditions:P_0(x) = 1, \\quad P_1(x) = x.\n\nThe first four Legendre polynomials resulting from this recurrence are the following:\n\nimport numpy as np\nfrom IPython.display import display, Math\n\ndef legendre_polynomial(n, x):\n    if n == 0:\n        return np.poly1d([1])\n    elif n == 1:\n        return x\n    else:\n        p0 = np.poly1d([1])\n        p1 = x\n        for k in range(2, n + 1):\n            p2 = ((2 * k - 1) * x * p1 - (k - 1) * p0) / k\n            p0, p1 = p1, p2\n        return p1\n\ndef legendre_coefficients(n):\n    x = np.poly1d([1, 0])  # Define a poly1d object to represent x\n    poly = legendre_polynomial(n, x)\n    return poly\n\ndef poly_to_latex(poly):\n    coeffs = poly.coefficients\n    variable = poly.variable\n    \n    terms = []\n    for i, coeff in enumerate(coeffs):\n        power = len(coeffs) - i - 1\n        if coeff == 0:\n            continue\n        coeff_str = f\"{coeff:.2g}\" if coeff not in {1, -1} or power == 0 else (\"-\" if coeff == -1 else \"\")\n        if power == 0:\n            term = f\"{coeff_str}\"\n        elif power == 1:\n            term = f\"{coeff_str}{variable}\"\n        else:\n            term = f\"{coeff_str}{variable}^{power}\"\n        terms.append(term)\n    \n    latex_poly = \" + \".join(terms).replace(\" + -\", \" - \")\n    return latex_poly\n\nfor n in range(4):\n    poly = legendre_coefficients(n)\n    display(Math(f\"P_{n}(x) = {poly_to_latex(poly)}\"))\n\n","type":"content","url":"/appendix-ivps#legendre-polynomials","position":19},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl4":"Chebyshev Polynomials","lvl3":"Orthogonal Polynomials","lvl2":"Quick Primer on Polynomials"},"type":"lvl4","url":"/appendix-ivps#chebyshev-polynomials","position":20},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl4":"Chebyshev Polynomials","lvl3":"Orthogonal Polynomials","lvl2":"Quick Primer on Polynomials"},"content":"There are two types of Chebyshev polynomials: Chebyshev polynomials of the first kind,  \\{ T_n(x) \\} , and Chebyshev polynomials of the second kind,  \\{ U_n(x) \\} . We typically focus on the first kind. They are defined on the interval [-1, 1] and satisfy the orthogonality condition:\\int_{-1}^{1} \\frac{T_n(x) T_m(x)}{\\sqrt{1 - x^2}} \\, dx = \n\\begin{cases}\n0 & \\text{if } n \\neq m, \\\\\n\\frac{\\pi}{2} & \\text{if } n = m \\neq 0, \\\\\n\\pi & \\text{if } n = m = 0.\n\\end{cases}\n\nThe Chebyshev polynomials of the first kind can be generated using the recurrence relation:T_{n+1}(x) = 2x T_n(x) - T_{n-1}(x),\n\nwith initial conditions:T_0(x) = 1, \\quad T_1(x) = x.\n\nThis recurrence relation also admits an explicit formula:T_n(x) = \\cos(n \\cos^{-1}(x)).\n\nLet’s now implement it in Python:\n\ndef chebyshev_polynomial(n, x):\n    if n == 0:\n        return np.poly1d([1])\n    elif n == 1:\n        return x\n    else:\n        t0 = np.poly1d([1])\n        t1 = x\n        for _ in range(2, n + 1):\n            t2 = 2 * x * t1 - t0\n            t0, t1 = t1, t2\n        return t1\n\ndef chebyshev_coefficients(n):\n    x = np.poly1d([1, 0])  # Define a poly1d object to represent x\n    poly = chebyshev_polynomial(n, x)\n    return poly\n\nfor n in range(4):\n    poly = chebyshev_coefficients(n)\n    display(Math(f\"T_{n}(x) = {poly_to_latex(poly)}\"))\n\n","type":"content","url":"/appendix-ivps#chebyshev-polynomials","position":21},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl4":"Hermite Polynomials","lvl3":"Orthogonal Polynomials","lvl2":"Quick Primer on Polynomials"},"type":"lvl4","url":"/appendix-ivps#hermite-polynomials","position":22},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl4":"Hermite Polynomials","lvl3":"Orthogonal Polynomials","lvl2":"Quick Primer on Polynomials"},"content":"Hermite polynomials  \\{ H_n(x) \\}  are defined on the entire real line and are orthogonal with respect to the weight function  w(x) = e^{-x^2} . They satisfy the orthogonality condition:\\int_{-\\infty}^{\\infty} H_n(x) H_m(x) e^{-x^2} \\, dx = \n\\begin{cases}\n0 & \\text{if } n \\neq m, \\\\\n2^n n! \\sqrt{\\pi} & \\text{if } n = m.\n\\end{cases}\n\nHermite polynomials can be generated using the recurrence relation:H_{n+1}(x) = 2x H_n(x) - 2n H_{n-1}(x),\n\nwith initial conditions:H_0(x) = 1, \\quad H_1(x) = 2x.\n\nThe following code computes the coefficients of the first four Hermite polynomials:\n\ndef hermite_polynomial(n, x):\n    if n == 0:\n        return np.poly1d([1])\n    elif n == 1:\n        return 2 * x\n    else:\n        h0 = np.poly1d([1])\n        h1 = 2 * x\n        for k in range(2, n + 1):\n            h2 = 2 * x * h1 - 2 * (k - 1) * h0\n            h0, h1 = h1, h2\n        return h1\n\ndef hermite_coefficients(n):\n    x = np.poly1d([1, 0])  # Define a poly1d object to represent x\n    poly = hermite_polynomial(n, x)\n    return poly\n\nfor n in range(4):\n    poly = hermite_coefficients(n)\n    display(Math(f\"H_{n}(x) = {poly_to_latex(poly)}\"))\n\n","type":"content","url":"/appendix-ivps#hermite-polynomials","position":23},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Collocation Conditions"},"type":"lvl2","url":"/appendix-ivps#collocation-conditions","position":24},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Collocation Conditions"},"content":"Consider a general ODE of the form:\\dot{y}(t) = f(y(t), t), \\quad y(t_0) = y_0,\n\nwhere  y(t) \\in \\mathbb{R}^n  is the state vector, and  f: \\mathbb{R}^n \\times \\mathbb{R} \\rightarrow \\mathbb{R}^n  is a known function. The goal is to approximate the solution  y(t)  over a given interval [t_0, t_f]. Collocation methods achieve this by:\n\nChoosing a basis to approximate  y(t)  using a finite sum of basis functions  \\phi_i(t) :y(t) \\approx \\sum_{i=0}^{N} c_i \\phi_i(t),\n\nwhere  \\{c_i\\}  are the coefficients to be determined.\n\nSelecting collocation points  t_1, t_2, \\ldots, t_N  within the interval [t_0, t_f]. These are typically chosen to be the roots of certain orthogonal polynomials, like Legendre or Chebyshev polynomials, or can be spread equally across the interval.\n\nEnforcing the ODE at the collocation points for each  t_j :\\dot{y}(t_j) = f(y(t_j), t_j).\n\nTo implement this, we differentiate the approximate solution  y(t)  with respect to time:\\dot{y}(t) \\approx \\sum_{i=0}^{N} c_i \\dot{\\phi}_i(t).\n\nSubstituting this into the ODE at the collocation points gives:\\sum_{i=0}^{N} c_i \\dot{\\phi}_i(t_j) = f\\left(\\sum_{i=0}^{N} c_i \\phi_i(t_j), t_j\\right), \\quad j = 1, \\ldots, N.\n\nThe collocation equations are formed by enforcing the ODE at all collocation points, leading to a system of nonlinear equations:\\sum_{i=0}^{N} c_i \\dot{\\phi}_i(t_j) - f\\left(\\sum_{i=0}^{N} c_i \\phi_i(t_j), t_j\\right) = 0, \\quad j = 1, \\ldots, N.\n\nFurthermore when solving an initial value problem (IVP),  we also need to incorporate the initial condition  y(t_0) = y_0  as an additional constraint:\\sum_{i=0}^{N} c_i \\phi_i(t_0) = y_0.\n\nThe collocation conditions and IVP condition are combined together to form a root-finding problem, which we can generically solve numerically using Newton’s method.","type":"content","url":"/appendix-ivps#collocation-conditions","position":25},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Common Numerical Integration Techniques as Collocation Methods"},"type":"lvl2","url":"/appendix-ivps#common-numerical-integration-techniques-as-collocation-methods","position":26},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Common Numerical Integration Techniques as Collocation Methods"},"content":"Many common numerical integration techniques can be viewed as special cases of collocation methods.\nWhile the general collocation method we discussed earlier applies to the entire interval [t_0, t_f], many numerical integration techniques can be viewed as collocation methods applied locally, step by step.\n\nIn practical numerical integration, we often divide the full interval [t_0, t_f] into smaller subintervals or steps. In general, this allows us to user simpler basis functions thereby reducing computational complexity, and gives us more flexibility in dynamically ajusting the step size using local error estimates. When we apply collocation locally, we’re essentially using the collocation method to “step” from t_n to t_{n+1}. As we did, earlier we still apply the following three steps:\n\nWe choose a basis function to approximate y(t) over [t_n, t_{n+1}].\n\nWe select collocation points within this interval.\n\nWe enforce the ODE at these points to determine the coefficients of our basis function.\n\nWe can make this idea clearer by re-deriving some of the numerical integration methods seen before using this perspective.","type":"content","url":"/appendix-ivps#common-numerical-integration-techniques-as-collocation-methods","position":27},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl3":"Explicit Euler Method","lvl2":"Common Numerical Integration Techniques as Collocation Methods"},"type":"lvl3","url":"/appendix-ivps#explicit-euler-method","position":28},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl3":"Explicit Euler Method","lvl2":"Common Numerical Integration Techniques as Collocation Methods"},"content":"For the Explicit Euler method, we use a linear basis function for each step:\\phi(t) = 1 + c(t - t_n)\n\nNote that we use (t - t_n) rather than just t because we’re approximating the solution locally, relative to the start of each step. We then choose one collocation point at t_{n+1} where we have:y'(t_{n+1}) = c = f(y_n, t_n)\n\nOur local approximation is:y(t) \\approx y_n + c(t - t_n)\n\nAt t = t_{n+1}, this gives:y_{n+1} = y_n + c(t_{n+1} - t_n) = y_n + hf(y_n, t_n)\n\nwhere h = t_{n+1} - t_n. This is the classic Euler update formula.","type":"content","url":"/appendix-ivps#explicit-euler-method","position":29},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl3":"Implicit Euler Method","lvl2":"Common Numerical Integration Techniques as Collocation Methods"},"type":"lvl3","url":"/appendix-ivps#implicit-euler-method","position":30},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl3":"Implicit Euler Method","lvl2":"Common Numerical Integration Techniques as Collocation Methods"},"content":"The Implicit Euler method uses the same linear basis function:\\phi(t) = 1 + c(t - t_n)\n\nAgain, we choose one collocation point at t_{n+1}. The main difference is that we enforce the ODE using y_{n+1}:y'(t_{n+1}) = c = f(y_{n+1}, t_{n+1})\n\nOur approximation remains:y(t) \\approx y_n + c(t - t_n)\n\nAt t = t_{n+1}, this leads to the implicit equation:y_{n+1} = y_n + hf(y_{n+1}, t_{n+1})","type":"content","url":"/appendix-ivps#implicit-euler-method","position":31},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl3":"Trapezoidal Method","lvl2":"Common Numerical Integration Techniques as Collocation Methods"},"type":"lvl3","url":"/appendix-ivps#trapezoidal-method","position":32},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl3":"Trapezoidal Method","lvl2":"Common Numerical Integration Techniques as Collocation Methods"},"content":"The Trapezoidal method uses a quadratic basis function:\\phi(t) = 1 + c(t - t_n) + a(t - t_n)^2\n\nWe use two collocation points: t_n and t_{n+1}. Enforcing the ODE at these points gives:\n\nAt t_n:y'(t_n) = c = f(y_n, t_n)\n\nAt t_{n+1}:y'(t_{n+1}) = c + 2ah = f(y_n + ch + ah^2, t_{n+1})\n\nOur approximation is:y(t) \\approx y_n + c(t - t_n) + a(t - t_n)^2\n\nAt t = t_{n+1}, this gives:y_{n+1} = y_n + ch + ah^2\n\nSolving the system of equations leads to the trapezoidal update:y_{n+1} = y_n + \\frac{h}{2}[f(y_n, t_n) + f(y_{n+1}, t_{n+1})]","type":"content","url":"/appendix-ivps#trapezoidal-method","position":33},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl3":"Runge-Kutta Methods","lvl2":"Common Numerical Integration Techniques as Collocation Methods"},"type":"lvl3","url":"/appendix-ivps#runge-kutta-methods","position":34},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl3":"Runge-Kutta Methods","lvl2":"Common Numerical Integration Techniques as Collocation Methods"},"content":"Higher-order Runge-Kutta methods can also be interpreted as collocation methods. The RK4 method corresponds to a collocation method using a cubic polynomial basis:\\phi(t) = 1 + c_1(t - t_n) + c_2(t - t_n)^2 + c_3(t - t_n)^3\n\nHere, we’re using a cubic polynomial to approximate the solution over each step, rather than the linear or quadratic approximations of the other methods above. For RK4, we use four collocation points:\n\nt_n (the start of the step)\n\nt_n + h/2\n\nt_n + h/2\n\nt_n + h (the end of the step)\n\nThese points are called the “Gauss-Lobatto” points, scaled to our interval [t_n, t_n + h].\nThe RK4 method enforces the ODE at these collocation points, leading to four stages:\\begin{aligned}\nk_1 &= hf(y_n, t_n) \\\\\nk_2 &= hf(y_n + \\frac{1}{2}k_1, t_n + \\frac{h}{2}) \\\\\nk_3 &= hf(y_n + \\frac{1}{2}k_2, t_n + \\frac{h}{2}) \\\\\nk_4 &= hf(y_n + k_3, t_n + h)\n\\end{aligned}\n\nThe final update formula for RK4 can be derived by solving the system of equations resulting from enforcing the ODE at our collocation points:y_{n+1} = y_n + \\frac{1}{6}(k_1 + 2k_2 + 2k_3 + k_4)","type":"content","url":"/appendix-ivps#runge-kutta-methods","position":35},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Example: Solving a Simple ODE by Collocation"},"type":"lvl2","url":"/appendix-ivps#example-solving-a-simple-ode-by-collocation","position":36},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Example: Solving a Simple ODE by Collocation"},"content":"Consider a simple ODE:\\frac{dy}{dt} = -y, \\quad y(0) = 1, \\quad t \\in [0, 2]\n\nThe analytical solution is y(t) = e^{-t}. We apply the collocation method with a monomial basis of order N:\\phi_i(t) = t^i, \\quad i = 0, 1, \\ldots, N\n\nWe select N equally spaced points \\{t_1, \\ldots, t_N\\} in [0, 2] as collocation points.\n\nimport numpy as np\nfrom scipy.optimize import root\nimport matplotlib.pyplot as plt\n\ndef ode_function(y, t):\n    \"\"\"Define the ODE: dy/dt = -y\"\"\"\n    return -y\n\ndef solve_ode_collocation(ode_func, t_span, y0, order):\n    t0, tf = t_span\n    n_points = order + 1  # number of collocation points\n    t_points = np.linspace(t0, tf, n_points)\n    \n    def collocation_residuals(coeffs):\n        residuals = []\n        # Initial condition residual\n        y_init = sum(c * t_points[0]**i for i, c in enumerate(coeffs))\n        residuals.append(y_init - y0)\n        # Collocation point residuals\n        for t in t_points[1:]:  # Skip the first point as it's used for initial condition\n            y = sum(c * t**i for i, c in enumerate(coeffs))\n            dy_dt = sum(c * i * t**(i-1) for i, c in enumerate(coeffs) if i > 0)\n            residuals.append(dy_dt - ode_func(y, t))\n        return residuals\n\n    # Initial guess for coefficients\n    initial_coeffs = [y0] + [0] * order\n\n    # Solve the system of equations with more robust settings\n    solution = root(collocation_residuals, initial_coeffs, \n                   method='hybr', options={'maxfev': 10000, 'xtol': 1e-8})\n    \n    if not solution.success:\n        # Try with a different method\n        solution = root(collocation_residuals, initial_coeffs, \n                       method='lm', options={'maxiter': 5000})\n        \n    if not solution.success:\n        print(f\"Warning: Collocation solver did not fully converge for order {order}\")\n        # Continue anyway with the best solution found\n\n    coeffs = solution.x\n\n    # Generate solution\n    t_fine = np.linspace(t0, tf, 100)\n    y_solution = sum(c * t_fine**i for i, c in enumerate(coeffs))\n\n    return t_fine, y_solution, t_points, coeffs\n\n# Example usage\nt_span = (0, 2)\ny0 = 1\norders = [1, 2, 3, 4, 5]  # Different polynomial orders to try\n\nplt.figure(figsize=(12, 8))\n\nfor order in orders:\n    t, y, t_collocation, coeffs = solve_ode_collocation(ode_function, t_span, y0, order)\n    \n    # Calculate y values at collocation points\n    y_collocation = sum(c * t_collocation**i for i, c in enumerate(coeffs))\n    \n    # Plot the results\n    plt.plot(t, y, label=f'Order {order}')\n    plt.scatter(t_collocation, y_collocation, s=50, zorder=5)\n\n# Plot the analytical solution\nt_analytical = np.linspace(t_span[0], t_span[1], 100)\ny_analytical = y0 * np.exp(-t_analytical)\nplt.plot(t_analytical, y_analytical, 'k--', label='Analytical')\n\nplt.xlabel('t')\nplt.ylabel('y')\nplt.title('ODE Solutions: dy/dt = -y, y(0) = 1')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Print error for each order\nprint(\"Maximum absolute errors:\")\nfor order in orders:\n    t, y, _, _ = solve_ode_collocation(ode_function, t_span, y0, order)\n    y_true = y0 * np.exp(-t)\n    max_error = np.max(np.abs(y - y_true))\n    print(f\"Order {order}: {max_error:.6f}\")","type":"content","url":"/appendix-ivps#example-solving-a-simple-ode-by-collocation","position":37},{"hierarchy":{"lvl1":"Nonlinear Programming"},"type":"lvl1","url":"/appendix-nlp","position":0},{"hierarchy":{"lvl1":"Nonlinear Programming"},"content":"Unless specific assumptions are made on the dynamics and cost structure, a DOCP is, in its most general form, a nonlinear mathematical program (commonly referred to as an NLP, not to be confused with Natural Language Processing). An NLP can be formulated as follows:\\begin{aligned}\n\\text{minimize } & f(\\mathbf{x}) \\\\\n\\text{subject to } & \\mathbf{g}(\\mathbf{x}) \\leq \\mathbf{0} \\\\\n& \\mathbf{h}(\\mathbf{x}) = \\mathbf{0}\n\\end{aligned}\n\nWhere:\n\nf: \\mathbb{R}^n \\to \\mathbb{R} is the objective function\n\n\\mathbf{g}: \\mathbb{R}^n \\to \\mathbb{R}^m represents inequality constraints\n\n\\mathbf{h}: \\mathbb{R}^n \\to \\mathbb{R}^\\ell represents equality constraints\n\nUnlike unconstrained optimization commonly used in deep learning, the optimality of a solution in constrained optimization must consider both the objective value and constraint feasibility. To illustrate this, consider the following problem, which includes both equality and inequality constraints:\\begin{align*}\n\\text{Minimize} \\quad & f(x_1, x_2) = (x_1 - 1)^2 + (x_2 - 2.5)^2 \\\\\n\\text{subject to} \\quad & g(x_1, x_2) = (x_1 - 1)^2 + (x_2 - 1)^2 \\leq 1.5, \\\\\n& h(x_1, x_2) = x_2 - \\left(0.5 \\sin(2 \\pi x_1) + 1.5\\right) = 0.\n\\end{align*}\n\nIn this example, the objective function f(x_1, x_2) is quadratic, the inequality constraint g(x_1, x_2) defines a circular feasible region centered at (1, 1) with a radius of \\sqrt{1.5} and the equality constraint h(x_1, x_2) requires x_2 to lie on a sine wave function. The following code demonstrates the difference between the unconstrained, and constrained solutions to this problem.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import minimize\n\n# Define the objective function\ndef objective(x):\n    return (x[0] - 1)**2 + (x[1] - 2.5)**2\n\n# Define the inequality constraint function\ndef constraint(x):\n    return -(x[0] - 1)**2 - (x[1] - 1)**2 + 1.5\n\n# Define the gradient of the objective function\ndef objective_gradient(x):\n    return np.array([2*(x[0] - 1), 2*(x[1] - 2.5)])\n\n# Define the gradient of the inequality constraint function\ndef constraint_gradient(x):\n    return np.array([-2*(x[0] - 1), -2*(x[1] - 1)])\n\n# Define the sine wave equality constraint function\ndef sine_wave_equality_constraint(x):\n    return x[1] - (0.5 * np.sin(2 * np.pi * x[0]) + 1.5)\n\n# Define the gradient of the sine wave equality constraint function\ndef sine_wave_equality_constraint_gradient(x):\n    return np.array([-np.pi * np.cos(2 * np.pi * x[0]), 1])\n\n# Define the constraints including the sine wave equality constraint\nsine_wave_constraints = [{'type': 'ineq', 'fun': constraint, 'jac': constraint_gradient},  # Inequality constraint\n                         {'type': 'eq', 'fun': sine_wave_equality_constraint, 'jac': sine_wave_equality_constraint_gradient}]  # Sine wave equality constraint\n\n# Define only the inequality constraint\ninequality_constraints = [{'type': 'ineq', 'fun': constraint, 'jac': constraint_gradient}]\n\n# Initial guess\nx0 = [1.25, 1.5]\n\n# Solve the optimization problem with the sine wave equality constraint\nres_sine_wave_constraint = minimize(objective, x0, method='SLSQP', jac=objective_gradient, \n                                    constraints=sine_wave_constraints, options={'disp': False})\n\nx_opt_sine_wave_constraint = res_sine_wave_constraint.x\n\n# Solve the optimization problem with only the inequality constraint\nres_inequality_only = minimize(objective, x0, method='SLSQP', jac=objective_gradient, \n                               constraints=inequality_constraints, options={'disp': False})\n\nx_opt_inequality_only = res_inequality_only.x\n\n# Solve the unconstrained optimization problem for reference\nres_unconstrained = minimize(objective, x0, method='SLSQP', jac=objective_gradient, options={'disp': False})\nx_opt_unconstrained = res_unconstrained.x\n\n# Generate data for visualization\nx = np.linspace(-1, 4, 400)\ny = np.linspace(-1, 4, 400)\nX, Y = np.meshgrid(x, y)\nZ = (X - 1)**2 + (Y - 2.5)**2  # Objective function values\nconstraint_values = (X - 1)**2 + (Y - 1)**2\n\n# Data for sine wave constraint\nx_sine = np.linspace(-1, 4, 400)\ny_sine = 0.5 * np.sin(2 * np.pi * x_sine) + 1.5\n\n# Visualization with Improved Color Scheme\nplt.figure(figsize=(8, 6))\nplt.contourf(X, Y, Z, levels=100, cmap='viridis', alpha=0.6)  # Heatmap for the objective function\n\n# Plot all the optimal points\nplt.plot(x_opt_inequality_only[0], x_opt_inequality_only[1], 'ro', label='Optimal Solution (Inequality Only)', markersize=8, markeredgecolor='black')\nplt.plot(x_opt_sine_wave_constraint[0], x_opt_sine_wave_constraint[1], 'mo', label='Optimal Solution (Sine Wave Equality & Inequality)', markersize=8, markeredgecolor='black')\nplt.plot(x_opt_unconstrained[0], x_opt_unconstrained[1], 'co', label='Unconstrained Minimum', markersize=8, markeredgecolor='black')\n\n# Adjust constraint boundary colors\nplt.contour(X, Y, constraint_values, levels=[1.5], colors='navy', linewidths=2, linestyles='dashed')\nplt.contourf(X, Y, constraint_values, levels=[0, 1.5], colors='skyblue', alpha=0.3)\n\n# Plot the sine wave equality constraint with a high contrast color\nplt.plot(x_sine, y_sine, 'lime', linestyle='--', linewidth=2, label='Sine Wave Equality Constraint')\n\nplt.xlim([-1, 4])\nplt.ylim([-1, 4])\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.title('Example NLP')\nplt.legend(loc='upper left', fontsize='small', edgecolor='black', fancybox=True)\nplt.grid(True)\n# Set the aspect ratio to be equal so the circle appears correctly\nplt.gca().set_aspect('equal', adjustable='box')\nplt.show()\n\n","type":"content","url":"/appendix-nlp","position":1},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl3":"Karush-Kuhn-Tucker (KKT) conditions"},"type":"lvl3","url":"/appendix-nlp#karush-kuhn-tucker-kkt-conditions","position":2},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl3":"Karush-Kuhn-Tucker (KKT) conditions"},"content":"While this example is simple enough to convince ourselves visually of the solution to this particular problem, it falls short of providing us with actionable chracterization of what constitutes and optimal solution in general.\nThe Karush-Kuhn-Tucker (KKT) conditions provide us with an answer to this problem by generalizing the first-order optimality conditions in unconstrained optimization to problems involving both equality and inequality constraints.\nThis result relies on the construction of an auxiliary function called the Lagrangian, defined as:\\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\mu}, \\boldsymbol{\\lambda})=f(\\mathbf{x})+\\boldsymbol{\\mu}^{\\top} \\mathbf{g}(\\mathbf{x})+\\boldsymbol{\\lambda}^{\\top} \\mathbf{h}(\\mathbf{x})\n\nwhere \\boldsymbol{\\mu} \\in \\mathbb{R}^m and \\boldsymbol{\\lambda} \\in \\mathbb{R}^\\ell are known as Lagrange multipliers. The first-order optimality conditions then state that if \\mathbf{x}^*, then there must exist corresponding Lagrange multipliers \\boldsymbol{\\mu}^* and \\boldsymbol{\\lambda}^* such that:\n\nThe gradient of the Lagrangian with respect to \\mathbf{x} must be zero at the optimal point (stationarity):\\nabla_x \\mathcal{L}(\\mathbf{x}^*, \\boldsymbol{\\mu}^*, \\boldsymbol{\\lambda}^*) = \\nabla f(\\mathbf{x}^*) + \\sum_{i=1}^m \\mu_i^* \\nabla g_i(\\mathbf{x}^*) + \\sum_{j=1}^\\ell \\lambda_j^* \\nabla h_j(\\mathbf{x}^*) = \\mathbf{0}\n\nIn the case where we only have equality constraints, this means that the gradient of the objective and that of constraint are parallel to each other at the optimum but point in opposite directions.\n\nA valid solution of a NLP is one which satisfies all the constraints (primal feasibility)\\begin{aligned}\n   \\mathbf{g}(\\mathbf{x}^*) &\\leq \\mathbf{0}, \\enspace \\text{and} \\enspace \\mathbf{h}(\\mathbf{x}^*) &= \\mathbf{0}\n   \\end{aligned}\n\nFurthermore, the Lagrange multipliers for inequality constraints must be non-negative (dual feasibility)\\boldsymbol{\\mu}^* \\geq \\mathbf{0}\n\nThis condition stems from the fact that the inequality constraints can only push the solution in one direction.\n\nFinally, for each inequality constraint, either the constraint is active (equality holds) or its corresponding Lagrange multiplier is zero at an optimal solution (complementary slackness)\\mu_i^* g_i(\\mathbf{x}^*) = 0, \\quad \\forall i = 1,\\ldots,m\n\nLet’s now solve our example problem above, this time using \n\nIpopt via the \n\nPyomo interface so that we can access the Lagrange multipliers found by the solver.\n\nfrom pyomo.environ import *\nfrom pyomo.opt import SolverFactory\nimport math\n\n# Define the Pyomo model\nmodel = ConcreteModel()\n\n# Define the variables\nmodel.x1 = Var(initialize=1.25)\nmodel.x2 = Var(initialize=1.5)\n\n# Define the objective function\ndef objective_rule(model):\n    return (model.x1 - 1)**2 + (model.x2 - 2.5)**2\nmodel.obj = Objective(rule=objective_rule, sense=minimize)\n\n# Define the inequality constraint (circle)\ndef inequality_constraint_rule(model):\n    return (model.x1 - 1)**2 + (model.x2 - 1)**2 <= 1.5\nmodel.ineq_constraint = Constraint(rule=inequality_constraint_rule)\n\n# Define the equality constraint (sine wave) using Pyomo's math functions\ndef equality_constraint_rule(model):\n    return model.x2 == 0.5 * sin(2 * math.pi * model.x1) + 1.5\nmodel.eq_constraint = Constraint(rule=equality_constraint_rule)\n\n# Create a suffix component to capture dual values\nmodel.dual = Suffix(direction=Suffix.IMPORT)\n\n# Create a solver\nsolver=SolverFactory('ipopt')\n\n# Solve the problem\nresults = solver.solve(model, tee=False)\n\n# Check if the solver found an optimal solution\nif (results.solver.status == SolverStatus.ok and \n    results.solver.termination_condition == TerminationCondition.optimal):\n    \n    # Print the results\n    print(f\"x1: {value(model.x1)}\")\n    print(f\"x2: {value(model.x2)}\")\n    \n    # Print the objective value\n    print(f\"Objective value: {value(model.obj)}\")\n\n    # Print the Lagrange multipliers (dual values)\n    print(\"\\nLagrange multipliers:\")\n    ineq_lambda = None\n    eq_lambda = None\n    for c in model.component_objects(Constraint, active=True):\n        for index in c:\n            dual_val = model.dual[c[index]]\n            print(f\"{c.name}[{index}]: {dual_val}\")\n            if c.name == \"ineq_constraint\":\n                ineq_lambda = dual_val\n            elif c.name == \"eq_constraint\":\n                eq_lambda = dual_val\nelse:\n    print(\"Solver did not find an optimal solution.\")\n    print(f\"Solver Status: {results.solver.status}\")\n    print(f\"Termination Condition: {results.solver.termination_condition}\")\n\nAfter running the code above, we can observe the Lagrange multipliers. The Lagrange multiplier associated with the inequality constraint is very small (close to zero), suggesting that the inequality constraint is not active at the optimal solution—meaning that the solution point lies inside the circle defined by this constraint. This can be verified visually in the figure above. As for the equality constraint, its corresponding Lagrange multiplier is non-zero, indicating that this constraint is active at the optimal solution. In general, when we find a Lagrange multiplier close to zero (like the one for the inequality constraint), it means that constraint is not “binding”—the optimal solution does not lie on the boundary defined by this constraint. In contrast, a non-zero Lagrange multiplier, such as the one for the equality constraint, indicates that the constraint is active and that any relaxation would directly affect the objective function’s value, as required by the stationarity condition.","type":"content","url":"/appendix-nlp#karush-kuhn-tucker-kkt-conditions","position":3},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl3":"Lagrange Multiplier Theorem"},"type":"lvl3","url":"/appendix-nlp#lagrange-multiplier-theorem","position":4},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl3":"Lagrange Multiplier Theorem"},"content":"The KKT conditions introduced above characterize the solution structure of constrained optimization problems with equality constraints. In this particular context, these conditions are referred to as the first-order optimality conditions, as part of the Lagrange multiplier theorem. Let’s just re-state them in that simpler setting:\n\nLagrange Multiplier Theorem\n\nConsider the constrained optimization problem:\\begin{aligned}\n\\min_{\\mathbf{x}} \\quad & f(\\mathbf{x}) \\\\\n\\text{subject to} \\quad & h_i(\\mathbf{x}) = 0, \\quad i = 1, \\ldots, m\n\\end{aligned}\n\nwhere \\mathbf{x} \\in \\mathbb{R}^n, f: \\mathbb{R}^n \\to \\mathbb{R}, and h_i: \\mathbb{R}^n \\to \\mathbb{R} for i = 1, \\ldots, m.\n\nAssume that:\n\nf and h_i are continuously differentiable functions.\n\nThe gradients \\nabla h_i(\\mathbf{x}^*) are linearly independent at the optimal point \\mathbf{x}^*.\n\nThen, there exist unique Lagrange multipliers \\lambda_i^* \\in \\mathbb{R}, i = 1, \\ldots, m, such that the following first-order optimality conditions hold:\n\nStationarity: \\nabla f(\\mathbf{x}^*) + \\sum_{i=1}^m \\lambda_i^* \\nabla h_i(\\mathbf{x}^*) = \\mathbf{0}\n\nPrimal feasibility: h_i(\\mathbf{x}^*) = 0, for i = 1, \\ldots, m\n\nNote that both the stationarity and primal feasibility statements are simply saying that the derivative of the Lagrangian in either the primal or dual variables must be zero at an optimal constrained solution. In other words:\\nabla_{\\mathbf{x}, \\boldsymbol{\\lambda}} L(\\mathbf{x}^*, \\boldsymbol{\\lambda}^*) = \\mathbf{0}\n\nLetting \\mathbf{F}(\\mathbf{x}, \\boldsymbol{\\lambda}) stand for \\nabla_{\\mathbf{x}, \\boldsymbol{\\lambda}} L(\\mathbf{x}, \\boldsymbol{\\lambda}), the Lagrange multipliers theorem tells us that an optimal primal-dual pair is actually a zero of that function \\mathbf{F}: the derivative of the Lagrangian. Therefore, we can use this observation to craft a solution method for solving equality constrained optimization using Newton’s method, which is a numerical procedure for finding zeros of a nonlinear function.","type":"content","url":"/appendix-nlp#lagrange-multiplier-theorem","position":5},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl3":"Newton’s Method"},"type":"lvl3","url":"/appendix-nlp#newtons-method","position":6},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl3":"Newton’s Method"},"content":"Newton’s method is a numerical procedure for solving root-finding problems. These are nonlinear systems of equations of the form:\n\nFind \\mathbf{z}^* \\in \\mathbb{R}^n such that \\mathbf{F}(\\mathbf{z}^*) = \\mathbf{0}\n\nwhere \\mathbf{F}: \\mathbb{R}^n \\to \\mathbb{R}^n is a continuously differentiable function. Newton’s method then consists in applying the following sequence of iterates:\\mathbf{z}^{k+1} = \\mathbf{z}^k - [\\nabla \\mathbf{F}(\\mathbf{z}^k)]^{-1} \\mathbf{F}(\\mathbf{z}^k)\n\nwhere \\mathbf{z}^k is the k-th iterate, and \\nabla \\mathbf{F}(\\mathbf{z}^k) is the Jacobian matrix of \\mathbf{F} evaluated at \\mathbf{z}^k.\n\nNewton’s method exhibits local quadratic convergence: if the initial guess \\mathbf{z}^0 is sufficiently close to the true solution \\mathbf{z}^*, and \\nabla \\mathbf{F}(\\mathbf{z}^*) is nonsingular, the method converges quadratically to \\mathbf{z}^* \n\nOrtega & Rheinboldt (1970). However, the method is sensitive to the initial guess; if it’s too far from the desired solution, Newton’s method might fail to converge or converge to a different root. To mitigate this problem, a set of techniques known as numerical continuation methods \n\nAllgower & Georg (1990) have been developed. These methods effectively enlarge the basin of attraction of Newton’s method by solving a sequence of related problems, progressing from an easy one to the target problem. This approach is reminiscent of several concepts in machine learning and statistical inference: curriculum learning in machine learning, where models are trained on increasingly complex data; tempering in Markov Chain Monte Carlo (MCMC) samplers, which gradually adjusts the target distribution to improve mixing; and modern diffusion models, which use a similar concept of gradually transforming noise into structured data.","type":"content","url":"/appendix-nlp#newtons-method","position":7},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl4":"Efficient Implementation of Newton’s Method","lvl3":"Newton’s Method"},"type":"lvl4","url":"/appendix-nlp#efficient-implementation-of-newtons-method","position":8},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl4":"Efficient Implementation of Newton’s Method","lvl3":"Newton’s Method"},"content":"Note that each step of Newton’s method involves computing the inverse of a Jacobian matrix. However, a cardinal rule in numerical linear algebra is to avoid computing matrix inverses explicitly: rarely, if ever, should there be a np.lindex.inv in your code. Instead, the numerically stable and computationally efficient approach is to solve a linear system of equations at each step.\nGiven the Newton’s method iterate:\\mathbf{z}^{k+1} = \\mathbf{z}^k - [\\nabla \\mathbf{F}(\\mathbf{z}^k)]^{-1} \\mathbf{F}(\\mathbf{z}^k)\n\nWe can reformulate this as a two-step procedure:\n\nSolve the linear system: \\underbrace{[\\nabla \\mathbf{F}(\\mathbf{z}^k)]}_{\\mathbf{A}} \\Delta \\mathbf{z}^k = -\\mathbf{F}(\\mathbf{z}^k)\n\nUpdate: \\mathbf{z}^{k+1} = \\mathbf{z}^k + \\Delta \\mathbf{z}^k\n\nThe structure of the linear system in step 1 often allows for specialized solution methods. In the context of automatic differentiation, matrix-free linear solvers are particularly useful. These solvers can find a solution without explicitly forming the matrix A, requiring only the ability to evaluate matrix-vector or vector-matrix products. Typical examples of such methods include classical matrix-splitting methods (e.g., Richardson iteration) or conjugate gradient methods through \n\nsparse.linalg.cg for example. Another useful method is the Generalized Minimal Residual method (GMRES) implemented in SciPy via \n\nsparse.linalg.gmres, which is useful when facing non-symmetric and indefinite systems.\n\nBy inspecting the structure of matrix \\mathbf{A} in the specific application where the function \\mathbf{F} is the derivative of the Lagrangian, we will also uncover an important structure known as the KKT matrix. This structure will then allow us to derive a Quadratic Programming (QP) sub-problem as part of a larger iterative procedure for solving equality and inequality constrained problems via Sequential Quadratic Programming (SQP).","type":"content","url":"/appendix-nlp#efficient-implementation-of-newtons-method","position":9},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl3":"Solving Equality Constrained Programs with Newton’s Method"},"type":"lvl3","url":"/appendix-nlp#solving-equality-constrained-programs-with-newtons-method","position":10},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl3":"Solving Equality Constrained Programs with Newton’s Method"},"content":"To solve equality-constrained optimization problems using Newton’s method, we begin by recognizing that the problem reduces to finding a zero of the function \\mathbf{F}(\\mathbf{z}) = \\nabla_{\\mathbf{x}, \\boldsymbol{\\lambda}} L(\\mathbf{x}, \\boldsymbol{\\lambda}). Here, \\mathbf{F} represents the derivative of the Lagrangian function, and \\mathbf{z} = (\\mathbf{x}, \\boldsymbol{\\lambda}) combines both the primal variables \\mathbf{x} and the dual variables (Lagrange multipliers) \\boldsymbol{\\lambda}. Explicitly, we have:\\mathbf{F}(\\mathbf{z}) = \\begin{bmatrix} \\nabla_{\\mathbf{x}} L(\\mathbf{x}, \\boldsymbol{\\lambda}) \\\\ \\mathbf{h}(\\mathbf{x}) \\end{bmatrix} = \\begin{bmatrix} \\nabla f(\\mathbf{x}) + \\sum_{i=1}^m \\lambda_i \\nabla h_i(\\mathbf{x}) \\\\ \\mathbf{h}(\\mathbf{x}) \\end{bmatrix}.\n\nNewton’s method involves linearizing \\mathbf{F}(\\mathbf{z}) around the current iterate \\mathbf{z}^k = (\\mathbf{x}^k, \\boldsymbol{\\lambda}^k) and then solving the resulting linear system. At each iteration k, Newton’s method updates the current estimate by solving the linear system:\\mathbf{z}^{k+1} = \\mathbf{z}^k - [\\nabla \\mathbf{F}(\\mathbf{z}^k)]^{-1} \\mathbf{F}(\\mathbf{z}^k).\n\nHowever, instead of explicitly inverting the Jacobian matrix \\nabla \\mathbf{F}(\\mathbf{z}^k), we solve the linear system:\\underbrace{\\nabla \\mathbf{F}(\\mathbf{z}^k)}_{\\mathbf{A}} \\Delta \\mathbf{z}^k = -\\mathbf{F}(\\mathbf{z}^k),\n\nwhere \\Delta \\mathbf{z}^k = (\\Delta \\mathbf{x}^k, \\Delta \\boldsymbol{\\lambda}^k) represents the Newton step for the primal and dual variables. Substituting the expression for \\mathbf{F}(\\mathbf{z}) and its Jacobian, the system becomes:\\begin{bmatrix}\n\\nabla^2_{\\mathbf{x}\\mathbf{x}} L(\\mathbf{x}^k, \\boldsymbol{\\lambda}^k) & \\nabla \\mathbf{h}(\\mathbf{x}^k)^T \\\\\n\\nabla \\mathbf{h}(\\mathbf{x}^k) & \\mathbf{0}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\Delta \\mathbf{x}^k \\\\\n\\Delta \\boldsymbol{\\lambda}^k\n\\end{bmatrix}\n=\n-\n\\begin{bmatrix}\n\\nabla f(\\mathbf{x}^k) + \\nabla \\mathbf{h}(\\mathbf{x}^k)^T \\boldsymbol{\\lambda}^k \\\\\n\\mathbf{h}(\\mathbf{x}^k)\n\\end{bmatrix}.\n\nThe matrix on the left-hand side is known as the KKT matrix, as it stems from the Karush-Kuhn-Tucker conditions for this optimization problem\nThe solution of this system provides the updates \\Delta \\mathbf{x}^k and \\Delta \\boldsymbol{\\lambda}^k, which are then used to update the primal and dual variables:\\mathbf{x}^{k+1} = \\mathbf{x}^k + \\Delta \\mathbf{x}^k, \\quad \\boldsymbol{\\lambda}^{k+1} = \\boldsymbol{\\lambda}^k + \\Delta \\boldsymbol{\\lambda}^k.","type":"content","url":"/appendix-nlp#solving-equality-constrained-programs-with-newtons-method","position":11},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl4":"Demonstration","lvl3":"Solving Equality Constrained Programs with Newton’s Method"},"type":"lvl4","url":"/appendix-nlp#demonstration","position":12},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl4":"Demonstration","lvl3":"Solving Equality Constrained Programs with Newton’s Method"},"content":"The following code demonstates how we can implement this idea in Jax. In this demonstration, we are minimizing a quadratic objective function subject to a single equality constraint, a problem formally stated as follows:\\begin{aligned}\n\\min_{x \\in \\mathbb{R}^2} \\quad & f(x) = (x_1 - 2)^2 + (x_2 - 1)^2 \\\\\n\\text{subject to} \\quad & h(x) = x_1^2 + x_2^2 - 1 = 0\n\\end{aligned}\n\nGeometrically speaking, the constraint h(x) describes a unit circle centered at the origin. To solve this problem using the method of Lagrange multipliers, we form the Lagrangian:L(x, \\lambda) = f(x) + \\lambda h(x) = (x_1 - 2)^2 + (x_2 - 1)^2 + \\lambda(x_1^2 + x_2^2 - 1)\n\nFor this particular problem, it happens so that we can also find an analytical without even having to use Newton’s method. From the first-order optimality conditions, we obtain the following linear system of equations:\\begin{align*}\n   2(x_1 - 2) + 2\\lambda x_1 &= 0 \\\\\n   2(x_2 - 1) + 2\\lambda x_2 &= 0 \\\\\n   x_1^2 + x_2^2 - 1 &= 0\\\\\n\\end{align*}\n\nFrom the first two equations, we then get:x_1 = \\frac{2}{1 + \\lambda}, \\quad x_2 = \\frac{1}{1 + \\lambda}\n\nwhich we can substitute these into the 3rd constraint equation to obtain:(\\frac{2}{1 + \\lambda})^2 + (\\frac{1}{1 + \\lambda})^2 = 1 \\Leftrightarrow \\lambda = \\sqrt{5} - 1\n\nThis value of the Lagrange multiplier can then be backsubstituted into the above equations to obtain x_1 = \\frac{2}{\\sqrt{5}} and x_2 =  \\frac{1}{\\sqrt{5}}.\nWe can verify numerically (and visually on the following graph) that the point (2/\\sqrt{5}, 1/\\sqrt{5}) is indeed the point on the unit circle closest to (2, 1).\n\nimport jax\nimport jax.numpy as jnp\nfrom jax import grad, jit, jacfwd\nimport matplotlib.pyplot as plt\n\n# Define the objective function and constraint\ndef f(x):\n    return (x[0] - 2)**2 + (x[1] - 1)**2\n\ndef g(x):\n    return x[0]**2 + x[1]**2 - 1\n\n# Lagrangian\ndef L(x, lambda_):\n    return f(x) + lambda_ * g(x)\n\n# Gradient and Hessian of Lagrangian\ngrad_L_x = jit(grad(L, argnums=0))\ngrad_L_lambda = jit(grad(L, argnums=1))\nhess_L_xx = jit(jacfwd(grad_L_x, argnums=0))\nhess_L_xlambda = jit(jacfwd(grad_L_x, argnums=1))\n\n# Newton's method\n@jit\ndef newton_step(x, lambda_):\n    grad_x = grad_L_x(x, lambda_)\n    grad_lambda = grad_L_lambda(x, lambda_)\n    hess_xx = hess_L_xx(x, lambda_)\n    hess_xlambda = hess_L_xlambda(x, lambda_).reshape(-1)\n    \n    # Construct the full KKT matrix\n    kkt_matrix = jnp.block([\n        [hess_xx, hess_xlambda.reshape(-1, 1)],\n        [hess_xlambda, jnp.array([[0.0]])]\n    ])\n    \n    # Construct the right-hand side\n    rhs = jnp.concatenate([-grad_x, -jnp.array([grad_lambda])])\n    \n    # Solve the KKT system\n    delta = jnp.linalg.solve(kkt_matrix, rhs)\n    \n    return x + delta[:2], lambda_ + delta[2]\n\ndef solve_constrained_optimization(x0, lambda0, max_iter=100, tol=1e-6):\n    x, lambda_ = x0, lambda0\n    \n    for i in range(max_iter):\n        x_new, lambda_new = newton_step(x, lambda_)\n        if jnp.linalg.norm(jnp.concatenate([x_new - x, jnp.array([lambda_new - lambda_])])) < tol:\n            break\n        x, lambda_ = x_new, lambda_new\n    \n    return x, lambda_, i+1\n\n# Analytical solution\ndef analytical_solution():\n    x1 = 2 / jnp.sqrt(5)\n    x2 = 1 / jnp.sqrt(5)\n    lambda_opt = jnp.sqrt(5) - 1\n    return jnp.array([x1, x2]), lambda_opt\n\n# Solve the problem numerically\nx0 = jnp.array([0.5, 0.5])\nlambda0 = 0.0\nx_opt_num, lambda_opt_num, iterations = solve_constrained_optimization(x0, lambda0)\n\n# Compute analytical solution\nx_opt_ana, lambda_opt_ana = analytical_solution()\n\n# Verify the result\nprint(\"\\nNumerical Solution:\")\nprint(f\"Constraint violation: {g(x_opt_num):.6f}\")\nprint(f\"Objective function value: {f(x_opt_num):.6f}\")\n\nprint(\"\\nAnalytical Solution:\")\nprint(f\"Constraint violation: {g(x_opt_ana):.6f}\")\nprint(f\"Objective function value: {f(x_opt_ana):.6f}\")\n\nprint(\"\\nComparison:\")\nx_diff = jnp.linalg.norm(x_opt_num - x_opt_ana)\nlambda_diff = jnp.abs(lambda_opt_num - lambda_opt_ana)\nprint(f\"Difference in x: {x_diff}\")\nprint(f\"Difference in lambda: {lambda_diff}\")\n\n# Precision test\nrtol = 1e-5  # relative tolerance\natol = 1e-8  # absolute tolerance\n\nx_close = jnp.allclose(x_opt_num, x_opt_ana, rtol=rtol, atol=atol)\nlambda_close = jnp.isclose(lambda_opt_num, lambda_opt_ana, rtol=rtol, atol=atol)\n\nprint(\"\\nPrecision Test:\")\nprint(f\"x values are close: {x_close}\")\nprint(f\"lambda values are close: {lambda_close}\")\n\nif x_close and lambda_close:\n    print(\"The numerical solution matches the analytical solution within the specified tolerance.\")\nelse:\n    print(\"The numerical solution differs from the analytical solution more than the specified tolerance.\")\n\n# Visualize the result\nplt.figure(figsize=(12, 10))\n\n# Create a mesh for the contour plot\nx1_range = jnp.linspace(-1.5, 2.5, 100)\nx2_range = jnp.linspace(-1.5, 2.5, 100)\nX1, X2 = jnp.meshgrid(x1_range, x2_range)\nZ = jnp.array([[f(jnp.array([x1, x2])) for x1 in x1_range] for x2 in x2_range])\n\n# Plot filled contours\ncontour = plt.contourf(X1, X2, Z, levels=50, cmap='viridis', alpha=0.7, extent=[-1.5, 2.5, -1.5, 2.5])\nplt.colorbar(contour, label='Objective Function Value')\n\n# Plot the constraint\ntheta = jnp.linspace(0, 2*jnp.pi, 100)\nx1 = jnp.cos(theta)\nx2 = jnp.sin(theta)\nplt.plot(x1, x2, color='red', linewidth=2, label='Constraint')\n\n# Plot the optimal points (numerical and analytical) and initial point\nplt.scatter(x_opt_num[0], x_opt_num[1], color='red', s=100, edgecolor='white', linewidth=2, label='Numerical Optimal Point')\nplt.scatter(x_opt_ana[0], x_opt_ana[1], color='blue', s=100, edgecolor='white', linewidth=2, label='Analytical Optimal Point')\nplt.scatter(x0[0], x0[1], color='green', s=100, edgecolor='white', linewidth=2, label='Initial Point')\n\n# Add labels and title\nplt.xlabel('x1', fontsize=12)\nplt.ylabel('x2', fontsize=12)\nplt.title('Constrained Optimization: Numerical vs Analytical Solution', fontsize=14)\nplt.legend(fontsize=10)\nplt.grid(True, linestyle='--', alpha=0.7)\n\n# Set the axis limits explicitly\nplt.xlim(-1.5, 2.5)\nplt.ylim(-1.5, 2.5)\n\nplt.tight_layout()\nplt.show()\n\n","type":"content","url":"/appendix-nlp#demonstration","position":13},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl2":"The SQP Approach: Taylor Expansion and Quadratic Approximation"},"type":"lvl2","url":"/appendix-nlp#the-sqp-approach-taylor-expansion-and-quadratic-approximation","position":14},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl2":"The SQP Approach: Taylor Expansion and Quadratic Approximation"},"content":"Sequential Quadratic Programming (SQP) tackles the problem of solving constrained programs by iteratively solving a sequence of simpler subproblems. Specifically, these subproblems are quadratic programs (QPs) that approximate the original problem around the current iterate by using a quadratic model of the objective function and a linear model of the constraints. Suppose we have the following optimization problem with equality constraints:\\begin{aligned}\n\\min_{\\mathbf{x}} \\quad & f(\\mathbf{x}) \\\\\n\\text{subject to} \\quad & \\mathbf{h}(\\mathbf{x}) = \\mathbf{0}.\n\\end{aligned}\n\nAt each iteration k, we approximate the objective function f(\\mathbf{x}) using a second-order Taylor expansion around the current iterate \\mathbf{x}^k. The standard Taylor expansion for f would be:\\begin{align*}\nf(\\mathbf{x}) \\approx f(\\mathbf{x}^k) + \\nabla f(\\mathbf{x}^k)^T (\\mathbf{x} - \\mathbf{x}^k) + \\frac{1}{2} (\\mathbf{x} - \\mathbf{x}^k)^T \\nabla^2 f(\\mathbf{x}^k) (\\mathbf{x} - \\mathbf{x}^k).\n\\end{align*}\n\nThis expansion uses the Hessian of the objective function \\nabla^2 f(\\mathbf{x}^k) to capture the curvature of f. However, in the context of constrained optimization, we also need to account for the effect of the constraints on the local behavior of the solution. If we were to use only \\nabla^2 f(\\mathbf{x}^k), we would not capture the influence of the constraints on the curvature of the feasible region. The resulting subproblem might then lead to steps that violate the constraints or are less effective in achieving convergence. The choice that we make instead is to use the Hessian of the Lagrangian, \\nabla^2_{\\mathbf{x}\\mathbf{x}} L(\\mathbf{x}^k, \\boldsymbol{\\lambda}^k), leading to the following quadratic model:f(\\mathbf{x}) \\approx f(\\mathbf{x}^k) + \\nabla f(\\mathbf{x}^k)^T (\\mathbf{x} - \\mathbf{x}^k) + \\frac{1}{2} (\\mathbf{x} - \\mathbf{x}^k)^T \\nabla^2_{\\mathbf{x}\\mathbf{x}} L(\\mathbf{x}^k, \\boldsymbol{\\lambda}^k) (\\mathbf{x} - \\mathbf{x}^k).\n\nSimilarly, the equality constraints \\mathbf{h}(\\mathbf{x}) are linearized around \\mathbf{x}^k:\\mathbf{h}(\\mathbf{x}) \\approx \\mathbf{h}(\\mathbf{x}^k) + \\nabla \\mathbf{h}(\\mathbf{x}^k) (\\mathbf{x} - \\mathbf{x}^k).\n\nCombining these approximations, we obtain a Quadratic Programming (QP) subproblem, which approximates our original problem locally at \\mathbf{x}^k but is easier to solve:\\begin{aligned}\n\\text{Minimize} \\quad & \\nabla f(\\mathbf{x}^k)^T \\Delta \\mathbf{x} + \\frac{1}{2} \\Delta \\mathbf{x}^T \\nabla^2_{\\mathbf{x}\\mathbf{x}} L(\\mathbf{x}^k, \\boldsymbol{\\lambda}^k) \\Delta \\mathbf{x} \\\\\n\\text{subject to} \\quad & \\nabla \\mathbf{h}(\\mathbf{x}^k) \\Delta \\mathbf{x} + \\mathbf{h}(\\mathbf{x}^k) = \\mathbf{0},\n\\end{aligned}\n\nwhere \\Delta \\mathbf{x} = \\mathbf{x} - \\mathbf{x}^k. The QP subproblem solved at each iteration focuses on finding the optimal step direction \\Delta \\mathbf{x} for the primal variables.\nWhile solving this QP, we obtain not only the step \\Delta \\mathbf{x} but also the associated Lagrange multipliers for the QP subproblem, which correspond to an updated dual variable vector \\boldsymbol{\\lambda}^{k+1}. More specifically, after solving the QP, we use \\Delta \\mathbf{x}^k to update the primal variables:\\begin{align*}\n\\mathbf{x}^{k+1} = \\mathbf{x}^k + \\Delta \\mathbf{x}^k.\n\\end{align*}\n\nSimultaneously, the Lagrange multipliers from the QP provide the updated dual variables \\boldsymbol{\\lambda}^{k+1}.\nWe summarize the SQP algorithm in the following pseudo-code:\n\nSequential Quadratic Programming (SQP)\n\nInput: Initial estimate \\mathbf{x}^0, initial Lagrange multipliers \\boldsymbol{\\lambda}^0, tolerance \\epsilon > 0.\n\nOutput: Solution \\mathbf{x}^*, Lagrange multipliers \\boldsymbol{\\lambda}^*.\n\nProcedure:\n\nCompute the QP Solution: Solve the QP subproblem to obtain \\Delta \\mathbf{x}^k. The QP solver also provides the updated Lagrange multipliers \\boldsymbol{\\lambda}^{k+1} associated with the constraints.\n\nUpdate the Estimates: Update the primal variables:\\mathbf{x}^{k+1} = \\mathbf{x}^k + \\Delta \\mathbf{x}^k.\n\nSet the dual variables to the updated values \\boldsymbol{\\lambda}^{k+1} from the QP solution.\n\nRepeat Until Convergence: Continue iterating until \\|\\Delta \\mathbf{x}^k\\| < \\epsilon and the KKT conditions are satisfied.","type":"content","url":"/appendix-nlp#the-sqp-approach-taylor-expansion-and-quadratic-approximation","position":15},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl3":"Connection to Newton’s Method in the Equality-Constrained Case","lvl2":"The SQP Approach: Taylor Expansion and Quadratic Approximation"},"type":"lvl3","url":"/appendix-nlp#connection-to-newtons-method-in-the-equality-constrained-case","position":16},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl3":"Connection to Newton’s Method in the Equality-Constrained Case","lvl2":"The SQP Approach: Taylor Expansion and Quadratic Approximation"},"content":"The QP subproblem in SQP is directly related to applying Newton’s method for equality-constrained optimization. To see this, note that the KKT matrix of the QP subproblem is:\\begin{align*}\n\\begin{bmatrix}\n\\nabla^2_{\\mathbf{x}\\mathbf{x}} L(\\mathbf{x}^k, \\boldsymbol{\\lambda}^k) & \\nabla \\mathbf{h}(\\mathbf{x}^k)^T \\\\\n\\nabla \\mathbf{h}(\\mathbf{x}^k) & \\mathbf{0}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\Delta \\mathbf{x}^k \\\\\n\\Delta \\boldsymbol{\\lambda}^k\n\\end{bmatrix}\n=\n-\n\\begin{bmatrix}\n\\nabla f(\\mathbf{x}^k) + \\nabla \\mathbf{h}(\\mathbf{x}^k)^T \\boldsymbol{\\lambda}^k \\\\\n\\mathbf{h}(\\mathbf{x}^k)\n\\end{bmatrix}\n\\end{align*}\n\nThis is exactly the same linear system that have to solve when applying Newton’s method to the KKT conditions of the original program! Thus, solving the QP subproblem at each iteration of SQP is equivalent to taking a Newton step on the KKT conditions of the original nonlinear problem.","type":"content","url":"/appendix-nlp#connection-to-newtons-method-in-the-equality-constrained-case","position":17},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl2":"SQP for Inequality-Constrained Optimization"},"type":"lvl2","url":"/appendix-nlp#sqp-for-inequality-constrained-optimization","position":18},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl2":"SQP for Inequality-Constrained Optimization"},"content":"So far, we’ve applied the ideas behind Sequential Quadratic Programming (SQP) to problems with only equality constraints. Now, let’s extend this framework to handle optimization problems that also include inequality constraints.\nConsider a general nonlinear optimization problem that includes both equality and inequality constraints:\\begin{align*}\n\\min_{\\mathbf{x}} \\quad & f(\\mathbf{x}) \\\\\n\\text{subject to} \\quad & \\mathbf{g}(\\mathbf{x}) \\leq \\mathbf{0}, \\\\\n& \\mathbf{h}(\\mathbf{x}) = \\mathbf{0}.\n\\end{align*}\n\nAs we did earlier, we approximate this problem by constructing a quadratic approximation to the objective and a linearization of the constraints. QP subproblem at each iteration is then formulated as:\\begin{align*}\n\\text{Minimize} \\quad & \\nabla f(\\mathbf{x}^k)^T \\Delta \\mathbf{x} + \\frac{1}{2} \\Delta \\mathbf{x}^T \\nabla^2_{\\mathbf{x}\\mathbf{x}} L(\\mathbf{x}^k, \\boldsymbol{\\lambda}^k, \\boldsymbol{\\nu}^k) \\Delta \\mathbf{x} \\\\\n\\text{subject to} \\quad & \\nabla \\mathbf{g}(\\mathbf{x}^k) \\Delta \\mathbf{x} + \\mathbf{g}(\\mathbf{x}^k) \\leq \\mathbf{0}, \\\\\n& \\nabla \\mathbf{h}(\\mathbf{x}^k) \\Delta \\mathbf{x} + \\mathbf{h}(\\mathbf{x}^k) = \\mathbf{0},\n\\end{align*}\n\nwhere \\Delta \\mathbf{x} = \\mathbf{x} - \\mathbf{x}^k represents the step direction for the primal variables. The following pseudocode outlines the steps involved in applying SQP to a problem with both equality and inequality constraints:\n\nSequential Quadratic Programming (SQP) with Inequality Constraints\n\nInput: Initial estimate \\mathbf{x}^0, initial multipliers \\boldsymbol{\\lambda}^0, \\boldsymbol{\\nu}^0, tolerance \\epsilon > 0.\n\nOutput: Solution \\mathbf{x}^*, Lagrange multipliers \\boldsymbol{\\lambda}^*, \\boldsymbol{\\nu}^*.\n\nProcedure:\n\nInitialization:\nSet k = 0.\n\nRepeat:\n\na. Construct the QP Subproblem:\nFormulate the QP subproblem using the current iterate \\mathbf{x}^k, \\boldsymbol{\\lambda}^k, and \\boldsymbol{\\nu}^k:\\begin{aligned}\n   \\text{Minimize} \\quad & \\nabla f(\\mathbf{x}^k)^T \\Delta \\mathbf{x} + \\frac{1}{2} \\Delta \\mathbf{x}^T \\nabla^2_{\\mathbf{x}\\mathbf{x}} L(\\mathbf{x}^k, \\boldsymbol{\\lambda}^k, \\boldsymbol{\\nu}^k) \\Delta \\mathbf{x} \\\\\n   \\text{subject to} \\quad & \\nabla \\mathbf{g}(\\mathbf{x}^k) \\Delta \\mathbf{x} + \\mathbf{g}(\\mathbf{x}^k) \\leq \\mathbf{0}, \\\\\n   & \\nabla \\mathbf{h}(\\mathbf{x}^k) \\Delta \\mathbf{x} + \\mathbf{h}(\\mathbf{x}^k) = \\mathbf{0}.\n   \\end{aligned}\n\nb. Solve the QP Subproblem:\nSolve for \\Delta \\mathbf{x}^k and obtain the updated Lagrange multipliers \\boldsymbol{\\lambda}^{k+1} and \\boldsymbol{\\nu}^{k+1}.\n\nc. Update the Estimates:\nUpdate the primal variables and multipliers:\\mathbf{x}^{k+1} = \\mathbf{x}^k + \\Delta \\mathbf{x}^k.\n\nd. Check for Convergence:\nIf \\|\\Delta \\mathbf{x}^k\\| < \\epsilon and the KKT conditions are satisfied, stop. Otherwise, set k = k + 1 and repeat.\n\nReturn:\n\\mathbf{x}^* = \\mathbf{x}^{k+1}, \\boldsymbol{\\lambda}^* = \\boldsymbol{\\lambda}^{k+1}, \\boldsymbol{\\nu}^* = \\boldsymbol{\\nu}^{k+1}.","type":"content","url":"/appendix-nlp#sqp-for-inequality-constrained-optimization","position":19},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl3":"Demonstration with JAX and CVXPy","lvl2":"SQP for Inequality-Constrained Optimization"},"type":"lvl3","url":"/appendix-nlp#demonstration-with-jax-and-cvxpy","position":20},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl3":"Demonstration with JAX and CVXPy","lvl2":"SQP for Inequality-Constrained Optimization"},"content":"Consider the following equality and inequality-constrained problem:\\begin{align*}\n\\min_{x \\in \\mathbb{R}^2} \\quad & f(x) = (x_1 - 2)^2 + (x_2 - 1)^2 \\\\\n\\text{subject to} \\quad & g(x) = x_1^2 - x_2 \\leq 0  \\\\\n& h(x) = x_1^2 + x_2^2 - 1 = 0\n\\end{align*}\n\nThis example builds on our previous one but adds a parabola-shaped inequality constraint. We require our solution to lie not only on the circle defining our equality constraint but also below the parabola. To solve the QP subproblem, we will be using the \n\nCVXPY package. While the Lagrangian and derivatives could be computed easily by hand, we use \n\nJAX for generality:\n\nimport jax\nimport jax.numpy as jnp\nfrom jax import grad, jit, jacfwd, hessian\nimport numpy as np\nimport cvxpy as cp\nimport matplotlib.pyplot as plt\n\n# Define the objective function and constraints\n@jit\ndef f(x):\n    return (x[0] - 2)**2 + (x[1] - 1)**2\n\n@jit\ndef g(x):\n    return jnp.array([x[0]**2 + x[1]**2 - 1])\n\n@jit\ndef h(x):\n    return jnp.array([x[0]**2 - x[1]])  # Corrected inequality constraint: x[1] <= x[0]^2\n\n# Compute gradients and Jacobians using JAX\ngrad_f = jit(grad(f))\nhess_f = jit(hessian(f))\njac_g = jit(jacfwd(g))\njac_h = jit(jacfwd(h))\n\n@jit\ndef lagrangian(x, lambda_, nu):\n    return f(x) + jnp.dot(lambda_, g(x)) + jnp.dot(nu, h(x))\n\nhess_L = jit(hessian(lagrangian, argnums=0))\n\ndef solve_qp_subproblem(x, lambda_, nu):\n    n = len(x)\n    delta_x = cp.Variable(n)\n    \n    # Convert JAX arrays to numpy for cvxpy\n    grad_f_np = np.array(grad_f(x))\n    hess_L_np = np.array(hess_L(x, lambda_, nu))\n    jac_g_np = np.array(jac_g(x))\n    jac_h_np = np.array(jac_h(x))\n    g_np = np.array(g(x))\n    h_np = np.array(h(x))\n    \n    obj = cp.Minimize(grad_f_np.T @ delta_x + 0.5 * cp.quad_form(delta_x, hess_L_np))\n    \n    constraints = [\n        jac_g_np @ delta_x + g_np == 0,\n        jac_h_np @ delta_x + h_np <= 0\n    ]\n    \n    prob = cp.Problem(obj, constraints)\n    prob.solve()\n    \n    return delta_x.value, prob.constraints[0].dual_value, prob.constraints[1].dual_value\n\ndef sqp(x0, max_iter=100, tol=1e-6):\n    x = x0\n    lambda_ = jnp.zeros(1)\n    nu = jnp.zeros(1)\n    \n    for i in range(max_iter):\n        delta_x, new_lambda, new_nu = solve_qp_subproblem(x, lambda_, nu)\n        \n        if jnp.linalg.norm(delta_x) < tol:\n            break\n        \n        x = x + delta_x\n        lambda_ = new_lambda\n        nu = new_nu\n        \n    return x, lambda_, nu, i+1\n\n# Initial point\nx0 = jnp.array([0.5, 0.5])\n\n# Solve using SQP\nx_opt, lambda_opt, nu_opt, iterations = sqp(x0)\n\nprint(f\"Optimal x: {x_opt}\")\nprint(f\"Optimal lambda: {lambda_opt}\")\nprint(f\"Optimal nu: {nu_opt}\")\nprint(f\"Iterations: {iterations}\")\n\n# Visualize the result\nplt.figure(figsize=(12, 10))\n\n# Create a mesh for the contour plot\nx1_range = jnp.linspace(-1.5, 2.5, 100)\nx2_range = jnp.linspace(-1.5, 2.5, 100)\nX1, X2 = jnp.meshgrid(x1_range, x2_range)\nZ = jnp.array([[f(jnp.array([x1, x2])) for x1 in x1_range] for x2 in x2_range])\n\n# Plot filled contours\ncontour = plt.contourf(X1, X2, Z, levels=50, cmap='viridis', alpha=0.7)\nplt.colorbar(contour, label='Objective Function Value')\n\n# Plot the equality constraint\ntheta = jnp.linspace(0, 2*jnp.pi, 100)\nx1_eq = jnp.cos(theta)\nx2_eq = jnp.sin(theta)\nplt.plot(x1_eq, x2_eq, color='red', linewidth=2, label='Equality Constraint')\n\n# Plot the inequality constraint and shade the feasible region\nx1_ineq = jnp.linspace(-1.5, 2.5, 100)\nx2_ineq = x1_ineq**2\nplt.plot(x1_ineq, x2_ineq, color='orange', linewidth=2, label='Inequality Constraint')\n\n# Shade the feasible region for the inequality constraint\nx2_lower = jnp.minimum(x2_ineq, 2.5)\nplt.fill_between(x1_ineq, -1.5, x2_lower, color='gray', alpha=0.2, hatch='\\\\/...', label='Feasible Region')\n\n# Plot the optimal and initial points\nplt.scatter(x_opt[0], x_opt[1], color='red', s=100, edgecolor='white', linewidth=2, label='Optimal Point')\nplt.scatter(x0[0], x0[1], color='green', s=100, edgecolor='white', linewidth=2, label='Initial Point')\n\n# Add labels and title\nplt.xlabel('x1', fontsize=12)\nplt.ylabel('x2', fontsize=12)\nplt.title('SQP for Inequality Constraints with CVXPY and JAX', fontsize=14)\nplt.legend(fontsize=10, loc='upper center')\nplt.grid(True, linestyle='--', alpha=0.7)\n\n# Set the axis limits explicitly\nplt.xlim(-1.5, 2.5)\nplt.ylim(-1.5, 2.5)\n\nplt.tight_layout()\nplt.show()\n\n# Verify the result\nprint(f\"\\nEquality constraint violation: {g(x_opt)[0]:.6f}\")\nprint(f\"Inequality constraint violation: {h(x_opt)[0]:.6f}\")\nprint(f\"Objective function value: {f(x_opt):.6f}\")\n\n","type":"content","url":"/appendix-nlp#demonstration-with-jax-and-cvxpy","position":21},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl2":"The Arrow-Hurwicz-Uzawa algorithm"},"type":"lvl2","url":"/appendix-nlp#the-arrow-hurwicz-uzawa-algorithm","position":22},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl2":"The Arrow-Hurwicz-Uzawa algorithm"},"content":"While the SQP method addresses constrained optimization problems by sequentially solving quadratic subproblems, an alternative approach emerges from viewing constrained optimization as a min-max problem. This perspective leads to a simpler algorithm, originally introduced by the Arrow-Hurwicz-Uzawa \n\nArrow et al. (1958). Consider the following general constrained optimization problem encompassing both equality and inequality constraints:\\begin{aligned}\n\\min_{\\mathbf{x}} \\quad & f(\\mathbf{x}) \\\\\n\\text{subject to} \\quad & \\mathbf{g}(\\mathbf{x}) \\leq \\mathbf{0} \\\\\n& \\mathbf{h}(\\mathbf{x}) = \\mathbf{0}\n\\end{aligned}\n\nUsing the Lagrangian function L(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu}) = f(\\mathbf{x}) + \\boldsymbol{\\mu}^T \\mathbf{g}(\\mathbf{x}) + \\boldsymbol{\\lambda}^T \\mathbf{h}(\\mathbf{x}), we can reformulate this problem as the following min-max problem:\\min_{\\mathbf{x}} \\max_{\\boldsymbol{\\lambda}, \\boldsymbol{\\mu} \\geq 0} L(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu})\n\nThe role of each component in this min-max structure can be understood as follows:\n\nThe outer minimization over \\mathbf{x} finds the feasible point that minimizes the objective function f(\\mathbf{x}).\n\nThe maximization over \\boldsymbol{\\mu} \\geq 0 ensures that inequality constraints \\mathbf{g}(\\mathbf{x}) \\leq \\mathbf{0} are satisfied. If any inequality constraint is violated, the corresponding term in \\boldsymbol{\\mu}^T \\mathbf{g}(\\mathbf{x}) can be made arbitrarily large by choosing a large enough \\mu_i.\n\nThe maximization over \\boldsymbol{\\lambda} ensures that equality constraints \\mathbf{h}(\\mathbf{x}) = \\mathbf{0} are satisfied.\n\nUsing this observation, we can devise an algorithm which, like SQP, will update both the primal and dual variables at every step. But rather than using second-order optimization, we will simply use a first-order gradient update step: a descent step in the primal variable, and an ascent step in the dual one. The corresponding procedure, when implemented by gradient descent, is called Gradient Ascent Descent in the learning and optimization communities. In the case of equality constraints only, the algorithm looks like the following:\n\nArrow-Hurwicz-Uzawa for equality constraints only\n\nInput: Initial guess \\mathbf{x}^0, \\boldsymbol{\\lambda}^0, step sizes \\alpha, \\beta\nOutput: Optimal \\mathbf{x}^*, \\boldsymbol{\\lambda}^*\n\n1: for k = 0, 1, 2, \\ldots until convergence do\n\n2:     \\mathbf{x}^{k+1} = \\mathbf{x}^k - \\alpha \\nabla_{\\mathbf{x}} L(\\mathbf{x}^k, \\boldsymbol{\\lambda}^k)  (Primal update)\n\n3:     \\boldsymbol{\\lambda}^{k+1} = \\boldsymbol{\\lambda}^k + \\beta \\nabla_{\\boldsymbol{\\lambda}} L(\\mathbf{x}^{k+1}, \\boldsymbol{\\lambda}^k)  (Dual update)\n\n4: end for\n\n5: return \\mathbf{x}^k, \\boldsymbol{\\lambda}^k\n\nNow to account for the fact that the Lagrange multiplier needs to be non-negative for inequality constraints, we can use our previous idea from projected gradient descent for bound constraints and consider a projection, or clipping step to ensure that this condition is satisfied throughout. In this case, the algorithm looks like the following:\n\nArrow-Hurwicz-Uzawa for equality and inequality constraints\n\nInput: Initial guess \\mathbf{x}^0, \\boldsymbol{\\lambda}^0, \\boldsymbol{\\mu}^0 \\geq 0, step sizes \\alpha, \\beta, \\gamma\nOutput: Optimal \\mathbf{x}^*, \\boldsymbol{\\lambda}^*, \\boldsymbol{\\mu}^*\n\n1: for k = 0, 1, 2, \\ldots until convergence do\n\n2:     \\mathbf{x}^{k+1} = \\mathbf{x}^k - \\alpha \\nabla_{\\mathbf{x}} L(\\mathbf{x}^k, \\boldsymbol{\\lambda}^k, \\boldsymbol{\\mu}^k)  (Primal update)\n\n3:     \\boldsymbol{\\lambda}^{k+1} = \\boldsymbol{\\lambda}^k + \\beta \\, \\nabla_{\\boldsymbol{\\lambda}} L(\\mathbf{x}^{k+1}, \\boldsymbol{\\lambda}^k, \\boldsymbol{\\mu}^k)  (Dual update for equality constraints)\n\n4:     \\boldsymbol{\\mu}^{k+1} = [\\boldsymbol{\\mu}^k + \\gamma \\nabla_{\\boldsymbol{\\mu}} L(\\mathbf{x}^{k+1}, \\boldsymbol{\\lambda}^k, \\boldsymbol{\\mu}^k)]_+  (Dual update with clipping for inequality constraints)\n\n5: end for\n\n6: return \\mathbf{x}^k, \\boldsymbol{\\lambda}^k, \\boldsymbol{\\mu}^k\n\nHere, [\\cdot]_+ denotes the projection onto the non-negative orthant, ensuring that \\boldsymbol{\\mu} remains non-negative.\n\nHowever, as it is widely known from the lessons of GAN (Generative Adversarial Network) training \n\nGoodfellow et al. (2014), Gradient Descent Ascent (GDA) can fail to converge or suffer from instability. The Arrow-Hurwicz-Uzawa algorithm, also known as the first-order Lagrangian method, is known to converge only locally, in the vicinity of an optimal primal-dual pair.\n\nimport jax\nimport jax.numpy as jnp\nfrom jax import grad, jit, value_and_grad\nimport optax\nimport matplotlib.pyplot as plt\n\n# Define the objective function and constraints\n@jit\ndef f(x):\n    return (x[0] - 2)**2 + (x[1] - 1)**2\n\n@jit\ndef g(x):\n    return jnp.array([x[0]**2 + x[1]**2 - 1])\n\n@jit\ndef h(x):\n    return jnp.array([x[0]**2 - x[1]])  # Inequality constraint: x[1] <= x[0]^2\n\n# Define the Lagrangian\n@jit\ndef lagrangian(x, lambda_, mu):\n    return f(x) + jnp.dot(lambda_, g(x)) + jnp.dot(mu, h(x))\n\n# Compute gradients of the Lagrangian\ngrad_L_x = jit(grad(lagrangian, argnums=0))\ngrad_L_lambda = jit(grad(lagrangian, argnums=1))\ngrad_L_mu = jit(grad(lagrangian, argnums=2))\n\n# Define the Arrow-Hurwicz-Uzawa update step\n@jit\ndef update(carry, t):\n    x, lambda_, mu, opt_state_x, opt_state_lambda, opt_state_mu = carry\n    \n    # Compute gradients\n    grad_x = grad_L_x(x, lambda_, mu)\n    grad_lambda = grad_L_lambda(x, lambda_, mu)\n    grad_mu = grad_L_mu(x, lambda_, mu)\n    \n    # Update primal variables (minimization)\n    updates_x, opt_state_x = optimizer_x.update(grad_x, opt_state_x)\n    x = optax.apply_updates(x, updates_x)\n    \n    # Update dual variables (maximization)\n    updates_lambda, opt_state_lambda = optimizer_lambda.update(grad_lambda, opt_state_lambda)\n    lambda_ = optax.apply_updates(lambda_, -updates_lambda)  # Positive update for maximization\n    \n    updates_mu, opt_state_mu = optimizer_mu.update(grad_mu, opt_state_mu)\n    mu = optax.apply_updates(mu, -updates_mu)  # Positive update for maximization\n    \n    # Project mu onto the non-negative orthant\n    mu = jnp.maximum(mu, 0)\n    \n    return (x, lambda_, mu, opt_state_x, opt_state_lambda, opt_state_mu), x\n\ndef arrow_hurwicz_uzawa(x0, lambda0, mu0, max_iter=1000):\n    # Initialize optimizers\n    global optimizer_x, optimizer_lambda, optimizer_mu\n    optimizer_x = optax.adam(learning_rate=0.01)\n    optimizer_lambda = optax.adam(learning_rate=0.01)\n    optimizer_mu = optax.adam(learning_rate=0.01)\n    \n    opt_state_x = optimizer_x.init(x0)\n    opt_state_lambda = optimizer_lambda.init(lambda0)\n    opt_state_mu = optimizer_mu.init(mu0)\n    \n    init_carry = (x0, lambda0, mu0, opt_state_x, opt_state_lambda, opt_state_mu)\n    \n    # Use jax.lax.scan for the optimization loop\n    (x, lambda_, mu, _, _, _), trajectory = jax.lax.scan(update, init_carry, jnp.arange(max_iter))\n    \n    return x, lambda_, mu, trajectory\n\n# Initial point\nx0 = jnp.array([0.5, 0.5])\nlambda0 = jnp.zeros(1)\nmu0 = jnp.zeros(1)\n\n# Solve using Arrow-Hurwicz-Uzawa\nx_opt, lambda_opt, mu_opt, trajectory = arrow_hurwicz_uzawa(x0, lambda0, mu0, max_iter=1000)\n\nprint(f\"Final x: {x_opt}\")\nprint(f\"Final lambda: {lambda_opt}\")\nprint(f\"Final mu: {mu_opt}\")\n\n# Visualize the result\nplt.figure(figsize=(12, 10))\n\n# Create a mesh for the contour plot\nx1_range = jnp.linspace(-1.5, 2.5, 100)\nx2_range = jnp.linspace(-1.5, 2.5, 100)\nX1, X2 = jnp.meshgrid(x1_range, x2_range)\nZ = jnp.array([[f(jnp.array([x1, x2])) for x1 in x1_range] for x2 in x2_range])\n\n# Plot filled contours\ncontour = plt.contourf(X1, X2, Z, levels=50, cmap='viridis', alpha=0.7)\nplt.colorbar(contour, label='Objective Function Value')\n\n# Plot the equality constraint\ntheta = jnp.linspace(0, 2*jnp.pi, 100)\nx1_eq = jnp.cos(theta)\nx2_eq = jnp.sin(theta)\nplt.plot(x1_eq, x2_eq, color='red', linewidth=2, label='Equality Constraint')\n\n# Plot the inequality constraint and shade the feasible region\nx1_ineq = jnp.linspace(-1.5, 2.5, 100)\nx2_ineq = x1_ineq**2\nplt.plot(x1_ineq, x2_ineq, color='orange', linewidth=2, label='Inequality Constraint')\n\n# Shade the feasible region for the inequality constraint\nx2_lower = jnp.minimum(x2_ineq, 2.5)\nplt.fill_between(x1_ineq, -1.5, x2_lower, color='gray', alpha=0.2, hatch='\\\\/...', label='Feasible Region')\n\n# Plot the optimal and initial points\nplt.scatter(x_opt[0], x_opt[1], color='red', s=100, edgecolor='white', linewidth=2, label='Final Point')\nplt.scatter(x0[0], x0[1], color='green', s=100, edgecolor='white', linewidth=2, label='Initial Point')\n\n# Plot the optimization trajectory using scatter plot\nscatter = plt.scatter(trajectory[:, 0], trajectory[:, 1], c=jnp.arange(len(trajectory)), \n                      cmap='cool', s=10, alpha=0.5)\nplt.colorbar(scatter, label='Iteration')\n\n# Add labels and title\nplt.xlabel('x1', fontsize=12)\nplt.ylabel('x2', fontsize=12)\nplt.title('Arrow-Hurwicz-Uzawa Algorithm with JAX and Adam (Corrected Min/Max)', fontsize=14)\nplt.legend(fontsize=10, loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=3)\nplt.grid(True, linestyle='--', alpha=0.7)\n\n# Set the axis limits explicitly\nplt.xlim(-1.5, 2.5)\nplt.ylim(-1.5, 2.5)\n\nplt.tight_layout()\nplt.show()\n\n\n# Verify the result\nprint(f\"\\nEquality constraint violation: {g(x_opt)[0]:.6f}\")\nprint(f\"Inequality constraint violation: {h(x_opt)[0]:.6f}\")\nprint(f\"Objective function value: {f(x_opt):.6f}\")\n\n","type":"content","url":"/appendix-nlp#the-arrow-hurwicz-uzawa-algorithm","position":23},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl2":"Projected Gradient Descent"},"type":"lvl2","url":"/appendix-nlp#projected-gradient-descent","position":24},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl2":"Projected Gradient Descent"},"content":"The Arrow-Hurwicz-Uzawa algorithm provided a way to handle constraints through dual variables and a primal-dual update scheme. Another commonly used approach for constrained optimization is Projected Gradient Descent (PGD). The idea is simple: take a gradient descent step as if the problem were unconstrained, then project the result back onto the feasible set. Formally:\\mathbf{x}_{k+1} = \\mathcal{P}_C\\big(\\mathbf{x}_k - \\alpha \\nabla f(\\mathbf{x}_k)\\big),\n\nwhere \\mathcal{P}_C is the projection onto the feasible set C, \\alpha is the step size, and f(\\mathbf{x}) is the objective function.\n\nPGD is particularly effective when the projection is computationally cheap. A common example is box constraints (or bound constraints), where the feasible set is a hyperrectangle:C = \\{ \\mathbf{x} \\mid \\mathbf{x}_{\\mathrm{lb}} \\leq \\mathbf{x} \\leq \\mathbf{x}_{\\mathrm{ub}} \\}.\n\nIn this case, the projection reduces to an element-wise clipping operation:[\\mathcal{P}_C(\\mathbf{x})]_i = \\max\\big(\\min([\\mathbf{x}]_i, [\\mathbf{x}_{\\mathrm{ub}}]_i), [\\mathbf{x}_{\\mathrm{lb}}]_i\\big).\n\nFor bound-constrained problems, PGD is almost as easy to implement as standard gradient descent because the projection step is just a clipping operation. For more general constraints, however, the projection may require solving a separate optimization problem, which can be as hard as the original task. Here is the algorithm for a problem of the form:\\begin{aligned}\n\\min_{\\mathbf{x}} \\quad & f(\\mathbf{x}) \\\\\n\\text{subject to} \\quad & \\mathbf{x}_{\\mathrm{lb}} \\leq \\mathbf{x} \\leq \\mathbf{x}_{\\mathrm{ub}}.\n\\end{aligned}\n\nProjected Gradient Descent for Bound Constraints\n\nInput: Initial point \\mathbf{x}_0, step size \\alpha, bounds \\mathbf{x}_{\\mathrm{lb}}, \\mathbf{x}_{\\mathrm{ub}},\nmaximum iterations \\max_\\text{iter}, tolerance \\varepsilon\n\nInitialize k = 0\n\nWhile k < \\max_\\text{iter} and not converged:\n\nCompute gradient: \\mathbf{g}_k = \\nabla f(\\mathbf{x}_k)\n\nUpdate: \\mathbf{x}_{k+1} = \\text{clip}(\\mathbf{x}_k - \\alpha \\mathbf{g}_k,\\; \\mathbf{x}_{\\mathrm{lb}}, \\mathbf{x}_{\\mathrm{ub}})\n\nCheck convergence: if \\|\\mathbf{x}_{k+1} - \\mathbf{x}_k\\| < \\varepsilon, stop\n\nk = k + 1\n\nReturn \\mathbf{x}_k\n\nThe clipping function is defined as:\\text{clip}(x, x_{\\mathrm{lb}}, x_{\\mathrm{ub}}) = \\max\\big(\\min(x, x_{\\mathrm{ub}}), x_{\\mathrm{lb}}\\big).\n\nUnder mild conditions such as Lipschitz continuity of the gradient, PGD converges to a stationary point of the constrained problem. Its simplicity and low cost make it a common choice whenever the projection can be computed efficiently.","type":"content","url":"/appendix-nlp#projected-gradient-descent","position":25},{"hierarchy":{"lvl1":"Bibliography"},"type":"lvl1","url":"/bibliography","position":0},{"hierarchy":{"lvl1":"Bibliography"},"content":"","type":"content","url":"/bibliography","position":1},{"hierarchy":{"lvl1":"Policy Parametrization Methods"},"type":"lvl1","url":"/cadp","position":0},{"hierarchy":{"lvl1":"Policy Parametrization Methods"},"content":"In the previous chapter, we explored various approaches to approximate dynamic programming, focusing on ways to handle large state spaces through function approximation. However, these methods still face significant challenges when dealing with large or continuous action spaces. The need to maximize over actions during the Bellman operator evaluation becomes computationally prohibitive as the action space grows.\n\nThis chapter explores a natural evolution of these ideas: rather than exhaustively searching over actions, we can parameterize and directly optimize the policy itself. We begin by examining how fitted Q methods, while effective for handling large state spaces, still struggle with action space complexity.","type":"content","url":"/cadp","position":1},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl2":"Embedded Optimization"},"type":"lvl2","url":"/cadp#embedded-optimization","position":2},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl2":"Embedded Optimization"},"content":"Recall that in fitted Q methods, the main idea is to compute the Bellman operator only at a subset of all states, relying on function approximation to generalize to the remaining states. At each step of the successive approximation loop, we build a dataset of input state-action pairs mapped to their corresponding optimality operator evaluations:\\mathcal{D}_n = \\{((s, a), (Lq)(s, a; \\boldsymbol{\\theta}_n)) \\mid (s,a) \\in \\mathcal{B}\\}\n\nThis dataset is then fed to our function approximator (neural network, random forest, linear model) to obtain the next set of parameters:\\boldsymbol{\\theta}_{n+1} \\leftarrow \\texttt{fit}(\\mathcal{D}_n)\n\nWhile this strategy allows us to handle very large or even infinite (continuous) state spaces, it still requires maximizing over actions (\\max_{a \\in A}) during the dataset creation when computing the operator L for each basepoint. This maximization becomes computationally expensive for large action spaces. A natural improvement is to add another level of optimization: for each sample added to our regression dataset, we can employ numerical optimization methods to find actions that maximize the Bellman operator for the given state.\n\nFitted Q-Iteration with Explicit Optimization\n\nInput Given an MDP (S, A, P, R, \\gamma), base points \\mathcal{B}, function approximator class q(s,a; \\boldsymbol{\\theta}), maximum iterations N, tolerance \\varepsilon > 0\n\nOutput Parameters \\boldsymbol{\\theta} for Q-function approximation\n\nInitialize \\boldsymbol{\\theta}_0 (e.g., for zero initialization)\n\nn \\leftarrow 0\n\nrepeat\n\n\\mathcal{D} \\leftarrow \\emptyset // Regression Dataset\n\nFor each (s,a,r,s') \\in \\mathcal{B}: // Assumes Monte Carlo Integration with one sample\n\ny_{s,a} \\leftarrow r + \\gamma \\texttt{maximize}(q(s', \\cdot; \\boldsymbol{\\theta}_n)) // s' and \\boldsymbol{\\theta}_n are kept fixed\n\n\\mathcal{D} \\leftarrow \\mathcal{D} \\cup \\{((s,a), y_{s,a})\\}\n\n\\boldsymbol{\\theta}_{n+1} \\leftarrow \\texttt{fit}(\\mathcal{D})\n\n\\delta \\leftarrow \\frac{1}{|\\mathcal{D}||A|}\\sum_{(s,a) \\in \\mathcal{D} \\times A} (q(s,a; \\boldsymbol{\\theta}_{n+1}) - q(s,a; \\boldsymbol{\\theta}_n))^2\n\nn \\leftarrow n + 1\n\nuntil (\\delta < \\varepsilon or n \\geq N)\n\nreturn \\boldsymbol{\\theta}_n\n\nThe above pseudocode introduces a generic \\texttt{maximize} routine which represents any numerical optimization method that searches for an action maximizing the given function. This approach is versatile and can be adapted to different types of action spaces. For continuous action spaces, we can employ standard nonlinear optimization methods like gradient descent or L-BFGS (e.g., using scipy.optimize.minimize). For large discrete action spaces, we can use integer programming solvers - linear integer programming if the Q-function approximator is linear in actions, or mixed-integer nonlinear programming (MINLP) solvers for nonlinear Q-functions. The choice of solver depends on the structure of our Q-function approximator and the constraints on our action space.","type":"content","url":"/cadp#embedded-optimization","position":3},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"Amortized Optimization Approach","lvl2":"Embedded Optimization"},"type":"lvl3","url":"/cadp#amortized-optimization-approach","position":4},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"Amortized Optimization Approach","lvl2":"Embedded Optimization"},"content":"This process is computationally intensive. A natural question is whether we can “amortize” some of this computation by replacing the explicit optimization for each sample with a direct mapping that gives us an approximate maximizer directly.\nFor Q-functions, recall that the operator is given by:(\\mathrm{L}q)(s,a) = r(s,a) + \\gamma \\int p(ds'|s,a)\\max_{a' \\in \\mathcal{A}(s')} q(s', a')\n\nIf q^* is the optimal state-action value function, then v^*(s) = \\max_a q^*(s,a), and we can derive the optimal policy directly by computing the decision rule:\\pi^\\star(s) = \\arg\\max_{a \\in \\mathcal{A}(s)} q^\\star(s,a)\n\nSince q^* is a fixed point of L, we can write:\\begin{align*}\nq^\\star(s,a) &= (Lq^*)(s,a) \\\\\n&= r(s,a) + \\gamma \\int p(ds'|s,a) \\max_{a' \\in \\mathcal{A}(s')} q^\\star(s', a') \\\\\n&= r(s,a) + \\gamma \\int p(ds'|s,a) q^\\star(s', \\pi^\\star(s'))\n\\end{align*}\n\nNote that \\pi^\\star is implemented by our \\texttt{maximize} numerical solver in the procedure above. A practical strategy would be to collect these maximizer values at each step and use them to train a function approximator that directly predicts these solutions. Due to computational constraints, we might want to compute these exact maximizer values only for a subset of states, based on some computational budget, and use the fitted decision rule to generalize to the remaining states. This leads to the following amortized version:\n\nFitted Q-Iteration with Amortized Optimization\n\nInput Given an MDP (S, A, P, R, \\gamma), base points \\mathcal{B}, subset for exact optimization \\mathcal{B}_{\\text{opt}} \\subset \\mathcal{B}, Q-function approximator q(s,a; \\boldsymbol{\\theta}), policy approximator d(s; \\boldsymbol{w}), maximum iterations N, tolerance \\varepsilon > 0\n\nOutput Parameters \\boldsymbol{\\theta} for Q-function, \\boldsymbol{w} for policy\n\nInitialize \\boldsymbol{\\theta}_0, \\boldsymbol{w}_0\n\nn \\leftarrow 0\n\nrepeat\n\n\\mathcal{D}_q \\leftarrow \\emptyset // Q-function regression dataset\n\n\\mathcal{D}_d \\leftarrow \\emptyset // Policy regression dataset\n\nFor each (s,a,r,s') \\in \\mathcal{B}:\n\n// Determine next state’s action using either exact optimization or approximation\n\nif s' \\in \\mathcal{B}_{\\text{opt}} then\n\na^*_{s'} \\leftarrow \\texttt{maximize}(q(s', \\cdot; \\boldsymbol{\\theta}_n))\n\n\\mathcal{D}_d \\leftarrow \\mathcal{D}_d \\cup \\{(s', a^*_{s'})\\}\n\nelse\n\na^*_{s'} \\leftarrow d(s'; \\boldsymbol{w}_n)\n\n// Compute Q-function target using chosen action\n\ny_{s,a} \\leftarrow r + \\gamma q(s', a^*_{s'}; \\boldsymbol{\\theta}_n)\n\n\\mathcal{D}_q \\leftarrow \\mathcal{D}_q \\cup \\{((s,a), y_{s,a})\\}\n\n// Update both function approximators\n\n\\boldsymbol{\\theta}_{n+1} \\leftarrow \\texttt{fit}(\\mathcal{D}_q)\n\n\\boldsymbol{w}_{n+1} \\leftarrow \\texttt{fit}(\\mathcal{D}_d)\n\n// Compute convergence criteria\n\n\\delta_q \\leftarrow \\frac{1}{|\\mathcal{D}_q|}\\sum_{(s,a) \\in \\mathcal{D}_q} (q(s,a; \\boldsymbol{\\theta}_{n+1}) - q(s,a; \\boldsymbol{\\theta}_n))^2\n\n\\delta_d \\leftarrow \\frac{1}{|\\mathcal{D}_d|}\\sum_{(s,a^*) \\in \\mathcal{D}_d} \\|a^* - d(s; \\boldsymbol{w}_{n+1})\\|^2\n\nn \\leftarrow n + 1\n\nuntil (\\max(\\delta_q, \\delta_d) \\geq \\varepsilon or n \\geq N)\n\nreturn \\boldsymbol{\\theta}_n, \\boldsymbol{w}_n\n\nAn important observation about this procedure is that the policy d(s; \\boldsymbol{w}) is being trained on a dataset \\mathcal{D}_d containing optimal actions computed with respect to an evolving Q-function. Specifically, at iteration n, we collect pairs (s', a^*_{s'}) where a^*_{s'} = \\arg\\max_a q(s', a; \\boldsymbol{\\theta}_n). However, after updating to \\boldsymbol{\\theta}_{n+1}, these actions may no longer be optimal with respect to the new Q-function.\n\nA natural approach to handle this staleness would be to maintain only the most recent optimization data. We could modify our procedure to keep a sliding window of K iterations, where at iteration n, we only use data from iterations max(0, n-K) to n. This would be implemented by augmenting each entry in \\mathcal{D}_d with a timestamp:\\mathcal{D}_\\pi^t = \\{(s', a^*_{s'}, t) \\mid t \\in \\{n-K,\\ldots,n\\}\\}\n\nwhere t indicates the iteration at which the optimal action was computed. When fitting the policy network, we would then only use data points that are at most K iterations old:\\boldsymbol{w}_{n+1} \\leftarrow \\texttt{fit}(\\{(s', a^*_{s'}) \\mid (s', a^*_{s'}, t) \\in \\mathcal{D}_\\pi^t, n-K \\leq t \\leq n\\})\n\nThis introduces a trade-off between using more data (larger K) versus using more recent, accurate data (smaller K). The choice of K would depend on how quickly the Q-function evolves and the computational budget available for computing exact optimal actions.\n\nNow the main issue with this approach, apart from the intrinsic out-of-distribution drift that we are trying to track, is that it requires “ground truth” - samples of optimal actions computed by the actual solver. This raises a natural question: how few samples do we actually need? Could we even envision eliminating the solver entirely? What seems impossible at first glance turns out to be achievable. The intuition is that as our policy improves at selecting actions, we can bootstrap from these increasingly better choices. As we continuously amortize these improving actions over time, it creates a virtuous cycle of self-improvement towards the optimal policy. But for this bootstrapping process to work, we need careful management. Move too quickly and the process may become unstable. Let’s examine how this balance can be achieved.","type":"content","url":"/cadp#amortized-optimization-approach","position":5},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl2":"Deterministic Parametrized Policies"},"type":"lvl2","url":"/cadp#deterministic-parametrized-policies","position":6},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl2":"Deterministic Parametrized Policies"},"content":"In this section, we consider deterministic parametrized policies of the form d(s; \\boldsymbol{w}) which directly output an action given a state. This approach differs from stochastic policies that output probability distributions over actions, making it particularly suitable for continuous control problems where the optimal policy is often deterministic. We’ll see how fitted Q-value methods can be naturally extended to simultaneously learn both the Q-function and such a deterministic policy.","type":"content","url":"/cadp#deterministic-parametrized-policies","position":7},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"Neural Fitted Q-iteration for Continuous Actions (NFQCA)","lvl2":"Deterministic Parametrized Policies"},"type":"lvl3","url":"/cadp#neural-fitted-q-iteration-for-continuous-actions-nfqca","position":8},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"Neural Fitted Q-iteration for Continuous Actions (NFQCA)","lvl2":"Deterministic Parametrized Policies"},"content":"To develop this approach, let’s first consider an idealized setting where we have access to q^\\star, the optimal Q-function. Then we can state our goal as finding policy parameters \\boldsymbol{w} that maximize q^\\star with respect to the actions chosen by our policy across the state space:\\max_{\\boldsymbol{w}} q^*(s, d(s; \\boldsymbol{w})) \\quad \\text{for all } s\n\nHowever, it’s computationally infeasible to satisfy this condition for every possible state s, especially in large or continuous state spaces. To address this, we assume a distribution of states, denoted \\mu(s), and take the expectation, leading to the problem:\\max_{\\boldsymbol{w}} \\mathbb{E}_{s \\sim \\mu(s)}[q^*(s, d(s; \\boldsymbol{w}))]\n\nHowever in practice, we do not have access to q^*. Instead, we need to approximate q^* with a Q-function q(s, a; \\boldsymbol{\\theta}), parameterized by \\boldsymbol{\\theta}, which we will learn simultaneously with the policy function d(s; \\boldsymbol{w}). Given a samples of initial states drawn from \\mu, we then maximize this objective via a Monte Carlo surrogate problem:\\max_{\\boldsymbol{w}} \\mathbb{E}_{s \\sim \\mu(s)}[q(s, d(s; \\boldsymbol{w}); \\boldsymbol{\\theta})] \\approx\n\\max_{\\boldsymbol{w}} \\frac{1}{|\\mathcal{B}|} \\sum_{s \\in \\mathcal{B}}  q(s, d(s; \\boldsymbol{w}); \\boldsymbol{\\theta})\n\nWhen using neural networks to parametrize q and d, we obtain the Neural Fitted Q-Iteration with Continuous Actions (NFQCA) algorithm proposed by \n\nHafner & Riedmiller (2011).\n\nNeural Fitted Q-Iteration with Continuous Actions (NFQCA)\n\nInput MDP (S, A, P, R, \\gamma), base points \\mathcal{B}, Q-function q(s,a; \\boldsymbol{\\theta}), policy d(s; \\boldsymbol{w})\n\nOutput Parameters \\boldsymbol{\\theta} for Q-function, \\boldsymbol{w} for policy\n\nInitialize \\boldsymbol{\\theta}_0, \\boldsymbol{w}_0\n\nfor n = 0,1,2,... do\n\n\\mathcal{D}_q \\leftarrow \\emptyset\n\nFor each (s,a,r,s') \\in \\mathcal{B}:\n\na'_{s'} \\leftarrow d(s'; \\boldsymbol{w}_n)\n\ny_{s,a} \\leftarrow r + \\gamma q(s', a'_{s'}; \\boldsymbol{\\theta}_n)\n\n\\mathcal{D}_q \\leftarrow \\mathcal{D}_q \\cup \\{((s,a), y_{s,a})\\}\n\n\\boldsymbol{\\theta}_{n+1} \\leftarrow \\texttt{fit}(\\mathcal{D}_q)\n\n\\boldsymbol{w}_{n+1} \\leftarrow \\texttt{minimize}_{\\boldsymbol{w}} -\\frac{1}{|\\mathcal{B}|} \\sum_{(s,a,r,s') \\in \\mathcal{B}} q(s, d(s; \\boldsymbol{w}); \\boldsymbol{\\theta}_{n+1})\n\nreturn \\boldsymbol{\\theta}_n, \\boldsymbol{w}_n\n\nIn practice, both the fit and minimize operations above are implemented using gradient descent. For the Q-function, the fit operation minimizes the mean squared error between the network’s predictions and the target values:\\texttt{fit}(\\mathcal{D}_q) = \\arg\\min_{\\boldsymbol{\\theta}} \\frac{1}{|\\mathcal{D}_q|} \\sum_{((s,a), y) \\in \\mathcal{D}_q} (q(s,a; \\boldsymbol{\\theta}) - y)^2\n\nFor the policy update, the minimize operation uses gradient descent on the composition of the “critic” network q and the “actor” network d. This results in the following update rule:\\boldsymbol{w}_{n+1} = \\boldsymbol{w}_n + \\alpha \\nabla_{\\boldsymbol{w}} \\left(\\frac{1}{|\\mathcal{B}|} \\sum_{(s,a,r,s') \\in \\mathcal{B}} q(s, d(s; \\boldsymbol{w}); \\boldsymbol{\\theta}_{n+1})\\right)\n\nwhere \\alpha is the learning rate. Both operations can be efficiently implemented using modern automatic differentiation libraries and stochastic gradient descent variants like Adam or RMSProp.","type":"content","url":"/cadp#neural-fitted-q-iteration-for-continuous-actions-nfqca","position":9},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"Deep Deterministic Policy Gradient (DDPG)","lvl2":"Deterministic Parametrized Policies"},"type":"lvl3","url":"/cadp#deep-deterministic-policy-gradient-ddpg","position":10},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"Deep Deterministic Policy Gradient (DDPG)","lvl2":"Deterministic Parametrized Policies"},"content":"Just as DQN adapted Neural Fitted Q-Iteration to the online setting, DDPG \n\nLillicrap et al. (2015) extends NFQCA to learn from data collected online. Like NFQCA, DDPG simultaneously learns a Q-function and a deterministic policy that maximizes it, but differs in how it collects and processes data.\n\nInstead of maintaining a fixed set of basepoints, DDPG uses a replay buffer that continuously stores new transitions as the agent interacts with the environment. Since the policy is deterministic, exploration becomes challenging. DDPG addresses this by adding noise to the policy’s actions during data collection:a = d(s; \\boldsymbol{w}) + \\mathcal{N}\n\nwhere \\mathcal{N} represents exploration noise drawn from an Ornstein-Uhlenbeck (OU) process. The OU process is particularly well-suited for control tasks as it generates temporally correlated noise, leading to smoother exploration trajectories compared to independent random noise. It is defined by the stochastic differential equation:d\\mathcal{N}_t = \\theta(\\mu - \\mathcal{N}_t)dt + \\sigma dW_t\n\nwhere \\mu is the long-term mean value (typically set to 0), \\theta determines how strongly the noise is pulled toward this mean, \\sigma scales the random fluctuations, and dW_t is a Wiener process (continuous-time random walk). For implementation, we discretize this continuous-time process using the Euler-Maruyama method:\\mathcal{N}_{t+1} = \\mathcal{N}_t + \\theta(\\mu - \\mathcal{N}_t)\\Delta t + \\sigma\\sqrt{\\Delta t}\\epsilon_t\n\nwhere \\Delta t is the time step and \\epsilon_t \\sim \\mathcal{N}(0,1) is standard Gaussian noise. Think of this process like a spring mechanism: when the noise value \\mathcal{N}_t deviates from \\mu, the term \\theta(\\mu - \\mathcal{N}_t)\\Delta t acts like a spring force, continuously pulling it back. Unlike a spring, however, this return to \\mu is not oscillatory - it’s more like motion through a viscous fluid, where the force simply decreases as the noise gets closer to \\mu. The random term \\sigma\\sqrt{\\Delta t}\\epsilon_t then adds perturbations to this smooth return trajectory. This creates noise that wanders away from \\mu (enabling exploration) but is always gently pulled back (preventing the actions from wandering too far), with \\theta controlling the strength of this pulling force.\n\nThe policy gradient update follows the same principle as NFQCA:\\nabla_{\\boldsymbol{w}} \\mathbb{E}_{s \\sim \\mu(s)}[q(s, d(s; \\boldsymbol{w}); \\boldsymbol{\\theta})]\n\nWe then embed this exploration mechanism into the data collection procedure and use the same flattened FQI structure that we adopted in DQN. Similar to DQN, flattening the outer-inner optimization structure leads to the need for target networks - both for the Q-function and the policy.\n\nDeep Deterministic Policy Gradient (DDPG)\n\nInput MDP (S, A, P, R, \\gamma), Q-network q(s,a; \\boldsymbol{\\theta}), policy network d(s; \\boldsymbol{w}), learning rates \\alpha_q, \\alpha_d, replay buffer size B, mini-batch size b, target update frequency K\n\nInitialize\n\nParameters \\boldsymbol{\\theta}_0, \\boldsymbol{w}_0 randomly\n\nTarget parameters: \\boldsymbol{\\theta}_{target} \\leftarrow \\boldsymbol{\\theta}_0, \\boldsymbol{w}_{target} \\leftarrow \\boldsymbol{w}_0\n\nInitialize replay buffer \\mathcal{R} with capacity B\n\nInitialize exploration noise process \\mathcal{N}\n\nn \\leftarrow 0\n\nwhile training:\n\nObserve current state s\n\nSelect action with noise: a = d(s; \\boldsymbol{w}_n) + \\mathcal{N}\n\nExecute a, observe reward r and next state s'\n\nStore (s,a,r,s') in \\mathcal{R}, replacing oldest if full\n\nSample mini-batch of b transitions (s_i,a_i,r_i,s'_i) from \\mathcal{R}\n\nFor each sampled transition:\n\ny_i \\leftarrow r_i + \\gamma q(s'_i, d(s'_i; \\boldsymbol{w}_{target}); \\boldsymbol{\\theta}_{target})\n\nUpdate Q-network: \\boldsymbol{\\theta}_{n+1} \\leftarrow \\boldsymbol{\\theta}_n - \\alpha_q \\nabla_{\\boldsymbol{\\theta}} \\frac{1}{b}\\sum_i(y_i - q(s_i,a_i;\\boldsymbol{\\theta}_n))^2\n\nUpdate policy: \\boldsymbol{w}_{n+1} \\leftarrow \\boldsymbol{w}_n + \\alpha_d \\frac{1}{b}\\sum_i \\nabla_a q(s_i,a;\\boldsymbol{\\theta}_{n+1})|_{a=d(s_i;\\boldsymbol{w}_n)} \\nabla_{\\boldsymbol{w}} d(s_i;\\boldsymbol{w}_n)\n\nIf n \\bmod K = 0:\n\n\\boldsymbol{\\theta}_{target} \\leftarrow \\boldsymbol{\\theta}_n\n\n\\boldsymbol{w}_{target} \\leftarrow \\boldsymbol{w}_n\n\nn \\leftarrow n + 1\n\nreturn \\boldsymbol{\\theta}_n, \\boldsymbol{w}_n","type":"content","url":"/cadp#deep-deterministic-policy-gradient-ddpg","position":11},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"Twin Delayed Deep Deterministic Policy Gradient (TD3)","lvl2":"Deterministic Parametrized Policies"},"type":"lvl3","url":"/cadp#twin-delayed-deep-deterministic-policy-gradient-td3","position":12},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"Twin Delayed Deep Deterministic Policy Gradient (TD3)","lvl2":"Deterministic Parametrized Policies"},"content":"While DDPG provided a foundation for continuous control with deep RL, it suffers from similar overestimation issues as DQN. TD3 \n\nFujimoto et al. (2018) addresses these challenges through three key modifications: double Q-learning to reduce overestimation bias, delayed policy updates to reduce per-update error, and target policy smoothing to prevent exploitation of Q-function errors.\n\nTwin Delayed Deep Deterministic Policy Gradient (TD3)\n\nInput MDP (S, A, P, R, \\gamma), twin Q-networks q^A(s,a; \\boldsymbol{\\theta}^A), q^B(s,a; \\boldsymbol{\\theta}^B), policy network d(s; \\boldsymbol{w}), learning rates \\alpha_q, \\alpha_d, replay buffer size B, mini-batch size b, policy delay d, noise scale \\sigma, noise clip c, exploration noise std \\sigma_{explore}\n\nInitialize\n\nParameters \\boldsymbol{\\theta}^A_0, \\boldsymbol{\\theta}^B_0, \\boldsymbol{w}_0 randomly\n\nTarget parameters: \\boldsymbol{\\theta}^A_{target} \\leftarrow \\boldsymbol{\\theta}^A_0, \\boldsymbol{\\theta}^B_{target} \\leftarrow \\boldsymbol{\\theta}^B_0, \\boldsymbol{w}_{target} \\leftarrow \\boldsymbol{w}_0\n\nInitialize replay buffer \\mathcal{R} with capacity B\n\nn \\leftarrow 0\n\nwhile training:\n\nObserve current state s\n\nSelect action with Gaussian noise: a = d(s; \\boldsymbol{w}_n) + \\epsilon, where \\epsilon \\sim \\mathcal{N}(0, \\sigma_{explore})\n\nExecute a, observe reward r and next state s'\n\nStore (s,a,r,s') in \\mathcal{R}, replacing oldest if full\n\nSample mini-batch of b transitions (s_i,a_i,r_i,s'_i) from \\mathcal{R}\n\nFor each sampled transition:\n\n\\tilde{a}_i \\leftarrow d(s'_i; \\boldsymbol{w}_{target}) + \\text{clip}(\\mathcal{N}(0, \\sigma), -c, c)  // Add clipped noise\n\nq_{target} \\leftarrow \\min(q^A(s'_i, \\tilde{a}_i; \\boldsymbol{\\theta}^A_{target}), q^B(s'_i, \\tilde{a}_i; \\boldsymbol{\\theta}^B_{target}))\n\ny_i \\leftarrow r_i + \\gamma q_{target}\n\nUpdate Q-networks:\n\n\\boldsymbol{\\theta}^A_{n+1} \\leftarrow \\boldsymbol{\\theta}^A_n - \\alpha_q \\nabla_{\\boldsymbol{\\theta}} \\frac{1}{b}\\sum_i(y_i - q^A(s_i,a_i;\\boldsymbol{\\theta}^A_n))^2\n\n\\boldsymbol{\\theta}^B_{n+1} \\leftarrow \\boldsymbol{\\theta}^B_n - \\alpha_q \\nabla_{\\boldsymbol{\\theta}} \\frac{1}{b}\\sum_i(y_i - q^B(s_i,a_i;\\boldsymbol{\\theta}^B_n))^2\n\nIf n \\bmod d = 0:  // Delayed policy update\n\nUpdate policy: \\boldsymbol{w}_{n+1} \\leftarrow \\boldsymbol{w}_n + \\alpha_d \\frac{1}{b}\\sum_i \\nabla_a q^A(s_i,a;\\boldsymbol{\\theta}^A_{n+1})|_{a=d(s_i;\\boldsymbol{w}_n)} \\nabla_{\\boldsymbol{w}} d(s_i;\\boldsymbol{w}_n)\n\nSoft update of target networks:\n\n\\boldsymbol{\\theta}^A_{target} \\leftarrow \\tau\\boldsymbol{\\theta}^A_{n+1} + (1-\\tau)\\boldsymbol{\\theta}^A_{target}\n\n\\boldsymbol{\\theta}^B_{target} \\leftarrow \\tau\\boldsymbol{\\theta}^B_{n+1} + (1-\\tau)\\boldsymbol{\\theta}^B_{target}\n\n\\boldsymbol{w}_{target} \\leftarrow \\tau\\boldsymbol{w}_{n+1} + (1-\\tau)\\boldsymbol{w}_{target}\n\nn \\leftarrow n + 1\n\nreturn \\boldsymbol{\\theta}^A_n, \\boldsymbol{\\theta}^B_n, \\boldsymbol{w}_n\n\nSimilar to Double Q-learning, TD3 decouples selection from evaluation when forming the targets. However, instead of intertwining the two existing online and target networks, TD3 suggests learning two Q-functions simultaneously and uses their minimum when computing target values to help combat the overestimation bias further.\n\nFurthermore, when computing target Q-values, TD3 adds small random noise to the target policy’s actions and clips it to keep the perturbations bounded. This regularization technique essentially implements a form of “policy smoothing” that prevents the policy from exploiting areas where the Q-function may have erroneously high values:$$\\tilde{a} = d(s'; \\boldsymbol{w}_{target}) + \\text{clip}(\\mathcal{N}(0, \\sigma), -c, c)$$\n\nWhile DDPG used the OU process which generates temporally correlated noise, TD3’s authors found that simple uncorrelated Gaussian noise works just as well for exploration. It is also easier to implement and tune since you only need to set a single parameter (\\sigma_{explore}) for exploration rather than the multiple parameters required by the OU process (\\theta, \\mu, \\sigma).\n\nFinally, TD3 updates the policy network (and target networks) less frequently than the Q-networks, typically once every d Q-function updates. This helps reduce the per-update error and gives the Q-functions time to become more accurate before they are used to update the policy.","type":"content","url":"/cadp#twin-delayed-deep-deterministic-policy-gradient-td3","position":13},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl2":"Soft Actor-Critic"},"type":"lvl2","url":"/cadp#soft-actor-critic","position":14},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl2":"Soft Actor-Critic"},"content":"Adapting the intuition of NFQCA to the smooth Bellman optimality equations leads us to the soft actor-critic algorithm \n\nHaarnoja et al. (2018). To understand this connection, let’s first examine how the smooth Bellman equations emerge naturally from entropy regularization.\n\nConsider the standard Bellman operator augmented with an entropy term. The smooth Bellman operator \\mathrm{L}_\\beta takes the form:(\\mathrm{L}_\\beta v)(s) = \\max_{\\pi \\in \\Pi^{MR}}\\{\\mathbb{E}_{a \\sim \\pi}[r(s,a) + \\gamma v(s')] + \\beta\\mathcal{H}(\\pi)\\}\n\nwhere \\mathcal{H}(\\pi) = -\\mathbb{E}_{a \\sim \\pi}[\\log \\pi(a|s)] represents the entropy of the policy. To find the solution to the optimization problem embedded in the operator \\mathrm{L}_\\beta, we set the functional derivative of the objective with respect to the decision rule to zero:\\frac{\\delta}{\\delta \\pi(a|s)} \\left[\\int_A \\pi(a|s)(r(s,a) + \\gamma v(s'))da - \\beta\\int_A \\pi(a|s)\\log \\pi(a|s)da \\right] = 0\n\nEnforcing that \\int_A \\pi(a|s)da = 1 leads to the following Lagrangian:r(s,a) + \\gamma v(s') - \\beta(1 + \\log \\pi(a|s)) - \\lambda(s) = 0\n\nSolving for \\pi shows that the optimal policy is a Boltzmann distribution\\pi^*(a|s) = \\frac{\\exp(\\frac{1}{\\beta}(r(s,a) + \\gamma \\mathbb{E}_{s'}[v(s')]))}{Z(s)}\n\nWhen we substitute this optimal policy back into the entropy-regularized objective, we obtain:\\begin{align*}\nv(s) &= \\mathbb{E}_{a \\sim d^*}[r(s,a) + \\gamma v(s')] + \\beta\\mathcal{H}(d^*) \\\\\n&= \\beta \\log \\int_A \\exp(\\frac{1}{\\beta}(r(s,a) + \\gamma \\mathbb{E}_{s'}[v(s')]))da\n\\end{align*}\n\nAs we saw at the beginning of this chapter, the smooth Bellman optimality operator for Q-factors is defined as:(\\mathrm{L}_\\beta q)(s,a) = r(s,a) + \\gamma \\mathbb{E}_{s'}\\left[\\beta \\log \\int_A \\exp(\\frac{1}{\\beta}q(s',a'))da'\\right]\n\nThis operator maintains the contraction property of its standard counterpart, guaranteeing a unique fixed point q^*. The optimal policy takes the form:d^*(a|s) = \\frac{\\exp(\\frac{1}{\\beta}q^*(s,a))}{Z(s)}\n\nwhere Z(s) = \\int_A \\exp(\\frac{1}{\\beta}q^*(s,a))da. The optimal value function can be recovered as:v^*(s) = \\beta \\log \\int_A \\exp(\\frac{1}{\\beta}q^*(s,a))da","type":"content","url":"/cadp#soft-actor-critic","position":15},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"Fitted Q-Iteration for the Smooth Bellman Equations","lvl2":"Soft Actor-Critic"},"type":"lvl3","url":"/cadp#fitted-q-iteration-for-the-smooth-bellman-equations","position":16},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"Fitted Q-Iteration for the Smooth Bellman Equations","lvl2":"Soft Actor-Critic"},"content":"Following the principles of fitted value iteration, we can approximate approximate the effect of the smooth Bellman operator by computing it exactly at a number of basepoints and generalizing elsewhere using function approximation. Concretely, given a collection of states s_i and actions a_i, we would compute regression target values:y_i = r(s_i,a_i) + \\gamma \\mathbb{E}_{s'}\\left[\\beta \\log \\int_A \\exp(\\frac{1}{\\beta}q_\\theta(s',a'))da'\\right]\n\nand fit our Q-function approximator by minimizing:\\min_\\theta \\sum_i (q_\\theta(s_i,a_i) - y_i)^2\n\nThe expectation over next states can be handled through Monte Carlo estimation using samples from the environment: given a transition (s_i,a_i,s'_i), we can approximate:\\mathbb{E}_{s'}\\left[\\beta \\log \\int_A \\exp(\\frac{1}{\\beta}q_\\theta(s',a'))da'\\right] \\approx \\beta \\log \\int_A \\exp(\\frac{1}{\\beta}q_\\theta(s'_i,a'))da'\n\nHowever, we still face the challenge of computing the integral over actions. This motivates maintaining separate function approximators for both Q and V, using samples from the current policy to estimate the value function:v_\\psi(s) \\approx \\mathbb{E}_{a \\sim d(\\cdot|s;\\phi)}\\left[q_\\theta(s,a) - \\beta \\log d(a|s;\\phi)\\right]\n\nBy maintaining both approximators, we can estimate targets using sampled actions from our policy. Specifically, if we have a transition (s_i,a_i,s'_i) and sample a'_i \\sim d(\\cdot|s'_i;\\phi), our target becomes:y_i = r(s_i,a_i) + \\gamma\\left(q_\\theta(s'_i,a'_i) - \\beta \\log d(a'_i|s'_i;\\phi)\\right)\n\nThis approach exists only due to the dual representation of the smooth Bellman equations as an entropy-regularized problem, which transforms the intractable log-sum-exp into a form we can estimate efficiently through sampling.","type":"content","url":"/cadp#fitted-q-iteration-for-the-smooth-bellman-equations","position":17},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"Approximating Boltzmann Policies by Gaussians","lvl2":"Soft Actor-Critic"},"type":"lvl3","url":"/cadp#approximating-boltzmann-policies-by-gaussians","position":18},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"Approximating Boltzmann Policies by Gaussians","lvl2":"Soft Actor-Critic"},"content":"The entropy-regularized objective and the smooth Bellman equation are mathematically equivalent. However, both formulations face a practical challenge: they require evaluating an intractable integral due to the Boltzmann distribution. Soft Actor-Critic (SAC) addresses this problem by approximating the optimal policy with a simpler, more tractable Gaussian distribution. Given the optimal soft policy:d^*(a|s) = \\frac{\\exp(\\frac{1}{\\beta}q^*(s,a))}{Z(s)}\n\nwe seek to approximate it with a Gaussian policy:d(a|s;\\phi) = \\mathcal{N}(\\mu_\\phi(s), \\sigma_\\phi(s))\n\nThis approximation task naturally raises the question of how to measure the “closeness” between the target Boltzmann distribution and a candidate Gaussian approximation. Following common practice in deep learning, we employ the Kullback-Leibler (KL) divergence as our measure of distributional distance. To find the best approximation, we minimize the KL divergence between our policy and the optimal policy, using our current estimate q_\\theta of q^*:\\operatorname{minimize}_{\\phi} \\mathbb{E}_{s \\sim \\mu(s)}\\left[D_{KL}\\left(d(\\cdot|s;\\phi) \\| \\frac{\\exp(\\frac{1}{\\beta}q_\\theta(s,\\cdot))}{Z(s)}\\right)\\right]\n\nHowever, an important question remains: how can we solve this optimization problem when it involves the intractable partition function Z(s)? To see this, recall that for two distributions p and q, the KL divergence takes the form D_{KL}(p\\|q) = \\mathbb{E}_{x \\sim p}[\\log p(x) - \\log q(x)]. Let’s denote the target Boltzmann distribution based on our current Q-estimate as:d_\\theta(a|s) = \\frac{\\exp(\\frac{1}{\\beta}q_\\theta(s,a))}{Z_\\theta(s)}\n\nThen the KL minimization becomes:\\begin{align*}\nD_{KL}(d(\\cdot|s;\\phi)\\|d_\\theta) &= \\mathbb{E}_{a \\sim d(\\cdot|s;\\phi)}[\\log d(a|s;\\phi) - \\log d_\\theta(a|s)] \\\\\n&= \\mathbb{E}_{a \\sim d(\\cdot|s;\\phi)}\\left[\\log d(a|s;\\phi) - \\log \\left(\\frac{\\exp(\\frac{1}{\\beta}q_\\theta(s,a))}{Z_\\theta(s)}\\right)\\right] \\\\\n&= \\mathbb{E}_{a \\sim d(\\cdot|s;\\phi)}\\left[\\log d(a|s;\\phi) - \\frac{1}{\\beta}q_\\theta(s,a) + \\log Z_\\theta(s)\\right]\n\\end{align*}\n\nSince \\log Z(s) is constant with respect to \\phi, minimizing this KL divergence is equivalent to:\\operatorname{minimize}_{\\phi} \\mathbb{E}_{s \\sim \\mu(s)}\\mathbb{E}_{a \\sim d(\\cdot|s;\\phi)}[\\log d(a|s;\\phi) - \\frac{1}{\\beta}q_\\theta(s,a)]","type":"content","url":"/cadp#approximating-boltzmann-policies-by-gaussians","position":19},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"Reparameterizating the Objective","lvl2":"Soft Actor-Critic"},"type":"lvl3","url":"/cadp#reparameterizating-the-objective","position":20},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"Reparameterizating the Objective","lvl2":"Soft Actor-Critic"},"content":"One last challenge remains: \\phi appears in the distribution underlying the inner expectation, as well as in the integrand. This setting departs from standard empirical risk minimization (ERM) in supervised learning where the distribution of the data (e.g., cats and dogs in image classification) remains fixed regardless of model parameters. Here, however, the “data” - our sampled actions - depends directly on the parameters \\phi we’re trying to optimize.\n\nThis dependence prevents us from simply using sample average estimators and differentiating through them, as we typically do in supervised learning. The challenge of correctly and efficiently estimating such derivatives has been extensively studied in the simulation literature under the umbrella of “derivative estimation.” SAC adopts a particular solution known as the reparameterization trick in deep learning (or the IPA estimator in simulation literature). This approach transforms the problem by pushing \\phi inside the expectation through a change of variables.\n\nTo address this, we can express our Gaussian policy through a deterministic function f_\\phi that transforms noise samples to actions:f_\\phi(s,\\epsilon) = \\mu_\\phi(s) + \\sigma_\\phi(s)\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,1)\n\nThis transformation allows us to rewrite our objective using an expectation over the fixed noise distribution:\\begin{align*}\n&\\mathbb{E}_{s \\sim \\mu(s)}\\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,1)}[\\log d(f_\\phi(s,\\epsilon)|s;\\phi) - \\frac{1}{\\beta}q_\\theta(s,f_\\phi(s,\\epsilon))]\n\\end{align*}\n\nNow \\phi appears only in the integrand through the function f_\\phi, not in the sampling distribution. The objective involves two terms. First, the log-probability of our Gaussian policy has a simple closed form:\\log d(f_\\phi(s,\\epsilon)|s;\\phi) = -\\frac{1}{2}\\log(2\\pi\\sigma_\\phi(s)^2) - \\frac{(f_\\phi(s,\\epsilon)-\\mu_\\phi(s))^2}{2\\sigma_\\phi(s)^2}\n\nSecond, \\phi enters through the composition of q^\\star with f_\\phi: q^\\star(s,f_\\phi(s,\\epsilon)). The chain rule for this composition would involve derivatives of both functions. While this might be problematic if the Q-factors were to come from outside of our control (ie. not in the computational graph), but since SAC learns it simultaneously with the policy, then we can simply compute all required derivatives through automatic differentiation.\n\nThis composition of policy and value functions - where f_\\phi enters as input to q_\\theta - directly parallels the structure we encountered in deterministic policy methods like NFQCA and DDPG. In those methods, we optimized:\\max_{\\phi} \\mathbb{E}_{s \\sim \\mu(s)}[q_\\theta(s, f_\\phi(s))]\n\nwhere f_\\phi(s) was a deterministic policy. SAC extends this idea to stochastic policies by having f_\\phi transform both state and noise:\\max_{\\phi} \\mathbb{E}_{s \\sim \\mu(s)}\\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,1)}[q_\\theta(s,f_\\phi(s,\\epsilon))]\n\nThus, rather than learning a single action for each state as in DDPG, we learn a function that transforms random noise into actions, explicitly parameterizing a distribution over actions while maintaining the same underlying principle of differentiating through composed policy and value functions.\n\nSoft Actor-Critic\n\nInput MDP (S, A, P, R, \\gamma), Q-networks q^1(s,a; \\boldsymbol{\\theta}^1), q^2(s,a; \\boldsymbol{\\theta}^2), value network v(s; \\boldsymbol{\\psi}), policy network d(a|s; \\boldsymbol{\\phi}), learning rates \\alpha_q, \\alpha_v, \\alpha_\\pi, replay buffer size B, mini-batch size b, target smoothing coefficient \\tau\n\nInitialize\n\nParameters \\boldsymbol{\\theta}^1_0, \\boldsymbol{\\theta}^2_0, \\boldsymbol{\\psi}_0, \\boldsymbol{\\phi}_0 randomly\n\nTarget parameters: \\boldsymbol{\\bar{\\psi}}_0 \\leftarrow \\boldsymbol{\\psi}_0\n\nInitialize replay buffer \\mathcal{R} with capacity B\n\nwhile training:\n\nObserve current state s\n\nSample action from policy: a \\sim d(a|s; \\boldsymbol{\\phi})\n\nExecute a, observe reward r and next state s'\n\nStore (s, a, r, s') in \\mathcal{R}, replacing oldest if full\n\nSample mini-batch of b transitions (s_i, a_i, r_i, s'_i) from \\mathcal{R}\n\nUpdate Value Network:\n\nCompute target for value network:y_v = \\mathbb{E}_{a' \\sim d(\\cdot|s'; \\boldsymbol{\\phi})} \\left[ \\min \\left( q^1(s', a'; \\boldsymbol{\\theta}^1), q^2(s', a'; \\boldsymbol{\\theta}^2) \\right) - \\alpha \\log d(a'|s'; \\boldsymbol{\\phi}) \\right]\n\nUpdate \\boldsymbol{\\psi} via gradient descent:\\boldsymbol{\\psi} \\leftarrow \\boldsymbol{\\psi} - \\alpha_v \\nabla_{\\boldsymbol{\\psi}} \\frac{1}{b} \\sum_i (v(s_i; \\boldsymbol{\\psi}) - y_v)^2\n\nUpdate Q-Networks:\n\nCompute targets for Q-networks:y_q = r_i + \\gamma \\cdot v(s'_i; \\boldsymbol{\\bar{\\psi}})\n\nUpdate \\boldsymbol{\\theta}^1 and \\boldsymbol{\\theta}^2 via gradient descent:\\boldsymbol{\\theta}^j \\leftarrow \\boldsymbol{\\theta}^j - \\alpha_q \\nabla_{\\boldsymbol{\\theta}^j} \\frac{1}{b} \\sum_i (q^j(s_i, a_i; \\boldsymbol{\\theta}^j) - y_q)^2, \\quad j \\in \\{1, 2\\}\n\nUpdate Policy Network:\n\nSample actions a \\sim d(\\cdot|s_i; \\boldsymbol{\\phi}) for each s_i in the mini-batch\n\nUpdate \\boldsymbol{\\phi} via gradient ascent:\\boldsymbol{\\phi} \\leftarrow \\boldsymbol{\\phi} + \\alpha_\\pi \\nabla_{\\boldsymbol{\\phi}} \\frac{1}{b} \\sum_i \\left[ \\alpha \\log d(a|s_i; \\boldsymbol{\\phi}) - q^1(s_i, a; \\boldsymbol{\\theta}^1) \\right]\n\nUpdate Target Value Network:\\boldsymbol{\\bar{\\psi}} \\leftarrow \\tau \\boldsymbol{\\psi} + (1 - \\tau) \\boldsymbol{\\bar{\\psi}}\n\nreturn Learned parameters \\boldsymbol{\\theta}^1, \\boldsymbol{\\theta}^2, \\boldsymbol{\\psi}, \\boldsymbol{\\phi}","type":"content","url":"/cadp#reparameterizating-the-objective","position":21},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl2":"Derivative Estimation for Stochastic Optimization"},"type":"lvl2","url":"/cadp#derivative-estimation-for-stochastic-optimization","position":22},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl2":"Derivative Estimation for Stochastic Optimization"},"content":"Consider optimizing an objective that involves an expectation:J(\\theta) = \\mathbb{E}_{x \\sim p(x;\\theta)}[f(x,\\theta)]\n\nFor concreteness, let’s examine a simple example where x \\sim \\mathcal{N}(\\theta,1) and f(x,\\theta) = x^2\\theta. The derivative we seek is:\\frac{d}{d\\theta}J(\\theta) = \\frac{d}{d\\theta}\\int x^2\\theta p(x;\\theta)dx\n\nWhile we can compute this exactly for the Gaussian example, this is often impossible for more general problems. We might then be tempted to approximate our objective using samples:J(\\theta) \\approx \\frac{1}{N}\\sum_{i=1}^N f(x_i,\\theta), \\quad x_i \\sim p(x;\\theta)\n\nThen differentiate this approximation:\\frac{d}{d\\theta}J(\\theta) \\approx \\frac{1}{N}\\sum_{i=1}^N \\frac{\\partial}{\\partial \\theta}f(x_i,\\theta)\n\nHowever, this naive approach ignores that the samples themselves depend on \\theta. The correct derivative requires the product rule:\\frac{d}{d\\theta}J(\\theta) = \\int \\frac{\\partial}{\\partial \\theta}[f(x,\\theta)p(x;\\theta)]dx = \\int \\left[\\frac{\\partial f}{\\partial \\theta}p(x;\\theta) + f(x,\\theta)\\frac{\\partial p(x;\\theta)}{\\partial \\theta}\\right]dx\n\nThe issue here is that while the first term could be numerically integrated using the Monte Carlo, the second one can’t as it’s not an expectation.\n\nWould there be a way to transform our objective in such a way that the Monte Carlo estimator for the objective could be differentiated directly while ensuring that the resulting derivative is unbiased? We will see that there are two main solutions to that problem: by doing a change of measure, or a change of variables.","type":"content","url":"/cadp#derivative-estimation-for-stochastic-optimization","position":23},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"The Likelihood Ratio Method","lvl2":"Derivative Estimation for Stochastic Optimization"},"type":"lvl3","url":"/cadp#the-likelihood-ratio-method","position":24},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"The Likelihood Ratio Method","lvl2":"Derivative Estimation for Stochastic Optimization"},"content":"One solution comes from rewriting our objective using any distribution q(x):J(\\theta) = \\int f(x,\\theta)\\frac{p(x;\\theta)}{q(x)}q(x)dx = \\mathbb{E}_{x \\sim q(x)}\\left[f(x,\\theta)\\frac{p(x;\\theta)}{q(x)}\\right]\n\nLet’s write this more functionally by defining:J(\\theta) = \\mathbb{E}_{x \\sim q(x)}[h(x,\\theta)]\n, \\enspace h(x,\\theta) \\equiv f(x,\\theta)\\frac{p(x;\\theta)}{q(x)}\n\nNow when we differentiate J, it’s clear that we must take the partial derivative of h with respect to its second argument:\\frac{d}{d\\theta}J(\\theta) = \\mathbb{E}_{x \\sim q(x)}\\left[\\frac{\\partial h}{\\partial \\theta}(x,\\theta)\\right] = \\mathbb{E}_{x \\sim q(x)}\\left[f(x,\\theta)\\frac{\\partial}{\\partial \\theta}\\frac{p(x;\\theta)}{q(x)} + \\frac{p(x;\\theta)}{q(x)}\\frac{\\partial f}{\\partial \\theta}(x,\\theta)\\right]\n\nThe so-called “score function” derivative estimator is obtained for the choice of q(x) = p(x;\\theta), where the ratio simplifies to 1 and its derivative becomes the score function:\\frac{d}{d\\theta}J(\\theta) = \\mathbb{E}_{x \\sim p(x;\\theta)}\\left[f(x,\\theta)\\frac{\\partial \\log p(x,\\theta)}{\\partial \\theta} + \\frac{\\partial f(x,\\theta)}{\\partial \\theta}\\right]","type":"content","url":"/cadp#the-likelihood-ratio-method","position":25},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"The Reparameterization Trick","lvl2":"Derivative Estimation for Stochastic Optimization"},"type":"lvl3","url":"/cadp#the-reparameterization-trick","position":26},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"The Reparameterization Trick","lvl2":"Derivative Estimation for Stochastic Optimization"},"content":"An alternative approach eliminates the \\theta-dependence in the sampling distribution by expressing x through a deterministic transformation of the noise:x = g(\\epsilon,\\theta), \\quad \\epsilon \\sim q(\\epsilon)\n\nTherefore if we want to sample from some target distribution p(x;\\theta), we can do so by first sampling from a simple base distribution q(\\epsilon) (like a standard normal) and then transforming those samples through a carefully chosen function g. If g(\\cdot,\\theta) is invertible, the change of variables formula tells us how these distributions relate:p(x;\\theta) = q(g^{-1}(x,\\theta))\\left|\\det\\frac{\\partial g^{-1}(x,\\theta)}{\\partial x}\\right| = q(\\epsilon)\\left|\\det\\frac{\\partial g(\\epsilon,\\theta)}{\\partial \\epsilon}\\right|^{-1}\n\nFor example, if we want to sample from any multivariate Gaussian distributions with covariance matrix \\Sigma and mean \\mu, it suffices to be able to sample from a standard normal noise and compute the linear transformation:x = \\mu + \\Sigma^{1/2}\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,I)\n\nwhere \\Sigma^{1/2} is the matrix square root obtained via Cholesky decomposition. In the univariate case, this transformation is simply:x = \\mu + \\sigma \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,1)\n\nwhere \\sigma = \\sqrt{\\sigma^2} is the standard deviation (square root of the variance).","type":"content","url":"/cadp#the-reparameterization-trick","position":27},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl4":"Common Examples of Reparameterization","lvl3":"The Reparameterization Trick","lvl2":"Derivative Estimation for Stochastic Optimization"},"type":"lvl4","url":"/cadp#common-examples-of-reparameterization","position":28},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl4":"Common Examples of Reparameterization","lvl3":"The Reparameterization Trick","lvl2":"Derivative Estimation for Stochastic Optimization"},"content":"","type":"content","url":"/cadp#common-examples-of-reparameterization","position":29},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl5":"The Truncated Normal Distribution","lvl4":"Common Examples of Reparameterization","lvl3":"The Reparameterization Trick","lvl2":"Derivative Estimation for Stochastic Optimization"},"type":"lvl5","url":"/cadp#the-truncated-normal-distribution","position":30},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl5":"The Truncated Normal Distribution","lvl4":"Common Examples of Reparameterization","lvl3":"The Reparameterization Trick","lvl2":"Derivative Estimation for Stochastic Optimization"},"content":"When we need samples constrained to an interval [a,b], we can use the truncated normal distribution. To sample from it, we transform uniform noise through the inverse cumulative distribution function (CDF) of the standard normal:x = \\Phi^{-1}(u\\Phi(b) + (1-u)\\Phi(a)), \\quad u \\sim \\text{Uniform}(0,1)\n\nHere:\n\n\\Phi(z) = \\frac{1}{2}\\left[1 + \\text{erf}\\left(\\frac{z}{\\sqrt{2}}\\right)\\right] is the CDF of the standard normal distribution\n\n\\Phi^{-1} is its inverse (the quantile function)\n\n\\text{erf}(z) = \\frac{2}{\\sqrt{\\pi}}\\int_0^z e^{-t^2}dt is the error function\n\nThe resulting samples follow a normal distribution restricted to [a,b], with the density properly normalized over this interval.","type":"content","url":"/cadp#the-truncated-normal-distribution","position":31},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl5":"The Kumaraswamy Distribution","lvl4":"Common Examples of Reparameterization","lvl3":"The Reparameterization Trick","lvl2":"Derivative Estimation for Stochastic Optimization"},"type":"lvl5","url":"/cadp#the-kumaraswamy-distribution","position":32},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl5":"The Kumaraswamy Distribution","lvl4":"Common Examples of Reparameterization","lvl3":"The Reparameterization Trick","lvl2":"Derivative Estimation for Stochastic Optimization"},"content":"When we need samples in the unit interval [0,1], a natural choice might be the Beta distribution. However, its inverse CDF doesn’t have a closed form. Instead, we can use the Kumaraswamy distribution as a convenient approximation, which allows for a simple reparameterization:x = (1-(1-u^{\\alpha})^{1/\\beta}), \\quad u \\sim \\text{Uniform}(0,1)\n\nwhere:\n\n\\alpha, \\beta > 0 are shape parameters that control the distribution\n\n\\alpha determines the concentration around 0\n\n\\beta determines the concentration around 1\n\nThe distribution is similar to Beta(α,β) but with analytically tractable CDF and inverse CDF\n\nThe Kumaraswamy distribution has density:f(x; \\alpha, \\beta) = \\alpha\\beta x^{\\alpha-1}(1-x^{\\alpha})^{\\beta-1}, \\quad x \\in [0,1]","type":"content","url":"/cadp#the-kumaraswamy-distribution","position":33},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl5":"The Gumbel-Softmax Distribution","lvl4":"Common Examples of Reparameterization","lvl3":"The Reparameterization Trick","lvl2":"Derivative Estimation for Stochastic Optimization"},"type":"lvl5","url":"/cadp#the-gumbel-softmax-distribution","position":34},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl5":"The Gumbel-Softmax Distribution","lvl4":"Common Examples of Reparameterization","lvl3":"The Reparameterization Trick","lvl2":"Derivative Estimation for Stochastic Optimization"},"content":"When sampling from a categorical distribution with probabilities \\{\\pi_i\\}, one approach uses \\text{Gumbel}(0,1) noise combined with the argmax of log-perturbed probabilities:\\text{argmax}_i(\\log \\pi_i + g_i), \\quad g_i \\sim \\text{Gumbel}(0,1)\n\nThis approach, known in machine learning as the Gumbel-Max trick, relies on sampling Gumbel noise from uniform random variables through the transformation g_i = -\\log(-\\log(u_i)) where u_i \\sim \\text{Uniform}(0,1). To see why this gives us samples from the categorical distribution, consider the probability of selecting category i:\\begin{align*}\nP(\\text{argmax}_j(\\log \\pi_j + g_j) = i) &= P(\\log \\pi_i + g_i > \\log \\pi_j + g_j \\text{ for all } j \\neq i) \\\\\n&= P(g_i - g_j > \\log \\pi_j - \\log \\pi_i \\text{ for all } j \\neq i)\n\\end{align*}\n\nSince the difference of two Gumbel random variables follows a logistic distribution, g_i - g_j \\sim \\text{Logistic}(0,1), and these differences are independent for different j (due to the independence of the original Gumbel variables), we can write:\\begin{align*}\nP(\\text{argmax}_j(\\log \\pi_j + g_j) = i) &= \\prod_{j \\neq i} P(g_i - g_j > \\log \\pi_j - \\log \\pi_i) \\\\\n&= \\prod_{j \\neq i} \\frac{\\pi_i}{\\pi_i + \\pi_j} = \\pi_i\n\\end{align*}\n\nThe last equality requires some additional algebra to show, but follows from the fact that these probabilities must sum to 1 over all i.\n\nWhile we have shown that the Gumbel-Max trick gives us exact samples from a categorical distribution, the argmax operation isn’t differentiable. For stochastic optimization problems of the form:\\mathbb{E}_{x \\sim p(x;\\theta)}[f(x)] = \\mathbb{E}_{\\epsilon \\sim \\text{Gumbel}(0,1)}[f(g(\\epsilon,\\theta))]\n\nwe need g to be differentiable with respect to \\theta. This leads us to consider a continuous relaxation where we replace the hard argmax with a temperature-controlled softmax:z_i = \\frac{\\exp((\\log \\pi_i + g_i)/\\tau)}{\\sum_j \\exp((\\log \\pi_j + g_j)/\\tau)}\n\nAs \\tau \\to 0, this approximation approaches the argmax:\\lim_{\\tau \\to 0} \\frac{\\exp(x_i/\\tau)}{\\sum_j \\exp(x_j/\\tau)} = \\begin{cases} 1 & \\text{if } x_i = \\max_j x_j \\\\ 0 & \\text{otherwise} \\end{cases}\n\nThe resulting distribution over the probability simplex is called the Gumbel-Softmax (or Concrete) distribution. The temperature parameter \\tau controls the discreteness of our samples: smaller values give samples closer to one-hot vectors but with less stable gradients, while larger values give smoother gradients but more diffuse samples.","type":"content","url":"/cadp#the-gumbel-softmax-distribution","position":35},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"Numerical Analysis of Gradient Estimators","lvl2":"Derivative Estimation for Stochastic Optimization"},"type":"lvl3","url":"/cadp#numerical-analysis-of-gradient-estimators","position":36},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"Numerical Analysis of Gradient Estimators","lvl2":"Derivative Estimation for Stochastic Optimization"},"content":"Let us examine the behavior of our three gradient estimators for the stochastic optimization objective:J(\\theta) = \\mathbb{E}_{x \\sim \\mathcal{N}(\\theta,1)}[x^2\\theta]\n\nTo get an analytical expression for the derivative, first note that we can factor out \\theta to obtain J(\\theta) = \\theta\\mathbb{E}[x^2] where x \\sim \\mathcal{N}(\\theta,1). By definition of the variance, we know that \\text{Var}(x) = \\mathbb{E}[x^2] - (\\mathbb{E}[x])^2, which we can rearrange to \\mathbb{E}[x^2] = \\text{Var}(x) + (\\mathbb{E}[x])^2. Since x \\sim \\mathcal{N}(\\theta,1), we have \\text{Var}(x) = 1 and \\mathbb{E}[x] = \\theta, therefore \\mathbb{E}[x^2] = 1 + \\theta^2. This gives us:J(\\theta) = \\theta(1 + \\theta^2)\n\nNow differentiating with respect to \\theta using the product rule yields:\\frac{d}{d\\theta}J(\\theta) = 1 + 3\\theta^2\n\nFor concreteness, we fix \\theta = 1.0 and analyze samples drawn using Monte Carlo estimation with batch size 1000 and 1000 independent trials. Evaluating at \\theta = 1 gives us \\frac{d}{d\\theta}J(\\theta)\\big|_{\\theta=1} = 1 + 3(1)^2 = 4, which serves as our ground truth against which we compare our estimators:\n\nFirst, we consider the naive estimator that incorrectly differentiates the Monte Carlo approximation:\\hat{g}_{\\text{naive}}(\\theta) = \\frac{1}{N}\\sum_{i=1}^N x_i^2\n\nFor x \\sim \\mathcal{N}(1,1), we have \\mathbb{E}[x^2] = \\theta^2 + 1 = 2.0 and \\mathbb{E}[\\hat{g}_{\\text{naive}}] = 2.0. We should therefore expect a bias of about -2 in our experiment.\n\nThen we compute the score function estimator:\\hat{g}_{\\text{SF}}(\\theta) = \\frac{1}{N}\\sum_{i=1}^N \\left[x_i^2\\theta(x_i - \\theta) + x_i^2\\right]\n\nThis estimator is unbiased with \\mathbb{E}[\\hat{g}_{\\text{SF}}] = 4\n\nFinally, through the reparameterization x = \\theta + \\epsilon where \\epsilon \\sim \\mathcal{N}(0,1), we obtain:\\hat{g}_{\\text{RT}}(\\theta) = \\frac{1}{N}\\sum_{i=1}^N \\left[2\\theta(\\theta + \\epsilon_i) + (\\theta + \\epsilon_i)^2\\right]\n\nThis estimator is also unbiased with \\mathbb{E}[\\hat{g}_{\\text{RT}}] = 4.\n\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nkey = jax.random.PRNGKey(0)\n\n# Define the objective function f(x,θ) = x²θ where x ~ N(θ, 1)\ndef objective(x, theta):\n    return x**2 * theta\n\n# Naive Monte Carlo gradient estimation\n@jax.jit\ndef naive_gradient_batch(key, theta):\n    samples = jax.random.normal(key, (1000,)) + theta\n    # Use jax.grad on the objective with respect to theta\n    grad_fn = jax.grad(lambda t: jnp.mean(objective(samples, t)))\n    return grad_fn(theta)\n\n# Score function estimator (REINFORCE)\n@jax.jit\ndef score_function_batch(key, theta):\n    samples = jax.random.normal(key, (1000,)) + theta\n    # f(x,θ) * ∂logp(x|θ)/∂θ + ∂f(x,θ)/∂θ\n    # score function for N(θ,1) is (x-θ)\n    score = samples - theta\n    return jnp.mean(objective(samples, theta) * score + samples**2)\n\n# Reparameterization gradient\n@jax.jit\ndef reparam_gradient_batch(key, theta):\n    eps = jax.random.normal(key, (1000,))\n    # Use reparameterization x = θ + ε, ε ~ N(0,1)\n    grad_fn = jax.grad(lambda t: jnp.mean(objective(t + eps, t)))\n    return grad_fn(theta)\n\n# Run trials\nn_trials = 1000\ntheta = 1.0\ntrue_grad = 3 + theta**2\n\nkeys = jax.random.split(key, n_trials)\nnaive_estimates = jnp.array([naive_gradient_batch(k, theta) for k in keys])\nscore_estimates = jnp.array([score_function_batch(k, theta) for k in keys])\nreparam_estimates = jnp.array([reparam_gradient_batch(k, theta) for k in keys])\n\n# Create violin plots with individual points\nplt.figure(figsize=(12, 6))\ndata = [naive_estimates, score_estimates, reparam_estimates]\ncolors = ['#ff9999', '#66b3ff', '#99ff99']\n\nparts = plt.violinplot(data, showextrema=False)\nfor i, pc in enumerate(parts['bodies']):\n    pc.set_facecolor(colors[i])\n    pc.set_alpha(0.7)\n\n# Add box plots\nplt.boxplot(data, notch=True, showfliers=False)\n\n# Add true gradient line\nplt.axhline(y=true_grad, color='r', linestyle='--', label='True Gradient')\n\nplt.xticks([1, 2, 3], ['Naive', 'Score Function', 'Reparam'])\nplt.ylabel('Gradient Estimate')\nplt.title(f'Gradient Estimators (θ={theta}, true grad={true_grad:.2f})')\nplt.grid(True, alpha=0.3)\nplt.legend()\n\n# Print statistics\nmethods = {\n    'Naive': naive_estimates,\n    'Score Function': score_estimates, \n    'Reparameterization': reparam_estimates\n}\n\nfor name, estimates in methods.items():\n    bias = jnp.mean(estimates) - true_grad\n    variance = jnp.var(estimates)\n    print(f\"\\n{name}:\")\n    print(f\"Mean: {jnp.mean(estimates):.6f}\")\n    print(f\"Bias: {bias:.6f}\")\n    print(f\"Variance: {variance:.6f}\")\n    print(f\"MSE: {bias**2 + variance:.6f}\")\n\nThe numerical experiments coroborate our theory. The naive estimator consistently underestimates the true gradient by 2.0, though it maintains a relatively small variance. This systematic bias would make it unsuitable for optimization despite its low variance. The score function estimator corrects this bias but introduces substantial variance. While unbiased, this estimator would require many samples to achieve reliable gradient estimates. Finally, the reparameterization trick achieves a much lower variance while remaining unbiased. While this experiment is for didactic purposes only, it reproduces what is commonly found in practice: that when applicable, the reparameterization estimator tends to perform better than the score function counterpart.","type":"content","url":"/cadp#numerical-analysis-of-gradient-estimators","position":37},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl2":"Score Function Gradient Estimation in Reinforcement Learning"},"type":"lvl2","url":"/cadp#score-function-gradient-estimation-in-reinforcement-learning","position":38},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl2":"Score Function Gradient Estimation in Reinforcement Learning"},"content":"Let G(\\tau) \\equiv \\sum_{t=0}^T r(s_t, a_t) be the sum of undiscounted rewards in a trajectory \\tau. The stochastic optimization problem we face is to maximize:J(\\boldsymbol{w}) = \\mathbb{E}_{\\tau \\sim p(\\tau;\\boldsymbol{w})}[G(\\tau)]\n\nwhere \\tau = (s_0,a_0,s_1,a_1,...) is a trajectory and G(\\tau) is the total return.\nApplying the score function estimator, we get:\\begin{align*}\n\\nabla_{\\boldsymbol{w}}J(\\boldsymbol{w}) &= \\nabla_{\\boldsymbol{w}}\\mathbb{E}_{\\tau}[G(\\tau)] \\\\\n&= \\mathbb{E}_{\\tau}\\left[G(\\tau)\\nabla_{\\boldsymbol{w}}\\log p(\\tau;\\boldsymbol{w})\\right] \\\\\n&= \\mathbb{E}_{\\tau}\\left[G(\\tau)\\nabla_{\\boldsymbol{w}}\\sum_{t=0}^T\\log d(a_t|s_t;\\boldsymbol{w})\\right] \\\\\n&= \\mathbb{E}_{\\tau}\\left[G(\\tau)\\sum_{t=0}^T\\nabla_{\\boldsymbol{w}}\\log d(a_t|s_t;\\boldsymbol{w})\\right]\n\\end{align*}\n\nWe have eliminated the need to know the transition probabilities in this estimator since the probability of a trajectory factorizes as:p(\\tau;\\boldsymbol{w}) = p(s_0)\\prod_{t=0}^T d(a_t|s_t;\\boldsymbol{w})p(s_{t+1}|s_t,a_t)\n\nTherefore, only the policy depends on \\boldsymbol{w}. When taking the logarithm of this product, we get a sum where all the \\boldsymbol{w}-independent terms vanish. The final estimator samples trajectories under the distribution p(\\tau; \\boldsymbol{w}) and computes:\\nabla_{\\boldsymbol{w}}J(\\boldsymbol{w}) \\approx \\frac{1}{N}\\sum_{i=1}^N\\left[G(\\tau^{(i)})\\sum_{t=0}^T\\nabla_{\\boldsymbol{w}}\\log d(a_t^{(i)}|s_t^{(i)};\\boldsymbol{w})\\right]\n\nThis is a direct application of the score function estimator. However, we rarely use this form in practice and instead make several improvements to further reduce the variance.","type":"content","url":"/cadp#score-function-gradient-estimation-in-reinforcement-learning","position":39},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"Leveraging Conditional Independence","lvl2":"Score Function Gradient Estimation in Reinforcement Learning"},"type":"lvl3","url":"/cadp#leveraging-conditional-independence","position":40},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"Leveraging Conditional Independence","lvl2":"Score Function Gradient Estimation in Reinforcement Learning"},"content":"Given the Markov property of the MDP, rewards r_k for k < t are conditionally independent of action a_t given the history h_t = (s_0,a_0,...,s_{t-1},a_{t-1},s_t). This allows us to only need to consider future rewards when computing policy gradients.\\begin{align*}\n\\nabla_{\\boldsymbol{w}}J(\\boldsymbol{w}) &= \\mathbb{E}_{\\tau}\\left[\\sum_{t=0}^T\\nabla_{\\boldsymbol{w}}\\log d(a_t|s_t;\\boldsymbol{w})\\sum_{k=0}^T r_k\\right] \\\\\n&= \\mathbb{E}_{\\tau}\\left[\\sum_{t=0}^T\\nabla_{\\boldsymbol{w}}\\log d(a_t|s_t;\\boldsymbol{w})\\left(\\sum_{k=0}^{t-1} r_k + \\sum_{k=t}^T r_k\\right)\\right] \\\\\n&= \\mathbb{E}_{\\tau}\\left[\\sum_{t=0}^T\\nabla_{\\boldsymbol{w}}\\log d(a_t|s_t;\\boldsymbol{w})\\sum_{k=t}^T r_k\\right]\n\\end{align*}\n\nThe condition independence assumption means that the term \\mathbb{E}_{\\tau}\\left[\\sum_{t=0}^T\\nabla_{\\boldsymbol{w}}\\log d(a_t|s_t;\\boldsymbol{w})\\sum_{k=0}^{t-1} r_k \\right] vanishes. To see this, let’s factor the trajectory distribution as:p(\\tau) = p(s_0,...,s_t,a_0,...,a_{t-1})\\cdot d(a_t|s_t;\\boldsymbol{w})\\cdot p(s_{t+1},...,s_T,a_{t+1},...,a_T|s_t,a_t)\n\nWe can now re-write a single term of this summation as:\\mathbb{E}_{\\tau}\\left[\\nabla_{\\boldsymbol{w}}\\log d(a_t|s_t;\\boldsymbol{w})\\sum_{k=0}^{t-1} r_k\\right] = \\mathbb{E}_{s_{0:t},a_{0:t-1}}\\left[\\sum_{k=0}^{t-1} r_k \\cdot \\mathbb{E}_{a_t}\\left[\\nabla_{\\boldsymbol{w}}\\log d(a_t|s_t;\\boldsymbol{w})\\right]\\right]\n\nThe inner expectation is zero because\\begin{align*}\n\\mathbb{E}_{a_t}\\left[\\nabla_{\\boldsymbol{w}}\\log d(a_t|s_t;\\boldsymbol{w})\\right] &= \\int \\nabla_{\\boldsymbol{w}}\\log d(a_t|s_t;\\boldsymbol{w})d(a_t|s_t;\\boldsymbol{w})da_t \\\\\n&= \\int \\frac{\\nabla_{\\boldsymbol{w}}d(a_t|s_t;\\boldsymbol{w})}{d(a_t|s_t;\\boldsymbol{w})}d(a_t|s_t;\\boldsymbol{w})da_t \\\\\n&= \\int \\nabla_{\\boldsymbol{w}}d(a_t|s_t;\\boldsymbol{w})da_t \\\\\n&= \\nabla_{\\boldsymbol{w}}\\int d(a_t|s_t;\\boldsymbol{w})da_t \\\\\n&= \\nabla_{\\boldsymbol{w}}1 = 0\n\\end{align*}\n\nThe Monte Carlo estimator becomes:\\nabla_{\\boldsymbol{w}}J(\\boldsymbol{w}) \\approx \\frac{1}{N}\\sum_{i=1}^N\\left[\\sum_{t=0}^T\\nabla_{\\boldsymbol{w}}\\log d(a_t^{(i)}|s_t^{(i)};\\boldsymbol{w})\\sum_{k=t}^T r_k^{(i)}\\right]\n\nThe benefit of this estimator compared to the naive one is that it generally has less variance. More formally, we can show that this estimator arises from the application of a variance reduction technique known as the Extended Conditional Monte Carlo Method.","type":"content","url":"/cadp#leveraging-conditional-independence","position":41},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"Variance Reduction via Control Variates","lvl2":"Score Function Gradient Estimation in Reinforcement Learning"},"type":"lvl3","url":"/cadp#variance-reduction-via-control-variates","position":42},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"Variance Reduction via Control Variates","lvl2":"Score Function Gradient Estimation in Reinforcement Learning"},"content":"A control variate is a zero-mean random variable that we subtract from our estimator to reduce variance. Given an estimator Z and a control variate C with \\mathbb{E}[C]=0, we can construct a new unbiased estimator:Z_{\\text{cv}} = Z - \\alpha C\n\nwhere \\alpha is a coefficient we can choose. The variance of this new estimator is:\\text{Var}(Z_{\\text{cv}}) = \\text{Var}(Z) + \\alpha^2\\text{Var}(C) - 2\\alpha\\text{Cov}(Z,C)\n\nThe optimal \\alpha that minimizes this variance is:\\alpha^* = \\frac{\\text{Cov}(Z,C)}{\\text{Var}(C)}\n\nIn the reinforcement learning setting, we usually choose C_t = \\nabla_{\\boldsymbol{w}}\\log d(a_t|s_t;\\boldsymbol{w}) as our control variate at each timestep. For a given state s_t, our estimator at time t is:Z_t = \\nabla_{\\boldsymbol{w}}\\log d(a_t|s_t;\\boldsymbol{w})\\sum_{k=t}^T r_k\n\nOur control variate estimator becomes:Z_{t,\\text{cv}} = Z_t - \\alpha_t^* C_t = \\nabla_{\\boldsymbol{w}}\\log d(a_t|s_t;\\boldsymbol{w})(\\sum_{k=t}^T r_k - \\alpha_t^*)\n\nFollowing the general theory, and using the fact that \\mathbb{E}[C_t|s_t] = 0  the optimal coefficient is:\\begin{align*}\n\\alpha^*_t = \\frac{\\text{Cov}(Z_t,C_t|s_t)}{\\text{Var}(C_t|s_t)} &= \\frac{\\mathbb{E}[Z_tC_t^T|s_t] - \\mathbb{E}[Z_t|s_t]\\mathbb{E}[C_t^T|s_t]}{\\mathbb{E}[C_tC_t^T|s_t] - \\mathbb{E}[C_t|s_t]\\mathbb{E}[C_t^T|s_t]} \\\\\n&= \\frac{\\mathbb{E}[\\nabla_{\\boldsymbol{w}}\\log d(a_t|s_t;\\boldsymbol{w})\\nabla_{\\boldsymbol{w}}\\log d(a_t|s_t;\\boldsymbol{w})^T\\sum_{k=t}^T r_k|s_t] - 0}{\\mathbb{E}[\\nabla_{\\boldsymbol{w}}\\log d(a_t|s_t;\\boldsymbol{w})\\nabla_{\\boldsymbol{w}}\\log d(a_t|s_t;\\boldsymbol{w})^T|s_t] - 0} \\\\\n&= \\frac{\\mathbb{E}[\\|\\nabla_{\\boldsymbol{w}}\\log d(a_t|s_t;\\boldsymbol{w})\\|^2\\sum_{k=t}^T r_k|s_t]}{\\mathbb{E}[\\|\\nabla_{\\boldsymbol{w}}\\log d(a_t|s_t;\\boldsymbol{w})\\|^2|s_t]} \\\\\n&= \\frac{\\mathbb{E}_{a_t|s_t}[\\|\\nabla_{\\boldsymbol{w}}\\log d(a_t|s_t;\\boldsymbol{w})\\|^2]\\mathbb{E}[\\sum_{k=t}^T r_k|s_t]}{\\mathbb{E}_{a_t|s_t}[\\|\\nabla_{\\boldsymbol{w}}\\log d(a_t|s_t;\\boldsymbol{w})\\|^2]} \\\\\n&= \\mathbb{E}[\\sum_{k=t}^T r_k|s_t] = v^{d_{\\boldsymbol{w}}}(s_t)\n\\end{align*}\n\nTherefore, our variance-reduced estimator becomes:Z_{\\text{cv},t} = \\nabla_{\\boldsymbol{w}}\\log d(a_t|s_t;\\boldsymbol{w})\\left(\\sum_{k=t}^T r_k - v^{d_{\\boldsymbol{w}}}(s_t)\\right)\n\nIn practice when implementing this estimator, we won’t have access to the true value function. So as we did earlier for NFQCA or SAC, we commonly learn that value function simultaneously with the policy. Do do so, we could either using a fitted value approach, or even more simply just regress from states to sum of rewards to learn what Williams 1992 called a “baseline”:\n\nPolicy Gradient with Simple Baseline\n\nInput: Policy parameterization d(a|s;\\boldsymbol{w}), baseline function b(s;\\boldsymbol{\\theta})Output: Updated policy parameters \\boldsymbol{w}Hyperparameters: Learning rates \\alpha_w, \\alpha_\\theta, number of episodes N, episode length T\n\nInitialize parameters \\boldsymbol{w}, \\boldsymbol{\\theta}\n\nFor episode = 1, ..., N do:\n\nCollect trajectory \\tau = (s_0, a_0, r_0, ..., s_T, a_T, r_T) using policy d(a|s;\\boldsymbol{w})\n\nCompute returns for each timestep: R_t = \\sum_{k=t}^T r_k\n\nUpdate baseline: \\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} + \\alpha_\\theta \\nabla_{\\boldsymbol{\\theta}}\\sum_{t=0}^T (R_t - b(s_t;\\boldsymbol{\\theta}))^2\n\nFor t = 0, ..., T do:\n\nUpdate policy: \\boldsymbol{w} \\leftarrow \\boldsymbol{w} + \\alpha_w \\nabla_{\\boldsymbol{w}}\\log d(a_t|s_t;\\boldsymbol{w})(R_t - b(s_t;\\boldsymbol{\\theta}))\n\nReturn \\boldsymbol{w}\n\nWhen implementing this algorithm nowadays, we always using mini-batching to make full use of our GPUs. Therefore, a more representative variant for this algorithm would be:\n\nPolicy Gradient with Optimal Control Variate and Mini-batches\n\nInput: Policy parameterization d(a|s;\\boldsymbol{w}), value function v(s;\\boldsymbol{\\theta})Output: Updated policy parameters \\boldsymbol{w}Hyperparameters: Learning rates \\alpha_w, \\alpha_\\theta, number of iterations N, episode length T, batch size B, mini-batch size M\n\nInitialize parameters \\boldsymbol{w}, \\boldsymbol{\\theta}\n\nFor iteration = 1, ..., N:\n\nInitialize empty buffer \\mathcal{D}\n\nFor b = 1, ..., B:\n\nCollect trajectory \\tau_b = (s_0, a_0, r_0, ..., s_T, a_T, r_T) using policy d(a|s;\\boldsymbol{w})\n\nCompute returns: R_t = \\sum_{k=t}^T r_k for all t\n\nStore tuple (s_t, a_t, R_t)_{t=0}^T in \\mathcal{D}\n\nCompute value targets: v_{\\text{target}}(s) = \\frac{1}{|\\mathcal{D}_s|}\\sum_{(s,\\cdot,R) \\in \\mathcal{D}_s} R\n\nFor value_epoch = 1, ..., K:\n\nSample mini-batch \\mathcal{B}_v of size M from \\mathcal{D}\n\nCompute value loss: L_v = \\frac{1}{M}\\sum_{(s,\\cdot,R) \\in \\mathcal{B}_v} (v(s;\\boldsymbol{\\theta}) - v_{\\text{target}}(s))^2\n\nUpdate value function: \\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\alpha_\\theta \\nabla_{\\boldsymbol{\\theta}}L_v\n\nCompute advantages: A(s,a) = R - v(s;\\boldsymbol{\\theta}) for all (s,a,R) \\in \\mathcal{D}\n\nNormalize advantages: A \\leftarrow \\frac{A - \\mu_A}{\\sigma_A}\n\nFor policy_epoch = 1, ..., J:\n\nSample mini-batch \\mathcal{B}_\\pi of size M from \\mathcal{D}\n\nCompute policy loss: L_\\pi = -\\frac{1}{M}\\sum_{(s,a,\\cdot) \\in \\mathcal{B}_\\pi} \\log d(a|s;\\boldsymbol{w})A(s,a)\n\nUpdate policy: \\boldsymbol{w} \\leftarrow \\boldsymbol{w} - \\alpha_w \\nabla_{\\boldsymbol{w}}L_\\pi\n\nReturn \\boldsymbol{w}","type":"content","url":"/cadp#variance-reduction-via-control-variates","position":43},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"Generalized Advantage Estimator","lvl2":"Score Function Gradient Estimation in Reinforcement Learning"},"type":"lvl3","url":"/cadp#generalized-advantage-estimator","position":44},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"Generalized Advantage Estimator","lvl2":"Score Function Gradient Estimation in Reinforcement Learning"},"content":"Given our control variate estimator with baseline v(s), we have:\\nabla_{\\boldsymbol{w}}\\log d(a_t|s_t;\\boldsymbol{w})(G_t - v(s_t))\n\nwhere G_t is the return \\sum_{k=t}^T r_k. We can improve this estimator by considering how it relates to the advantage function, defined as:A(s_t,a_t) = q(s_t,a_t) - v(s_t)\n\nwhere q(s_t,a_t) is the action-value function. Due to the Bellman equation:q(s_t,a_t) = \\mathbb{E}_{s_{t+1},r_t}[r_t + \\gamma v(s_{t+1})|s_t,a_t]\n\nThis leads to the one-step TD error:\\delta_t = r_t + \\gamma v(s_{t+1}) - v(s_t)\n\nNow, let’s decompose our original term:\\begin{align*}\nG_t - v(s_t) &= r_t + \\gamma G_{t+1} - v(s_t) \\\\\n&= r_t + \\gamma v(s_{t+1}) - v(s_t) + \\gamma(G_{t+1} - v(s_{t+1})) \\\\\n&= \\delta_t + \\gamma(G_{t+1} - v(s_{t+1}))\n\\end{align*}\n\nExpanding recursively:\\begin{align*}\nG_t - v(s_t) &= \\delta_t + \\gamma(G_{t+1} - v(s_{t+1})) \\\\\n&= \\delta_t + \\gamma[\\delta_{t+1} + \\gamma(G_{t+2} - v(s_{t+2}))] \\\\\n&= \\delta_t + \\gamma\\delta_{t+1} + \\gamma^2\\delta_{t+2} + ... + \\gamma^{T-t}\\delta_T\n\\end{align*}\n\nGAE generalizes this by introducing a weighted version with parameter \\lambda:A^{\\text{GAE}(\\gamma,\\lambda)}(s_t,a_t) = (1-\\lambda)\\sum_{k=0}^{\\infty}\\lambda^k\\sum_{l=0}^k \\gamma^l\\delta_{t+l}\n\nWhich simplifies to:A^{\\text{GAE}(\\gamma,\\lambda)}(s_t,a_t) = \\sum_{l=0}^{\\infty}(\\gamma\\lambda)^l\\delta_{t+l}\n\nThis formulation allows us to trade off bias and variance through \\lambda:\n\n\\lambda=0: one-step TD error (low variance, high bias)\n\n\\lambda=1: Monte Carlo estimate (high variance, low bias)\n\nThe general GAE algorithm with mini-batches is the following:\n\nPolicy Gradient with GAE and Mini-batches\n\nInput: Policy parameterization d(a|s;\\boldsymbol{w}), value function v(s;\\boldsymbol{\\theta})Output: Updated policy parameters \\boldsymbol{w}Hyperparameters: Learning rates \\alpha_w, \\alpha_\\theta, number of iterations N, episode length T, batch size B, mini-batch size M, discount \\gamma, GAE parameter \\lambda\n\nInitialize parameters \\boldsymbol{w}, \\boldsymbol{\\theta}\n\nFor iteration = 1, ..., N:\n\nInitialize empty buffer \\mathcal{D}\n\nFor b = 1, ..., B:\n\nCollect trajectory \\tau_b = (s_0, a_0, r_0, ..., s_T, a_T, r_T) using policy d(a|s;\\boldsymbol{w})\n\nCompute TD errors: \\delta_t = r_t + \\gamma v(s_{t+1};\\boldsymbol{\\theta}) - v(s_t;\\boldsymbol{\\theta}) for all t\n\nCompute GAE advantages:\n\nInitialize A_T = 0\n\nFor t = T-1, ..., 0:\n\nA_t = \\delta_t + (\\gamma\\lambda)A_{t+1}\n\nStore tuple (s_t, a_t, A_t, v(s_t;\\boldsymbol{\\theta}))_{t=0}^T in \\mathcal{D}\n\nFor value_epoch = 1, ..., K:\n\nSample mini-batch \\mathcal{B}_v of size M from \\mathcal{D}\n\nCompute value loss: L_v = \\frac{1}{M}\\sum_{(s,\\cdot,\\cdot,v_{\\text{old}}) \\in \\mathcal{B}_v} (v(s;\\boldsymbol{\\theta}) - v_{\\text{old}})^2\n\nUpdate value function: \\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\alpha_\\theta \\nabla_{\\boldsymbol{\\theta}}L_v\n\nNormalize advantages: A \\leftarrow \\frac{A - \\mu_A}{\\sigma_A}\n\nFor policy_epoch = 1, ..., J:\n\nSample mini-batch \\mathcal{B}_\\pi of size M from \\mathcal{D}\n\nCompute policy loss: L_\\pi = -\\frac{1}{M}\\sum_{(s,a,A,\\cdot) \\in \\mathcal{B}_\\pi} \\log d(a|s;\\boldsymbol{w})A\n\nUpdate policy: \\boldsymbol{w} \\leftarrow \\boldsymbol{w} - \\alpha_w \\nabla_{\\boldsymbol{w}}L_\\pi\n\nReturn \\boldsymbol{w}\n\nWith \\lambda = 0, the GAE advantage estimator becomes just the one-step TD error:A^{\\text{GAE}(\\gamma,0)}(s_t,a_t) = \\delta_t = r_t + \\gamma v(s_{t+1}) - v(s_t)\n\nThe non-batched, purely online, GAE(0) algorithm then becomes:\n\nActor-Critic with TD(0)\n\nInput: Policy parameterization d(a|s;\\boldsymbol{w}), value function v(s;\\boldsymbol{\\theta})Output: Updated policy parameters \\boldsymbol{w}Hyperparameters: Learning rates \\alpha_w, \\alpha_\\theta, number of episodes N, episode length T, discount \\gamma\n\nInitialize parameters \\boldsymbol{w}, \\boldsymbol{\\theta}\n\nFor episode = 1, ..., N do:\n\nInitialize state s_0\n\nFor t = 0, ..., T do:\n\nSample action: a_t \\sim d(\\cdot|s_t;\\boldsymbol{w})\n\nExecute a_t, observe r_t, s_{t+1}\n\nCompute TD error: \\delta_t = r_t + \\gamma v(s_{t+1};\\boldsymbol{\\theta}) - v(s_t;\\boldsymbol{\\theta})\n\nUpdate value function: \\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} + \\alpha_\\theta \\delta_t \\nabla_{\\boldsymbol{\\theta}}v(s_t;\\boldsymbol{\\theta})\n\nUpdate policy: \\boldsymbol{w} \\leftarrow \\boldsymbol{w} + \\alpha_w \\nabla_{\\boldsymbol{w}}\\log d(a_t|s_t;\\boldsymbol{w})\\delta_t\n\nReturn \\boldsymbol{w}\n\nThis version was first derived by Richard Sutton in his 1984 PhD thesis. He called it the Adaptive Heuristic Actor-Critic algorithm. As far as I know, it was not derived using the score function method outlined here, but rather through intuitive reasoning (great intuition!).","type":"content","url":"/cadp#generalized-advantage-estimator","position":45},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"The Policy Gradient Theorem","lvl2":"Score Function Gradient Estimation in Reinforcement Learning"},"type":"lvl3","url":"/cadp#the-policy-gradient-theorem","position":46},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"The Policy Gradient Theorem","lvl2":"Score Function Gradient Estimation in Reinforcement Learning"},"content":"Sutton 1999 provided an expression for the gradient of the infinite discounted return with respect to the parameters of a parameterized policy. Here is an alternative derivation by considering a bilevel optimization problem:\\max_{\\mathbf{w}} \\alpha^\\top \\mathbf{v}_\\gamma^{d^\\infty}\n\nsubject to:(\\mathbf{I} - \\gamma \\mathbf{P}_d) \\mathbf{v}_\\gamma^{d^\\infty} = \\mathbf{r}_d\n\nThe Implicit Function Theorem states that if there is a solution to the problem F(\\mathbf{v}, \\mathbf{w}) = 0, then we can “reparameterize” our problem as F(\\mathbf{v}(\\mathbf{w}), \\mathbf{w}) where \\mathbf{v}(\\mathbf{w}) is an implicit function of \\mathbf{w}. If the Jacobian \\frac{\\partial F}{\\partial \\mathbf{v}} is invertible, then:\\frac{d\\mathbf{v}(\\mathbf{w})}{d\\mathbf{w}} = -\\left(\\frac{\\partial F(\\mathbf{v}(\\mathbf{w}), \\mathbf{w})}{\\partial \\mathbf{x}}\\right)^{-1}\\frac{\\partial F(\\mathbf{v}(\\mathbf{w}), \\mathbf{w})}{\\partial \\mathbf{w}}\n\nHere we made it clear in our notation that the derivative must be evaluated at root (\\mathbf{v}(\\mathbf{w}), \\mathbf{w}) of F. For the remaining of this derivation, we will drop this dependence to make notation more compact.\n\nApplying this to our case with F(\\mathbf{v}, \\mathbf{w}) = (\\mathbf{I} - \\gamma \\mathbf{P}_d)\\mathbf{v} - \\mathbf{r}_d:\\frac{\\partial \\mathbf{v}_\\gamma^{d^\\infty}}{\\partial \\mathbf{w}} = (\\mathbf{I} - \\gamma \\mathbf{P}_d)^{-1}\\left(\\frac{\\partial \\mathbf{r}_d}{\\partial \\mathbf{w}} + \\gamma \\frac{\\partial \\mathbf{P}_d}{\\partial \\mathbf{w}}\\mathbf{v}_\\gamma^{d^\\infty}\\right)\n\nThen:\\begin{align*}\n\\nabla_{\\mathbf{w}}J(\\mathbf{w}) &= \\alpha^\\top \\frac{\\partial \\mathbf{v}_\\gamma^{d^\\infty}}{\\partial \\mathbf{w}} \\\\\n&= \\mathbf{x}_\\alpha^\\top\\left(\\frac{\\partial \\mathbf{r}_d}{\\partial \\mathbf{w}} + \\gamma \\frac{\\partial \\mathbf{P}_d}{\\partial \\mathbf{w}}\\mathbf{v}_\\gamma^{d^\\infty}\\right)\n\\end{align*}\n\nwhere we have defined the discounted state visitation distribution:\\mathbf{x}_\\alpha^\\top \\equiv \\alpha^\\top(\\mathbf{I} - \\gamma \\mathbf{P}_d)^{-1}.\n\nRemember the vector notation for MDPs:\\begin{align*}\n[\\mathbf{r}_d]_s &= \\sum_a d(a|s)r(s,a) \\\\\n[\\mathbf{P}_d]_{ss'} &= \\sum_a d(a|s)P(s'|s,a)\n\\end{align*}\n\nThen taking the derivatives gives us:\\begin{align*}\n\\left[\\frac{\\partial \\mathbf{r}_d}{\\partial \\mathbf{w}}\\right]_s &= \\sum_a \\nabla_{\\mathbf{w}}d(a|s)r(s,a) \\\\\n\\left[\\frac{\\partial \\mathbf{P}_d}{\\partial \\mathbf{w}}\\mathbf{v}_\\gamma^{d^\\infty}\\right]_s &= \\sum_a \\nabla_{\\mathbf{w}}d(a|s)\\sum_{s'} P(s'|s,a)v_\\gamma^{d^\\infty}(s')\n\\end{align*}\n\nSubstituting back:\\begin{align*}\n\\nabla_{\\mathbf{w}}J(\\mathbf{w}) &= \\sum_s x_\\alpha(s)\\left(\\sum_a \\nabla_{\\mathbf{w}}d(a|s)r(s,a) + \\gamma\\sum_a \\nabla_{\\mathbf{w}}d(a|s)\\sum_{s'} P(s'|s,a)v_\\gamma^{d^\\infty}(s')\\right) \\\\\n&= \\sum_s x_\\alpha(s)\\sum_a \\nabla_{\\mathbf{w}}d(a|s)\\left(r(s,a) + \\gamma \\sum_{s'} P(s'|s,a)v_\\gamma^{d^\\infty}(s')\\right)\n\\end{align*}\n\nThis is the policy gradient theorem, where x_\\alpha(s) is the discounted state visitation distribution and the term in parentheses is the state-action value function q(s,a).","type":"content","url":"/cadp#the-policy-gradient-theorem","position":47},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"Normalized Discounted State Visitation Distribution","lvl2":"Score Function Gradient Estimation in Reinforcement Learning"},"type":"lvl3","url":"/cadp#normalized-discounted-state-visitation-distribution","position":48},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"Normalized Discounted State Visitation Distribution","lvl2":"Score Function Gradient Estimation in Reinforcement Learning"},"content":"The discounted state visitation x_\\alpha(s) is not normalized. Therefore the expression we obtained above is not an expectation. However, we can tranform it into one by normalizing by 1-  \\gamma. Note that for any initial distribution \\alpha:\\sum_s x_\\alpha(s) = \\alpha^\\top(\\mathbf{I} - \\gamma \\mathbf{P}_d)^{-1}\\mathbf{1} = \\frac{\\alpha^\\top\\mathbf{1}}{1-\\gamma} = \\frac{1}{1-\\gamma}\n\nTherefore, defining the normalized state distribution \\mu_\\alpha(s) = (1-\\gamma)x_\\alpha(s), we can write:\\begin{align*}\n\\nabla_{\\mathbf{w}}J(\\mathbf{w}) &= \\frac{1}{1-\\gamma}\\sum_s \\mu_\\alpha(s)\\sum_a \\nabla_{\\mathbf{w}}d(a|s)\\left(r(s,a) + \\gamma \\sum_{s'} P(s'|s,a)v_\\gamma^{d^\\infty}(s')\\right) \\\\\n&= \\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim\\mu_\\alpha}\\left[\\sum_a \\nabla_{\\mathbf{w}}d(a|s)Q(s,a)\\right]\n\\end{align*}\n\nNow we have expressed the policy gradient theorem in terms of expectations under the normalized discounted state visitation distribution. But what does sampling from \\mu_\\alpha means? Recall that \\mathbf{x}_\\alpha^\\top = \\alpha^\\top(\\mathbf{I} - \\gamma \\mathbf{P}_d)^{-1}. Using the Neumann series expansion (valid when \\|\\gamma \\mathbf{P}_d\\| < 1, which holds for \\gamma < 1 since \\mathbf{P}_d is a stochastic matrix) we have:\\boldsymbol{\\mu}_\\alpha^\\top = (1-\\gamma)\\alpha^\\top\\sum_{k=0}^{\\infty} (\\gamma \\mathbf{P}_d)^k\n\nWe can then factor out the first term from this summation to obtain:\ns\\begin{align*}\n\\boldsymbol{\\mu}_\\alpha^\\top &= (1-\\gamma)\\alpha^\\top\\sum_{k=0}^{\\infty} (\\gamma \\mathbf{P}_d)^k \\\\\n&= (1-\\gamma)\\alpha^\\top + (1-\\gamma)\\alpha^\\top\\sum_{k=1}^{\\infty} (\\gamma \\mathbf{P}_d)^k \\\\\n&= (1-\\gamma)\\alpha^\\top + (1-\\gamma)\\alpha^\\top\\gamma\\mathbf{P}_d\\sum_{k=0}^{\\infty} (\\gamma \\mathbf{P}_d)^k \\\\\n&= (1-\\gamma)\\alpha^\\top + \\gamma\\boldsymbol{\\mu}_\\alpha^\\top \\mathbf{P}_d\n\\end{align*}\n\nThe balance equation :\\boldsymbol{\\mu}_\\alpha^\\top = (1-\\gamma)\\alpha^\\top + \\gamma\\boldsymbol{\\mu}_\\alpha^\\top \\mathbf{P}_d\n\nshows that \\boldsymbol{\\mu}_\\alpha is a mixture distribution: with probability 1-\\gamma you draw a state from the initial distribution \\alpha (reset), and with probability \\gamma you follow the policy dynamics \\mathbf{P}_d from the current state (continue). This interpretation directly connects to the geometric process: at each step you either terminate and resample from \\alpha (with probability 1-\\gamma) or continue following the policy (with probability \\gamma).\n\nimport numpy as np\n\ndef sample_from_discounted_visitation(\n    alpha, \n    policy, \n    transition_model, \n    gamma, \n    n_samples=1000\n):\n    \"\"\"Sample states from the discounted visitation distribution.\n    \n    Args:\n        alpha: Initial state distribution (vector of probabilities)\n        policy: Function (state -> action probabilities)\n        transition_model: Function (state, action -> next state probabilities)\n        gamma: Discount factor\n        n_samples: Number of states to sample\n    \n    Returns:\n        Array of sampled states\n    \"\"\"\n    samples = []\n    n_states = len(alpha)\n    \n    # Initialize state from alpha\n    current_state = np.random.choice(n_states, p=alpha)\n    \n    for _ in range(n_samples):\n        samples.append(current_state)\n        \n        # With probability (1-gamma): reset\n        if np.random.random() > gamma:\n            current_state = np.random.choice(n_states, p=alpha)\n        # With probability gamma: continue\n        else:\n            # Sample action from policy\n            action_probs = policy(current_state)\n            action = np.random.choice(len(action_probs), p=action_probs)\n            \n            # Sample next state from transition model\n            next_state_probs = transition_model(current_state, action)\n            current_state = np.random.choice(n_states, p=next_state_probs)\n    \n    return np.array(samples)\n\n# Example usage for a simple 2-state MDP\nalpha = np.array([0.7, 0.3])  # Initial distribution\npolicy = lambda s: np.array([0.8, 0.2])  # Dummy policy\ntransition_model = lambda s, a: np.array([0.9, 0.1])  # Dummy transitions\ngamma = 0.9\n\nsamples = sample_from_discounted_visitation(alpha, policy, transition_model, gamma)\n\n# Check empirical distribution\nprint(\"Empirical state distribution:\")\nprint(np.bincount(samples) / len(samples))\n\nWhile the math shows that sampling from the discounted visitation distribution \\boldsymbol{\\mu}_\\alpha would give us unbiased policy gradient estimates, Thomas (2014) demonstrated that this implementation can be detrimental to performance in practice. The issue arises because terminating trajectories early (with probability 1-\\gamma) reduces the effective amount of data we collect from each trajectory. This early termination weakens the learning signal, as many trajectories don’t reach meaningful terminal states or rewards.\n\nTherefore, in practice, we typically sample complete trajectories from the undiscounted process (i.e., run the policy until natural termination or a fixed horizon) while still using \\gamma in the advantage estimation. This approach preserves the full learning signal from each trajectory\nand has been empirically shown to lead to better performance.\n\nThis is one of several cases in RL where the theoretically optimal procedure differs from the best practical implementation.","type":"content","url":"/cadp#normalized-discounted-state-visitation-distribution","position":49},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl2":"Policy Optimization with a Model"},"type":"lvl2","url":"/cadp#policy-optimization-with-a-model","position":50},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl2":"Policy Optimization with a Model"},"content":"In this section, we’ll explore how incorporating a model of the dynamics can help us design better policy gradient estimators. Let’s begin with a pure model-free approach that uses a critic to maximize a deterministic policy:J(\\boldsymbol{w}) = \\mathbb{E}_{s\\sim\\rho}[Q(s,d(s;\\boldsymbol{w}))], \\enspace \\nabla_{\\boldsymbol{w}} J(\\boldsymbol{w}) = \\mathbb{E}_{s\\sim\\rho}[\\nabla_a Q(s,a)|_{a=d(s;\\boldsymbol{w})} \\nabla_{\\boldsymbol{w}} d(s;\\boldsymbol{w})]\n\nUsing the recursive structure of the Bellman equations, we can unroll our objective one step ahead:J(\\boldsymbol{w}) = \\mathbb{E}_{s\\sim\\rho}[r(s,d(s;\\boldsymbol{w})) + \\gamma\\mathbb{E}_{s'\\sim p(\\cdot|s,d(s;\\boldsymbol{w}))}[Q(s',d(s';\\boldsymbol{w}))]],\n\nTo differentiate this objective, we need access to both a model of the dynamics and the reward function, as shown in the following expression:\\nabla_{\\boldsymbol{w}} J(\\boldsymbol{w}) = \\mathbb{E}_{s\\sim\\rho}[\\nabla_a r(s,a)|_{a=d(s;\\boldsymbol{w})}\\nabla_{\\boldsymbol{w}} d(s;\\boldsymbol{w}) + \n\\gamma(\\mathbb{E}_{s'\\sim p(\\cdot|s,d(s;\\boldsymbol{w}))}[\\nabla_a Q(s',a)|_{a=d(s';\\boldsymbol{w})}\\nabla_{\\boldsymbol{w}} d(s';\\boldsymbol{w})] + \\\\\\nabla_{\\boldsymbol{w}} d(s;\\boldsymbol{w})\\int_{s'} \\nabla_a p(s'|s,a)|_{a=d(s;\\boldsymbol{w})}Q(s',d(s';\\boldsymbol{w}))ds')]\n\nWhile \\boldsymbol{w} doesn’t appear in the outer expectation over initial states, it affects the inner expectation over next states—a distributional dependency. As a result, the product rule of calculus yields two terms: the first being an expectation, and the second being problematic for sample average estimation. However, we have tools to address this challenge: we can either apply the reparameterization trick to the dynamics or use score function estimators.\n\nFor the reparameterization approach, assuming s' = f(s,a,\\xi;\\boldsymbol{w}) where \\xi is the noise variable:\\begin{align*}\nJ^{\\text{DPMB-R}}(\\boldsymbol{w}) &= \\mathbb{E}_{s\\sim\\rho,\\xi}[r(s,d(s;\\boldsymbol{w})) + \\gamma Q(f(s,d(s;\\boldsymbol{w}),\\xi),d(f(s,d(s;\\boldsymbol{w}),\\xi);\\boldsymbol{w}))]\\\\\n\\nabla_{\\boldsymbol{w}} J^{\\text{DPMB-R}}(\\boldsymbol{w}) &= \\mathbb{E}_{s\\sim\\rho,\\xi}[\\nabla_a r(s,a)|_{a=d(s;\\boldsymbol{w})}\\nabla_{\\boldsymbol{w}} d(s;\\boldsymbol{w}) + \\\\\n&\\gamma(\\nabla_a Q(s',a)|_{a=d(s';\\boldsymbol{w})}\\nabla_{\\boldsymbol{w}} d(s';\\boldsymbol{w}) + \n\\nabla_{s'} Q(s',d(s';\\boldsymbol{w}))\\nabla_{\\boldsymbol{w}} f(s,d(s;\\boldsymbol{w}),\\xi)\\nabla_{\\boldsymbol{w}} d(s;\\boldsymbol{w}))]\n\\end{align*}\n\nAs for the score function approach:\\begin{align*}\n\\nabla_{\\boldsymbol{w}} J^{\\text{DPMB-SF}}(\\boldsymbol{w}) = \\mathbb{E}_{s\\sim\\rho}[&\\nabla_a r(s,a)|_{a=d(s;\\boldsymbol{w})}\\nabla_{\\boldsymbol{w}} d(s;\\boldsymbol{w}) + \\\\\n&\\gamma\\mathbb{E}_{s'\\sim p(\\cdot|s,d(s;\\boldsymbol{w}))}[\\nabla_{\\boldsymbol{w}} \\log p(s'|s,d(s;\\boldsymbol{w}))Q(s',d(s';\\boldsymbol{w})) + \\nabla_a Q(s',a)|_{a=d(s';\\boldsymbol{w})}\\nabla_{\\boldsymbol{w}} d(s';\\boldsymbol{w})]]\n\\end{align*}\n\nWe’ve now developed a hybrid algorithm that combines model-based and model-free approaches while integrating derivative estimators for stochastic dynamics with a deterministic policy parameterization. While this hybrid estimator remains relatively unexplored in practice, it could prove valuable for systems with specific structural properties.\n\nConsider a robotics scenario with hybrid continuous-discrete dynamics: a robotic arm operates in continuous space but interacts with discrete object states. While the arm’s policy remains differentiable (\\nabla_{\\boldsymbol{w}} d), the object state transitions follow categorical distributions. In this case, reparameterization becomes impractical, but the score function approach is viable if we can compute \\nabla_{\\boldsymbol{w}} \\log p(s'|s,d(s;\\boldsymbol{w})) from the known transition model. Similar structures arise in manufacturing processes, where machine actions might be continuous and differentiable, but material state transitions often follow discrete steps with known probabilities. Note that both approaches require knowledge of transition probabilities and won’t work with pure black-box simulators or systems where we can only sample transitions without probability estimates.\n\nAnother dimension to explore in our algorithm design is the number of steps we wish to unroll our model. This allows us to better understand and control the bias-variance tradeoffs in our methods.","type":"content","url":"/cadp#policy-optimization-with-a-model","position":51},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"Backpropagation Policy Optimization","lvl2":"Policy Optimization with a Model"},"type":"lvl3","url":"/cadp#backpropagation-policy-optimization","position":52},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"Backpropagation Policy Optimization","lvl2":"Policy Optimization with a Model"},"content":"The questions of derivative estimators only arise with stochastic dynamics. For systems with deterministic dynamics and a deterministic policy, the one-step gradient unroll simplifies to:\\begin{align*}\n\\nabla_{\\boldsymbol{w}} J(\\boldsymbol{w}) = &\\mathbb{E}_{s\\sim\\rho}[\\nabla_a r(s,a)|_{a=d(s;\\boldsymbol{w})}\\nabla_{\\boldsymbol{w}} d(s;\\boldsymbol{w}) + \\\\\n&\\gamma(\\nabla_a Q(s',a)|_{a=d(s';\\boldsymbol{w})}\\nabla_{\\boldsymbol{w}} d(s';\\boldsymbol{w}) + \n\\nabla_{\\boldsymbol{w}} d(s;\\boldsymbol{w})\\nabla_a f(s,a)|_{a=d(s;\\boldsymbol{w})}\\nabla_{s'} Q(s',d(s';\\boldsymbol{w})))]\n\\end{align*}\n\nwhere s' = f(s,d(s;\\boldsymbol{w})) is the deterministic next state. Notice the resemblance between this expression and that obtained for \\nabla_{\\boldsymbol{w}} J^{\\text{DPMB-R}} above. They are essentially the same except that in the reparameterized case, the dynamics have made the dependence on the noise variable explicit and the outer expectation has been updated accordingly. This similarity arises because differentiation through reparameterized dynamics models is, in essence, backpropagation: it tracks the propagation of perturbations through a computation graph—which we refer to as a stochastic computation graph in this setting.\n\nStill under this simplified setting with deterministic policies and dynamics, we can extend the expression for the gradient through n-steps of model unroll, leading to:\\begin{align*}\n\\nabla_{\\boldsymbol{w}} J(\\boldsymbol{w}) = &\\mathbb{E}_{s\\sim\\rho}[\\sum_{t=0}^{n-1} \\gamma^t(\\nabla_a r(s_t,a_t)|_{a_t=d(s_t;\\boldsymbol{w})}\\nabla_{\\boldsymbol{w}} d(s_t;\\boldsymbol{w}) + \\nabla_{s_t} r(s_t,d(s_t;\\boldsymbol{w}))\\nabla_{\\boldsymbol{w}} s_t) + \\\\\n&\\gamma^n(\\nabla_a Q(s_n,a)|_{a=d(s_n;\\boldsymbol{w})}\\nabla_{\\boldsymbol{w}} d(s_n;\\boldsymbol{w}) + \\nabla_{s_n} Q(s_n,d(s_n;\\boldsymbol{w}))\\nabla_{\\boldsymbol{w}} s_n)]\n\\end{align*}\n\nwhere s_{t+1} = f(s_t,d(s_t;\\boldsymbol{w})) for t=0,\\ldots,n-1, s_0=s, and \\nabla_{\\boldsymbol{w}} s_t follows the recursive relationship:\\nabla_{\\boldsymbol{w}} s_{t+1} = \\nabla_a f(s_t,a)|_{a=d(s_t;\\boldsymbol{w})}\\nabla_{\\boldsymbol{w}} d(s_t;\\boldsymbol{w}) + \\nabla_s f(s_t,d(s_t;\\boldsymbol{w}))\\nabla_{\\boldsymbol{w}} s_t\n\nwith base case \\nabla_{\\boldsymbol{w}} s_0 = 0 since the initial state does not depend on the policy parameters.\n\nAt both ends of the spectrum, we have that for n=0, we fall back to the pure critic NFQCA-like approach, and for n=\\infty, we don’t bootstrap at all and are fully model-based without a critic. The pure model-based critic-free approach to optimization is what we may refer to as a backpropagation-based policy optimization (BPO).\n\nBut just as backpropagation through RNNs or very deep networks can be challenging due to exploding and vanishing gradients, “vanilla” Backpropagation Policy Optimization (BPO) without a critic can severely suffer from the curse of horizon. This is because it essentially implements single shooting optimization. Using a critic can be an effective remedy to this problem, allowing us to better control the bias-variance tradeoff while preserving gradient information that would be lost with a more drastic truncated backpropagation approach.","type":"content","url":"/cadp#backpropagation-policy-optimization","position":53},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"Stochastic Value Gradient (SVG)","lvl2":"Policy Optimization with a Model"},"type":"lvl3","url":"/cadp#stochastic-value-gradient-svg","position":54},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"Stochastic Value Gradient (SVG)","lvl2":"Policy Optimization with a Model"},"content":"The stochastic value gradient framework of Heess (2015) applies the recipe for policy optimization with a model using reparameterized dynamics and action selection via randomized policies. In this setting, the stochastic policy model based reparameterized estimator over n steps is\\begin{align*}\nJ^{\\text{SPMB-R-N}}(\\boldsymbol{w}) &= \\mathbb{E}_{s\\sim\\rho,\\{\\epsilon_i\\}_{i=0}^{n-1},\\{\\xi_i\\}_{i=0}^{n-1}}[\\sum_{i=0}^{n-1} \\gamma^i r(s_i,d(s_i,\\epsilon_i;\\boldsymbol{w})) + \\gamma^n Q(s_n,d(s_n,\\epsilon_n;\\boldsymbol{w}))]\n\\end{align*}\n\nwhere s_0 = s and for i \\geq 0, s_{i+1} = f(s_i,d(s_i,\\epsilon_i;\\boldsymbol{w}),\\xi_i). The gradient becomes:\\begin{align*}\n\\nabla_{\\boldsymbol{w}} J^{\\text{SPMB-R-N}}(\\boldsymbol{w}) &= \\mathbb{E}_{s\\sim\\rho,\\{\\epsilon_i\\}_{i=0}^{n-1},\\{\\xi_i\\}_{i=0}^{n-1}}[\\sum_{i=0}^{n-1} \\gamma^i \\left(\\nabla_a r(s_i,a)|_{a=d(s_i,\\epsilon_i;\\boldsymbol{w})}\\nabla_{\\boldsymbol{w}} d(s_i,\\epsilon_i;\\boldsymbol{w}) + \\nabla_{s_i} r(s_i,d(s_i,\\epsilon_i;\\boldsymbol{w}))\\nabla_{\\boldsymbol{w}} s_i\\right) \\enspace + \\\\\n&\\qquad \\gamma^n(\\nabla_a Q(s_n,a)|_{a=d(s_n,\\epsilon_n;\\boldsymbol{w})}\\nabla_{\\boldsymbol{w}} d(s_n,\\epsilon_n;\\boldsymbol{w}) + \\nabla_{s_n} Q(s_n,d(s_n,\\epsilon_n;\\boldsymbol{w}))\\nabla_{\\boldsymbol{w}} s_n)]\n\\end{align*}\n\nwhere \\nabla_{\\boldsymbol{w}} s_0 = 0 and for i \\geq 0:\\nabla_{\\boldsymbol{w}} s_{i+1} = \\nabla_a f(s_i,a,\\xi_i)|_{a=d(s_i,\\epsilon_i;\\boldsymbol{w})}\\nabla_{\\boldsymbol{w}} d(s_i,\\epsilon_i;\\boldsymbol{w}) + \\nabla_{s_i} f(s_i,d(s_i,\\epsilon_i;\\boldsymbol{w}),\\xi_i)\\nabla_{\\boldsymbol{w}} s_i\n\nWhile we could implement this expression for the gradient ourselves, this approach is much easier, less error-prone, and most likely better optimized for performance when using automatic differentiation. Given  a set of rollouts (for which we know the primitive noise variables), then we can compute the monte carlo objective:\\hat{J}^{\\text{SPMB-R-N}}(\\boldsymbol{w}) = \\frac{1}{M}\\sum_{m=1}^M [\\sum_{i=0}^{n-1} \\gamma^i r(s_i^m,d(s_i^m,\\epsilon_i^m;\\boldsymbol{w})) + \\gamma^n Q(s_n^m,d(s_n^m,\\epsilon_n^m;\\boldsymbol{w}))]\n\nwhere the states are generated recursively using the known noise variables: starting with initial state s_0^m, each subsequent state is computed as s_{i+1}^m = f(s_i^m,d(s_i^m,\\epsilon_i^m;\\boldsymbol{w}),\\xi_i^m). Thus,  a trajectory is completely determined by just the sequence of noise variables:(\\epsilon_0^m, \\xi_0^m, \\epsilon_1^m, \\xi_1^m, ..., \\epsilon_{n-1}^m, \\xi_{n-1}^m, \\epsilon_n^m) where \\epsilon_i^m are the action noise variables and \\xi_i^m are the dynamics noise variables.\n\nThe choice of unroll steps n gives us precise control over the balance between model-based and critic-based components in our gradient estimator. At one extreme, setting n = 0 yields a purely model-free algorithm known as SVG(0) (Heess et al., 2015), which relies entirely on the critic for value estimation:\\hat{J}^{\\text{SPMB-R-0}}(\\boldsymbol{w}) = \\frac{1}{M}\\sum_{m=1}^M Q(s_0^m,d(s_0^m,\\epsilon_0^m;\\boldsymbol{w}))\n\nAt the other extreme, as n \\to \\infty, we can drop the critic term (since \\gamma^n Q \\to 0) to obtain a purely model-based algorithm, SVG(∞):\\hat{J}^{\\text{SPMB-R-$\\infty$}}(\\boldsymbol{w}) = \\frac{1}{M}\\sum_{m=1}^M \\sum_{i=0}^{\\infty} \\gamma^i r(s_i^m,d(s_i^m,\\epsilon_i^m;\\boldsymbol{w}))\n\nIn the 2015 paper, the authors make a specific choice to reparameterize both the dynamics and action selections using normal distributions. For the policy, they use:a_t = d(s_t;\\boldsymbol{w}) + \\sigma(s_t;\\boldsymbol{w})\\epsilon_t, \\quad \\epsilon_t \\sim \\mathcal{N}(0,I)\n\nwhere d(s_t;\\boldsymbol{w}) predicts the mean action and \\sigma(s_t;\\boldsymbol{w}) predicts the standard deviation. For the dynamics:s_{t+1} = \\mu(s_t,a_t;\\boldsymbol{\\phi}) + \\Sigma(s_t,a_t;\\boldsymbol{\\phi})\\xi_t, \\quad \\xi_t \\sim \\mathcal{N}(0,I)\n\nwhere \\mu(s_t,a_t;\\boldsymbol{\\phi}) predicts the mean next state and \\Sigma(s_t,a_t;\\boldsymbol{\\phi}) predicts the covariance matrix.\n\nUnder this reparameterization, the n-step surrogate loss becomes:\\begin{align*}\n\\hat{J}^{\\text{SPMB-R-n}}(\\boldsymbol{w}) = \\frac{1}{M}&\\sum_{m=1}^M [\\sum_{t=0}^{n-1} \\gamma^t r(s_t^m(\\boldsymbol{w}),d(s_t^m(\\boldsymbol{w});\\boldsymbol{w}) + \\sigma(s_t^m(\\boldsymbol{w});\\boldsymbol{w})\\epsilon_t^m) + \\\\\n&\\gamma^n Q(s_n^m(\\boldsymbol{w}),d(s_n^m(\\boldsymbol{w});\\boldsymbol{w}) + \\sigma(s_n^m(\\boldsymbol{w});\\boldsymbol{w})\\epsilon_n^m)]\n\\end{align*}\n\nwhere:s_{t+1}^m(\\boldsymbol{w}) = \\mu(s_t^m(\\boldsymbol{w}),d(s_t^m(\\boldsymbol{w});\\boldsymbol{w}) + \\sigma(s_t^m(\\boldsymbol{w});\\boldsymbol{w})\\epsilon_t^m;\\boldsymbol{\\phi}) + \\Sigma(s_t^m(\\boldsymbol{w}),a_t^m(\\boldsymbol{w});\\boldsymbol{\\phi})\\xi_t^m\n\nStochastic Value Gradients (SVG) Infinity (automatic differentiation)\n\nInput Initial state distribution \\rho_0(s), policy networks d(s;\\boldsymbol{w}) and \\sigma(s;\\boldsymbol{w}), dynamics model networks \\mu(s,a;\\boldsymbol{\\phi}) and \\Sigma(s,a;\\boldsymbol{\\phi}), reward function r(s,a), rollout horizon T, learning rate \\alpha_w, batch size N\n\nInitialize\n\nPolicy parameters \\boldsymbol{w}_0 randomly\n\nn \\leftarrow 0\n\nwhile not converged:\n\nSample batch of N initial states s_0^i \\sim \\rho_0(s)\n\nSample noise sequences \\epsilon_{0:T}^i, \\xi_{0:T}^i \\sim \\mathcal{N}(0,I) for i=1,\\ldots,N\n\nCompute objective using autodiff-enabled computation graph:\n\nFor each i=1,\\ldots,N:\n\nInitialize s_0^i(\\boldsymbol{w}) = s_0^i\n\nFor t=0 to T:\n\na_t^i(\\boldsymbol{w}) = d(s_t^i(\\boldsymbol{w});\\boldsymbol{w}) + \\sigma(s_t^i(\\boldsymbol{w});\\boldsymbol{w})\\epsilon_t^i\n\ns_{t+1}^i(\\boldsymbol{w}) = \\mu(s_t^i(\\boldsymbol{w}),a_t^i(\\boldsymbol{w});\\boldsymbol{\\phi}) + \\Sigma(s_t^i(\\boldsymbol{w}),a_t^i(\\boldsymbol{w});\\boldsymbol{\\phi})\\xi_t^i\n\nJ(\\boldsymbol{w}) = \\frac{1}{N}\\sum_{i=1}^N \\sum_{t=0}^T \\gamma^t r(s_t^i(\\boldsymbol{w}),a_t^i(\\boldsymbol{w}))\n\nCompute gradient using autodiff: \\nabla_{\\boldsymbol{w}}J\n\nUpdate policy: \\boldsymbol{w}_{n+1} \\leftarrow \\boldsymbol{w}_n + \\alpha_w \\nabla_{\\boldsymbol{w}}J\n\nn \\leftarrow n + 1\n\nreturn \\boldsymbol{w}_n","type":"content","url":"/cadp#stochastic-value-gradient-svg","position":55},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"Noise Inference in SVG","lvl2":"Policy Optimization with a Model"},"type":"lvl3","url":"/cadp#noise-inference-in-svg","position":56},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"Noise Inference in SVG","lvl2":"Policy Optimization with a Model"},"content":"The method we’ve presented so far assumes we have direct access to the noise variables \\epsilon and \\xi used to generate trajectories. This works well in the on-policy setting where we generate our own data. However, in off-policy scenarios where we receive externally generated trajectories, we need to infer these noise variables—a process the authors call noise inference.\n\nFor the Gaussian case discussed above, this inference is straightforward. Given an observed scalar action a_t and the current policy parameters \\boldsymbol{w}, we can recover the action noise \\epsilon_t through inverse reparameterization:\\epsilon_t = \\frac{a_t - d(s_t;\\boldsymbol{w})}{\\sigma(s_t;\\boldsymbol{w})}\n\nSimilarly, for the dynamics noise where states are typically vector-valued:\\xi_t = \\Sigma(s_t,a_t;\\boldsymbol{\\phi})^{-1}(s_{t+1} - \\mu(s_t,a_t;\\boldsymbol{\\phi}))\n\nThis simple inversion is possible because the Gaussian reparameterization is an affine transformation, which is invertible as long as \\sigma(s_t;\\boldsymbol{w}) is non-zero for scalar actions and \\Sigma(s_t,a_t;\\boldsymbol{\\phi}) is full rank for vector-valued states. The same principle extends naturally to vector-valued actions, where \\sigma would be replaced by a full covariance matrix.\n\nMore generally, this idea of invertible transformations can be extended far beyond simple Gaussian reparameterization. We could consider a sequence of invertible transformations:z_0 \\sim \\mathcal{N}(0,I) \\xrightarrow{f_1} z_1 \\xrightarrow{f_2} z_2 \\xrightarrow{f_3} \\cdots \\xrightarrow{f_K} z_K = a_t\n\nwhere each f_k is an invertible neural network layer. The forward process can be written compactly as:a_t = (f_K \\circ f_{K-1} \\circ \\cdots \\circ f_1)(z_0;\\boldsymbol{w})\n\nThis is the idea behind normalizing flows: a series of invertible transformations that can transform a simple base distribution (like a standard normal) into a complex target distribution while maintaining exact invertibility.\n\nThe noise inference in this case would involve applying the inverse transformations:z_0 = (f_1^{-1} \\circ \\cdots \\circ f_K^{-1})(a_t;\\boldsymbol{w})\n\nThis approach offers several advantages:\n\nMore expressive policies and dynamics models capable of capturing multimodal distributions\n\nExact likelihood computation through the change of variables formula (can be useful for computing the log prob terms in entropy regularization for example)\n\nPrecise noise inference through the guaranteed invertibility of the flow\n\nAs far as I know, this approach has not been explored in the literature.","type":"content","url":"/cadp#noise-inference-in-svg","position":57},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"DPG as a Special Case of SAC","lvl2":"Policy Optimization with a Model"},"type":"lvl3","url":"/cadp#dpg-as-a-special-case-of-sac","position":58},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl3":"DPG as a Special Case of SAC","lvl2":"Policy Optimization with a Model"},"content":"At first glance, SAC and DPG might appear to be fundamentally different approaches to policy optimization. SAC begins with the principle of entropy maximization and policy distribution matching through KL divergence minimization, while DPG directly optimizes a deterministic policy to maximize expected Q-values. However, we can show that DPG emerges as a special case of SAC as we take the temperature parameter to zero.\n\nConvergence of SAC to DPG\n\nLet d(\\cdot|s;\\boldsymbol{w}_\\alpha) be the optimal stochastic policy for SAC with temperature \\alpha, and d(s;\\boldsymbol{w}_{DPG}) be the optimal deterministic policy gradient solution. Under appropriate assumptions, as \\alpha \\to 0:d(a|s;\\boldsymbol{w}_\\alpha) \\to \\delta(a - d(s;\\boldsymbol{w}_{DPG}))\n\nAssumptions:\n\nThe stochastic policy class is Gaussian with learnable mean and standard deviation:d(a|s;\\boldsymbol{w}) = \\mathcal{N}(\\mu_w(s), \\sigma_w(s)^2)\n\nThe SAC objective for policy improvement uses the soft Q-function:\\boldsymbol{w}^*_\\alpha = \\arg\\min_w \\mathbb{E}_{s\\sim\\rho}\\left[D_{KL}\\left(d(\\cdot|s;\\boldsymbol{w}) \\| \\frac{\\exp(Q_{soft}(s,\\cdot)/\\alpha)}{\\int \\exp(Q_{soft}(s,b)/\\alpha)db}\\right)\\right]\n\nwhere Q_{soft} follows the soft Bellman equation:Q_{soft}(s,a) = r(s,a) + \\gamma \\mathbb{E}_{s' \\sim P}\\left[\\mathbb{E}_{a' \\sim d(\\cdot|s')}\\left[Q_{soft}(s',a') - \\alpha \\log d(a'|s')\\right]\\right]\n\nThe DPG objective with a deterministic policy uses the standard Q-function:\\boldsymbol{w}^*_{DPG} = \\arg\\max_w \\mathbb{E}_{s\\sim\\rho}\\left[Q(s,d(s;\\boldsymbol{w}))\\right]\n\nwhere Q follows the standard Bellman equation:Q(s,a) = r(s,a) + \\gamma \\mathbb{E}_{s' \\sim P}[Q(s',d(s';\\boldsymbol{w}))]\n\nQ_{soft}(s,a) and Q(s,a) are continuous and achieve their maxima for each state s.\n\nAt the fixed point of the soft Bellman equation, as \\alpha \\to 0, the entropy term -\\alpha \\log d(a|s) vanishes, and Q_{soft} \\to Q. This implies that the SAC target distribution, which is proportional to \\exp(Q_{soft}(s,a)/\\alpha), becomes:\\lim_{\\alpha \\to 0} \\frac{\\exp(Q(s,a)/\\alpha)}{\\int \\exp(Q(s,b)/\\alpha) db} = \\delta(a - \\arg\\max_a Q(s,a)),\n\nby Laplace’s method. The target distribution thus collapses to a delta function centered at the deterministic optimal action \\arg\\max_a Q(s,a).\n\nThe KL divergence term in the SAC objective measures the divergence between the stochastic policy d(a|s;\\boldsymbol{w}) (Gaussian) and this target distribution. For a Gaussian \\mathcal{N}(\\mu, \\sigma^2) and a delta function \\delta(a - a^*), we derive:D_{KL}(\\mathcal{N}(\\mu, \\sigma^2) \\| \\delta(a - a^*)) = \\lim_{\\epsilon \\to 0} D_{KL}(\\mathcal{N}(\\mu, \\sigma^2) \\| \\mathcal{N}(a^*, \\epsilon^2)),\n\nwhere \\mathcal{N}(a^*, \\epsilon^2) is a Gaussian approximation of the delta. Using the KL formula:D_{KL} = \\frac{1}{2}\\left[\\log\\left(\\frac{\\epsilon^2}{\\sigma^2}\\right) + \\frac{\\sigma^2}{\\epsilon^2} + \\frac{(\\mu - a^*)^2}{\\epsilon^2} - 1\\right].\n\nTaking the limit \\epsilon \\to 0, the divergence diverges unless \\mu = a^* and \\sigma = 0, where it becomes zero. Thus, minimizing the SAC objective drives \\mu_w(s) \\to \\arg\\max_a Q(s,a) and \\sigma_w(s) \\to 0.\n\nConsequently, the stochastic policy converges to a delta function:\\lim_{\\alpha \\to 0} d(a|s;\\boldsymbol{w}^*_\\alpha) = \\delta(a - \\arg\\max_a Q(s,a)) = \\delta(a - d(s;\\boldsymbol{w}^*_{DPG})).","type":"content","url":"/cadp#dpg-as-a-special-case-of-sac","position":59},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl2":"Policy Optimization with a Trust Region"},"type":"lvl2","url":"/cadp#policy-optimization-with-a-trust-region","position":60},{"hierarchy":{"lvl1":"Policy Parametrization Methods","lvl2":"Policy Optimization with a Trust Region"},"content":"Trust region methods in optimization approximate the objective function with a simpler local model within a region where we “trust” this approximation to be good. This brings about the need to define what we mean by a local region, and therefore to pick a geometry which suits our problem.\n\nIn standard optimization in the Euclidean space on \\mathbb{R}^n, at each iteration k, we create a quadratic approximation around the current point x_k:m_k(p) = f(x_k) + g_k^T p + \\frac{1}{2}p^T B_k p\n\nwhere g_k = \\nabla f(x_k) is the gradient and B_k approximates the Hessian. The trust region constrains updates using Euclidean distance:\\min_p m_k(p) \\text{ subject to } \\|p\\| \\leq \\Delta_k\n\nHowever, when optimizing over probability distributions p(x;\\theta), the Euclidean geometry becomes unnatural. Instead, the Kullback-Leibler divergence provides a more natural mean of measuring proximity:D_{KL}(p(x;\\theta) || p(x;\\theta_k)) = \\int p(x;\\theta) \\log\\left(\\frac{p(x;\\theta)}{p(x;\\theta_k)}\\right)dx\n\nThis leads to the following trust region subproblem:\\min_\\theta m_k(\\theta) \\text{ subject to } D_{KL}(p(x;\\theta) || p(x;\\theta_k)) \\leq \\Delta_k\n\nFor exponential families, the KL divergence locally reduces to a quadratic form involving the Fisher Information Matrix I(\\theta_k):D_{KL}(p(x;\\theta) || p(x;\\theta_k)) \\approx \\frac{1}{2}(\\theta - \\theta_k)^T I(\\theta_k)(\\theta - \\theta_k)\n\nIn both cases, after solving for the step, we evaluate the actual versus predicted reduction ratio:\\rho_k = \\frac{f(x_k) - f(x_k + p)}{m_k(0) - m_k(p)}\n\nThis ratio determines both step acceptance and trust region adjustment:\\Delta_{k+1} = \\begin{cases}\n\\alpha_1 \\Delta_k & \\text{if } \\rho_k < \\eta_1 \\text{ (poor prediction)} \\\\\n\\Delta_k & \\text{if } \\eta_1 \\leq \\rho_k < \\eta_2 \\text{ (acceptable)} \\\\\n\\alpha_2 \\Delta_k & \\text{if } \\rho_k \\geq \\eta_2 \\text{ (very good)}\n\\end{cases}\n\nThe method accepts steps when the model prediction is sufficiently accurate:x_{k+1} = \\begin{cases}\nx_k + p & \\text{if } \\rho_k > \\eta_1 \\\\\nx_k & \\text{otherwise}\n\\end{cases}","type":"content","url":"/cadp#policy-optimization-with-a-trust-region","position":61},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time"},"type":"lvl1","url":"/cocp","position":0},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time"},"content":"As in the discrete-time setting, we work with three continuous-time variants that differ only in how the objective is written while sharing the same dynamics, path constraints, and bounds. The path constraints \\mathbf{g}(\\mathbf{x}(t),\\mathbf{u}(t))\\le \\mathbf{0} are pointwise in time, and the bounds \\mathbf{x}_{\\min}\\le \\mathbf{x}(t)\\le \\mathbf{x}_{\\max} and \\mathbf{u}_{\\min}\\le \\mathbf{u}(t)\\le \\mathbf{u}_{\\max} are understood in the same pointwise sense.\n\nMayer Problem\\begin{aligned}\n    \\text{minimize} \\quad & c(\\mathbf{x}(t_f)) \\\\\n    \\text{subject to} \\quad & \\dot{\\mathbf{x}}(t) = \\mathbf{f}(\\mathbf{x}(t), \\mathbf{u}(t)) \\\\\n                            & \\mathbf{g}(\\mathbf{x}(t), \\mathbf{u}(t)) \\leq \\mathbf{0} \\\\\n                            & \\mathbf{x}_{\\text{min}} \\leq \\mathbf{x}(t) \\leq \\mathbf{x}_{\\text{max}} \\\\\n                            & \\mathbf{u}_{\\text{min}} \\leq \\mathbf{u}(t) \\leq \\mathbf{u}_{\\text{max}} \\\\\n    \\text{given} \\quad & \\mathbf{x}(t_0) = \\mathbf{x}_0 \\enspace .\n\\end{aligned}\n\nLagrange Problem\\begin{aligned}\n    \\text{minimize} \\quad & \\int_{t_0}^{t_f} c(\\mathbf{x}(t), \\mathbf{u}(t)) \\, dt \\\\\n    \\text{subject to} \\quad & \\dot{\\mathbf{x}}(t) = \\mathbf{f}(\\mathbf{x}(t), \\mathbf{u}(t)) \\\\\n                            & \\mathbf{g}(\\mathbf{x}(t), \\mathbf{u}(t)) \\leq \\mathbf{0} \\\\\n                            & \\mathbf{x}_{\\text{min}} \\leq \\mathbf{x}(t) \\leq \\mathbf{x}_{\\text{max}} \\\\\n                            & \\mathbf{u}_{\\text{min}} \\leq \\mathbf{u}(t) \\leq \\mathbf{u}_{\\text{max}} \\\\\n    \\text{given} \\quad & \\mathbf{x}(t_0) = \\mathbf{x}_0 \\enspace .\n\\end{aligned}\n\nBolza Problem\\begin{aligned}\n    \\text{minimize} \\quad & c(\\mathbf{x}(t_f)) + \\int_{t_0}^{t_f} c(\\mathbf{x}(t), \\mathbf{u}(t)) \\, dt \\\\\n    \\text{subject to} \\quad & \\dot{\\mathbf{x}}(t) = \\mathbf{f}(\\mathbf{x}(t), \\mathbf{u}(t)) \\\\\n                            & \\mathbf{g}(\\mathbf{x}(t), \\mathbf{u}(t)) \\leq \\mathbf{0} \\\\\n                            & \\mathbf{x}_{\\text{min}} \\leq \\mathbf{x}(t) \\leq \\mathbf{x}_{\\text{max}} \\\\\n                            & \\mathbf{u}_{\\text{min}} \\leq \\mathbf{u}(t) \\leq \\mathbf{u}_{\\text{max}} \\\\\n    \\text{given} \\quad & \\mathbf{x}(t_0) = \\mathbf{x}_0 \\enspace .\n\\end{aligned}\n\nThese three forms are different lenses on the same task. Bolza contains both terminal and running terms. Lagrange can be turned into Mayer by augmenting the state with an accumulator:\\dot{z}(t)=c(\\mathbf{x}(t),\\mathbf{u}(t)),\\quad z(t_0)=0,\\quad \\text{minimize } z(t_f),\n\nwith the original dynamics left unchanged. Mayer is a special case of Bolza with zero running cost. We will use these equivalences freely, since a numerical method cares only about what must be evaluated and where those evaluations are taken.\n\nWith this catalog in place, we now pass from functions to finite representations.","type":"content","url":"/cocp","position":1},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl2":"Direct Transcription Methods"},"type":"lvl2","url":"/cocp#direct-transcription-methods","position":2},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl2":"Direct Transcription Methods"},"content":"The discrete-time problems of the previous chapter already suggested how to proceed: we convert a continuous problem into one over finitely many numbers by deciding where to look at the trajectories and how to interpolate between those looks. We place a mesh t_0<t_1<\\cdots<t_N=t_f and, inside each window [t_k,t_{k+1}], select a small set of interior fractions \\{\\tau_j\\} on the reference interval [0,1]. The running cost is additive over windows, so we write it as a sum of local integrals, map each window to [0,1], and approximate each local integral by a quadrature rule with nodes \\tau_j and weights w_j. This produces\\int_{t_0}^{t_f} c(\\mathbf{x}(t),\\mathbf{u}(t))\\,dt\n\\;\\approx\\;\n\\sum_{k=0}^{N-1} h_k \\sum_{j=1}^q w_j\\, c\\!\\big(\\mathbf{x}(t_k+h_k\\tau_j),\\,\\mathbf{u}(t_k+h_k\\tau_j)\\big),\n\nwith h_k=t_{k+1}-t_k. The dynamics are treated in the same way by the fundamental theorem of calculus,\\mathbf{x}(t_{k+1})-\\mathbf{x}(t_k)=\\int_{t_k}^{t_{k+1}} \\mathbf{f}(\\mathbf{x}(t),\\mathbf{u}(t))\\,dt\n\\;\\approx\\;\nh_k \\sum_{j=1}^q b_j\\, \\mathbf{f}\\!\\big(\\mathbf{x}(t_k+h_k\\tau_j),\\,\\mathbf{u}(t_k+h_k\\tau_j)\\big),\n\nso the places where we “pay” running cost are the same places where we “account” for state changes. Path constraints and bounds are then enforced at the same interior times. In the infinite-horizon discounted case, the same formulas apply with an extra factor e^{-\\rho(t_k+h_k\\tau_j)} multiplying the weights in the cost.\n\nThe values \\mathbf{x}(t_k+h_k\\tau_j) and \\mathbf{u}(t_k+h_k\\tau_j) do not exist a priori. We create them by a finite representation. One option is shooting: parameterize \\mathbf{u} on the mesh, integrate the ODE across each window with a chosen numerical step, and read interior values from that step. Another is collocation: represent \\mathbf{x} inside each window by a local polynomial and choose its coefficients so that the ODE holds at the interior nodes. Both constructions lead to the same structure: a nonlinear program whose objective is a composite quadrature of the running term (plus any terminal term in the Bolza case) and whose constraints are algebraic relations that encode the ODE and the pointwise inequalities at the selected nodes.\n\nSpecific choices recover familiar schemes. If we use the left endpoint as the single interior node, we obtain the forward Euler transcription. If we use both endpoints with equal weights, we obtain the trapezoidal transcription. Higher-order rules arise when we include interior nodes and richer polynomials for \\mathbf{x}. What matters here is the unifying picture: choose nodes, translate integrals into weighted sums, and couple those evaluations to a finite trajectory representation so that cost and physics are enforced at the same places. This is the organizing idea that will guide the rest of the chapter.","type":"content","url":"/cocp#direct-transcription-methods","position":3},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl3":"Discretizing cost and dynamics together","lvl2":"Direct Transcription Methods"},"type":"lvl3","url":"/cocp#discretizing-cost-and-dynamics-together","position":4},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl3":"Discretizing cost and dynamics together","lvl2":"Direct Transcription Methods"},"content":"In a continuous-time OCP, integrals appear twice: in the objective, which accumulates running cost over time, and implicitly in the dynamics, since state changes over any interval are the integral of the vector field. To compute, we must approximate both the integrals and the unknown functions \\mathbf{x}(t) and \\mathbf{u}(t) with finitely many numbers that an optimizer can manipulate.\n\nA natural way to do this is to lay down a finite set of time points (a mesh) over the horizon. You can think of the mesh as a grid we overlay on the “true” trajectories that exist as mathematical objects but are not directly accessible. Our aim is to approximate those trajectories and their integrals using values and simple models tied to the mesh. Using the same mesh for both the cost and the dynamics keeps the representation coherent: we evaluate what we pay and how the state changes at consistent times.\n\nConcretely, we begin by choosing a mesht_0<t_1<\\cdots<t_N=t_f,\\qquad h_k:=t_{k+1}-t_k .\n\nThe running cost is additive over disjoint intervals. When the horizon [t_0,t_f] is partitioned by the mesh, additivity (linearity) of the integral gives\\int_{t_0}^{t_f} c(\\mathbf{x}(t),\\mathbf{u}(t))\\,dt\n\\;=\\; \\sum_{k=0}^{N-1} \\int_{t_k}^{t_{k+1}} c(\\mathbf{x}(t),\\mathbf{u}(t))\\,dt .\n\nThis identity is exact: it is just the additivity (linearity) of the Lebesgue/Riemann integral over a partition. No approximation has been made yet. Approximations enter only when we later replace each window integral by a quadrature rule: a finite set of nodes and positive weights prescribing an integral approximation. This sets the table for three important ingredients that we will use throughout the chapter.\n\nFirst, it turns a global object into local contributions that live on each [t_k,t_{k+1}]. Numerical integration is most effective when it is composite: we approximate each small interval integral and then sum the results. Doing so controls error uniformly, because the global quadrature error is the accumulation of local errors that shrink with the step size. It also allows non-uniform steps h_k=t_{k+1}-t_k, which we will use later for mesh refinement.\n\nSecond, the split aligns the cost with the local dynamics constraints. On each interval the ODE can be written, by the fundamental theorem of calculus, as\\mathbf{x}(t_{k+1})-\\mathbf{x}(t_k)=\\int_{t_k}^{t_{k+1}} \\mathbf{f}(\\mathbf{x}(t),\\mathbf{u}(t))\\,dt.\n\nWhen we approximate this integral, we introduce interior evaluation points t_{k,j}\\in[t_k,t_{k+1}]. Using the same points in the cost and in the dynamics ties \\mathbf{x} and \\mathbf{u} together coherently: the places where we “pay” for running cost are also the places where we enforce the ODE. This avoids a mismatch between where we approximate the objective and where we impose feasibility.\n\nThird, the decomposition yields a nonlinear program with sparse structure. Each interval contributes a small block to the objective and constraints that depends only on variables from that interval (and its endpoints). Modern solvers exploit this banded sparsity to scale to long horizons.\n\nWith the split justified, we standardize the approximation. Map each interval to a reference domain via t=t_k+h_k\\tau with \\tau\\in[0,1] and dt=h_k\\,d\\tau. A quadrature rule on [0,1] is specified by evaluation points \\{\\tau_j\\}_{j=1}^q \\subset [0,1] and positive weights \\{w_j\\}_{j=1}^q such that, for a smooth \\phi,\\int_0^1 \\phi(\\tau)\\,d\\tau \\;\\approx\\; \\sum_{j=1}^q w_j\\,\\phi(\\tau_j).\n\nApplying it on each interval gives\\int_{t_k}^{t_{k+1}} c(\\mathbf{x}(t),\\mathbf{u}(t))\\,dt\n\\;\\approx\\;\nh_k\\sum_{j=1}^q w_j\\, c\\!\\big(\\mathbf{x}(t_k+h_k\\tau_j),\\,\\mathbf{u}(t_k+h_k\\tau_j)\\big).\n\nSumming these window contributions gives a composite approximation of the integral over [t_0,t_f]:\\int_{t_0}^{t_f} c(\\mathbf{x}(t),\\mathbf{u}(t))\\,dt\n\\;\\approx\\;\n\\sum_{k=0}^{N-1} h_k \\sum_{j=1}^q w_j\\, c\\!\\big(\\mathbf{x}(t_{k,j}),\\mathbf{u}(t_{k,j})\\big).\n\nThe outer index k indicates the window; the inner index i indicates the samples within that window; the factor h_k appears from the change of variables.\n\nThe dynamics admit the same treatment. By the fundamental theorem of calculus,\\mathbf{x}(t_{k+1})-\\mathbf{x}(t_k)\n=\\int_{t_k}^{t_{k+1}} \\dot{\\mathbf{x}}(t)\\,dt\n=\\int_{t_k}^{t_{k+1}} \\mathbf{f}(\\mathbf{x}(t),\\mathbf{u}(t))\\,dt .\n\nReplacing this integral by a quadrature rule that uses the same nodes produces the window defect relation\\mathbf{x}_{k+1}-\\mathbf{x}_k\n\\;\\approx\\;\nh_k\\sum_{j=1}^q b_j\\, \\mathbf{f}\\!\\big(\\mathbf{x}(t_{k,j}),\\mathbf{u}(t_{k,j})\\big),\n\nwhere \\{b_j\\} are the weights used for the ODE. Path constraints \\mathbf{g}(\\mathbf{x}(t),\\mathbf{u}(t))\\le 0 are imposed at selected nodes t_{k,j} in the same spirit. Using the same evaluation points for cost and dynamics keeps the representation coherent: we “pay” running cost and “account” for state changes at the same times.","type":"content","url":"/cocp#discretizing-cost-and-dynamics-together","position":5},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl4":"On the choice of interior points","lvl3":"Discretizing cost and dynamics together","lvl2":"Direct Transcription Methods"},"type":"lvl4","url":"/cocp#on-the-choice-of-interior-points","position":6},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl4":"On the choice of interior points","lvl3":"Discretizing cost and dynamics together","lvl2":"Direct Transcription Methods"},"content":"Once we select a mesh t_0<\\cdots<t_N and interior fractions \\{\\tau_j\\}_{j=1}^q per window [t_k,t_{k+1}], we need \\mathbf{x}(t_{k,j}) and \\mathbf{u}(t_{k,j}) at the evaluation times t_{k,j} := t_k + h_k\\tau_j. These values do not preexist. They come from one of two constructions that align with the standard quadrature taxonomy: step-function based and interpolating-function based rules.","type":"content","url":"/cocp#on-the-choice-of-interior-points","position":7},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl5":"Step-function based construction (piecewise constants; rectangle or midpoint)","lvl4":"On the choice of interior points","lvl3":"Discretizing cost and dynamics together","lvl2":"Direct Transcription Methods"},"type":"lvl5","url":"/cocp#step-function-based-construction-piecewise-constants-rectangle-or-midpoint","position":8},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl5":"Step-function based construction (piecewise constants; rectangle or midpoint)","lvl4":"On the choice of interior points","lvl3":"Discretizing cost and dynamics together","lvl2":"Direct Transcription Methods"},"content":"Here we approximate the relevant time functions by step functions on each window. For controls, a common choice is piecewise-constant:\\mathbf{u}(t)=\\mathbf{u}_k\\quad\\text{for }t\\in[t_k,t_{k+1}].\n\nFor the running cost and the vector field, the corresponding quadrature is a rectangle rule on [t_k,t_{k+1}]. Using the left endpoint gives\\int_{t_k}^{t_{k+1}} c(\\mathbf{x}(t),\\mathbf{u}(t))\\,dt\n\\;\\approx\\; h_k\\,c(\\mathbf{x}_k,\\mathbf{u}_k),\n\nand replacing the dynamics integral by the same step-function idea yields the forward Euler relation\\mathbf{x}_{k+1}=\\mathbf{x}_k+h_k\\,\\mathbf{f}(\\mathbf{x}_k,\\mathbf{u}_k,t_k).\n\nIf we prefer the midpoint rectangle rule, we sample at t_{k+\\frac12}=t_k+\\tfrac{h_k}{2}. In practice we then generate \\mathbf{x}_{k+\\frac12} by a half-step of the chosen integrator, and set \\mathbf{u}_{k+\\frac12}=\\mathbf{u}_k (piecewise-constant) or an average if we allow a short linear segment. Either way, interior values come from integrating forward given a step-function model for \\mathbf{u} and a rectangle-rule view of the integrals. This is the shooting viewpoint. Single shooting keeps only control parameters as decision variables; multiple shooting adds the window-start states and enforces step consistency.","type":"content","url":"/cocp#step-function-based-construction-piecewise-constants-rectangle-or-midpoint","position":9},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl5":"Interpolating-function based construction (low-order polynomials; trapezoid, Simpson, Gauss/Radau/Lobatto)","lvl4":"On the choice of interior points","lvl3":"Discretizing cost and dynamics together","lvl2":"Direct Transcription Methods"},"type":"lvl5","url":"/cocp#interpolating-function-based-construction-low-order-polynomials-trapezoid-simpson-gauss-radau-lobatto","position":10},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl5":"Interpolating-function based construction (low-order polynomials; trapezoid, Simpson, Gauss/Radau/Lobatto)","lvl4":"On the choice of interior points","lvl3":"Discretizing cost and dynamics together","lvl2":"Direct Transcription Methods"},"content":"Here we approximate time functions by polynomials on each window. If we interpolate \\mathbf{x}(t) linearly between endpoints, the cost naturally uses the trapezoidal rule\\int_{t_k}^{t_{k+1}} c\\,dt\\;\\approx\\;\\tfrac{h_k}{2}\\big[c(\\mathbf{x}_k,\\mathbf{u}_k)+c(\\mathbf{x}_{k+1},\\mathbf{u}_{k+1})\\big],\n\nand the dynamics use the matched trapezoidal defect\\mathbf{x}_{k+1}=\\mathbf{x}_k+\\tfrac{h_k}{2}\\Big[\\mathbf{f}(\\mathbf{x}_k,\\mathbf{u}_k,t_k)+\\mathbf{f}(\\mathbf{x}_{k+1},\\mathbf{u}_{k+1},t_{k+1})\\Big].\n\nWith a quadratic interpolation that includes the midpoint, Simpson’s rule appears in the cost and the Hermite–Simpson relations tie \\mathbf{x}_{k+\\frac12} to endpoint values and slopes. More generally, collocation chooses interior nodes on [t_k,t_{k+1}] (equally spaced gives Newton–Cotes like trapezoid or Simpson; Gaussian points give Gauss, Radau, or Lobatto schemes) and enforces the ODE at those nodes:\\frac{d}{dt}\\mathbf{x}(t_{k,j})=\\mathbf{f}\\!\\big(\\mathbf{x}(t_{k,j}),\\mathbf{u}(t_{k,j}),t_{k,j}\\big),\n\nwith continuity at endpoints. The interior values \\mathbf{x}(t_{k,j}) are evaluations of the decision polynomials; \\mathbf{u}(t_{k,j}) follows from the chosen control interpolation (constant, linear, or quadratic). The running cost is evaluated by the same interpolatory quadrature at the same nodes, which keeps “where we pay” aligned with “where we enforce.”","type":"content","url":"/cocp#interpolating-function-based-construction-low-order-polynomials-trapezoid-simpson-gauss-radau-lobatto","position":11},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl2":"Polynomial Interpolation"},"type":"lvl2","url":"/cocp#polynomial-interpolation","position":12},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl2":"Polynomial Interpolation"},"content":"We often want to construct a function that passes through a given set of points. For example, suppose we know a function should satisfy:f(x_0) = y_0, \\quad f(x_1) = y_1, \\quad \\dots, \\quad f(x_m) = y_m.\n\nThese are called interpolation constraints. Our goal is to find a function f(x) that satisfies all of them exactly.\n\nTo make the problem tractable, we restrict ourselves to a class of functions. In polynomial interpolation, we assume that f(x) is a polynomial of degree at most N. That means we are trying to find coefficients c_0, \\dots, c_N such thatf(x) = \\sum_{n=0}^N c_n \\, \\phi_n(x),\n\nwhere the functions \\phi_n(x) form a basis for the space of polynomials. The most common choice is the monomial basis, where \\phi_n(x) = x^n. This gives:f(x) = c_0 + c_1 x + c_2 x^2 + \\dots + c_N x^N.\n\nOther valid bases include Legendre, Chebyshev, and Lagrange polynomials, each chosen for specific numerical properties. But all span the same function space.\n\nTo find a unique solution, we need the number of unknowns (the c_n) to match the number of constraints. Since a degree-N polynomial has N+1 coefficients, we need:N + 1 = m + 1 \\quad \\Rightarrow \\quad N = m.\n\nSo if we want a function that passes through 4 points, we need a cubic polynomial (N = 3). Choosing a higher degree than necessary would give us infinitely many solutions; a lower degree may make the problem unsolvable.","type":"content","url":"/cocp#polynomial-interpolation","position":13},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl4":"Solving for the Coefficients (Monomial Basis)","lvl2":"Polynomial Interpolation"},"type":"lvl4","url":"/cocp#solving-for-the-coefficients-monomial-basis","position":14},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl4":"Solving for the Coefficients (Monomial Basis)","lvl2":"Polynomial Interpolation"},"content":"If we fix the basis functions to be monomials, we can build a system of equations by plugging in each x_i into f(x). This gives:\\begin{aligned}\nf(x_0) &= c_0 + c_1 x_0 + c_2 x_0^2 + \\dots + c_N x_0^N = y_0 \\\\\nf(x_1) &= c_0 + c_1 x_1 + c_2 x_1^2 + \\dots + c_N x_1^N = y_1 \\\\\n&\\vdots \\\\\nf(x_m) &= c_0 + c_1 x_m + c_2 x_m^2 + \\dots + c_N x_m^N = y_m\n\\end{aligned}\n\nThis system can be written in matrix form as:\\begin{bmatrix}\n1 & x_0 & x_0^2 & \\cdots & x_0^N \\\\\n1 & x_1 & x_1^2 & \\cdots & x_1^N \\\\\n\\vdots & \\vdots & \\vdots & & \\vdots \\\\\n1 & x_m & x_m^2 & \\cdots & x_m^N\n\\end{bmatrix}\n\\begin{bmatrix}\nc_0 \\\\ c_1 \\\\ \\vdots \\\\ c_N\n\\end{bmatrix}\n=\n\\begin{bmatrix}\ny_0 \\\\ y_1 \\\\ \\vdots \\\\ y_m\n\\end{bmatrix}\n\nThe matrix on the left is called the Vandermonde matrix. Solving this system gives the coefficients c_n that define the interpolating polynomial.","type":"content","url":"/cocp#solving-for-the-coefficients-monomial-basis","position":15},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl4":"Using a Different Basis","lvl2":"Polynomial Interpolation"},"type":"lvl4","url":"/cocp#using-a-different-basis","position":16},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl4":"Using a Different Basis","lvl2":"Polynomial Interpolation"},"content":"We don’t have to use monomials. We can pick any set of basis functions \\phi_n(x), such as Chebyshev or Fourier modes, and follow the same steps. The interpolating function becomes:f(x) = \\sum_{n=0}^N c_n \\, \\phi_n(x),\n\nand each interpolation constraint becomes:f(x_i) = \\sum_{n=0}^N c_n \\, \\phi_n(x_i) = y_i.\n\nAssembling these into a system gives:\\begin{bmatrix}\n\\phi_0(x_0) & \\phi_1(x_0) & \\dots & \\phi_N(x_0) \\\\\n\\phi_0(x_1) & \\phi_1(x_1) & \\dots & \\phi_N(x_1) \\\\\n\\vdots & \\vdots & & \\vdots \\\\\n\\phi_0(x_m) & \\phi_1(x_m) & \\dots & \\phi_N(x_m)\n\\end{bmatrix}\n\\begin{bmatrix}\nc_0 \\\\ c_1 \\\\ \\vdots \\\\ c_N\n\\end{bmatrix}\n=\n\\begin{bmatrix}\ny_0 \\\\ y_1 \\\\ \\vdots \\\\ y_m\n\\end{bmatrix}\n\nFrom here, we solve as before and reconstruct f(x).","type":"content","url":"/cocp#using-a-different-basis","position":17},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl4":"Derivative Constraints","lvl2":"Polynomial Interpolation"},"type":"lvl4","url":"/cocp#derivative-constraints","position":18},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl4":"Derivative Constraints","lvl2":"Polynomial Interpolation"},"content":"Sometimes, instead of a value constraint f(x_i) = y_i, we want to impose a slope constraint f'(x_i) = s_i. This is common in applications like spline interpolation or collocation methods, where derivative information is available from an ODE.\n\nSincef(x) = \\sum_{n=0}^N c_n \\phi_n(x) \\quad \\Rightarrow \\quad f'(x) = \\sum_{n=0}^N c_n \\phi_n'(x),\n\nwe can directly write the slope constraint:f'(x_i) = \\sum_{n=0}^N c_n \\phi_n'(x_i) = s_i.\n\nTo enforce this, we replace one of the interpolation equations in our system with this slope constraint. The resulting system still has m+1 equations and N+1 = m+1 unknowns.\n\nConcretely, suppose we have k+1 value constraints at nodes X=\\{x_0,\\ldots,x_k\\} with values Y=\\{y_0,\\ldots,y_k\\} and r slope constraints at nodes Z=\\{z_1,\\ldots,z_r\\} with slopes S=\\{s_1,\\ldots,s_r\\}, with k+1+r=N+1. The linear system for the coefficients \\mathbf{c}=[c_0,\\ldots,c_N]^\\top is\\begin{bmatrix}\n\\phi_0(x_0) & \\phi_1(x_0) & \\cdots & \\phi_N(x_0) \\\\\n\\vdots & \\vdots & & \\vdots \\\\\n\\phi_0(x_k) & \\phi_1(x_k) & \\cdots & \\phi_N(x_k) \\\\\n\\phi_0'(z_1) & \\phi_1'(z_1) & \\cdots & \\phi_N'(z_1) \\\\\n\\vdots & \\vdots & & \\vdots \\\\\n\\phi_0'(z_r) & \\phi_1'(z_r) & \\cdots & \\phi_N'(z_r)\n\\end{bmatrix}\n\\begin{bmatrix}\nc_0 \\\\ c_1 \\\\ \\vdots \\\\ c_N\n\\end{bmatrix}\n=\n\\begin{bmatrix}\ny_0 \\\\ \\vdots \\\\ y_k \\\\ s_1 \\\\ \\vdots \\\\ s_r\n\\end{bmatrix}.\n\nIf a value and a slope are imposed at the same node, take z_j=x_i and include both the value row and the derivative row; the system remains square. In the monomial basis, the top block is the Vandermonde matrix and the derivative block has entries n\\,x^{\\,n-1}. Once solved for \\mathbf{c}, reconstructf(x)=\\sum_{n=0}^N c_n\\,\\phi_n(x).","type":"content","url":"/cocp#derivative-constraints","position":19},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl4":"Interpolating ODE Trajectories (Collocation)","lvl2":"Polynomial Interpolation"},"type":"lvl4","url":"/cocp#interpolating-ode-trajectories-collocation","position":20},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl4":"Interpolating ODE Trajectories (Collocation)","lvl2":"Polynomial Interpolation"},"content":"Having established the general framework of interpolation, we now apply these concepts to the specific context of approximating trajectories governed by ordinary differential equations.  The idea of applying polynomial interpolation with derivative constraints yields a method known as “collocation”. More precisely, a degree-s collocation method is a way to discretize an ordinary differential equation (ODE) by approximating the solution on each time interval with a polynomial of degree s, and then enforcing that this polynomial satisfies the ODE exactly at s carefully chosen points (the collocation nodes).\n\nConsider a dynamical system described by the ordinary differential equation:\\dot{\\mathbf{x}}(t) = \\mathbf{f}(\\mathbf{x}(t),\\mathbf{u}(t),t)\n\nwhere \\mathbf{x}(t) represents the state trajectory and \\mathbf{u}(t) denotes the control input.\nLet us focus on a single mesh interval [t_k,t_{k+1}] with step size h_k := t_{k+1}-t_k. To work with a standardized domain, we introduce the transformation t = t_k + h_k\\tau that maps the physical interval [t_k,t_{k+1}] to the reference interval [0,1]. On this reference interval, we select a set of collocation nodes \\{\\tau_j\\}_{j=0}^K \\subset [0,1].\n\nOur goal is now to approximate the unknown trajectory using a polynomial of degree K. Using a monomial basis, we represent (parameterize) our trajectory as:\\mathbf{x}_h(\\tau) := \\sum_{n=0}^K \\mathbf{a}_n\\,\\tau^n\n\nwhere \\mathbf{a}_n \\in \\mathbb{R}^d are coefficient vectors to be determined.\nCollocation enforces the differential equation at a chosen set of nodes on [0,1]. Depending on the node family, these nodes may be interior-only or may include one or both endpoints. With the polynomial state model, we can differentiate analytically. Using the change of variables t=t_k+h_k\\,\\tau, we obtain:\\dot{\\mathbf{x}}_h(t_k+h_k\\,\\tau_j) = \\frac{1}{h_k} \\sum_{n=1}^K n\\,\\mathbf{a}_n\\,\\tau_j^{n-1}\n\nThe collocation condition requires that this polynomial derivative equals the right-hand side of the ODE at each collocation node \\tau_j:\\frac{1}{h_k} \\sum_{n=1}^K n\\,\\mathbf{a}_n\\,\\tau_j^{n-1} = \\mathbf{f}\\left( \\sum_{n=0}^K \\mathbf{a}_n\\,\\tau_j^{n},\\ \\mathbf{u}_j,\\ t_k+h_k\\,\\tau_j \\right), \\quad \\text{for each collocation node } \\tau_j.\n\nwhere \\mathbf{u}_j represents the control value at node \\tau_j.","type":"content","url":"/cocp#interpolating-ode-trajectories-collocation","position":21},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl5":"Boundary Conditions and Node Families","lvl4":"Interpolating ODE Trajectories (Collocation)","lvl2":"Polynomial Interpolation"},"type":"lvl5","url":"/cocp#boundary-conditions-and-node-families","position":22},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl5":"Boundary Conditions and Node Families","lvl4":"Interpolating ODE Trajectories (Collocation)","lvl2":"Polynomial Interpolation"},"content":"\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import Rectangle\nimport matplotlib.patches as mpatches\nfrom scipy.interpolate import CubicSpline\n\n# Set up the figure with subplots\nfig = plt.figure(figsize=(16, 12))\n\n# Create a 2x2 subplot layout\ngs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n\ndef create_collocation_plot(ax, title, node_positions, slope_nodes, endpoint_types):\n    \"\"\"\n    Create a collocation method illustration\n    \n    Parameters:\n    - ax: matplotlib axis\n    - title: plot title\n    - node_positions: list of x-positions for nodes (normalized 0-1)\n    - slope_nodes: list of booleans indicating which nodes enforce slopes\n    - endpoint_types: tuple of (left_type, right_type) where type is 'slope', 'eval', or 'continuity'\n    \"\"\"\n    \n    # Time interval\n    t_start, t_end = 0, 1\n    \n    # Create a smooth trajectory curve for illustration\n    x = np.linspace(t_start, t_end, 100)\n    # Create an S-shaped curve to represent state trajectory\n    y = 0.3 + 0.4 * np.sin(3 * np.pi * x) * np.exp(-2 * x)\n    \n    # Build a cubic spline of the trajectory for consistent slope evaluation\n    spline = CubicSpline(x, y, bc_type='natural')\n    \n    # Plot the trajectory\n    ax.plot(x, y, 'k-', linewidth=2, label='State trajectory x(t)')\n    \n    # Plot collocation points\n    for i, pos in enumerate(node_positions):\n        t_node = t_start + pos * (t_end - t_start)\n        y_node = 0.3 + 0.4 * np.sin(3 * np.pi * t_node) * np.exp(-2 * t_node)\n        \n        # Endpoints are rendered in the endpoint section (as squares). Skip here.\n        if np.isclose(t_node, 0.0) or np.isclose(t_node, 1.0):\n            continue\n\n        if slope_nodes[i]:\n            # Blue dot for slope constraint nodes\n            ax.plot(t_node, y_node, 'bo', markersize=8, markerfacecolor='blue', \n                   markeredgecolor='darkblue', linewidth=1.5)\n            # Add tangent line to show slope constraint (centered on node)\n            dt = 0.08  # Half-length for symmetric extension\n            t_prev = max(0, t_node - dt)\n            t_next = min(1, t_node + dt)\n            \n            # Calculate slope from the spline derivative (matches plotted curve)\n            slope = spline.derivative()(t_node)\n            \n            # Create symmetric tangent line centered on the node\n            y_prev = y_node + slope * (t_prev - t_node)\n            y_next = y_node + slope * (t_next - t_node)\n            ax.plot([t_prev, t_next], [y_prev, y_next], 'r--', alpha=0.8, linewidth=2)\n        else:\n            # Green dot for evaluation-only nodes\n            ax.plot(t_node, y_node, 'go', markersize=8, markerfacecolor='lightgreen', \n                   markeredgecolor='darkgreen', linewidth=1.5)\n    \n    # Handle endpoints specially (always render as squares if applicable)\n    endpoints = [(0, 'left'), (1, 'right')]\n    for pos, side in endpoints:\n        y_end = 0.3 + 0.4 * np.sin(3 * np.pi * pos) * np.exp(-2 * pos)\n        end_type = endpoint_types[0] if side == 'left' else endpoint_types[1]\n        \n        if end_type == 'slope':\n            ax.plot(pos, y_end, 'bs', markersize=10, markerfacecolor='blue', \n                   markeredgecolor='darkblue', linewidth=2)\n            # Add tangent line (centered on endpoint)\n            dt = 0.08  # Half-length for symmetric extension\n            \n            # Calculate slope from the spline derivative (matches plotted curve)\n            slope = spline.derivative()(pos)\n            \n            t_prev = pos - dt\n            t_next = pos + dt\n            y_prev = y_end + slope * (t_prev - pos)\n            y_next = y_end + slope * (t_next - pos)\n            ax.plot([t_prev, t_next], [y_prev, y_next], 'r--', alpha=0.8, linewidth=2)\n        elif end_type == 'eval':\n            ax.plot(pos, y_end, 'gs', markersize=10, markerfacecolor='lightgreen', \n                   markeredgecolor='darkgreen', linewidth=2)\n        elif end_type == 'continuity':\n            ax.plot(pos, y_end, 'ms', markersize=10, markerfacecolor='orange', \n                   markeredgecolor='darkorange', linewidth=2)\n    \n    # Add time markers\n    ax.axvline(x=0, color='gray', linestyle=':', alpha=0.5)\n    ax.axvline(x=1, color='gray', linestyle=':', alpha=0.5)\n    ax.text(0, -0.15, r'$t_k$', ha='center', va='top', fontsize=12)\n    ax.text(1, -0.15, r'$t_{k+1}$', ha='center', va='top', fontsize=12)\n    \n    # Formatting\n    ax.set_xlim(-0.1, 1.1)\n    ax.set_ylim(-0.2, 0.8)\n    ax.set_xlabel('Time', fontsize=12)\n    ax.set_ylabel('State', fontsize=12)\n    ax.set_title(title, fontsize=14, fontweight='bold')\n    ax.grid(True, alpha=0.3)\n\n# Define node positions for each method (normalized to [0,1])\n# Using 4 nodes total for fair comparison\n\n# Lobatto IIIA nodes (includes both endpoints)\nlobatto_nodes = [0.0, 0.276, 0.724, 1.0]\nlobatto_slopes = [True, True, True, True]\nlobatto_endpoints = ('slope', 'slope')\n\n# Radau IA nodes (includes left endpoint)\nradau1_nodes = [0.0, 0.155, 0.645, 0.955]\nradau1_slopes = [True, True, True, True]  # Collocation at left endpoint and interior nodes\nradau1_endpoints = ('slope', 'eval')  # Left: slope, Right: evaluation-only\n\n# Radau IIA nodes (includes right endpoint)\nradau2_nodes = [0.045, 0.355, 0.845, 1.0]\nradau2_slopes = [True, True, True, True]  # Collocation at interior nodes and right endpoint\nradau2_endpoints = ('continuity', 'slope')  # Left: continuity, Right: slope\n\n# Gauss nodes (no endpoints)\ngauss_nodes = [0.113, 0.387, 0.613, 0.887]\ngauss_slopes = [True, True, True, True]\ngauss_endpoints = ('eval', 'eval')\n\n# Create subplots\nax1 = fig.add_subplot(gs[0, 0])\ncreate_collocation_plot(ax1, 'Lobatto IIIA Method', lobatto_nodes, lobatto_slopes, lobatto_endpoints)\n\nax2 = fig.add_subplot(gs[0, 1])\ncreate_collocation_plot(ax2, 'Radau IA Method', radau1_nodes, radau1_slopes, radau1_endpoints)\n\nax3 = fig.add_subplot(gs[1, 0])\ncreate_collocation_plot(ax3, 'Radau IIA Method', radau2_nodes, radau2_slopes, radau2_endpoints)\n\nax4 = fig.add_subplot(gs[1, 1])\ncreate_collocation_plot(ax4, 'Gauss Method', gauss_nodes, gauss_slopes, gauss_endpoints)\n\n# Create legend\nlegend_elements = [\n    mpatches.Patch(color='blue', label='Slope constraint (f = dynamics)'),\n    mpatches.Patch(color='lightgreen', label='Polynomial evaluation only'),\n    mpatches.Patch(color='orange', label='Continuity constraint'),\n    plt.Line2D([0], [0], color='red', linestyle='--', alpha=0.7, label='Tangent (slope direction)'),\n    plt.Line2D([0], [0], color='black', linewidth=2, label='State trajectory')\n]\n\nfig.legend(handles=legend_elements, loc='lower center', bbox_to_anchor=(0.5, 0.03), \n           ncol=3, fontsize=12, frameon=True, fancybox=True, shadow=False)\n\n# Add main title\nfig.suptitle('Collocation Methods for Optimal Control\\n(Illustration of Node Types and Constraints)', \n             fontsize=16, fontweight='bold', y=0.95)\n\nplt.tight_layout(rect=[0.04, 0.10, 0.98, 0.93])\nplt.show()\n\nThe choice of collocation nodes determines how boundary conditions are handled and affects the resulting discretization properties. Three standard families are commonly used: Labatto, Randau and and Gauss.\n\nLet’s consider these three setup with more generality over any given basis. We start again by taking a mesh interval [t_k,t_{k+1}] of length h_k=t_{k+1}-t_k and reparametrize time byt = t_k + h_k\\,\\tau,\\qquad \\tau\\in[0,1].\n\nWe then choose to represent the (unknown) state by a degree–K polynomial\\mathbf{x}_h(\\tau) \\;=\\; \\sum_{n=0}^{K}\\mathbf{a}_n\\,\\phi_n(\\tau),\n\nwhere \\{\\phi_n\\}_{n=0}^K is any linearly independent basis of polynomials of degree \\le K, and \\mathbf{a}_0,\\dots,\\mathbf{a}_K are vector coefficients to be determined.\n\nThe collocation condition  mean that we require for the chosen K collocation points that:\\frac{d}{dt}\\mathbf{x}_h\\bigl(\\tau_j\\bigr) \\;=\\; \\mathbf{f}\\bigl(\\mathbf{x}_h(\\tau_j),\\mathbf{u}_h(\\tau_j),t_k+h_k\\tau_j\\bigr),\n\\qquad j=0,1,\\dots,K,\n\nwhich, using \\tfrac{d}{dt}=(1/h_k)\\tfrac{d}{d\\tau}, becomes\\underbrace{\\tfrac{1}{h_k}\\mathbf{x}_h'(\\tau_j)}_{\\text{polynomial slope at node}}\n\\;=\\;\n\\underbrace{\\mathbf{f}\\!\\bigl(\\mathbf{x}_h(\\tau_j),\\mathbf{u}_h(\\tau_j),t_k+h_k\\tau_j\\bigr)}_{\\text{ODE slope at same point}},\n\\qquad j=0,\\dots,K.\n\nPut simply: choose some nodes inside the interval, and at each of those nodes force the slope of the polynomial approximation to match the slope prescribed by the ODE. What we mean by the expression “collocation conditions” is simply to say that we want to satisfy a set of “slope-matching equations” at the chosen nodes.\n\nBy definition of the mesh variables,\\mathbf{x}_k := \\mathbf{x}_h(0),\\qquad \\mathbf{x}_{k+1} := \\mathbf{x}_h(1),\n\nand (if you sample the control at endpoints)\\mathbf{u}_k := \\mathbf{u}_h(0),\\qquad \\mathbf{u}_{k+1} := \\mathbf{u}_h(1).\n\nWith the monomial basis,\\phi_n(0)=\\delta_{n0}\\ \\Rightarrow\\ \\mathbf{x}_h(0)=\\sum_{n=0}^K \\mathbf{a}_n \\phi_n(0)=\\mathbf{a}_0=\\mathbf{x}_k,\\phi_n(1)=1\\ \\Rightarrow\\ \\mathbf{x}_h(1)=\\sum_{n=0}^K \\mathbf{a}_n=\\mathbf{x}_{k+1}.\n\nFor the derivative, \\phi_n'(\\tau)=n\\,\\tau^{n-1}, so\\mathbf{x}_h'(0)=\\sum_{n=0}^K \\mathbf{a}_n\\,\\phi_n'(0)=\\mathbf{a}_1,\n\\qquad\n\\mathbf{x}_h'(1)=\\sum_{n=1}^K n\\,\\mathbf{a}_n.\n\nWhen chaining intervals into a global trajectory, direct collocation enforces state continuity by construction: the variable \\mathbf{x}_{k+1} at the end of one interval is the same as the starting variable of the next. What is not enforced automatically is slope continuity; the derivative at the end of one interval generally does not match the derivative at the start of the next. Different collocation methods may have different slope continuity properties depending on the chosen collocation nodes.\n\nLobatto Nodes (endpoints included):\n\nThe family of Labotto methods correspond to any set of so-called Lobatto nodes \\{\\tau_j\\}_{j=0}^K with the specificity that we require \\tau_0=0 and \\tau_K=1. Let’s assume that we work with the power (monomial) basis \\phi_n(\\tau)=\\tau^n, so that\\mathbf{x}_h(\\tau)=\\sum_{n=0}^{K}\\mathbf{a}_n\\,\\tau^n,\n\nDifferentiating \\mathbf{x}_h with respect to \\tau gives\\frac{d\\mathbf{x}_h}{d\\tau}(\\tau)=\\sum_{n=0}^{K}\\mathbf{a}_n\\,\\phi_n'(\\tau),\n\nSince we have the chain rule \\frac{d}{dt} = \\frac{1}{h_k}\\frac{d}{d\\tau} from the time transformation t = t_k + h_k\\tau, the time derivative becomes\\frac{d\\mathbf{x}_h}{dt}(t_k + h_k\\tau) = \\frac{1}{h_k}\\frac{d\\mathbf{x}_h}{d\\tau}(\\tau) = \\frac{1}{h_k}\\sum_{n=0}^{K}\\mathbf{a}_n\\,\\phi_n'(\\tau).\n\nso the collocation equations at Lobatto nodes are\\frac{1}{h_k}\\sum_{n=0}^{K}\\mathbf{a}_n\\,\\phi_n'(\\tau_j)\n\\;=\\;\n\\mathbf{f}\\!\\Bigl(\\sum_{n=0}^{K}\\mathbf{a}_n\\,\\phi_n(\\tau_j),\\ \\mathbf{u}_h(\\tau_j),\\ t_k+h_k\\tau_j\\Bigr),\n\\qquad j=0,1,\\dots,K.\n\nFor j=0 and j=K, these conditions become:\\frac{1}{h_k}\\sum_{n=0}^{K}\\mathbf{a}_n\\,\\phi_n'(0)\n\\;=\\;\n\\mathbf{f}\\!\\Bigl(\\sum_{n=0}^{K}\\mathbf{a}_n\\,\\phi_n(0),\\ \\mathbf{u}_h(0),\\ t_k\\Bigr),\n\\qquad \\text{(left endpoint)}\\frac{1}{h_k}\\sum_{n=0}^{K}\\mathbf{a}_n\\,\\phi_n'(1)\n\\;=\\;\n\\mathbf{f}\\!\\Bigl(\\sum_{n=0}^{K}\\mathbf{a}_n\\,\\phi_n(1),\\ \\mathbf{u}_h(1),\\ t_{k+1}\\Bigr),\n\\qquad \\text{(right endpoint)}\n\nWith the monomial basis \\phi_n(\\tau)=\\tau^n, we have \\phi_n'(0)=n\\delta_{n,1} (only \\phi_1'=1, others vanish) and \\phi_n'(1)=n. Also, \\phi_n(0)=\\delta_{n,0} and \\phi_n(1)=1 for all n. This simplifies the endpoint conditions to:\\frac{\\mathbf{a}_1}{h_k} = \\mathbf{f}(\\mathbf{a}_0, \\mathbf{u}_h(0), t_k) = \\mathbf{f}(\\mathbf{x}_k, \\mathbf{u}_k, t_k),\n\\qquad \\text{(left endpoint slope)}\\frac{1}{h_k}\\sum_{n=1}^{K}n\\,\\mathbf{a}_n = \\mathbf{f}\\!\\Bigl(\\sum_{n=0}^{K}\\mathbf{a}_n, \\mathbf{u}_h(1), t_{k+1}\\Bigr) = \\mathbf{f}(\\mathbf{x}_{k+1}, \\mathbf{u}_{k+1}, t_{k+1}),\n\\qquad \\text{(right endpoint slope)}\n\nThese equations enforce that the polynomial’s slope at both endpoints matches the ODE’s prescribed slope, which is why the figure shows red tangent lines at both endpoints for Lobatto methods.\n\nRadau Nodes (one endpoint included):\nRadau points include only one endpoint. Radau-I includes the left endpoint (\\tau_0 = 0) while Radau-II includes the right endpoint (\\tau_K = 1). This means that a radau collocation is defined by any set of collocation nodes such that \\tau_K = 1. This translates into requireing that we match the ODE over the mesh only at the right endpoiunt in addition to the interior nodes.  As a consequence, we leave the solution unconstrained to take any value on the left endpoint. When chaining up multiple intervals across a global solution, this may pose some complication as we will no longer be able to ensure continuity as the slope at one endpoitn need not match that of the next endpoint. (But could you have a situation where slopes match but the states don’t line up?)\n\nAt the included endpoint the ODE is enforced (slope shown in the figure), while at the other endpoint continuity links adjacent intervals. For Radau-I with K+1 points:\\mathbf{x}_k = \\mathbf{x}_h(0) = \\mathbf{a}_0\n\nThe endpoint \\mathbf{x}_{k+1} = \\mathbf{x}_h(1) = \\sum_{n=0}^K \\mathbf{a}_n is not directly constrained by a collocation condition, requiring separate continuity enforcement between intervals.\n\nGauss Nodes (endpoints excluded):\nGauss points exclude both endpoints, using only interior points \\tau_j \\in (0,1) for j = 1,\\ldots,K. The ODE is enforced only at interior nodes; both endpoints are handled through separate continuity constraints:\\mathbf{x}_k = \\mathbf{x}_h(0) = \\mathbf{a}_0\\mathbf{x}_{k+1} = \\mathbf{x}_h(1) = \\sum_{n=0}^K \\mathbf{a}_n\n\nOrigins and Selection Criteria:\nThese node families derive from orthogonal polynomial theory. Gauss nodes correspond to roots of Legendre polynomials and provide optimal quadrature accuracy for smooth integrands. Radau nodes are roots of modified Legendre polynomials with one endpoint constraint, while Lobatto nodes include both endpoints and correspond to roots of derivatives of Legendre polynomials.\n\nFor optimal control applications, Radau-II nodes are often preferred because they provide implicit time-stepping behavior and good stability properties. Lobatto nodes simplify boundary condition handling but may require smaller time steps. Gauss nodes offer highest quadrature accuracy but complicate endpoint treatment.","type":"content","url":"/cocp#boundary-conditions-and-node-families","position":23},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl5":"Control Parameterization and Cost Integration","lvl4":"Interpolating ODE Trajectories (Collocation)","lvl2":"Polynomial Interpolation"},"type":"lvl5","url":"/cocp#control-parameterization-and-cost-integration","position":24},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl5":"Control Parameterization and Cost Integration","lvl4":"Interpolating ODE Trajectories (Collocation)","lvl2":"Polynomial Interpolation"},"content":"The control inputs can be handled with similar polynomial approximations. We may use piecewise-constant controls, piecewise-linear controls, or higher-order polynomial parameterizations of the form:\\mathbf{u}_h(\\tau) = \\sum_{n=0}^{K_u} \\mathbf{b}_n\\,\\tau^n\n\nwhere \\mathbf{u}_j = \\mathbf{u}_h(\\tau_j) represents the control values at each collocation point. This polynomial framework extends to cost function evaluation, where running costs are integrated using the same quadrature nodes and weights:\\int_{t_k}^{t_{k+1}} c\\,dt \\approx h_k\\sum_{j=0}^K w_j\\, c\\big(\\mathbf{x}_h(\\tau_j),\\mathbf{u}_h(\\tau_j), t_k+h_k\\,\\tau_j\\big)","type":"content","url":"/cocp#control-parameterization-and-cost-integration","position":25},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl2":"A Compendium of Direct Transcription Methods in Trajectory Optimization"},"type":"lvl2","url":"/cocp#a-compendium-of-direct-transcription-methods-in-trajectory-optimization","position":26},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl2":"A Compendium of Direct Transcription Methods in Trajectory Optimization"},"content":"The mesh and interior nodes are the common scaffold. What distinguishes one transcription from another is how we obtain values at those nodes and how we approximate the two integrals that appear implicitly and explicitly: the integral of the running cost and the integral that carries the state forward. In other words, we now commit to two design choices that mirror the previous section: a finite representation for \\mathbf{x}(t) and \\mathbf{u}(t) over each interval [t_i,t_{i+1}], and a quadrature rule whose nodes and weights are used consistently for both cost and dynamics. The result is always a sparse nonlinear program; the differences are in where we sample and how we tie samples together.\n\nBelow, each transcription should be read as “same grid, same interior points, same evaluations for cost and physics,” with only the local representation changing.","type":"content","url":"/cocp#a-compendium-of-direct-transcription-methods-in-trajectory-optimization","position":27},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl3":"Euler Collocation","lvl2":"A Compendium of Direct Transcription Methods in Trajectory Optimization"},"type":"lvl3","url":"/cocp#euler-collocation","position":28},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl3":"Euler Collocation","lvl2":"A Compendium of Direct Transcription Methods in Trajectory Optimization"},"content":"Work on one interval [t_k,t_{k+1}] of length h_k with the reparametrization t=t_k+h_k\\,\\tau, \\tau\\in[0,1]. Assume a degree 1 polynomial:\\mathbf{x}_h(\\tau)=\\sum_{n=0}^{1}\\mathbf{a}_n\\,\\phi_n(\\tau),\n\nfor any basis \\{\\phi_0,\\phi_1\\} of linear polynomials.\nEndpoint conditions give\\mathbf{x}_h(0)=\\mathbf{x}_k\\Rightarrow \\mathbf{a}_0=\\mathbf{x}_k,\\qquad\n\\mathbf{x}_h(1)=\\mathbf{x}_{k+1}\\Rightarrow \\mathbf{a}_1=\\mathbf{x}_{k+1}-\\mathbf{x}_k.\n\nby backsubstitution and because\\mathbf{x}_h(\\tau)=\\mathbf{a}_0+\\mathbf{a}_1\\,\\tau.\n\nFurthermore, the derivative with respect to \\tau is:\\frac{d}{d\\tau}\\mathbf{x}_h(\\tau)=\\mathbf{a}_1=\\mathbf{x}_{k+1}-\\mathbf{x}_k,\n\\qquad\n\\frac{d}{dt}=\\frac{1}{h_k}\\frac{d}{d\\tau}\n\\Rightarrow\n\\frac{d}{dt}\\mathbf{x}_h=\\frac{1}{h_k}\\,(\\mathbf{x}_{k+1}-\\mathbf{x}_k).\n\nBecause from the mapping t(\\tau)=t_k+h_k\\tau we can invert:\n\\tau(t)=\\frac{t-t_k}{h_k} and differentiating gives\\frac{d\\tau}{dt}=\\frac{1}{h_k}.\n\nThe collocation condition at a single Radau-II node \\tau=1:\\frac{1}{h_k}\\,\\mathbf{x}_h'(\\tau)\\Big|_{\\tau=1}\n\\;=\\;\n\\mathbf{f}\\!\\big(\\mathbf{x}_h(1),\\mathbf{u}_h(1),t_{k+1}\\big)\n\\;=\\;\n\\mathbf{f}\\!\\big(\\mathbf{x}_{k+1},\\mathbf{u}_{k+1},t_{k+1}\\big).\n\nBecause \\mathbf{x}_h is linear, \\mathbf{x}_h'(\\tau) is constant in \\tau, so \\mathbf{x}_h'(1)=\\mathbf{x}_h'(\\tau) for all \\tau. Moreover, linear interpolation between the two endpoints gives\\mathbf{x}_h'(\\tau)=\\mathbf{x}_{k+1}-\\mathbf{x}_k.\n\nSubstitute this into the collocation condition:\\frac{1}{h_k}\\big(\\mathbf{x}_{k+1}-\\mathbf{x}_k\\big) \\;=\\; \\mathbf{f}\\!\\big(\\mathbf{x}_{k+1},\\mathbf{u}_{k+1},t_{k+1}\\big),\n\nwhich is exactly the implicit Euler step{\\ \\mathbf{x}_{k+1}=\\mathbf{x}_k + h_k\\,\\mathbf{f}\\!\\big(\\mathbf{x}_{k+1},\\mathbf{u}_{k+1},t_{k+1}\\big)\\ }.\n\nThe overall direct transcription is then:\n\nImplicit–Euler Collocation (Radau-II, degree 1)\n\nLet t_0<\\cdots<t_N with h_i:=t_{i+1}-t_i. Decision variables are \\{\\mathbf{x}_i\\}_{i=0}^N, \\{\\mathbf{u}_i\\}_{i=0}^N. Solve\\begin{aligned}\n\\min\\ & c_T(\\mathbf{x}_N)\\;+\\;\\sum_{i=0}^{N-1} h_i\\,c(\\mathbf{x}_{i+1},\\mathbf{u}_{i+1})\\\\\n\\text{s.t.}\\ & \\mathbf{x}_{i+1}-\\mathbf{x}_i - h_i\\,\\mathbf{f}(\\mathbf{x}_{i+1},\\mathbf{u}_{i+1})=\\mathbf{0},\\quad i=0,\\ldots,N-1,\\\\\n& \\mathbf{g}(\\mathbf{x}_{i+1},\\mathbf{u}_{i+1})\\le \\mathbf{0},\\\\\n& \\mathbf{x}_{\\min}\\le \\mathbf{x}_i\\le \\mathbf{x}_{\\max},\\quad \\mathbf{u}_{\\min}\\le \\mathbf{u}_i\\le \\mathbf{u}_{\\max},\\\\\n& \\mathbf{x}_0=\\mathbf{x}(t_0).\n\\end{aligned}\n\nNote that:\n\nThe running cost and path constraints are evaluated at the same right-endpoint where the dynamics are enforced, keeping “where we pay” aligned with “where we enforce.”\n\nState continuity is automatic because \\mathbf{x}_{i+1} is a shared variable between adjacent intervals; slope continuity is not enforced unless you add it.\nHere’s an updated subsection that explicitly says what collocation nodes are chosen and why the trapezoidal defect uses them the way it does.\n\nSide remark. If you instead collocate at the left endpoint (Radau-I with \\tau=0) with the same linear model, you obtain \\frac{1}{h_k}(\\mathbf{x}_{k+1}-\\mathbf{x}_k)=\\mathbf{f}(\\mathbf{x}_k,\\mathbf{u}_k,t_k), i.e., the explicit Euler step. In that very precise sense, explicit Euler can be viewed as a (left-endpoint) degree-1 collocation scheme.\n\nExplicit–Euler Collocation (Radau-I, degree 1)\n\nLet t_0<\\cdots<t_N with h_i:=t_{i+1}-t_i. Decision variables are \\{\\mathbf{x}_i\\}_{i=0}^N and \\{\\mathbf{u}_i\\}_{i=0}^N. Solve\\begin{aligned}\n\\min\\ & c_T(\\mathbf{x}_N)\\;+\\;\\sum_{i=0}^{N-1} h_i\\,c(\\mathbf{x}_i,\\mathbf{u}_i)\\\\\n\\text{s.t.}\\ & \\mathbf{x}_{i+1}-\\mathbf{x}_i - h_i\\,\\mathbf{f}(\\mathbf{x}_i,\\mathbf{u}_i)=\\mathbf{0},\\quad i=0,\\ldots,N-1,\\\\\n& \\mathbf{g}(\\mathbf{x}_i,\\mathbf{u}_i)\\le \\mathbf{0},\\quad i=0,\\ldots,N-1,\\\\\n& \\mathbf{x}_{\\min}\\le \\mathbf{x}_i\\le \\mathbf{x}_{\\max},\\quad \\mathbf{u}_{\\min}\\le \\mathbf{u}_i\\le \\mathbf{u}_{\\max},\\\\\n& \\mathbf{x}_0=\\mathbf{x}(t_0).\n\\end{aligned}","type":"content","url":"/cocp#euler-collocation","position":29},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl3":"Trapezoidal collocation","lvl2":"A Compendium of Direct Transcription Methods in Trajectory Optimization"},"type":"lvl3","url":"/cocp#trapezoidal-collocation","position":30},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl3":"Trapezoidal collocation","lvl2":"A Compendium of Direct Transcription Methods in Trajectory Optimization"},"content":"In this scheme we take the two endpoints as the nodes on each interval:\\tau_0=0,\\qquad \\tau_1=1\\quad(\\text{``Lobatto with }K=1\").\n\nWe approximate \\mathbf{x} linearly over [t_i,t_{i+1}], and we evaluate both the running cost and the dynamics at these two nodes with equal weights. Because a linear polynomial has a constant derivative, we do not try to match the ODE’s slope at both endpoints (that would overconstrain a linear function). Instead, we enforce the ODE in its integral form over the interval and approximate the integral of \\mathbf{f} by the trapezoid rule using those two nodes. This makes the cost quadrature and the state-update (“defect”) use the same nodes and weights.\n\nTrapezoidal Collocation\n\nLet t_0<\\cdots<t_N with h_i:=t_{i+1}-t_i. Decision variables are \\{\\mathbf{x}_i\\}_{i=0}^N, \\{\\mathbf{u}_i\\}_{i=0}^N. Solve\\begin{aligned}\n\\min_{\\{\\mathbf{x}_i,\\mathbf{u}_i\\}}\\ & c_T(\\mathbf{x}_N)\\; +\\; \\sum_{i=0}^{N-1} \\tfrac{h_i}{2}\\,\\Big[c(\\mathbf{x}_i,\\mathbf{u}_i)+c(\\mathbf{x}_{i+1},\\mathbf{u}_{i+1})\\Big]\\\\\n\\text{s.t.}\\ & \\mathbf{x}_{i+1}-\\mathbf{x}_i \\;-\\; \\tfrac{h_i}{2}\\Big[\\mathbf{f}(\\mathbf{x}_i,\\mathbf{u}_i)+\\mathbf{f}(\\mathbf{x}_{i+1},\\mathbf{u}_{i+1})\\Big] \\;=\\; \\mathbf{0},\\quad i=0,\\ldots,N-1,\\\\\n& \\mathbf{g}(\\mathbf{x}_i,\\mathbf{u}_i) \\le \\mathbf{0},\\ \\ \\mathbf{g}(\\mathbf{x}_{i+1},\\mathbf{u}_{i+1}) \\le \\mathbf{0},\\\\\n& \\mathbf{x}_{\\min} \\le \\mathbf{x}_i \\le \\mathbf{x}_{\\max},\\ \\ \\mathbf{u}_{\\min} \\le \\mathbf{u}_i \\le \\mathbf{u}_{\\max},\\\\\n& \\mathbf{x}_0 = \\mathbf{x}(t_0).\n\\end{aligned}\n\nSummary: the collocation nodes for trapezoidal are the two endpoints; the state is linear on each interval; and the dynamics are enforced via the integrated ODE with the trapezoid rule at those two nodes, yielding the familiar trapezoidal defect.","type":"content","url":"/cocp#trapezoidal-collocation","position":31},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl3":"Hermite–Simpson (quadratic interpolation; midpoint included)","lvl2":"A Compendium of Direct Transcription Methods in Trajectory Optimization"},"type":"lvl3","url":"/cocp#hermite-simpson-quadratic-interpolation-midpoint-included","position":32},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl3":"Hermite–Simpson (quadratic interpolation; midpoint included)","lvl2":"A Compendium of Direct Transcription Methods in Trajectory Optimization"},"content":"On each interval [t_i,t_{i+1}] we pick three collocation nodes on the reference domain \\tau\\in[0,1]:\\tau_0=0,\\qquad \\tau_{1/2}=\\tfrac12,\\qquad \\tau_1=1.\n\nSo we evaluate at left, midpoint, right. These are the same three nodes used by Simpson’s rule (weights 1{:}4{:}1) for numerical quadrature. We let \\mathbf{x}_h be quadratic in \\tau. Two things then happen:\n\nIntegral (defect) enforcement with Simpson’s rule.\nWe enforce the ODE in integral form over the interval and approximate the integral of \\mathbf{f} with Simpson’s rule using the three nodes above. This yields the first constraint (the “Simpson defect”), which uses \\mathbf{f} evaluated at left, midpoint, and right.\n\nSlope matching at the midpoint (collocation).\nBecause a quadratic has limited shape, we don’t try to match slopes at both endpoints. Instead, we introduce midpoint variables (\\mathbf{x}_{i+\\frac12},\\mathbf{u}_{i+\\frac12}) and match the ODE at the midpoint. The second constraint below is exactly the midpoint collocation condition written in an equivalent Hermite form: it pins the midpoint state to the average of the endpoints plus a correction based on endpoint slopes, ensuring that the polynomial’s derivative is consistent with the ODE at \\tau=\\tfrac12.\n\nThis way, where we pay (Simpson quadrature) and where we enforce (midpoint collocation + Simpson defect) are aligned at the same three nodes, which is why the method is both accurate and well conditioned on smooth problems.\n\nHermite–Simpson Transcription\n\nLet t_0<\\cdots<t_N with h_i:=t_{i+1}-t_i and midpoints t_{i+\\frac12}. Decision variables are \\{\\mathbf{x}_i\\}_{i=0}^N, \\{\\mathbf{u}_i\\}_{i=0}^N, plus midpoint variables \\{\\mathbf{x}_{i+\\frac12},\\mathbf{u}_{i+\\frac12}\\}_{i=0}^{N-1}. Solve\\begin{aligned}\n\\min\\ & c_T(\\mathbf{x}_N)\\; +\\; \\sum_{i=0}^{N-1} \\tfrac{h_i}{6}\\Big[ c(\\mathbf{x}_i,\\mathbf{u}_i) + 4\\,c(\\mathbf{x}_{i+\\frac12},\\mathbf{u}_{i+\\frac12}) + c(\\mathbf{x}_{i+1},\\mathbf{u}_{i+1}) \\Big]\\\\\n\\text{s.t.}\\ & \\underbrace{\\mathbf{x}_{i+1}-\\mathbf{x}_i - \\tfrac{h_i}{6}\\Big[\\mathbf{f}(\\mathbf{x}_i,\\mathbf{u}_i) + 4\\,\\mathbf{f}(\\mathbf{x}_{i+\\frac12},\\mathbf{u}_{i+\\frac12}) + \\mathbf{f}(\\mathbf{x}_{i+1},\\mathbf{u}_{i+1})\\Big]}_{\\text{Simpson defect over }[t_i,t_{i+1}]} = \\mathbf{0},\\\\\n& \\underbrace{\\mathbf{x}_{i+\\frac12} - \\tfrac{\\mathbf{x}_i+\\mathbf{x}_{i+1}}{2} - \\tfrac{h_i}{8}\\Big[\\mathbf{f}(\\mathbf{x}_i,\\mathbf{u}_i) - \\mathbf{f}(\\mathbf{x}_{i+1},\\mathbf{u}_{i+1})\\Big]}_{\\text{midpoint collocation (slope matching at }t_{i+\\frac12}\\text{)}} = \\mathbf{0},\\\\\n& \\mathbf{g}(\\mathbf{x}_i,\\mathbf{u}_i) \\le \\mathbf{0},\\ \\ \\mathbf{g}(\\mathbf{x}_{i+\\frac12},\\mathbf{u}_{i+\\frac12}) \\le \\mathbf{0},\\ \\ \\mathbf{g}(\\mathbf{x}_{i+1},\\mathbf{u}_{i+1}) \\le \\mathbf{0},\\\\\n& \\mathbf{x}_{\\min} \\le \\mathbf{x}_i,\\mathbf{x}_{i+\\frac12} \\le \\mathbf{x}_{\\max},\\ \\ \\mathbf{u}_{\\min} \\le \\mathbf{u}_i,\\mathbf{u}_{i+\\frac12} \\le \\mathbf{u}_{\\max},\\\\\n& \\mathbf{x}_0 = \\mathbf{x}(t_0),\\quad i=0,\\ldots,N-1.\n\\end{aligned}\n\nCollocation nodes recap: \\tau=0,\\ \\tfrac12,\\ 1.\n\nThe midpoint is where we explicitly match the ODE slope (collocation).\n\nThe three nodes together are used for the Simpson integral of \\mathbf{f} (state update) and of c (cost), keeping physics and objective synchronized.","type":"content","url":"/cocp#hermite-simpson-quadratic-interpolation-midpoint-included","position":33},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl2":"Examples"},"type":"lvl2","url":"/cocp#examples","position":34},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl2":"Examples"},"content":"","type":"content","url":"/cocp#examples","position":35},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl3":"Compressor Surge Problem","lvl2":"Examples"},"type":"lvl3","url":"/cocp#compressor-surge-problem","position":36},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl3":"Compressor Surge Problem","lvl2":"Examples"},"content":"A compressor is a machine that raises the pressure of a gas by squeezing it into a smaller volume. You find them in natural gas pipelines, jet engines, and factories. But compressors can run into trouble if the flow of gas becomes too small. In that case, the machine can “stall” much like an airplane wing at too high an angle. Instead of moving forward, the gas briefly pushes back, creating strong pressure oscillations that can damage the compressor and anything connected to it.\n\nTo prevent this, engineers often add a close-coupled valve (CCV) at the outlet. The valve can quickly adjust the flow to keep the compressor away from these unstable conditions. Our goal is to design a control strategy for operating this valve so that the compressor never enters surge.\n\nFollowing  \n\nGravdahl & Egeland, 1997 and \n\nGrancharova & Johansen (2012), we model the compressor using a simplified second-order representation:\\begin{aligned}\n\\dot{x}_1 &= B(\\Psi_e(x_1) - x_2 - u) \\\\\n\\dot{x}_2 &= \\frac{1}{B}(x_1 - \\Phi(x_2))\n\\end{aligned}\n\nHere, \\mathbf{x} = [x_1, x_2]^T represents the state variables:\n\nx_1 is the normalized mass flow through the compressor.\n\nx_2 is the normalized pressure ratio across the compressor.\n\nThe control input u denotes the normalized mass flow through a CCV.\nThe functions \\Psi_e(x_1) and \\Phi(x_2) represent the characteristics of the compressor and valve, respectively:\\begin{aligned}\n\\Psi_e(x_1) &= \\psi_{c0} + H\\left(1 + 1.5\\left(\\frac{x_1}{W} - 1\\right) - 0.5\\left(\\frac{x_1}{W} - 1\\right)^3\\right) \\\\\n\\Phi(x_2) &= \\gamma \\operatorname{sign}(x_2) \\sqrt{|x_2|}\n\\end{aligned}\n\nThe system parameters are given as \\gamma = 0.5, B = 1, H = 0.18, \\psi_{c0} = 0.3, and W = 0.25.\n\nOne possible way to pose the problem \n\nGrancharova & Johansen (2012) is by penalizing deviations from the setpoints using a quadratic penalty in the instantaneous cost function as well as in the terminal one. Furthermore, we also penalize taking large actions (which are energy hungry and potentially unsafe) within the integral term. The idea of penalizing deviations throughout is a natural way of posing the problem when solving it via single shooting. Another alternative, which we will explore below, is to set the desired setpoint as a hard terminal constraint.\n\nThe control objective is to stabilize the system and prevent surge, formulated as a continuous-time optimal control problem (COCP) in the Bolza form:\\begin{aligned}\n\\text{minimize} \\quad & \\left[ \\int_0^T \\alpha(\\mathbf{x}(t) - \\mathbf{x}^*)^T(\\mathbf{x}(t) - \\mathbf{x}^*) + \\kappa u(t)^2 \\, dt\\right] + \\beta(\\mathbf{x}(T) - \\mathbf{x}^*)^T(\\mathbf{x}(T) - \\mathbf{x}^*) + R v^2  \\\\\n\\text{subject to} \\quad & \\dot{x}_1(t) = B(\\Psi_e(x_1(t)) - x_2(t) - u(t)) \\\\\n& \\dot{x}_2(t) = \\frac{1}{B}(x_1(t) - \\Phi(x_2(t))) \\\\\n& u_{\\text{min}} \\leq u(t) \\leq u_{\\text{max}} \\\\\n& -x_2(t) + 0.4 \\leq v \\\\\n& -v \\leq 0 \\\\\n& \\mathbf{x}(0) = \\mathbf{x}_0\n\\end{aligned}\n\nThe parameters \\alpha, \\beta, \\kappa, and R are non-negative weights that allow the designer to prioritize different aspects of performance (e.g., tight setpoint tracking vs. smooth control actions). We also constraint the control input to be within 0 \\leq u(t) \\leq 0.3 due to the physical limitations of the valve.\n\nThe authors in \n\nGrancharova & Johansen (2012) also add a soft path constraint x_2(t) \\geq 0.4 to ensure that we maintain a minimum pressure at all time. This is implemented as a soft constraint using slack variables. The reason that we have the term R v^2 in the objective is to penalizes violations of the soft constraint: we allow for deviations, but don’t want to do it too much.\n\nIn the experiment below, we choose the setpoint \\mathbf{x}^* = [0.40, 0.60]^T as it corresponds to an unstable equilibrium point. If we were to run the system without applying any control, we would see that the system starts to oscillate.\n\nimport numpy as np\nfrom scipy.optimize import minimize\nimport matplotlib.pyplot as plt\n\n# System parameters\ngamma, B, H, psi_c0, W = 0.5, 1, 0.18, 0.3, 0.25\nalpha, beta, kappa, R = 1, 0, 0.08, 0\nT, N = 12, 60\ndt = T / N\nx1_star, x2_star = 0.40, 0.60\n\ndef psi_e(x1):\n    return psi_c0 + H * (1 + 1.5 * ((x1 / W) - 1) - 0.5 * ((x1 / W) - 1)**3)\n\ndef phi(x2):\n    return gamma * np.sign(x2) * np.sqrt(np.abs(x2))\n\ndef system_dynamics(x, u):\n    x1, x2 = x\n    dx1dt = B * (psi_e(x1) - x2 - u)\n    dx2dt = (1 / B) * (x1 - phi(x2))\n    return np.array([dx1dt, dx2dt])\n\ndef euler_step(x, u, dt):\n    return x + dt * system_dynamics(x, u)\n\ndef instantenous_cost(x, u):\n    return (alpha * np.sum((x - np.array([x1_star, x2_star]))**2) + kappa * u**2)\n\ndef terminal_cost(x):\n    return beta * np.sum((x - np.array([x1_star, x2_star]))**2)\n\ndef objective_and_constraints(z):\n    u, v = z[:-1], z[-1]\n    x = np.zeros((N+1, 2))\n    x[0] = x0\n    obj = 0\n    cons = []\n    for i in range(N):\n        x[i+1] = euler_step(x[i], u[i], dt)\n        obj += dt * instantenous_cost(x[i], u[i])\n        cons.append(0.4 - x[i+1, 1] - v)\n    obj += terminal_cost(x[-1]) + R * v**2\n    return obj, np.array(cons)\n\ndef solve_trajectory_optimization(x0, u_init):\n    z0 = np.zeros(N + 1)\n    z0[:-1] = u_init\n    bounds = [(0, 0.3)] * N + [(0, None)]\n    result = minimize(\n        lambda z: objective_and_constraints(z)[0],\n        z0,\n        method='SLSQP',\n        bounds=bounds,\n        constraints={'type': 'ineq', 'fun': lambda z: -objective_and_constraints(z)[1]},\n        options={'disp': True, 'maxiter': 1000, 'ftol': 1e-6}\n    )\n    return result.x, result\n\ndef simulate_trajectory(x0, u):\n    x = np.zeros((N+1, 2))\n    x[0] = x0\n    for i in range(N):\n        x[i+1] = euler_step(x[i], u[i], dt)\n    return x\n\n# Run optimizations and simulations\nx0 = np.array([0.25, 0.25])\nt = np.linspace(0, T, N+1)\n\n# Optimized control starting from zero\nz_single_shooting, _ = solve_trajectory_optimization(x0, np.zeros(N))\nu_opt_shoot, v_opt_shoot = z_single_shooting[:-1], z_single_shooting[-1]\nx_opt_shoot = simulate_trajectory(x0, u_opt_shoot)\n\n# Do-nothing control (u = 0)\nu_nothing = np.zeros(N)\nx_nothing = simulate_trajectory(x0, u_nothing)\n\n# Plotting\nplt.figure(figsize=(15, 20))\n\n# State variables over time\nplt.subplot(3, 1, 1)\nplt.plot(t, x_opt_shoot[:, 0], label='x1 (opt from 0)')\nplt.plot(t, x_opt_shoot[:, 1], label='x2 (opt from 0)')\nplt.plot(t, x_nothing[:, 0], ':', label='x1 (do-nothing)')\nplt.plot(t, x_nothing[:, 1], ':', label='x2 (do-nothing)')\nplt.axhline(y=x1_star, color='r', linestyle='--', label='x1 setpoint')\nplt.axhline(y=x2_star, color='g', linestyle='--', label='x2 setpoint')\nplt.xlabel('Time')\nplt.ylabel('State variables')\nplt.title('State variables over time')\nplt.legend()\nplt.grid(True)\n\n# Phase portrait\nplt.subplot(3, 1, 2)\nplt.plot(x_opt_shoot[:, 0], x_opt_shoot[:, 1], label='Optimized from 0')\nplt.plot(x_nothing[:, 0], x_nothing[:, 1], ':', label='Do-nothing')\nplt.plot(x1_star, x2_star, 'r*', markersize=10, label='Setpoint')\nplt.xlabel('x1 (mass flow)')\nplt.ylabel('x2 (pressure)')\nplt.title('Phase portrait')\nplt.legend()\nplt.grid(True)\n\n# Control inputs\nplt.subplot(3, 1, 3)\nplt.plot(t[:-1], u_opt_shoot, label='Optimized from 0')\nplt.plot(t[:-1], u_nothing, ':', label='Do-nothing')\nplt.xlabel('Time')\nplt.ylabel('Control input (u)')\nplt.title('Control input over time')\nplt.legend()\nplt.grid(True)\n\n","type":"content","url":"/cocp#compressor-surge-problem","position":37},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl4":"Solution by Trapezoidal Collocation","lvl3":"Compressor Surge Problem","lvl2":"Examples"},"type":"lvl4","url":"/cocp#solution-by-trapezoidal-collocation","position":38},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl4":"Solution by Trapezoidal Collocation","lvl3":"Compressor Surge Problem","lvl2":"Examples"},"content":"Another way to pose the problem is by imposing a terminal state constraint on the system rather than through a penalty in the integral term. In the following experiment, we use a problem formulation of the form:\\begin{aligned}\n\\text{minimize} \\quad & \\left[ \\int_0^T \\kappa u(t)^2 \\, dt\\right] \\\\\n\\text{subject to} \\quad & \\dot{x}_1(t) = B(\\Psi_e(x_1(t)) - x_2(t) - u(t)) \\\\\n& \\dot{x}_2(t) = \\frac{1}{B}(x_1(t) - \\Phi(x_2(t))) \\\\\n& u_{\\text{min}} \\leq u(t) \\leq u_{\\text{max}} \\\\\n& \\mathbf{x}(0) = \\mathbf{x}_0 \\\\\n& \\mathbf{x}(T) = \\mathbf{x}^\\star\n\\end{aligned}\n\nWe then find a control function u(t) and state trajectory x(t) using the trapezoidal collocation method.\n\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.integrate import solve_ivp\nfrom scipy.interpolate import interp1d\nimport matplotlib.pyplot as plt\n\n# System parameters\ngamma, B, H, psi_c0, W = 0.5, 1, 0.18, 0.3, 0.25\nkappa = 0.08\nT, N = 12, 20  # Number of collocation points\nt = np.linspace(0, T, N)\ndt = T / (N - 1)\nx1_star, x2_star = 0.40, 0.60\n\ndef psi_e(x1):\n    return psi_c0 + H * (1 + 1.5 * ((x1 / W) - 1) - 0.5 * ((x1 / W) - 1)**3)\n\ndef phi(x2):\n    return gamma * np.sign(x2) * np.sqrt(np.abs(x2))\n\ndef system_dynamics(t, x, u_func):\n    x1, x2 = x\n    u = u_func(t)\n    dx1dt = B * (psi_e(x1) - x2 - u)\n    dx2dt = (1 / B) * (x1 - phi(x2))\n    return [dx1dt, dx2dt]\n\ndef objective(z):\n    x = z[:2*N].reshape((N, 2))\n    u = z[2*N:]\n    \n    # Trapezoidal rule for the cost function\n    cost = 0\n    for i in range(N-1):\n        cost += 0.5 * dt * (kappa * u[i]**2 + kappa * u[i+1]**2)\n    \n    return cost\n\ndef constraints(z):\n    x = z[:2*N].reshape((N, 2))\n    u = z[2*N:]\n    \n    cons = []\n    \n    # Dynamics constraints (trapezoidal rule)\n    for i in range(N-1):\n        f_i = system_dynamics(t[i], x[i], lambda t: u[i])\n        f_ip1 = system_dynamics(t[i+1], x[i+1], lambda t: u[i+1])\n        cons.extend(x[i+1] - x[i] - 0.5 * dt * (np.array(f_i) + np.array(f_ip1)))\n    \n    # Terminal constraint\n    cons.extend([x[-1, 0] - x1_star, x[-1, 1] - x2_star])\n    \n    # Initial condition constraint\n    cons.extend([x[0, 0] - x0[0], x[0, 1] - x0[1]])\n    \n    return np.array(cons)\n\ndef solve_trajectory_optimization(x0):\n    # Initial guess\n    x_init = np.linspace(x0, [x1_star, x2_star], N)\n    u_init = np.zeros(N)\n    z0 = np.concatenate([x_init.flatten(), u_init])\n    \n    # Bounds\n    bounds = [(None, None)] * (2*N)  # State variables\n    bounds += [(0, 0.3)] * N  # Control inputs\n    \n    # Constraints\n    cons = {'type': 'eq', 'fun': constraints}\n    \n    result = minimize(\n        objective,\n        z0,\n        method='SLSQP',\n        bounds=bounds,\n        constraints=cons,\n        options={'disp': True, 'maxiter': 1000, 'ftol': 1e-6}\n    )\n    return result.x, result\n\n# Run optimization\nx0 = np.array([0.5, 0.5])\nz_opt, result = solve_trajectory_optimization(x0)\nx_opt_coll = z_opt[:2*N].reshape((N, 2))\nu_opt_coll = z_opt[2*N:]\n\nprint(f\"Optimization successful: {result.success}\")\nprint(f\"Final objective value: {result.fun}\")\nprint(f\"Final state: x1 = {x_opt_coll[-1, 0]:.4f}, x2 = {x_opt_coll[-1, 1]:.4f}\")\nprint(f\"Target state: x1 = {x1_star:.4f}, x2 = {x2_star:.4f}\")\n\n# Create interpolated control function\nu_func = interp1d(t, u_opt_coll, kind='linear', bounds_error=False, fill_value=(u_opt_coll[0], u_opt_coll[-1]))\n\n# Solve IVP with the optimized control\nsol = solve_ivp(lambda t, x: system_dynamics(t, x, u_func), [0, T], x0, dense_output=True)\n\n# Generate solution points\nt_dense = np.linspace(0, T, 200)\nx_ivp = sol.sol(t_dense).T\n\n# Plotting\nplt.figure(figsize=(15, 20))\n\n# State variables over time\nplt.subplot(3, 1, 1)\nplt.plot(t, x_opt_coll[:, 0], 'bo-', label='x1 (collocation)')\nplt.plot(t, x_opt_coll[:, 1], 'ro-', label='x2 (collocation)')\nplt.plot(t_dense, x_ivp[:, 0], 'b--', label='x1 (integrated)')\nplt.plot(t_dense, x_ivp[:, 1], 'r--', label='x2 (integrated)')\nplt.axhline(y=x1_star, color='b', linestyle=':', label='x1 setpoint')\nplt.axhline(y=x2_star, color='r', linestyle=':', label='x2 setpoint')\nplt.xlabel('Time')\nplt.ylabel('State variables')\nplt.title('State variables over time')\nplt.legend()\nplt.grid(True)\n\n# Phase portrait\nplt.subplot(3, 1, 2)\nplt.plot(x_opt_coll[:, 0], x_opt_coll[:, 1], 'go-', label='Collocation')\nplt.plot(x_ivp[:, 0], x_ivp[:, 1], 'm--', label='Integrated')\nplt.plot(x1_star, x2_star, 'r*', markersize=10, label='Setpoint')\nplt.xlabel('x1 (mass flow)')\nplt.ylabel('x2 (pressure)')\nplt.title('Phase portrait')\nplt.legend()\nplt.grid(True)\n\n# Control inputs\nplt.subplot(3, 1, 3)\nplt.step(t, u_opt_coll, 'g-', where='post', label='Collocation')\nplt.plot(t_dense, u_func(t_dense), 'm--', label='Interpolated')\nplt.xlabel('Time')\nplt.ylabel('Control input (u)')\nplt.title('Control input over time')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\nYou can try to vary the number of collocation points in the code and observe how the state trajectory progressively matches the ground truth (the line denoted “integrated solution”). Note that this version of the code also lacks bound constraints on the variable x_2 to ensure a minimum pressure, as we did earlier. Consider this a good exercise to try on your own.","type":"content","url":"/cocp#solution-by-trapezoidal-collocation","position":39},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl4":"System Identification as Trajectory Optimization (Compressor Surge)","lvl3":"Compressor Surge Problem","lvl2":"Examples"},"type":"lvl4","url":"/cocp#system-identification-as-trajectory-optimization-compressor-surge","position":40},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl4":"System Identification as Trajectory Optimization (Compressor Surge)","lvl3":"Compressor Surge Problem","lvl2":"Examples"},"content":"We now turn the compressor surge model into a simple system identification task: estimate unknown parameters (here, the scalar B) from measured trajectories. This can be viewed as a trajectory optimization problem: choose parameters (and optionally states) to minimize reconstruction error while enforcing the dynamics.\n\nGiven time-aligned data \\{(\\mathbf{u}_k,\\mathbf{y}_k)\\}_{k=0}^{N}, model states \\mathbf{x}_k\\in\\mathbb{R}^d, outputs \\mathbf{y}_k\\approx \\mathbf{h}(\\mathbf{x}_k;\\boldsymbol{\\theta}), step \\Delta t, and dynamics \\mathbf{f}(\\mathbf{x},\\mathbf{u};\\boldsymbol{\\theta}), the simultaneous (full-discretization) viewpoint is\\begin{aligned}\n\\min_{\\boldsymbol{\\theta},\\,\\{\\mathbf{x}_k\\}} \\quad & \\sum_{k\\in K}\\;\\big\\|\\mathbf{y}_k - \\mathbf{h}(\\mathbf{x}_k;\\boldsymbol{\\theta})\\big\\|_2^2 \\\\\n\\text{s.t.}\\quad & \\mathbf{x}_{k+1} - \\mathbf{x}_k - \\Delta t\\,\\mathbf{f}(\\mathbf{x}_k,\\mathbf{u}_k;\\boldsymbol{\\theta}) = \\mathbf{0},\\quad k=0,\\ldots,N-1, \\\\\n& \\mathbf{x}_0 \\;\\text{given},\n\\end{aligned}\n\nwhile the single-shooting (recursive elimination) variant eliminates the states by simulating forward from \\mathbf{x}_0:J(\\boldsymbol{\\theta}) := \\sum_{k\\in K}\\;\\big\\|\\mathbf{y}_k - \\mathbf{h}(\\boldsymbol{\\phi}_k(\\boldsymbol{\\theta};\\mathbf{x}_0,\\mathbf{u}_{0:N-1})\\big\\|_2^2,\\quad \\min_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}),\n\nwhere \\boldsymbol{\\phi}_k denotes the state reached at step k by an RK4 rollout under parameter \\boldsymbol{\\theta}. In our demo the data grid and rollout grid coincide, so \\boldsymbol{\\phi}_k = \\mathbf{x}_k and no interpolation is required. We will identify B by fitting the model to data generated from the ground-truth B=1 system under randomized initial conditions and small input perturbations.\n\nimport numpy as np\nfrom scipy.integrate import solve_ivp\nimport matplotlib.pyplot as plt\n\n# System parameters\ngamma, B, H, psi_c0, W = 0.5, 1, 0.18, 0.3, 0.25\n\n# Simulation parameters\nT = 50  # Total simulation time\ndt = 0.1  # Time step\nt = np.arange(0, T + dt, dt)\nN = len(t)\n\n# Number of trajectories\nnum_trajectories = 10\n\ndef psi_e(x1):\n    return psi_c0 + H * (1 + 1.5 * ((x1 / W) - 1) - 0.5 * ((x1 / W) - 1)**3)\n\ndef phi(x2):\n    return gamma * np.sign(x2) * np.sqrt(np.abs(x2))\n\ndef system_dynamics(t, x, u):\n    x1, x2 = x\n    dx1dt = B * (psi_e(x1) - x2 - u)\n    dx2dt = (1 / B) * (x1 - phi(x2))\n    return [dx1dt, dx2dt]\n\n# \"Do nothing\" controller with small random noise\ndef u_func(t):\n    return np.random.normal(0, 0.01)  # Mean 0, standard deviation 0.01\n\n# Function to simulate a single trajectory\ndef simulate_trajectory(x0):\n    sol = solve_ivp(lambda t, x: system_dynamics(t, x, u_func(t)), [0, T], x0, t_eval=t, method='RK45')\n    return sol.y[0], sol.y[1]\n\n# Generate multiple trajectories\ntrajectories = []\ninitial_conditions = []\n\nfor i in range(num_trajectories):\n    # Randomize initial conditions around [0.5, 0.5]\n    x0 = np.array([0.5, 0.5]) + np.random.normal(0, 0.05, 2)\n    initial_conditions.append(x0)\n    x1, x2 = simulate_trajectory(x0)\n    trajectories.append((x1, x2))\n\n# Calculate control inputs (small random noise)\nu = np.array([u_func(ti) for ti in t])\n\n# Plotting\nplt.figure(figsize=(15, 15))\n\n# State variables over time\nplt.subplot(3, 1, 1)\nfor i, (x1, x2) in enumerate(trajectories):\n    plt.plot(t, x1, label=f'x1 (Traj {i+1})' if i == 0 else \"_nolegend_\")\n    plt.plot(t, x2, label=f'x2 (Traj {i+1})' if i == 0 else \"_nolegend_\")\nplt.xlabel('Time')\nplt.ylabel('State variables')\nplt.title('State variables over time (Multiple Trajectories)')\nplt.legend()\nplt.grid(True)\n\n# Phase portrait\nplt.subplot(3, 1, 2)\nfor x1, x2 in trajectories:\n    plt.plot(x1, x2)\n    plt.plot(x1[0], x2[0], 'bo', markersize=5)\n    plt.plot(x1[-1], x2[-1], 'ro', markersize=5)\nplt.xlabel('x1 (mass flow)')\nplt.ylabel('x2 (pressure)')\nplt.title('Phase portrait (Multiple Trajectories)')\nplt.grid(True)\n\n# Control input (small random noise)\nplt.subplot(3, 1, 3)\nplt.plot(t, u, 'k-')\nplt.xlabel('Time')\nplt.ylabel('Control input (u)')\nplt.title('Control input over time (Small random noise)')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n# Save the data\nnp.savez('_static/compressor_surge_data_multi.npz', t=t, trajectories=trajectories, u=u, initial_conditions=initial_conditions)\n\nprint(\"Data collection complete. Results saved to 'compressor_surge_data_multi.npz'\")\nprint(f\"Data shape: {num_trajectories} trajectories, each with {N} time steps\")\nprint(f\"Time range: 0 to {T} seconds\")\nprint(\"Initial conditions:\")\nfor i, x0 in enumerate(initial_conditions):\n    print(f\"  Trajectory {i+1}: x1 = {x0[0]:.4f}, x2 = {x0[1]:.4f}\")\n\nimport numpy as np\nfrom scipy.optimize import minimize\nimport matplotlib.pyplot as plt\n\n# Load the data\ndata = np.load('_static/compressor_surge_data_multi.npz', allow_pickle=True)\nt = data['t']\ntrajectories = data['trajectories']\nu = data['u']\ninitial_conditions = data['initial_conditions']\n\n# Known system parameters\ngamma, H, psi_c0, W = 0.5, 0.18, 0.3, 0.25\n# B is the parameter we want to identify\nB_true = 1.0  # True value, used for comparison\n\ndef psi_e(x1):\n    return psi_c0 + H * (1 + 1.5 * ((x1 / W) - 1) - 0.5 * ((x1 / W) - 1)**3)\n\ndef phi(x2):\n    return gamma * np.sign(x2) * np.sqrt(np.abs(x2))\n\ndef system_dynamics(t, x, u, B):\n    x1, x2 = x\n    dx1dt = B * (psi_e(x1) - x2 - u)\n    dx2dt = (1 / B) * (x1 - phi(x2))\n    return np.array([dx1dt, dx2dt])\n\ndef rk4_step(f, t, x, u, dt, B):\n    k1 = f(t, x, u, B)\n    k2 = f(t + 0.5*dt, x + 0.5*dt*k1, u, B)\n    k3 = f(t + 0.5*dt, x + 0.5*dt*k2, u, B)\n    k4 = f(t + dt, x + dt*k3, u, B)\n    return x + (dt/6) * (k1 + 2*k2 + 2*k3 + k4)\n\ndef simulate_trajectory(x0, B):\n    x = np.zeros((len(t), 2))\n    x[0] = x0\n    for i in range(1, len(t)):\n        x[i] = rk4_step(system_dynamics, t[i-1], x[i-1], u[i-1], t[i] - t[i-1], B)\n    return x\n\ndef objective(B):\n    error = 0\n    for i, (x1_obs, x2_obs) in enumerate(trajectories):\n        x_sim = simulate_trajectory(initial_conditions[i], B[0])\n        error += np.sum((x_sim[:, 0] - x1_obs)**2 + (x_sim[:, 1] - x2_obs)**2)\n    return error\n\n# Perform optimization\nresult = minimize(objective, x0=[1.5], method='Nelder-Mead', options={'disp': True})\n\nB_identified = result.x[0]\n\nprint(f\"True B: {B_true}\")\nprint(f\"Identified B: {B_identified}\")\nprint(f\"Relative error: {abs(B_identified - B_true) / B_true * 100:.2f}%\")\n\n# Plot results\nplt.figure(figsize=(15, 10))\n\n# Plot one trajectory for comparison\ntraj_index = 0\nx1_obs, x2_obs = trajectories[traj_index]\nx_sim = simulate_trajectory(initial_conditions[traj_index], B_identified)\n\nplt.subplot(2, 1, 1)\nplt.plot(t, x1_obs, 'b-', label='Observed x1')\nplt.plot(t, x2_obs, 'r-', label='Observed x2')\nplt.plot(t, x_sim[:, 0], 'b--', label='Simulated x1')\nplt.plot(t, x_sim[:, 1], 'r--', label='Simulated x2')\nplt.xlabel('Time')\nplt.ylabel('State variables')\nplt.title('Observed vs Simulated Trajectory')\nplt.legend()\nplt.grid(True)\n\nplt.subplot(2, 1, 2)\nplt.plot(x1_obs, x2_obs, 'g-', label='Observed')\nplt.plot(x_sim[:, 0], x_sim[:, 1], 'm--', label='Simulated')\nplt.xlabel('x1 (mass flow)')\nplt.ylabel('x2 (pressure)')\nplt.title('Phase Portrait: Observed vs Simulated')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n","type":"content","url":"/cocp#system-identification-as-trajectory-optimization-compressor-surge","position":41},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl3":"Flight Trajectory Optimization","lvl2":"Examples"},"type":"lvl3","url":"/cocp#flight-trajectory-optimization","position":42},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl3":"Flight Trajectory Optimization","lvl2":"Examples"},"content":"We consider a concrete task: computing a fuel-optimal trajectory between Montréal–Trudeau (CYUL) and Toronto Pearson (CYYZ), taking into account both aircraft dynamics and wind conditions along the route. For this demo, we leverage the excellent library \n\nOpenAP.top which provides direct transcription methods and airplane dynamics models \n\nSun, 2022. Furthermore, it allows us to import a a wind field comes from ERA5 \n\nC3S, 2018, a global atmospheric dataset. It combines historical observations from satellites, aircraft, and surface stations with a weather model to reconstruct the state of the atmosphere across space and time. In climate science, this is called a reanalysis.\n\nERA5 data is stored in GRIB files, a compact format widely used in meteorology. Each file contains a gridded field: values of wind and other variables arranged on a regular 4D lattice over longitude, latitude, altitude, and time. Since the aircraft rarely sits exactly on a grid point, we interpolate the wind components it sees as it moves.\n\nThe aircraft is modeled as a point mass with state\\mathbf{x}(t) = (x(t), y(t), h(t), m(t)),\n\nwhere (x, y) is horizontal position, h is altitude, and m is remaining mass. Controls are Mach number M(t), vertical speed v_s(t), and heading angle \\psi(t). The equations of motion combine airspeed and wind:\\begin{aligned}\n\\dot x &= v(M,h)\\cos\\psi\\cos\\gamma + u_w(x,y,h,t), \\\\\n\\dot y &= v(M,h)\\sin\\psi\\cos\\gamma + v_w(x,y,h,t), \\\\\n\\dot h &= v_s, \\\\\n\\dot m &= -\\,\\mathrm{FF}(T(h,M,v_s), h, M, v_s),\n\\end{aligned}\n\nwhere \\gamma = \\arcsin(v_s / v) is the flight path angle and \\mathrm{FF} is the fuel flow rate based on current conditions. The wind terms u_w and v_w are taken from ERA5 and interpolated in space and time.\n\nThe optimization minimizes fuel burn over the CYUL–CYYZ leg. But the same setup could be used to minimize arrival time, or some weighted combination of time, cost, and emissions.\n\nWe use OpenAP.top, which solves the problem using direct collocation at Legendre–Gauss–Lobatto (LGL) points. Each trajectory segment is mapped to the unit interval, the state is interpolated by Lagrange polynomials at nonuniform LGL nodes, and the dynamics are enforced at those points. Integration is done with matching quadrature weights.\n\nThis setup lets us optimize trajectories under realistic conditions by feeding in the appropriate ERA5 GRIB file (e.g., era5_mtl_20230601_12.grib). The result accounts for wind patterns (eg. headwinds, tailwinds, shear) along the corridor between Montréal and Toronto.\n\n# OpenAP.top demo with optional wind overlay – docs: https://github.com/junzis/openap-top\nfrom openap import top\nimport matplotlib.pyplot as plt\nimport os\n\n# Montreal region route (Canada): CYUL (Montréal–Trudeau) → CYYZ (Toronto)\nopt = top.CompleteFlight(\"A320\", \"CYUL\", \"CYYZ\", m0=0.85)\n\n# Optional: point to a local ERA5/GRIB file to enable wind (set env var OPENAP_WIND_GRIB)\n# If not set, look for a default small file produced by `_static/openap_fetch_era5.py`.\nfgrib = os.environ.get(\"OPENAP_WIND_GRIB\", \"_static/era5_mtl_20230601_12.grib\")\nwindfield = None\nif fgrib and os.path.exists(fgrib):\n    try:\n        windfield = top.tools.read_grids(fgrib)\n        opt.enable_wind(windfield)\n    except Exception:\n        windfield = None  # fall back silently if GRIB reading deps are missing\n\n# Solve for a fuel-optimal trajectory (CasADi direct collocation under the hood)\nflight = opt.trajectory(objective=\"fuel\")\n\n# Visualize; overlay wind barbs if windfield available\nif windfield is not None:\n    ax = top.vis.trajectory(flight, windfield=windfield, barb_steps=15)\nelse:\n    ax = top.vis.trajectory(flight)\n\ntitle = \"OpenAP.top fuel-optimal trajectory (A320: CYUL → CYYZ)\"\nif hasattr(ax, \"set_title\"):\n    ax.set_title(title)\nelse:\n    plt.title(title)\n\n","type":"content","url":"/cocp#flight-trajectory-optimization","position":43},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl3":"Hydro Cascade Scheduling with Physical Routing and Multiple Shooting","lvl2":"Examples"},"type":"lvl3","url":"/cocp#hydro-cascade-scheduling-with-physical-routing-and-multiple-shooting","position":44},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl3":"Hydro Cascade Scheduling with Physical Routing and Multiple Shooting","lvl2":"Examples"},"content":"Earlier in the book, we introduced a simplified view of hydro reservoir control, where the water level evolves in discrete time by accounting for inflow and outflow, with precipitation treated as a noisy input. While useful for learning and control design, this model abstracts away much of the physical behavior of actual rivers and dams.\n\nIn this chapter, we move toward a more realistic setup inspired by \n\nSavorgnan et al., 2011. We consider a series of dams arranged in a cascade, where the actions taken upstream influence downstream levels with a delay. The amount of power produced depends not only on how much water flows through the turbines, but also on the head (the vertical distance between the reservoir surface and the turbine outlet). The larger the head, the more potential energy is available for conversion into electricity, and the higher the power output.\n\nTo capture these effects, we follow a modeling approach inspired by the Saint-Venant equations, which describe how water levels and flows evolve in open channels. Instead of solving the full PDEs, we use a reduced model that approximates each dammed section of river (called a reach) as a lumped system governed by an ordinary differential equation. The main variable of interest is the water level h_r(t), which changes over time depending on how much water enters, how much is discharged through the turbines q_r(t), and how much is spilled s_r(t). The mass balance for reach r is written as:\\frac{d h_r(t)}{dt} = \\frac{1}{A_r} \\left( z_r(t) - q_r(t) - s_r(t) \\right),\n\nwhere A_r is the surface area of the reservoir, assumed constant. The inflow z_r(t) to a reach either comes from nature (for the first dam), or from the upstream turbine and spill discharge, delayed by a travel time \\tau_{r-1}:z_1(t) = \\text{inflow}(t), \\qquad\nz_r(t) = q_{r-1}(t - \\tau_{r-1}) + s_{r-1}(t - \\tau_{r-1}), \\quad \\text{for } r > 1.\n\nPower generation at each reach depends on how much water is discharged and the available head:P_r(t) = \\rho g \\eta \\, q_r(t) \\, H_r(h_r(t)),\n\nwhere \\rho is water density, g is gravitational acceleration, \\eta is turbine efficiency, and H_r(h_r(t)) denotes the head as a function of the water level. In some models, the head is approximated as the difference between the current level and a fixed tailwater height (the water level downstream of the dam, after it has passed through the turbine).\n\nThe operator’s goal is to meet a target generation profile P^\\text{ref}(t), such as one dictated by a market dispatch or load-following constraint. This leads to an objective that minimizes the deviation from the target over the full horizon:\\min_{\\{q_r(t), s_r(t)\\}} \\int_0^T \\left( \\sum_{r=1}^R P_r(t) - P^\\text{ref}(t) \\right)^2 dt.\n\nIn practice, this is combined with operational constraints: turbine capacity 0 \\le q_r(t) \\le \\bar{q}_r, spillway limits 0 \\le s_r(t) \\le \\bar{s}_r, and safe level bounds h_r^{\\min} \\le h_r(t) \\le h_r^{\\max}. Depending on the use case, one may also penalize spill to encourage water conservation, or penalize fast changes in levels for ecological reasons.\n\nWhat makes this problem particularly interesting is the coupling across space and time. An upstream reach cannot simply act in isolation: if the operator wants reach r to produce power at a specific time, the water must be released by reach r-1 sufficiently in advance. This coordination is further complicated by delays, nonlinearities in head-dependent power, and limited storage capacity.\n\nWe solve the problem using multiple shooting. Each reach is divided into local simulation segments over short time windows. Within each segment, the dynamics are integrated forward using the ODEs, and continuity constraints are added to ensure that the water levels match across segment boundaries. At the same time, the inflows passed from upstream reaches must arrive at the right time and be consistent with previous decisions. In discrete time, this gives rise to a set of state-update equations:h_r^{k+1} = h_r^k + \\Delta t \\cdot \\frac{1}{A_r}(z_r^k - q_r^k - s_r^k),\n\nwith delays handled by shifting z_r^k according to the appropriate travel time. These constraints are enforced as part of a nonlinear program, alongside the power tracking objective and control bounds.\n\nCompared to the earlier inflow-outflow model, this richer setup introduces more structure, but also more opportunity. The cascade now behaves like a coordinated team: upstream reservoirs can store water in anticipation of future needs, while downstream dams adjust their output to match arrivals and avoid overflows. The optimization reveals not just a schedule, but a strategy for how the entire system should act together to meet demand.\n\n# Instrumented MSD hydro demo with heterogeneity + diagnostics\n# - Breaks symmetry to avoid trivial identical plots\n# - Adds rich diagnostics to explain flat levels and equalities\n#\n# This cell runs end-to-end and shows plots + tables.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom dataclasses import dataclass\nfrom typing import Tuple\nfrom scipy.optimize import minimize\nfrom math import sqrt\nimport warnings\n\n# ---------- Model ----------\n\ng = 9.81  # m/s^2\n\n@dataclass\nclass ReachParams:\n    L: float\n    W: float\n    k_b: float\n    S_b: float\n    k_t: float\n    @property\n    def A_surf(self) -> float:\n        return self.L * self.W\n\ndef smooth_relu(x, eps=1e-9):\n    return 0.5*(x + np.sqrt(x*x + eps))\n\ndef q_bypass(H, rp: ReachParams):\n    H_eff = smooth_relu(H)\n    return rp.k_b * rp.S_b * np.sqrt(2*g*H_eff)\n\ndef muskingum_coeffs(K: float, X: float, dt: float) -> Tuple[float, float, float]:\n    D  = 2.0*K*(1.0 - X) + dt\n    C0 = (dt - 2.0*K*X) / D\n    C1 = (dt + 2.0*K*X) / D\n    C2 = (2.0*K*(1.0 - X) - dt) / D\n    return C0, C1, C2\n\ndef integrate_interval(H0, u, z, dt, nsub, rp: ReachParams):\n    \"\"\"Forward Euler. Returns Hend, avg_qout.\"\"\"\n    h = dt/nsub\n    H = H0\n    qsum = 0.0\n    for _ in range(nsub):\n        qb = q_bypass(H, rp)\n        qout = u + qb\n        dHdt = (z - qout) / rp.A_surf\n        H += h*dHdt\n        qsum += qout\n    return H, qsum/nsub\n\ndef shapes(M,N): return (M*(N+1), M*N, M*N)\n\ndef unpack(x, M, N):\n    nH, nu, nz = shapes(M,N)\n    H = x[:nH].reshape(M,N+1)\n    u = x[nH:nH+nu].reshape(M,N)\n    z = x[nH+nu:nH+nu+nz].reshape(M,N)\n    return H,u,z\n\ndef pack(H,u,z): return np.concatenate([H.ravel(), u.ravel(), z.ravel()])\n\n# ---------- Problem builder ----------\n\ndef make_params_hetero(M):\n    \"\"\"Heterogeneous reaches to break symmetry.\"\"\"\n    # Widths, spillway areas, and power coeffs vary by reach\n    W_list = np.linspace(80, 140, M)         # m\n    L_list = np.full(M, 4000.0)              # m\n    S_b_list = np.linspace(14.0, 20.0, M)    # m^2\n    k_t_list = np.linspace(7.5, 8.5, M)      # power coeff\n    k_b_list = np.linspace(0.55, 0.65, M)    # spill coeff\n    return [ReachParams(L=float(L_list[i]), W=float(W_list[i]),\n                        k_b=float(k_b_list[i]), S_b=float(S_b_list[i]),\n                        k_t=float(k_t_list[i])) for i in range(M)]\n\ndef build_demo(M=3, N=12, dt=900.0, seed=0, hetero=True):\n    rng = np.random.default_rng(seed)\n    params = make_params_hetero(M) if hetero else [ReachParams(4000.0, 100.0, 0.6, 18.26, 8.0) for _ in range(M)]\n\n    # initial levels (heterogeneous)\n    H0 = np.array([17.0, 16.7, 17.3][:M])\n\n    H_ref = np.array([17.0, 16.9, 17.1][:M]) if hetero else np.full(M, 17.0)\n    H_bounds = (16.0, 18.5)\n    u_bounds = (40.0, 160.0)\n\n    Qin_base = 300.0\n    Qin_ext = Qin_base + 30.0*np.sin(2*np.pi*np.arange(N)/N)  # stronger swing\n\n    Pref_raw = 60.0 + 15.0*np.sin(2*np.pi*(np.arange(N)-2)/N)\n\n    # default Muskingum parameters per link (M-1 links)\n    if M > 1:\n        K_list = list(np.linspace(1800.0, 2700.0, M-1))\n        X_list = [0.2]*(M-1)\n    else:\n        K_list = []\n        X_list = []\n\n    return dict(params=params, H0=H0, H_ref=H_ref, H_bounds=H_bounds,\n                u_bounds=u_bounds, Qin_ext=Qin_ext, Pref_raw=Pref_raw,\n                dt=dt, N=N, M=M, nsub=10,\n                muskingum=dict(K=K_list, X=X_list))\n\n# ---------- Objective / constraints / helpers ----------\n\ndef compute_total_power(H,u,params):\n    M,N = u.shape\n    Pn = np.zeros(N)\n    for n in range(N):\n        for i in range(M):\n            Pn[n] += params[i].k_t * u[i,n] * H[i,n]\n    return Pn\n\ndef decompose_objective(x, data, Pref, wP, wH, wDu):\n    H,u,z = unpack(x, data[\"M\"], data[\"N\"])\n    params, H_ref = data[\"params\"], data[\"H_ref\"]\n    track = np.sum((compute_total_power(H,u,params)-Pref)**2)\n    lvl   = np.sum((H[:,:-1]-H_ref[:,None])**2)\n    du    = np.sum((u[:,1:]-u[:,:-1])**2)\n    return dict(track=wP*track, lvl=wH*lvl, du=wDu*du, raw=dict(track=track,lvl=lvl,du=du))\n\ndef make_objective(data, Pref, wP=8.0, wH=0.02, wDu=1e-4):\n    params, H_ref, N, M = data[\"params\"], data[\"H_ref\"], data[\"N\"], data[\"M\"]\n    def obj(x):\n        H,u,z = unpack(x,M,N)\n        return (\n            wP*np.sum((compute_total_power(H,u,params)-Pref)**2)\n            + wH*np.sum((H[:,:-1]-H_ref[:,None])**2)\n            + wDu*np.sum((u[:,1:]-u[:,:-1])**2)\n        )\n    return obj, dict(wP=wP,wH=wH,wDu=wDu)\n\ndef make_constraints(data):\n    params, H0, Qin_ext, dt, N, M, nsub = (\n        data[\"params\"], data[\"H0\"], data[\"Qin_ext\"], data[\"dt\"], data[\"N\"], data[\"M\"], data[\"nsub\"]\n    )\n    cons = []\n    def init_fun(x):\n        H,u,z = unpack(x,M,N); return H[:,0]-H0\n    cons.append({'type':'eq','fun':init_fun})\n    def dyn_fun(x):\n        H,u,z = unpack(x,M,N)\n        res=[]\n        for i in range(M):\n            for n in range(N):\n                Hend, _ = integrate_interval(H[i,n], u[i,n], z[i,n], dt, nsub, params[i])\n                res.append(H[i,n+1]-Hend)\n        return np.array(res)\n    cons.append({'type':'eq','fun':dyn_fun})\n    def coup_fun(x):\n        H,u,z = unpack(x,M,N)\n        res=[]\n        # First reach is exogenous inflow per interval\n        for n in range(N):\n            res.append(z[0,n]-Qin_ext[n])\n        # Downstream links: Muskingum routing\n        K_list = data.get(\"muskingum\", {}).get(\"K\", [])\n        X_list = data.get(\"muskingum\", {}).get(\"X\", [])\n        for i in range(1,M):\n            # Seed condition for z[i,0]\n            _, I0 = integrate_interval(H[i-1,0], u[i-1,0], z[i-1,0], dt, nsub, params[i-1])\n            res.append(z[i,0] - I0)\n            # Coefficients\n            Ki = K_list[i-1] if i-1 < len(K_list) else 1800.0\n            Xi = X_list[i-1] if i-1 < len(X_list) else 0.2\n            C0, C1, C2 = muskingum_coeffs(Ki, Xi, dt)\n            # Recursion over intervals\n            for n in range(N-1):\n                # upstream interval-average outflows for n and n+1\n                _, I_n   = integrate_interval(H[i-1,n],   u[i-1,n],   z[i-1,n],   dt, nsub, params[i-1])\n                _, I_np1 = integrate_interval(H[i-1,n+1], u[i-1,n+1], z[i-1,n+1], dt, nsub, params[i-1])\n                res.append(z[i,n+1] - (C0*I_np1 + C1*I_n + C2*z[i,n]))\n        return np.array(res)\n    cons.append({'type':'eq','fun':coup_fun})\n    return cons\n\ndef make_bounds(data):\n    Hmin,Hmax = data[\"H_bounds\"]\n    umin,umax = data[\"u_bounds\"]\n    M,N = data[\"M\"], data[\"N\"]\n    nH,nu,nz = shapes(M,N)\n    lb = np.empty(nH+nu+nz); ub = np.empty_like(lb)\n    lb[:nH]=Hmin; ub[:nH]=Hmax\n    lb[nH:nH+nu]=umin; ub[nH:nH+nu]=umax\n    lb[nH+nu:]=0.0; ub[nH+nu:]=2000.0\n    return list(zip(lb,ub))\n\ndef residuals(x, data):\n    params, H0, Qin_ext, dt, N, M, nsub = (\n        data[\"params\"], data[\"H0\"], data[\"Qin_ext\"], data[\"dt\"], data[\"N\"], data[\"M\"], data[\"nsub\"]\n    )\n    H,u,z = unpack(x, M, N)\n    dyn = np.zeros((M,N)); coup = np.zeros((M,N))\n    for i in range(M):\n        for n in range(N):\n            Hend, qavg = integrate_interval(H[i,n], u[i,n], z[i,n], dt, nsub, params[i])\n            dyn[i,n] = H[i,n+1] - Hend\n            if i == 0:\n                coup[i,n] = z[i,n] - Qin_ext[n]\n            else:\n                # Muskingum residual, align on current index using n and n-1\n                Ki = data.get(\"muskingum\", {}).get(\"K\", [1800.0]*(M-1))[i-1]\n                Xi = data.get(\"muskingum\", {}).get(\"X\", [0.2]*(M-1))[i-1]\n                C0, C1, C2 = muskingum_coeffs(Ki, Xi, dt)\n                if n == 0:\n                    coup[i,n] = 0.0\n                else:\n                    _, I_nm1 = integrate_interval(H[i-1,n-1], u[i-1,n-1], z[i-1,n-1], dt, nsub, params[i-1])\n                    _, I_n   = integrate_interval(H[i-1,n],   u[i-1,n],   z[i-1,n],   dt, nsub, params[i-1])\n                    coup[i,n] = z[i,n] - (C0*I_n + C1*I_nm1 + C2*z[i,n-1])\n    return dyn, coup\n\n# ---------- Feasible initial guess with hetero controls ----------\n\ndef feasible_initial_guess(data):\n    \"\"\"Feasible x0 with nontrivial u by setting u at mid + per-reach pattern, then integrating to define H,z.\"\"\"\n    M,N,dt,nsub = data[\"M\"], data[\"N\"], data[\"dt\"], data[\"nsub\"]\n    params = data[\"params\"]\n    umin,umax = data[\"u_bounds\"]\n    Qin_ext = data[\"Qin_ext\"]\n\n    # pattern to break symmetry\n    base = 0.5*(umin+umax)\n    phase = np.linspace(0, np.pi/2, M)\n    tgrid = np.arange(N)\n    u_pattern = np.array([base + 25*np.sin(2*np.pi*(tgrid/N) + ph) for ph in phase])\n    u_pattern = np.clip(u_pattern, umin, umax)\n\n    H = np.zeros((M, N+1)); u = np.zeros((M, N)); z = np.zeros((M, N))\n    H[:,0] = data[\"H0\"]\n    # Set controls from pattern first\n    for i in range(M):\n        u[i,:] = u_pattern[i,:]\n\n    # First reach: exogenous inflow, integrate forward and record outflow averages\n    qavg_up = np.zeros((M, N))\n    for n in range(N):\n        z[0,n] = Qin_ext[n]\n        Hend, qavg = integrate_interval(H[0,n], u[0,n], z[0,n], dt, nsub, params[0])\n        H[0,n+1] = Hend\n        qavg_up[0,n] = qavg\n\n    # Downstream reaches with Muskingum routing\n    K_list = data.get(\"muskingum\", {}).get(\"K\", [1800.0]*(M-1))\n    X_list = data.get(\"muskingum\", {}).get(\"X\", [0.2]*(M-1))\n    for i in range(1,M):\n        Ki = K_list[i-1] if i-1 < len(K_list) else 1800.0\n        Xi = X_list[i-1] if i-1 < len(X_list) else 0.2\n        C0, C1, C2 = muskingum_coeffs(Ki, Xi, dt)\n        I = qavg_up[i-1,:]\n        # seed\n        z[i,0] = I[0]\n        # propagate recursively over time\n        for n in range(N-1):\n            z[i,n+1] = C0*I[n+1] + C1*I[n] + C2*z[i,n]\n        # integrate levels for reach i using routed inflow\n        for n in range(N):\n            Hend, qavg = integrate_interval(H[i,n], u[i,n], z[i,n], dt, nsub, params[i])\n            H[i,n+1] = Hend\n            qavg_up[i,n] = qavg\n    return pack(H,u,z)\n\ndef scale_pref(Pref_raw, x0, data):\n    H,u,z = unpack(x0, data[\"M\"], data[\"N\"])\n    P0 = compute_total_power(H,u,data[\"params\"])\n    s = max(np.mean(P0),1e-6)/max(np.mean(Pref_raw),1e-6)\n    return Pref_raw*s, P0\n\ndef run_demo(show: bool = True, save_path: str | None = 'hydro.png', verbose: bool = False):\n    \"\"\"Build, solve, and render the hydro demo.\n\n    Parameters\n    ----------\n    show : bool\n        If True, displays the matplotlib figure via plt.show().\n    save_path : str | None\n        If provided, saves the figure to this path.\n    verbose : bool\n        If True, prints diagnostic information.\n\n    Returns\n    -------\n    matplotlib.figure.Figure | None\n        Returns the Figure when show is False; otherwise returns None.\n    \"\"\"\n    # ---------- Solve ----------\n    data = build_demo(M=3, N=16, dt=900.0, hetero=True)\n    x0 = feasible_initial_guess(data)\n    Pref, P0 = scale_pref(data[\"Pref_raw\"], x0, data)\n\n    objective, weights = make_objective(data, Pref, wP=8.0, wH=0.02, wDu=5e-4)\n    # Suppress noisy SciPy warning about delta_grad during quasi-Newton updates\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\",\n            message=r\"delta_grad == 0.0\",\n            category=UserWarning,\n            module=r\"scipy\\.optimize\\.\\_differentiable_functions\",\n        )\n        res = minimize(\n            fun=objective,\n            x0=x0,\n            method='trust-constr',\n            bounds=make_bounds(data),\n            constraints=make_constraints(data),\n            options=dict(maxiter=1000, disp=verbose),\n        )\n\n    H,u,z = unpack(res.x, data[\"M\"], data[\"N\"])\n    P = compute_total_power(H,u,data[\"params\"])\n    dyn_res, coup_res = residuals(res.x, data)\n\n    # ---------- Diagnostics ----------\n    if verbose:\n        terms = decompose_objective(res.x, data, Pref, **weights)\n        print(\"\\n=== Objective decomposition ===\")\n        print({k: float(v) if not isinstance(v, dict) else {kk: float(vv) for kk,vv in v.items()} for k,v in terms.items()})\n\n        print(\"\\n=== Constraint residuals (max |.|) ===\")\n        print(\"dyn:\", float(np.max(np.abs(dyn_res)))), print(\"coup:\", float(np.max(np.abs(coup_res))))\n\n        # Muskingum coefficient sanity and residuals\n        if data.get(\"M\", 1) > 1:\n            K_list = data.get(\"muskingum\", {}).get(\"K\", [])\n            X_list = data.get(\"muskingum\", {}).get(\"X\", [])\n            coef_checks = []\n            mean_abs_res = []\n            for i in range(1, data[\"M\"]):\n                Ki = K_list[i-1] if i-1 < len(K_list) else 1800.0\n                Xi = X_list[i-1] if i-1 < len(X_list) else 0.2\n                C0, C1, C2 = muskingum_coeffs(Ki, Xi, data[\"dt\"])\n                coef_checks.append(dict(link=i, sum=float(C0+C1+C2), min_coef=float(min(C0,C1,C2))))\n                # compute mean abs residual for this link\n                res_vals = []\n                for n in range(data[\"N\"]-1):\n                    _, I_n   = integrate_interval(H[i-1,n],   u[i-1,n],   z[i-1,n],   data[\"dt\"], data[\"nsub\"], data[\"params\"][i-1])\n                    _, I_np1 = integrate_interval(H[i-1,n+1], u[i-1,n+1], z[i-1,n+1], data[\"dt\"], data[\"nsub\"], data[\"params\"][i-1])\n                    res_vals.append(float(abs(z[i,n+1] - (C0*I_np1 + C1*I_n + C2*z[i,n]))))\n                mean_abs_res.append(dict(link=i, mean_abs=float(np.mean(res_vals))))\n            print(\"\\n=== Muskingum coeff checks (sum, min_coef) ===\")\n            print(coef_checks)\n            print(\"=== Muskingum mean |residual| per link ===\")\n            print(mean_abs_res)\n\n    # Per-interval diagnostic table for each reach (kept for debugging but unused here)\n    def interval_table(i):\n        rp = data[\"params\"][i]\n        rows = []\n        for n in range(data[\"N\"]):\n            qb = q_bypass(H[i,n], rp)\n            net = z[i,n] - (u[i,n] + qb)\n            dH = data[\"dt\"]*net/rp.A_surf\n            rows.append(dict(interval=n, Hn=H[i,n], Hn1=H[i,n+1], u=u[i,n], z=z[i,n], qb=qb, net_flow=net, dH_pred=dH))\n        return pd.DataFrame(rows)\n\n    # summary and tables available to callers if needed\n    tables = [interval_table(i) for i in range(data[\"M\"])]\n    summary = pd.DataFrame([\n        dict(reach=i+1,\n             H_mean=float(np.mean(H[i])), H_std=float(np.std(H[i])),\n             u_mean=float(np.mean(u[i])), u_std=float(np.std(u[i])),\n             z_mean=float(np.mean(z[i])), z_std=float(np.std(z[i])))\n        for i in range(data[\"M\"])\n    ])\n\n    # ---------- Plots ----------\n    M,N = data[\"M\"], data[\"N\"]\n    t_nodes = np.arange(N+1)\n    t = np.arange(N)\n\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    fig.suptitle('Hydroelectric System Optimization Results', fontsize=16)\n\n    ax1 = axes[0, 0]\n    for i in range(M):\n        ax1.plot(t_nodes, H[i], marker='o', label=f'Reach {i+1}')\n    ax1.set_xlabel(\"Node n\"); ax1.set_ylabel(\"H [m]\"); ax1.set_title(\"Water Levels\")\n    ax1.grid(True); ax1.legend()\n\n    ax2 = axes[0, 1]\n    for i in range(M):\n        ax2.step(t, u[i], where='post', label=f'Reach {i+1}')\n    ax2.set_xlabel(\"Interval n\"); ax2.set_ylabel(\"u [m³/s]\"); ax2.set_title(\"Turbine Discharge\")\n    ax2.grid(True); ax2.legend()\n\n    ax3 = axes[1, 0]\n    for i in range(M):\n        ax3.step(t, z[i], where='post', label=f'Reach {i+1}')\n    ax3.set_xlabel(\"Interval n\"); ax3.set_ylabel(\"z [m³/s]\"); ax3.set_title(\"Inflow (Coupling)\")\n    ax3.grid(True); ax3.legend()\n\n    ax4 = axes[1, 1]\n    ax4.plot(t, P0, marker='s', label=\"Power @ x0\")\n    ax4.plot(t, P, marker='o', label=\"Power @ optimum\")\n    ax4.plot(t, Pref, marker='x', label=\"Scaled Pref\")\n    ax4.set_xlabel(\"Interval n\"); ax4.set_ylabel(\"Power units\"); ax4.set_title(\"Power Tracking\")\n    ax4.legend(); ax4.grid(True)\n\n    plt.tight_layout()\n    if save_path:\n        fig.savefig(save_path, bbox_inches='tight')\n    if show:\n        plt.show()\n        return None\n    return fig\n\n\n# Run the demo directly when loaded in a notebook cell\nrun_demo(show=True, save_path=None, verbose=False)\n\nThe figure shows the result of a multiple-shooting optimization applied to a three-reach hydroelectric cascade. The time horizon is discretized into 16 intervals, and SciPy’s trust-constr solver is used to find a feasible control sequence that satisfies mass balance, turbine and spillway limits, and Muskingum-style routing dynamics. Each reach integrates its own local ODE, with continuity constraints linking the flows between reaches.\n\nThe top-left panel shows the water levels in each reservoir. We observe that upstream reservoirs tend to increase their levels ahead of discharge events, building potential energy before releasing water downstream. The top-right panel shows turbine discharges for each reach. These vary smoothly and are temporally coordinated across the system. The bottom-right panel compares the total generation to a synthetic demand profile, which is generated by a sum of time-shifted sigmoids and normalized to be feasible given turbine capacities. The optimized schedule (orange) tracks this demand closely, while the initial guess (blue) lags behind. The bottom-left panel plots the routed inflows between reaches, which display the expected lag and smoothing effects from Muskingum routing. The interplay between these plots shows how the system anticipates, stores, and routes water to meet time-varying generation targets within physical and operational limits.","type":"content","url":"/cocp#hydro-cascade-scheduling-with-physical-routing-and-multiple-shooting","position":45},{"hierarchy":{"lvl1":"Dynamic Programming"},"type":"lvl1","url":"/dp","position":0},{"hierarchy":{"lvl1":"Dynamic Programming"},"content":"Unlike the methods we’ve discussed so far, dynamic programming takes a step back and considers an entire family of related problems rather than a single optimization problem. This approach, while seemingly more complex at first glance, can often lead to efficient solutions.\n\nDynamic programming leverage the solution structure underlying many control problems that allows for a decomposition it into smaller, more manageable subproblems. Each subproblem is itself an optimization problem, embedded within the larger whole. This recursive structure is the foundation upon which dynamic programming constructs its solutions.\n\nTo ground our discussion, let us return to the domain of discrete-time optimal control problems (DOCPs). These problems frequently arise from the discretization of continuous-time optimal control problems. While the focus here will be on deterministic problems, these concepts extend naturally to stochastic problems by taking the expectation over the random quantities.\n\nConsider a typical DOCP of Bolza type:\\begin{align*}\n\\text{minimize} \\quad & J \\triangleq c_\\mathrm{T}(\\mathbf{x}_T) + \\sum_{t=1}^{T-1} c_t(\\mathbf{x}_t, \\mathbf{u}_t) \\\\\n\\text{subject to} \\quad \n& \\mathbf{x}_{t+1} = \\mathbf{f}_t(\\mathbf{x}_t, \\mathbf{u}_t), \\quad t = 1, \\ldots, T-1, \\\\\n& \\mathbf{u}_{lb} \\leq \\mathbf{u}_t \\leq \\mathbf{u}_{ub}, \\quad t = 1, \\ldots, T, \\\\\n& \\mathbf{x}_{lb} \\leq \\mathbf{x}_t \\leq \\mathbf{x}_{ub}, \\quad t = 1, \\ldots, T, \\\\\n\\text{given} \\quad & \\mathbf{x}_1\n\\end{align*}\n\nRather than considering only the total cost from the initial time to the final time, dynamic programming introduces the concept of cost from an arbitrary point in time to the end. This leads to the definition of the “cost-to-go” or “value function” J_k(\\mathbf{x}_k):J_k(\\mathbf{x}_k) \\triangleq c_\\mathrm{T}(\\mathbf{x}_T) + \\sum_{t=k}^{T-1} c_t(\\mathbf{x}_t, \\mathbf{u}_t)\n\nThis function represents the total cost incurred from stage k onwards to the end of the time horizon, given that the system is initialized in state \\mathbf{x}_k at stage k. Suppose the problem has been solved from stage k+1 to the end, yielding the optimal cost-to-go J_{k+1}^\\star(\\mathbf{x}_{k+1}) for any state \\mathbf{x}_{k+1} at stage k+1. The question then becomes: how does this information inform the decision at stage k?\n\nGiven knowledge of the optimal behavior from k+1 onwards, the task reduces to determining the optimal action \\mathbf{u}_k at stage k. This control should minimize the sum of the immediate cost c_k(\\mathbf{x}_k, \\mathbf{u}_k) and the optimal future cost J_{k+1}^\\star(\\mathbf{x}_{k+1}), where \\mathbf{x}_{k+1} is the resulting state after applying action \\mathbf{u}_k. Mathematically, this is expressed as:J_k^\\star(\\mathbf{x}_k) = \\min_{\\mathbf{u}_k} \\left[ c_k(\\mathbf{x}_k, \\mathbf{u}_k) + J_{k+1}^\\star(\\mathbf{f}_k(\\mathbf{x}_k, \\mathbf{u}_k)) \\right]\n\nThis equation is known as Bellman’s equation, named after Richard Bellman, who formulated the principle of optimality:\n\nAn optimal policy has the property that whatever the previous state and decision, the remaining decisions must constitute an optimal policy with regard to the state resulting from the previous decision.\n\nIn other words, any sub-path of an optimal path, from any intermediate point to the end, must itself be optimal. This principle is the basis for the backward induction procedure which computes the optimal value function and provides closed-loop control capabilities without having to use an explicit NLP solver.\n\nDynamic programming can handle nonlinear systems and non-quadratic cost functions naturally. It provides a global optimal solution, when one exists, and can incorporate state and control constraints with relative ease. However, as the dimension of the state space increases, this approach suffers from what Bellman termed the “curse of dimensionality.” The computational complexity and memory requirements grow exponentially with the state dimension, rendering direct application of dynamic programming intractable for high-dimensional problems.\n\nFortunately, learning-based methods offer efficient tools to combat the curse of dimensionality on two fronts: by using function approximation (e.g., neural networks) to avoid explicit discretization, and by leveraging randomization through Monte Carlo methods inherent in the learning paradigm. Most of this course is dedicated to those ideas.","type":"content","url":"/dp","position":1},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Backward Recursion"},"type":"lvl3","url":"/dp#backward-recursion","position":2},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Backward Recursion"},"content":"The principle of optimality provides a methodology for solving optimal control problems. Beginning at the final time horizon and working backwards, at each stage the local optimization problem given by Bellman’s equation is solved. This process, termed backward recursion or backward induction, constructs the optimal value function stage by stage.\n\nBackward Recursion for Dynamic Programming\n\nInput: Terminal cost function c_\\mathrm{T}(\\cdot), stage cost functions c_t(\\cdot, \\cdot), system dynamics f_t(\\cdot, \\cdot), time horizon \\mathrm{T}\n\nOutput: Optimal value functions J_t^\\star(\\cdot) and optimal control policies \\mu_t^\\star(\\cdot) for t = 1, \\ldots, T\n\nInitialize J_T^\\star(\\mathbf{x}) = c_\\mathrm{T}(\\mathbf{x}) for all \\mathbf{x} in the state space\n\nFor t = T-1, T-2, \\ldots, 1:\n\nFor each state \\mathbf{x} in the state space:\n\nCompute J_t^\\star(\\mathbf{x}) = \\min_{\\mathbf{u}} \\left[ c_t(\\mathbf{x}, \\mathbf{u}) + J_{t+1}^\\star(f_t(\\mathbf{x}, \\mathbf{u})) \\right]\n\nCompute \\mu_t^\\star(\\mathbf{x}) = \\arg\\min_{\\mathbf{u}} \\left[ c_t(\\mathbf{x}, \\mathbf{u}) + J_{t+1}^\\star(f_t(\\mathbf{x}, \\mathbf{u})) \\right]\n\nEnd For\n\nEnd For\n\nReturn J_t^\\star(\\cdot), \\mu_t^\\star(\\cdot) for t = 1, \\ldots, T\n\nUpon completion of this backward pass, we now have access to the optimal control to take at any stage and in any state. Furthermore, we can simulate optimal trajectories from any initial state and applying the optimal policy at each stage to generate the optimal trajectory.\n\nBackward induction solves deterministic Bolza DOCP\n\nSetting. Let \\mathbf{x}_{t+1}=\\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t) for t=1,\\dots,T-1, with admissible action sets \\mathcal{U}_t(\\mathbf{x})\\neq\\varnothing. Let stage costs c_t(\\mathbf{x},\\mathbf{u}) and terminal cost c_\\mathrm{T}(\\mathbf{x}) be real-valued and bounded below. Assume for every (t,\\mathbf{x}) the one-step problem\\min_{\\mathbf{u}\\in\\mathcal{U}_t(\\mathbf{x})}\\big\\{c_t(\\mathbf{x},\\mathbf{u})+J_{t+1}^\\star(\\mathbf{f}_t(\\mathbf{x},\\mathbf{u}))\\big\\}\n\nadmits a minimizer (e.g., compact \\mathcal{U}_t(\\mathbf{x}) and continuity suffice).\n\nDefine J_T^\\star(\\mathbf{x}) \\equiv c_\\mathrm{T}(\\mathbf{x}) and for t=T-1,\\dots,1J_t^\\star(\\mathbf{x}) \\;\\triangleq\\; \\min_{\\mathbf{u}\\in\\mathcal{U}_t(\\mathbf{x})}\n\\Big[c_t(\\mathbf{x},\\mathbf{u})+J_{t+1}^\\star\\big(\\mathbf{f}_t(\\mathbf{x},\\mathbf{u})\\big)\\Big],\n\nand select any minimizer \\boldsymbol{\\mu}_t^\\star(\\mathbf{x})\\in\\arg\\min(\\cdot).\n\nClaim. For every initial state \\mathbf{x}_1, the control sequence\n\\boldsymbol{\\mu}_1^\\star(\\mathbf{x}_1),\\dots,\\boldsymbol{\\mu}_{T-1}^\\star(\\mathbf{x}_{T-1})\ngenerated by these selectors is optimal for the Bolza problem, and\nJ_1^\\star(\\mathbf{x}_1) equals the optimal cost. Moreover, J_t^\\star(\\cdot) is the optimal cost-to-go from stage t for every state, i.e., backward induction recovers the entire value function.\n\nWe give a direct proof by backward induction. The general idea is that any feasible sequence can be improved by replacing its tail with an optimal continuation, so optimal solutions can be built stage by stage. This is sometimes called a “cut-and-paste” argument.\n\nStep 1 (verification of the recursion at a fixed stage).Fix t\\in\\{1,\\dots,T-1\\} and \\mathbf{x}\\in\\mathbb{X}. Consider any admissible control sequence \\mathbf{u}_t,\\dots,\\mathbf{u}_{T-1} starting from \\mathbf{x}_t=\\mathbf{x} and define the induced states \\mathbf{x}_{k+1}=\\mathbf{f}_k(\\mathbf{x}_k,\\mathbf{u}_k). Its total cost from t isc_t(\\mathbf{x}_t,\\mathbf{u}_t)+\\sum_{k=t+1}^{T-1}c_k(\\mathbf{x}_k,\\mathbf{u}_k)+c_\\mathrm{T}(\\mathbf{x}_T).\n\nBy definition of J_{t+1}^\\star, the tail cost satisfies\\sum_{k=t+1}^{T-1}c_k(\\mathbf{x}_k,\\mathbf{u}_k)+c_\\mathrm{T}(\\mathbf{x}_T)\n\\;\\ge\\; J_{t+1}^\\star(\\mathbf{x}_{t+1})\n\\;=\\; J_{t+1}^\\star\\big(\\mathbf{f}_t(\\mathbf{x},\\mathbf{u}_t)\\big).\n\nHence the total cost is bounded below byc_t(\\mathbf{x},\\mathbf{u}_t)+J_{t+1}^\\star\\big(\\mathbf{f}_t(\\mathbf{x},\\mathbf{u}_t)\\big).\n\nTaking the minimum over \\mathbf{u}_t\\in\\mathcal{U}_t(\\mathbf{x}) yields\\text{(any admissible cost from $t$)}\\;\\ge\\;J_t^\\star(\\mathbf{x}).\n\\tag{$\\ast$}\n\nStep 2 (existence of an optimal prefix at stage t).By assumption, there exists \\boldsymbol{\\mu}_t^\\star(\\mathbf{x}) attaining the minimum in the definition of J_t^\\star(\\mathbf{x}). If we now paste to \\boldsymbol{\\mu}_t^\\star(\\mathbf{x}) an optimal tail policy from t+1 (whose existence we will establish inductively), the resulting sequence attains cost exactlyc_t\\big(\\mathbf{x},\\boldsymbol{\\mu}_t^\\star(\\mathbf{x})\\big)\n+J_{t+1}^\\star\\!\\Big(\\mathbf{f}_t\\big(\\mathbf{x},\\boldsymbol{\\mu}_t^\\star(\\mathbf{x})\\big)\\Big)\n=J_t^\\star(\\mathbf{x}),\n\nwhich matches the lower bound (\\ast); hence it is optimal from t.\n\nStep 3 (backward induction over time).Base case t=T. The statement holds because J_T^\\star(\\mathbf{x})=c_\\mathrm{T}(\\mathbf{x}) and there is no control to choose.\n\nInductive step. Assume the tail statement holds for t+1: from any state \\mathbf{x}_{t+1} there exists an optimal control sequence realizing J_{t+1}^\\star(\\mathbf{x}_{t+1}). Then by Steps 1–2, selecting \\boldsymbol{\\mu}_t^\\star(\\mathbf{x}_t) at stage t and concatenating the optimal tail from t+1 yields an optimal sequence from t with value J_t^\\star(\\mathbf{x}_t).\n\nBy backward induction, the claim holds for all t, in particular for t=1 and any initial \\mathbf{x}_1. Therefore the backward recursion both certifies optimality (verification) and constructs an optimal policy (synthesis), while recovering the full family \\{J_t^\\star\\}_{t=1}^T.\n\nNo “big NLP” required\n\nThe Bolza DOCP over the whole horizon couples all controls through the dynamics and is typically posed as a single large nonlinear program. The proof shows you can solve T-1 sequences of one-step problems instead: at each (t,\\mathbf{x}) minimize\\mathbf{u}\\mapsto c_t(\\mathbf{x},\\mathbf{u}) + J_{t+1}^\\star(\\mathbf{f}_t(\\mathbf{x},\\mathbf{u})).\n\nIn finite state–action spaces this becomes pure table lookup and argmin. In continuous spaces you still solve local one-step minimizations, but you avoid a monolithic horizon-coupled NLP.\n\nGraph interpretation (optional intuition)\n\nUnroll time to form a DAG whose nodes are (t,\\mathbf{x}) and whose edges correspond to feasible controls with edge weight c_t(\\mathbf{x},\\mathbf{u}). The terminal node cost is c_\\mathrm{T}(\\cdot). The Bolza problem is a shortest-path problem on this DAG. The equationJ_t^\\star(\\mathbf{x})=\\min_{\\mathbf{u}}\\{c_t(\\mathbf{x},\\mathbf{u})+J_{t+1}^\\star(\\mathbf{f}_t(\\mathbf{x},\\mathbf{u}))\\}\n\nis exactly the dynamic programming recursion for shortest paths on acyclic graphs, hence backward induction is optimal. ```{prf:remark} If minimizers may not exist\nReplace each \"min\" by \"inf\" in the definitions and state that for every $\\varepsilon>0$ there exist $\\varepsilon$-optimal selectors $\\boldsymbol{\\mu}_t^\\varepsilon(\\cdot)$ achieving cost within $\\varepsilon$ of $J_t^\\star(\\cdot)$. The same cut-and-paste and induction go through.\n``` ","type":"content","url":"/dp#backward-recursion","position":3},{"hierarchy":{"lvl1":"Dynamic Programming","lvl4":"Example: Optimal Harvest in Resource Management","lvl3":"Backward Recursion"},"type":"lvl4","url":"/dp#example-optimal-harvest-in-resource-management","position":4},{"hierarchy":{"lvl1":"Dynamic Programming","lvl4":"Example: Optimal Harvest in Resource Management","lvl3":"Backward Recursion"},"content":"Dynamic programming is often used in resource management and conservation biology to devise policies to be implemented by decision makers and stakeholders : for eg. in fishereries, or timber harvesting. Per \n\nConroy & Peterson (2013), we consider a population of a particular species, whose abundance we denote by x_t, where t represents discrete time steps. Our objective is to maximize the cumulative harvest over a finite time horizon, while also considering the long-term sustainability of the population. This optimization problem can be formulated as:\\text{maximize} \\quad \\sum_{t=t_0}^{t_f} F(x_t \\cdot h_t) + F_\\mathrm{T}(x_{t_f})\n\nHere, F(\\cdot) represents the immediate reward function associated with harvesting, h_t is the harvest rate at time t, and F_\\mathrm{T}(\\cdot) denotes a terminal value function that could potentially assign value to the final population state. In this particular problem, we assign no terminal value to the final population state, setting F_\\mathrm{T}(x_{t_f}) = 0 and allowing us to focus solely on the cumulative harvest over the time horizon.\n\nIn our model population model, the abundance of a specicy x ranges from 1 to 100 individuals. The decision variable is the harvest rate h, which can take values from the set D = \\{0, 0.1, 0.2, 0.3, 0.4, 0.5\\}. The population dynamics are governed by a modified logistic growth model:x_{t+1} = x_t + 0.3x_t(1 - x_t/125) - h_tx_t\n\nwhere the 0.3 represents the growth rate and 125 is the carrying capacity (the maximum population size given the available resources). The logistic growth model returns continuous values; however our DP formulation uses a discrete state space. Therefore, we also round the the outcomes to the nearest integer.\n\nApplying the principle of optimality, we can express the optimal value function J^\\star(x_t,t) recursively:J^\\star(x_t, t) = \\max_{h_t \\in D} (F(x, h, t) + J^*(x_{t+1}, t+1))\n\nwith the boundary condition J^*(x_{t_f}) = 0.\n\nIt’s worth noting that while this example uses a relatively simple model, the same principles can be applied to more complex scenarios involving stochasticity, multiple species interactions, or spatial heterogeneity.\n\nimport numpy as np\n\n# Parameters\nr_max = 0.3\nK = 125\nT = 20  # Number of time steps\nN_max = 100  # Maximum population size to consider\nh_max = 0.5  # Maximum harvest rate\nh_step = 0.1  # Step size for harvest rate\n\n# Create state and decision spaces\nN_space = np.arange(1, N_max + 1)\nh_space = np.arange(0, h_max + h_step, h_step)\n\n# Initialize value function and policy\nV = np.zeros((T + 1, len(N_space)))\npolicy = np.zeros((T, len(N_space)))\n\n# Terminal value function (F_T)\ndef terminal_value(N):\n    return 0\n\n# State return function (F)\ndef state_return(N, h):\n    return N * h\n\n# State dynamics function\ndef state_dynamics(N, h):\n    return N + r_max * N * (1 - N / K) - N * h\n\n# Backward iteration\nfor t in range(T - 1, -1, -1):\n    for i, N in enumerate(N_space):\n        max_value = float('-inf')\n        best_h = 0\n\n        for h in h_space:\n            if h > 1:  # Ensure harvest rate doesn't exceed 100%\n                continue\n\n            next_N = state_dynamics(N, h)\n            if next_N < 1:  # Ensure population doesn't go extinct\n                continue\n\n            next_N_index = np.searchsorted(N_space, next_N)\n            if next_N_index == len(N_space):\n                next_N_index -= 1\n\n            value = state_return(N, h) + V[t + 1, next_N_index]\n\n            if value > max_value:\n                max_value = value\n                best_h = h\n\n        V[t, i] = max_value\n        policy[t, i] = best_h\n\n# Function to simulate the optimal policy with conversion to Python floats\ndef simulate_optimal_policy(initial_N, T):\n    trajectory = [float(initial_N)]  # Ensure first value is a Python float\n    harvests = []\n\n    for t in range(T):\n        N = trajectory[-1]\n        N_index = np.searchsorted(N_space, N)\n        if N_index == len(N_space):\n            N_index -= 1\n\n        h = policy[t, N_index]\n        harvests.append(float(N * h))  # Ensure harvest is a Python float\n\n        next_N = state_dynamics(N, h)\n        trajectory.append(float(next_N))  # Ensure next population value is a Python float\n\n    return trajectory, harvests\n\n# Example usage\ninitial_N = 50\ntrajectory, harvests = simulate_optimal_policy(initial_N, T)\n\nprint(\"Optimal policy:\")\nprint(policy)\nprint(\"\\nPopulation trajectory:\", trajectory)\nprint(\"Harvests:\", harvests)\nprint(\"Total harvest:\", sum(harvests))\n\n","type":"content","url":"/dp#example-optimal-harvest-in-resource-management","position":5},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Handling Continuous Spaces with Interpolation"},"type":"lvl3","url":"/dp#handling-continuous-spaces-with-interpolation","position":6},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Handling Continuous Spaces with Interpolation"},"content":"In many real-world problems, such as our resource management example, the state space is inherently continuous. Dynamic programming, however, is usually defined on discrete state spaces. To reconcile this, we approximate the value function on a finite grid of points and use interpolation to estimate its value elsewhere.\n\nIn our earlier example, we acted as if population sizes could only be whole numbers: 1 fish, 2 fish, 3 fish. But real measurements don’t fit neatly. What do you do with a survey that reports 42.7 fish? Our reflex in the code example was to round to the nearest integer, effectively saying “let’s just call it 43.” This corresponds to nearest-neighbor interpolation, also known as discretization. It’s the zeroth-order case: you assume the value between grid points is constant and equal to the closest one. In practice, this amounts to overlaying a grid on the continuous landscape and forcing yourself to stand at the intersections. In our demo code, this step was carried out with \n\nnumpy.searchsorted.\n\nWhile easy to implement, nearest-neighbor interpolation can introduce artifacts:\n\nDecisions may change abruptly, even if the state only shifts slightly.\n\nPrecision is lost, especially in regimes where small variations matter.\n\nThe curse of dimensionality forces an impractically fine grid if many state variables are added.\n\nTo address these issues, we can use higher-order interpolation. Instead of taking the nearest neighbor, we estimate the value at off-grid points by leveraging multiple nearby values.","type":"content","url":"/dp#handling-continuous-spaces-with-interpolation","position":7},{"hierarchy":{"lvl1":"Dynamic Programming","lvl4":"Backward Recursion with Interpolation","lvl3":"Handling Continuous Spaces with Interpolation"},"type":"lvl4","url":"/dp#backward-recursion-with-interpolation","position":8},{"hierarchy":{"lvl1":"Dynamic Programming","lvl4":"Backward Recursion with Interpolation","lvl3":"Handling Continuous Spaces with Interpolation"},"content":"Suppose we have computed J_{k+1}^\\star(\\mathbf{x}) only at grid points \\mathbf{x} \\in \\mathcal{X}_\\text{grid}.\nTo evaluate Bellman’s equation at an arbitrary \\mathbf{x}_{k+1}, we interpolate.\nFormally, let I_{k+1}(\\mathbf{x}) be the interpolation operator that extends the value function from \\mathcal{X}_\\text{grid} to the continuous space. Then:J_k^\\star(\\mathbf{x}_k) \n= \\min_{\\mathbf{u}_k} \n\\Big[ c_k(\\mathbf{x}_k, \\mathbf{u}_k) \n+ I_{k+1}\\big(\\mathbf{f}_k(\\mathbf{x}_k, \\mathbf{u}_k)\\big) \\Big].\n\nFor instance, in one dimension, linear interpolation gives:I_{k+1}(x) = J_{k+1}^\\star(x_l) + \\frac{x - x_l}{x_u - x_l} \\big(J_{k+1}^\\star(x_u) - J_{k+1}^\\star(x_l)\\big),\n\nwhere x_l and x_u are the nearest grid points bracketing x. Linear interpolation is often sufficient, but higher-order methods (cubic splines, radial basis functions) can yield smoother and more accurate estimates. The choice of interpolation scheme and grid layout both affect accuracy and efficiency. A finer grid improves resolution but increases computational cost, motivating strategies like adaptive grid refinement or replacing interpolation altogether with parametric function approximation which we are going to see later in this book.\n\nIn higher-dimensional spaces, naive interpolation becomes prohibitively expensive due to the curse of dimensionality. Several approaches such as tensorized multilinear interpolation, radial basis functions, and machine learning models address this challenge by extending a common principle: they approximate the value function at unobserved points using information from a finite set of evaluations. However, as dimensionality continues to grow, even tensor methods face scalability limits, which is why flexible parametric models like neural networks have become essential tools for high-dimensional function approximation.\n\nBackward Recursion with Interpolation\n\nInput:\n\nTerminal cost c_\\mathrm{T}(\\cdot)\n\nStage costs c_t(\\cdot,\\cdot)\n\nDynamics f_t(\\cdot,\\cdot)\n\nTime horizon T\n\nState grid \\mathcal{X}_\\text{grid}\n\nAction set \\mathcal{U}\n\nInterpolation method \\mathcal{I}(\\cdot) (e.g., linear, cubic spline, RBF, neural network)\n\nOutput: Value functions J_t^\\star(\\cdot) and policies \\mu_t^\\star(\\cdot) for all t\n\nInitialize terminal values:\n\nCompute J_T^\\star(\\mathbf{x}) = c_\\mathrm{T}(\\mathbf{x}) for all \\mathbf{x} \\in \\mathcal{X}_\\text{grid}\n\nFit interpolator: I_T \\leftarrow \\mathcal{I}(\\{\\mathbf{x}, J_T^\\star(\\mathbf{x})\\}_{\\mathbf{x} \\in \\mathcal{X}_\\text{grid}})\n\nBackward recursion:For t = T-1, T-2, \\dots, 0:\n\na. Bellman update at grid points:For each \\mathbf{x} \\in \\mathcal{X}_\\text{grid}:\n\nFor each \\mathbf{u} \\in \\mathcal{U}:\n\nCompute next state: \\mathbf{x}_\\text{next} = f_t(\\mathbf{x}, \\mathbf{u})\n\nInterpolate future cost: \\hat{J}_{t+1}(\\mathbf{x}_\\text{next}) = I_{t+1}(\\mathbf{x}_\\text{next})\n\nCompute total cost: J_t(\\mathbf{x}, \\mathbf{u}) = c_t(\\mathbf{x}, \\mathbf{u}) + \\hat{J}_{t+1}(\\mathbf{x}_\\text{next})\n\nMinimize over actions: J_t^\\star(\\mathbf{x}) = \\min_{\\mathbf{u} \\in \\mathcal{U}} J_t(\\mathbf{x}, \\mathbf{u})\n\nStore optimal action: \\mu_t^\\star(\\mathbf{x}) = \\arg\\min_{\\mathbf{u} \\in \\mathcal{U}} J_t(\\mathbf{x}, \\mathbf{u})\n\nb. Fit interpolator for current stage:I_t \\leftarrow \\mathcal{I}(\\{\\mathbf{x}, J_t^\\star(\\mathbf{x})\\}_{\\mathbf{x} \\in \\mathcal{X}_\\text{grid}})\n\nReturn: Interpolated value functions \\{I_t\\}_{t=0}^T and policies \\{\\mu_t^\\star\\}_{t=0}^{T-1}","type":"content","url":"/dp#backward-recursion-with-interpolation","position":9},{"hierarchy":{"lvl1":"Dynamic Programming","lvl5":"Example: Optimal Harvest with Linear Interpolation","lvl4":"Backward Recursion with Interpolation","lvl3":"Handling Continuous Spaces with Interpolation"},"type":"lvl5","url":"/dp#example-optimal-harvest-with-linear-interpolation","position":10},{"hierarchy":{"lvl1":"Dynamic Programming","lvl5":"Example: Optimal Harvest with Linear Interpolation","lvl4":"Backward Recursion with Interpolation","lvl3":"Handling Continuous Spaces with Interpolation"},"content":"Here is a demonstration of the backward recursion procedure using linear interpolation.\n\nimport numpy as np\n\n# Parameters\nr_max = 0.3\nK = 125\nT = 20  # Number of time steps\nN_max = 100  # Maximum population size to consider\nh_max = 0.5  # Maximum harvest rate\nh_step = 0.1  # Step size for harvest rate\n\n# Create state and decision spaces\nN_space = np.arange(1, N_max + 1)\nh_space = np.arange(0, h_max + h_step, h_step)\n\n# Initialize value function and policy\nV = np.zeros((T + 1, len(N_space)))\npolicy = np.zeros((T, len(N_space)))\n\n# Terminal value function (F_T)\ndef terminal_value(N):\n    return 0\n\n# State return function (F)\ndef state_return(N, h):\n    return N * h\n\n# State dynamics function\ndef state_dynamics(N, h):\n    return N + r_max * N * (1 - N / K) - N * h\n\n# Function to linearly interpolate between grid points in N_space\ndef interpolate_value_function(V, N_space, next_N, t):\n    if next_N <= N_space[0]:\n        return V[t, 0]  # Below or at minimum population, return minimum value\n    if next_N >= N_space[-1]:\n        return V[t, -1]  # Above or at maximum population, return maximum value\n    \n    # Find indices to interpolate between\n    lower_idx = np.searchsorted(N_space, next_N) - 1\n    upper_idx = lower_idx + 1\n    \n    # Linear interpolation\n    N_lower = N_space[lower_idx]\n    N_upper = N_space[upper_idx]\n    weight = (next_N - N_lower) / (N_upper - N_lower)\n    return (1 - weight) * V[t, lower_idx] + weight * V[t, upper_idx]\n\n# Backward iteration with interpolation\nfor t in range(T - 1, -1, -1):\n    for i, N in enumerate(N_space):\n        max_value = float('-inf')\n        best_h = 0\n        \n        for h in h_space:\n            if h > 1:  # Ensure harvest rate doesn't exceed 100%\n                continue\n            \n            next_N = state_dynamics(N, h)\n            if next_N < 1:  # Ensure population doesn't go extinct\n                continue\n            \n            # Interpolate value for next_N\n            value = state_return(N, h) + interpolate_value_function(V, N_space, next_N, t + 1)\n            \n            if value > max_value:\n                max_value = value\n                best_h = h\n        \n        V[t, i] = max_value\n        policy[t, i] = best_h\n\n# Function to simulate the optimal policy using interpolation\ndef simulate_optimal_policy(initial_N, T):\n    trajectory = [initial_N]\n    harvests = []\n\n    for t in range(T):\n        N = trajectory[-1]\n        \n        # Interpolate optimal harvest rate\n        if N <= N_space[0]:\n            h = policy[t, 0]\n        elif N >= N_space[-1]:\n            h = policy[t, -1]\n        else:\n            lower_idx = np.searchsorted(N_space, N) - 1\n            upper_idx = lower_idx + 1\n            weight = (N - N_space[lower_idx]) / (N_space[upper_idx] - N_space[lower_idx])\n            h = (1 - weight) * policy[t, lower_idx] + weight * policy[t, upper_idx]\n        \n        harvests.append(float(N * h))  # Ensure harvest is a Python float\n        next_N = state_dynamics(N, h)\n        trajectory.append(float(next_N))  # Ensure next population value is a Python float\n\n    return trajectory, harvests\n\n# Example usage\ninitial_N = 50\ntrajectory, harvests = simulate_optimal_policy(initial_N, T)\n\nprint(\"Optimal policy:\")\nprint(policy)\nprint(\"\\nPopulation trajectory:\", trajectory)\nprint(\"Harvests:\", harvests)\nprint(\"Total harvest:\", sum(harvests))\n\nDue to pedagogical considerations, this example is using our own implementation of the linear interpolation procedure. However, a more general and practical approach would be to use a built-in interpolation procedure in Numpy. Because our state space has a single dimension, we can simply use \n\nscipy​.interpolate​.interp1d which offers various interpolation methods through its kind argument, which can take values in ‘linear’, ‘nearest’, ‘nearest-up’, ‘zero’, ‘slinear’, ‘quadratic’, ‘cubic’, ‘previous’, or ‘next’. ‘zero’, ‘slinear’, ‘quadratic’ and ‘cubic’.\n\nHere’s a more general implementation which here uses cubic interpolation through the scipy.interpolate.interp1d function:\n\nimport numpy as np\nfrom scipy.interpolate import interp1d\n\n# Parameters\nr_max = 0.3\nK = 125\nT = 20  # Number of time steps\nN_max = 100  # Maximum population size to consider\nh_max = 0.5  # Maximum harvest rate\nh_step = 0.1  # Step size for harvest rate\n\n# Create state and decision spaces\nN_space = np.arange(1, N_max + 1)\nh_space = np.arange(0, h_max + h_step, h_step)\n\n# Initialize value function and policy\nV = np.zeros((T + 1, len(N_space)))\npolicy = np.zeros((T, len(N_space)))\n\n# Terminal value function (F_T)\ndef terminal_value(N):\n    return 0\n\n# State return function (F)\ndef state_return(N, h):\n    return N * h\n\n# State dynamics function\ndef state_dynamics(N, h):\n    return N + r_max * N * (1 - N / K) - N * h\n\n# Function to create interpolation function for a given time step\ndef create_interpolator(V_t, N_space):\n    return interp1d(N_space, V_t, kind='cubic', bounds_error=False, fill_value=(V_t[0], V_t[-1]))\n\n# Backward iteration with interpolation\nfor t in range(T - 1, -1, -1):\n    interpolator = create_interpolator(V[t+1], N_space)\n    \n    for i, N in enumerate(N_space):\n        max_value = float('-inf')\n        best_h = 0\n\n        for h in h_space:\n            if h > 1:  # Ensure harvest rate doesn't exceed 100%\n                continue\n\n            next_N = state_dynamics(N, h)\n            if next_N < 1:  # Ensure population doesn't go extinct\n                continue\n\n            # Use interpolation to get the value for next_N\n            value = state_return(N, h) + interpolator(next_N)\n\n            if value > max_value:\n                max_value = value\n                best_h = h\n\n        V[t, i] = max_value\n        policy[t, i] = best_h\n\n# Function to simulate the optimal policy using interpolation\ndef simulate_optimal_policy(initial_N, T):\n    trajectory = [initial_N]\n    harvests = []\n\n    for t in range(T):\n        N = trajectory[-1]\n        \n        # Create interpolator for the policy at time t\n        policy_interpolator = interp1d(N_space, policy[t], kind='cubic', bounds_error=False, fill_value=(policy[t][0], policy[t][-1]))\n        \n        h = policy_interpolator(N)\n        harvests.append(float(N * h))  # Ensure harvest is a Python float\n\n        next_N = state_dynamics(N, h)\n        trajectory.append(float(next_N))  # Ensure next population value is a Python float\n\n    return trajectory, harvests\n\n# Example usage\ninitial_N = 50\ntrajectory, harvests = simulate_optimal_policy(initial_N, T)\n\nprint(\"Optimal policy (first few rows):\")\nprint(policy[:5])\nprint(\"\\nPopulation trajectory:\", trajectory)\nprint(\"Harvests:\", harvests)\nprint(\"Total harvest:\", sum(harvests))\n\n ## Linear Quadratic Regulator via Dynamic Programming\n\nLet us now consider a special case of our dynamic programming formulation: the discrete-time Linear Quadratic Regulator (LQR) problem. This example will illustrate how the structure of linear dynamics and quadratic costs leads to a particularly clean form of the backward recursion.\n\nConsider a linear time-invariant system with dynamics:\n\n$$\n\\mathbf{x}_{t+1} = A\\mathbf{x}_t + B\\mathbf{u}_t\n$$\n\nwhere $\\mathbf{x}_t \\in \\mathbb{R}^n$ is the state and $\\mathbf{u}_t \\in \\mathbb{R}^m$ is the control input.  \nThe cost function to be minimized is quadratic:\n\n$$\nJ = \\frac{1}{2}\\mathbf{x}_T^\\top S_T \\mathbf{x}_T + \\frac{1}{2}\\sum_{t=0}^{T-1} \\left(\\mathbf{x}_t^\\top Q \\mathbf{x}_t + \\mathbf{u}_t^\\top R \\mathbf{u}_t\\right)\n$$\n\nwhere $S_T \\geq 0$, $Q \\geq 0$, and $R > 0$ are symmetric matrices of appropriate dimensions.  \nOur goal is to find the optimal control sequence $\\mathbf{u}_t^*$ that minimizes $J$ over a fixed time horizon $[0, T]$, given an initial state $\\mathbf{x}_0$.\n\nLet's apply the principle of optimality to derive the backward recursion for this problem. We'll start at the final time step and work our way backward.\n\nAt $t = T$, the terminal cost is given by:\n\n$$\nJ_T^*(\\mathbf{x}_T) = \\frac{1}{2}\\mathbf{x}_T^\\top S_T \\mathbf{x}_T\n$$\n\nAt $t = T-1$, the cost-to-go is:\n\n$$\nJ_{T-1}(\\mathbf{x}_{T-1}, \\mathbf{u}_{T-1}) = \\frac{1}{2}\\mathbf{x}_{T-1}^\\top Q \\mathbf{x}_{T-1} + \\frac{1}{2}\\mathbf{u}_{T-1}^\\top R \\mathbf{u}_{T-1} + J_T^*(\\mathbf{x}_T)\n$$\n\nSubstituting the dynamics equation:\n\n$$\nJ_{T-1} = \\frac{1}{2}\\mathbf{x}_{T-1}^\\top Q \\mathbf{x}_{T-1} + \\frac{1}{2}\\mathbf{u}_{T-1}^\\top R \\mathbf{u}_{T-1} + \\frac{1}{2}(A\\mathbf{x}_{T-1} + B\\mathbf{u}_{T-1})^\\top S_T (A\\mathbf{x}_{T-1} + B\\mathbf{u}_{T-1})\n$$\n\nTo find the optimal control, we differentiate with respect to $\\mathbf{u}_{T-1}$ and set it to zero:\n\n$$\n\\frac{\\partial J_{T-1}}{\\partial \\mathbf{u}_{T-1}} = R\\mathbf{u}_{T-1} + B^\\top S_T (A\\mathbf{x}_{T-1} + B\\mathbf{u}_{T-1}) = 0\n$$\n\nSolving for $\\mathbf{u}_{T-1}^*$:\n\n$$\n\\mathbf{u}_{T-1}^* = -(R + B^\\top S_T B)^{-1}B^\\top S_T A\\mathbf{x}_{T-1}\n$$\n\nWe can define the gain matrix:\n\n$$\nK_{T-1} = (R + B^\\top S_T B)^{-1}B^\\top S_T A\n$$\n\nSo that $\\mathbf{u}_{T-1}^* = -K_{T-1}\\mathbf{x}_{T-1}$. The optimal cost-to-go at $T-1$ is then:\n\n$$\nJ_{T-1}^*(\\mathbf{x}_{T-1}) = \\frac{1}{2}\\mathbf{x}_{T-1}^\\top S_{T-1} \\mathbf{x}_{T-1}\n$$\n\nWhere $S_{T-1}$ is given by:\n\n$$\nS_{T-1} = Q + A^\\top S_T A - A^\\top S_T B(R + B^\\top S_T B)^{-1}B^\\top S_T A\n$$\n\nContinuing this process backward in time, we find that for any $t$:\n\n$$\n\\mathbf{u}_t^* = -K_t\\mathbf{x}_t\n$$\n\nWhere:\n\n$$\nK_t = (R + B^\\top S_{t+1} B)^{-1}B^\\top S_{t+1} A\n$$\n\nAnd the optimal cost-to-go is:\n\n$$\nJ_t^*(\\mathbf{x}_t) = \\frac{1}{2}\\mathbf{x}_t^\\top S_t \\mathbf{x}_t\n$$\n\nWhere $S_t$ satisfies the so-called discrete-time Riccati equation:\n\n$$\nS_t = Q + A^\\top S_{t+1} A - A^\\top S_{t+1} B(R + B^\\top S_{t+1} B)^{-1}B^\\top S_{t+1} A\n$$  \n### Example: Linear Quadratic Regulation of a Liquid Tank \n\nWe are dealing with a liquid-level control system for a storage tank. This system consists of a reservoir connected to a tank via valves. These valves are controlled by a gear train, which is driven by a DC motor. The motor, in turn, is controlled by an electronic amplifier. The goal is to maintain a constant liquid level in the tank, adjusting only when necessary.\n\nThe system is described by a third-order continuous-time model with the following state variables:\n- $x_1(t)$: the height of the liquid in the tank\n- $x_2(t)$: the angular position of the electric motor driving the valves\n- $x_3(t)$: the angular velocity of the motor\n\nThe input to the system, $u(t)$, represents the signal sent to the electronic amplifier connected to the motor.\nThe system dynamics are described by the following differential equations:\n\n$$\n\\begin{aligned}\n& \\dot{x}_1(t) = -2x_1(t) \\\\\n& \\dot{x}_2(t) = x_3(t) \\\\\n& \\dot{x}_3(t) = -10x_3(t) + 9000u(t)\n\\end{aligned}\n$$\n\nTo pose this as a discrete-time LQR problem, we need to discretize the continuous-time system. Let's assume a sampling time of $T_s$ seconds. We can use the forward Euler method for a simple discretization:\n\n$$\n\\begin{aligned}\n& x_1(k+1) = x_1(k) + T_s(-2x_1(k)) \\\\\n& x_2(k+1) = x_2(k) + T_sx_3(k) \\\\\n& x_3(k+1) = x_3(k) + T_s(-10x_3(k) + 9000u(k))\n\\end{aligned}\n$$\n\nThis can be written in the standard discrete-time state-space form:\n\n$x(k+1) = Ax(k) + Bu(k)$\n\nWhere:\n\n$x(k) = \\begin{bmatrix} x_1(k) \\\\ x_2(k) \\\\ x_3(k) \\end{bmatrix}$\n\n$A = \\begin{bmatrix} \n1-2T_s & 0 & 0 \\\\\n0 & 1 & T_s \\\\\n0 & 0 & 1-10T_s\n\\end{bmatrix}$\n\n$B = \\begin{bmatrix} \n0 \\\\\n0 \\\\\n9000T_s\n\\end{bmatrix}$\n\nThe goal of our LQR controller is to maintain the liquid level at a desired reference value while minimizing control effort. We can formulate this as a discrete-time LQR problem with the following cost function:\n\n$J = \\sum_{k=0}^{\\infty} \\left( (x_1(k) - x_{1,ref})^2 + ru^2(k) \\right)$\n\nWhere $x_{1,ref}$ is the reference liquid level and $r$ is a positive weight on the control input.\n\nTo put this in standard discrete-time LQR form, we rewrite the cost function as:\n\n$J = \\sum_{k=0}^{\\infty} \\left( x^\\mathrm{T}(k)Qx(k) + ru^2(k) \\right)$\n\nWhere:\n\n$Q = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{bmatrix}$\n\nThe discrete-time LQR problem is now to find the optimal control law $u^*(k) = -Kx(k)$ that minimizes this cost function, subject to the discrete-time system dynamics $x(k+1) = Ax(k) + Bu(k)$.\n\nThe solution involves solving the discrete-time algebraic Riccati equation:\n\n$P = A^TPA - A^TPB(B^TPB + R)^{-1}B^TPA + Q$\n\nWhere $R = r$ (a scalar in this case), to find the positive definite matrix $P$. Then, the optimal gain matrix $K$ is given by:\n\n$K = (B^TPB + R)^{-1}B^TPA$\n\nThis formulation ensures that:\n1. The liquid level ($x_1(k)$) is maintained close to the reference value.\n2. The system acts primarily when there's a change in the liquid level, as only $x_1(k)$ is directly penalized in the cost function.\n3. The control effort is minimized, ensuring smooth operation of the valves.\n\nBy tuning the weight $r$ and the sampling time $T_s$, we can balance the trade-off between maintaining the desired liquid level, the amount of control effort used, and the responsiveness of the system. ","type":"content","url":"/dp#example-optimal-harvest-with-linear-interpolation","position":11},{"hierarchy":{"lvl1":"Dynamic Programming","lvl2":"Stochastic Dynamic Programming and Markov Decision Processes"},"type":"lvl2","url":"/dp#stochastic-dynamic-programming-and-markov-decision-processes","position":12},{"hierarchy":{"lvl1":"Dynamic Programming","lvl2":"Stochastic Dynamic Programming and Markov Decision Processes"},"content":"While our previous discussion centered on deterministic systems, many real-world problems involve uncertainty. Stochastic Dynamic Programming (SDP) extends our framework to handle stochasticity in both the objective function and system dynamics. This extension naturally leads us to consider more general policy classes and to formalize when simpler policies suffice.","type":"content","url":"/dp#stochastic-dynamic-programming-and-markov-decision-processes","position":13},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Decision Rules and Policies","lvl2":"Stochastic Dynamic Programming and Markov Decision Processes"},"type":"lvl3","url":"/dp#decision-rules-and-policies","position":14},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Decision Rules and Policies","lvl2":"Stochastic Dynamic Programming and Markov Decision Processes"},"content":"Before diving into stochastic systems, we need to establish terminology for the different types of strategies a decision maker might employ. In the deterministic setting, we implicitly used feedback controllers of the form u(\\mathbf{x}, t). In the stochastic setting, we must be more precise about what information policies can use and how they select actions.\n\nA decision rule is a prescription for action selection in each state at a specified decision epoch. These rules can vary in their complexity based on two main criteria:\n\nDependence on history: Markovian or History-dependent\n\nAction selection method: Deterministic or Randomized\n\nMarkovian decision rules depend only on the current state, while history-dependent rules consider the entire sequence of past states and actions. Formally, a history h_t at time t is:h_t = (s_1, a_1, \\ldots, s_{t-1}, a_{t-1}, s_t)\n\nThe set of all possible histories at time t, denoted H_t, grows exponentially with t:\n\nH_1 = \\mathcal{S} (just the initial state)\n\nH_2 = \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S}\n\nH_t = \\mathcal{S} \\times (\\mathcal{A} \\times \\mathcal{S})^{t-1}\n\nDeterministic rules select an action with certainty, while randomized rules specify a probability distribution over the action space.\n\nThese classifications lead to four types of decision rules:\n\nMarkovian Deterministic (MD): \\pi_t: \\mathcal{S} \\rightarrow \\mathcal{A}_s\n\nMarkovian Randomized (MR): \\pi_t: \\mathcal{S} \\rightarrow \\mathcal{P}(\\mathcal{A}_s)\n\nHistory-dependent Deterministic (HD): \\pi_t: H_t \\rightarrow \\mathcal{A}_s\n\nHistory-dependent Randomized (HR): \\pi_t: H_t \\rightarrow \\mathcal{P}(\\mathcal{A}_s)\n\nwhere \\mathcal{P}(\\mathcal{A}_s) denotes the set of probability distributions over \\mathcal{A}_s.\n\nA policy \\boldsymbol{\\pi} is a sequence of decision rules, one for each decision epoch:\\boldsymbol{\\pi} = (\\pi_1, \\pi_2, ..., \\pi_{N-1})\n\nThe set of all policies of class K (where K \\in \\{HR, HD, MR, MD\\}) is denoted as \\Pi^K. These policy classes form a hierarchy:\\Pi^{MD} \\subset \\Pi^{MR} \\subset \\Pi^{HR}, \\quad \\Pi^{MD} \\subset \\Pi^{HD} \\subset \\Pi^{HR}\n\nThe largest set \\Pi^{HR} contains all possible policies. We ask: under what conditions can we restrict attention to the much simpler set \\Pi^{MD} without loss of optimality?\n\nNotation: rules vs. policies\n\nDecision rule (kernel). A map from information to action distributions:\n\nMarkov, deterministic:  \\pi_t:\\mathcal{S}\\to\\mathcal{A}_s\n\nMarkov, randomized:     \\pi_t(\\cdot\\mid s)\\in\\Delta(\\mathcal{A}_s)\n\nHistory-dependent:       \\pi_t(\\cdot\\mid h_t)\\in\\Delta(\\mathcal{A}_{s_t})\n\nPolicy (sequence). \\boldsymbol{\\pi}=(\\pi_1,\\pi_2,\\ldots).\n\nStationary policy. \\boldsymbol{\\pi}=\\mathrm{const}(\\pi) with \\pi_t\\equiv\\pi \\ \\forall t.By convention, we identify \\pi with its stationary policy \\mathrm{const}(\\pi) when no confusion arises.","type":"content","url":"/dp#decision-rules-and-policies","position":15},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Stochastic System Dynamics","lvl2":"Stochastic Dynamic Programming and Markov Decision Processes"},"type":"lvl3","url":"/dp#stochastic-system-dynamics","position":16},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Stochastic System Dynamics","lvl2":"Stochastic Dynamic Programming and Markov Decision Processes"},"content":"In the stochastic setting, our system evolution takes the form:\\mathbf{x}_{t+1} = \\mathbf{f}_t(\\mathbf{x}_t, \\mathbf{u}_t, \\mathbf{w}_t)\n\nHere, \\mathbf{w}_t represents a random disturbance or noise term at time t due to the inherent uncertainty in the system’s behavior. The stage cost function may also incorporate stochastic influences:c_t(\\mathbf{x}_t, \\mathbf{u}_t, \\mathbf{w}_t)\n\nIn this context, our objective shifts from minimizing a deterministic cost to minimizing the expected total cost:\\mathbb{E}\\left[c_\\mathrm{T}(\\mathbf{x}_T) + \\sum_{t=1}^{T-1} c_t(\\mathbf{x}_t, \\mathbf{u}_t, \\mathbf{w}_t)\\right]\n\nwhere the expectation is taken over the distributions of the random variables \\mathbf{w}_t. The principle of optimality still holds in the stochastic case, but Bellman’s optimality equation now involves an expectation:J_k^\\star(\\mathbf{x}_k) = \\min_{\\mathbf{u}_k} \\mathbb{E}_{\\mathbf{w}_k}\\left[c_k(\\mathbf{x}_k, \\mathbf{u}_k, \\mathbf{w}_k) + J_{k+1}^\\star(\\mathbf{f}_k(\\mathbf{x}_k, \\mathbf{u}_k, \\mathbf{w}_k))\\right]\n\nIn practice, this expectation is often computed by discretizing the distribution of \\mathbf{w}_k when the set of possible disturbances is very large or even continuous. Let’s say we approximate the distribution with K discrete values \\mathbf{w}_k^i, each occurring with probability p_k^i. Then our Bellman equation becomes:J_k^\\star(\\mathbf{x}_k) = \\min_{\\mathbf{u}_k} \\sum_{i=1}^K p_k^i \\left(c_k(\\mathbf{x}_k, \\mathbf{u}_k, \\mathbf{w}_k^i) + J_{k+1}^\\star(\\mathbf{f}_k(\\mathbf{x}_k, \\mathbf{u}_k, \\mathbf{w}_k^i))\\right)","type":"content","url":"/dp#stochastic-system-dynamics","position":17},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Optimality Equations in the Stochastic Setting","lvl2":"Stochastic Dynamic Programming and Markov Decision Processes"},"type":"lvl3","url":"/dp#optimality-equations-in-the-stochastic-setting","position":18},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Optimality Equations in the Stochastic Setting","lvl2":"Stochastic Dynamic Programming and Markov Decision Processes"},"content":"When dealing with stochastic systems, a central question arises: what information should our control policy use? In the most general case, a policy might use the entire history of observations and actions. However, as we’ll see, the Markovian structure of our problems allows for dramatic simplifications.\n\nLet h_t = (s_1, a_1, s_2, a_2, \\ldots, s_{t-1}, a_{t-1}, s_t) denote the complete history up to time t. In the stochastic setting, the history-based optimality equations become:u_t(h_t) = \\sup_{a\\in A_{s_t}}\\left\\{ r_t(s_t,a) + \\sum_{j\\in S} p_t(j\\mid s_t,a)\\, u_{t+1}(h_t,a,j) \\right\\},\\quad u_N(h_N)=r_N(s_N)\n\nwhere we now explicitly use the transition probabilities p_t(j|s_t,a) rather than a deterministic dynamics function.\n\nPrinciple of optimality for stochastic systems\n\nLet u_t^* be the optimal expected return from epoch t onward. Then:\n\na. u_t^* satisfies the optimality equations:u_t^*(h_t) = \\sup_{a \\in A_{s_t}} \\left\\{ r_t(s_t, a) + \\sum_{j \\in S} p_t(j|s_t, a) u_{t+1}^*(h_t, a, j) \\right\\}\n\nwith boundary condition u_N^*(h_N) = r_N(s_N).\n\nb. Any policy \\pi^* that selects actions attaining the supremum (or maximum) in the above equation at each history is optimal.\n\nIntuition: This formalizes Bellman’s principle of optimality: “An optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision.” The recursive structure means that optimal local decisions (choosing the best action at each step) lead to global optimality, even with uncertainty captured by the transition probabilities.\n\nA simplification occurs when we examine these history-based equations more closely. The Markov property of our system dynamics and rewards means that the optimal return actually depends on the history only through the current state:\n\nState sufficiency for stochastic MDPs\n\nIn finite-horizon stochastic MDPs with Markovian dynamics and rewards, the optimal return u_t^*(h_t) depends on the history only through the current state s_t. Thus we can write u_t^*(h_t) = v_t^*(s_t) for some function v_t^* that depends only on state and time.\n\nFollowing \n\nPuterman (1994) Theorem 4.4.2. We proceed by backward induction.\n\nBase case: At the terminal time N, we have u_N^*(h_N) = r_N(s_N) by the boundary condition. Since the terminal reward depends only on the final state s_N and not on how we arrived there, u_N^*(h_N) = u_N^*(s_N).\n\nInductive step: Assume u_{t+1}^*(h_{t+1}) depends on h_{t+1} only through s_{t+1} for all t+1, \\ldots, N. Then from the optimality equation:u_t^*(h_t) = \\sup_{a \\in A_{s_t}} \\left\\{ r_t(s_t, a) + \\sum_{j \\in S} p_t(j|s_t, a) u_{t+1}^*(h_t, a, j) \\right\\}\n\nBy the induction hypothesis, u_{t+1}^*(h_t, a, j) depends only on the next state j, so:u_t^*(h_t) = \\sup_{a \\in A_{s_t}} \\left\\{ r_t(s_t, a) + \\sum_{j \\in S} p_t(j|s_t, a) u_{t+1}^*(j) \\right\\}\n\nSince the expression in brackets depends on h_t only through the current state s_t (the rewards and transition probabilities are Markovian), we conclude that u_t^*(h_t) = u_t^*(s_t).\n\nIntuition: The Markov property means that the current state contains all information needed to predict future evolution. The past provides no additional value for decision-making. This result allows us to work with value functions v_t^*(s) indexed only by state and time, dramatically simplifying both theory and computation.\n\nThis state-sufficiency result, combined with the fact that randomization never helps when maximizing expected returns, leads to a dramatic simplification of the policy space:\n\nPolicy reduction for stochastic MDPs\n\nFor finite-horizon stochastic MDPs with finite state and action sets:\\sup_{\\pi \\in \\Pi^{\\mathrm{HR}}} v_\\pi(s,t) = \\max_{\\pi \\in \\Pi^{\\mathrm{MD}}} v_\\pi(s,t)\n\nThat is, there exists an optimal policy that is both deterministic and Markovian.\n\nSketch following \n\nPuterman (1994) Lemma 4.3.1 and Theorem 4.4.2. First, Lemma 4.3.1 shows that for any function w and any distribution q over actions, \\sup_a w(a) \\ge \\sum_a q(a) w(a). Thus randomization cannot improve the expected value over choosing a single maximizing action. Second, by state sufficiency (Proposition \n\nProposition 1 and \n\nPuterman (1994) Thm. 4.4.2(a)), the optimal return depends on the history only through (s_t,t). Therefore, selecting at each (s_t,t) an action that attains the maximum yields a deterministic Markov decision rule which is optimal whenever the maximum is attained. If only a supremum exists, \\varepsilon-optimal selectors exist by choosing actions within \\varepsilon of the supremum (see \n\nPuterman (1994) Thm. 4.3.4).\n\nIntuition: Even in stochastic systems, randomization in the policy doesn’t help when maximizing expected returns: you should always choose the action with the highest expected value. Combined with state sufficiency, this means simple state-to-action mappings are optimal.\n\nThese results justify focusing on deterministic Markov policies and lead to the backward recursion algorithm for stochastic systems:\n\nBackward Recursion for Stochastic Dynamic Programming\n\nInput: Terminal cost function c_\\mathrm{T}(\\cdot), stage cost functions c_t(\\cdot, \\cdot, \\cdot), system dynamics \\mathbf{f}_t(\\cdot, \\cdot, \\cdot), time horizon \\mathrm{T}, disturbance distributions\n\nOutput: Optimal value functions J_t^\\star(\\cdot) and optimal control policies \\mu_t^\\star(\\cdot) for t = 1, \\ldots, T\n\nInitialize J_T^\\star(\\mathbf{x}) = c_\\mathrm{T}(\\mathbf{x}) for all \\mathbf{x} in the state space\n\nFor t = T-1, T-2, \\ldots, 1:\n\nFor each state \\mathbf{x} in the state space:\n\nCompute J_t^\\star(\\mathbf{x}) = \\min_{\\mathbf{u}} \\mathbb{E}_{\\mathbf{w}_t}\\left[c_t(\\mathbf{x}, \\mathbf{u}, \\mathbf{w}_t) + J_{t+1}^\\star(\\mathbf{f}_t(\\mathbf{x}, \\mathbf{u}, \\mathbf{w}_t))\\right]\n\nCompute \\mu_t^\\star(\\mathbf{x}) = \\arg\\min_{\\mathbf{u}} \\mathbb{E}_{\\mathbf{w}_t}\\left[c_t(\\mathbf{x}, \\mathbf{u}, \\mathbf{w}_t) + J_{t+1}^\\star(\\mathbf{f}_t(\\mathbf{x}, \\mathbf{u}, \\mathbf{w}_t))\\right]\n\nEnd For\n\nEnd For\n\nReturn J_t^\\star(\\cdot), \\mu_t^\\star(\\cdot) for t = 1, \\ldots, T\n\nWhile SDP provides us with a framework to for handling uncertainty, it makes the curse of dimensionality even more difficult to handle in practice. Both the state space and the disturbance space must be discretized. This can lead to a combinatorial explosion in the number of scenarios to be evaluated at each stage.\n\nHowever, just as we tackled the challenges of continuous state spaces with discretization and interpolation, we can devise efficient methods to handle the additional complexity of evaluating expectations. This problem essentially becomes one of numerical integration. When the set of disturbances is continuous (as is often the case with continuous state spaces), we enter a domain where numerical quadrature methods could be applied. But these methods tend to scale poorly as the number of dimensions grows. This is where more efficient techniques, often rooted in Monte Carlo methods, come into play. The combination of two key ingredients emerges to tackle the curse of dimensionality:\n\nFunction approximation (through discretization, interpolation, neural networks, etc.)\n\nMonte Carlo integration (simulation)\n\nThese two elements essentially distill the key ingredients of machine learning, which is the direction we’ll be exploring in this course.","type":"content","url":"/dp#optimality-equations-in-the-stochastic-setting","position":19},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Example: Stochastic Optimal Harvest in Resource Management","lvl2":"Stochastic Dynamic Programming and Markov Decision Processes"},"type":"lvl3","url":"/dp#example-stochastic-optimal-harvest-in-resource-management","position":20},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Example: Stochastic Optimal Harvest in Resource Management","lvl2":"Stochastic Dynamic Programming and Markov Decision Processes"},"content":"Building upon our previous deterministic model, we now introduce stochasticity to more accurately reflect the uncertainties inherent in real-world resource management scenarios \n\nConroy & Peterson, 2013. As before, we consider a population of a particular species, whose abundance we denote by x_t, where t represents discrete time steps. Our objective remains to maximize the cumulative harvest over a finite time horizon, while also considering the long-term sustainability of the population. However, we now account for two sources of stochasticity: partial controllability of harvest and environmental variability affecting growth rates.\nThe optimization problem can be formulated as:\\text{maximize} \\quad \\mathbb{E}\\left[\\sum_{t=t_0}^{t_f} F(x_t \\cdot h_t)\\right]\n\nHere, F(\\cdot) represents the immediate reward function associated with harvesting, and h_t is the realized harvest rate at time t. The expectation \\mathbb{E}[\\cdot] over both harvest and growth rates, which we view as random variables.\nIn our stochastic model, the abundance x still ranges from 1 to 100 individuals. The decision variable is now the desired harvest rate d_t, which can take values from the set D = {0, 0.1, 0.2, 0.3, 0.4, 0.5}. However, the realized harvest rate h_t is stochastic and follows a discrete distribution:h_t = \\begin{cases}\n0.75d_t & \\text{with probability } 0.25 \\\\\nd_t & \\text{with probability } 0.5 \\\\\n1.25d_t & \\text{with probability } 0.25\n\\end{cases}\n\nBy expressing the harvest rate as a random variable, we mean to capture the fact that harvesting is a not completely under our control: we might obtain more or less what we had intended to. Furthermore, we generalize the population dynamics to the stochastic case via:\n\n$$\n\nx_{t+1} = x_t + r_tx_t(1 - x_t/K) - h_tx_t\n$$\n\nwhere K = 125 is the carrying capacity. The growth rate r_t is now stochastic and follows a discrete distribution:r_t = \\begin{cases}\n0.85r_{\\text{max}} & \\text{with probability } 0.25 \\\\\n1.05r_{\\text{max}} & \\text{with probability } 0.5 \\\\\n1.15r_{\\text{max}} & \\text{with probability } 0.25\n\\end{cases}\n\nwhere r_{\\text{max}} = 0.3 is the maximum growth rate.\nApplying the principle of optimality, we can express the optimal value function J^\\star(x_t, t) recursively:J^\\star(x_t, t) = \\max_{d(t) \\in D} \\mathbb{E}\\left[F(x_t \\cdot h_t) + J^\\star(x_{t+1}, t+1)\\right]\n\nwhere the expectation is taken over the harvest and growth rate random variables. The boundary condition remains J^*(x_{t_f}) = 0. We can now adapt our previous code to account for the stochasticity in our model. One important difference is that simulating a solution in this context requires multiple realizations of our process. This is an important consideration when evaluating reinforcement learning methods in practice, as success cannot be claimed based on a single successful trajectory.\n\nimport numpy as np\nfrom scipy.interpolate import interp1d\n\n# Parameters\nr_max = 0.3\nK = 125\nT = 30  # Number of time steps\nN_max = 100  # Maximum population size to consider\nh_max = 0.5  # Maximum harvest rate\nh_step = 0.1  # Step size for harvest rate\n\n# Create state and decision spaces\nN_space = np.linspace(1, N_max, 100)  # Using more granular state space\nh_space = np.arange(0, h_max + h_step, h_step)\n\n# Stochastic parameters\nh_outcomes = np.array([0.75, 1.0, 1.25])\nh_probs = np.array([0.25, 0.5, 0.25])\nr_outcomes = np.array([0.85, 1.05, 1.15]) * r_max\nr_probs = np.array([0.25, 0.5, 0.25])\n\n# Initialize value function and policy\nV = np.zeros((T + 1, len(N_space)))\npolicy = np.zeros((T, len(N_space)))\n\n# State return function (F)\ndef state_return(N, h):\n    return N * h\n\n# State dynamics function (stochastic)\ndef state_dynamics(N, h, r):\n    return N + r * N * (1 - N / K) - h * N\n\n# Function to create interpolation function for a given time step\ndef create_interpolator(V_t, N_space):\n    return interp1d(N_space, V_t, kind='linear', bounds_error=False, fill_value=(V_t[0], V_t[-1]))\n\n# Backward iteration with stochastic dynamics\nfor t in range(T - 1, -1, -1):\n    interpolator = create_interpolator(V[t+1], N_space)\n    \n    for i, N in enumerate(N_space):\n        max_value = float('-inf')\n        best_h = 0\n\n        for h in h_space:\n            if h > 1:  # Ensure harvest rate doesn't exceed 100%\n                continue\n\n            expected_value = 0\n            for h_factor, h_prob in zip(h_outcomes, h_probs):\n                for r_factor, r_prob in zip(r_outcomes, r_probs):\n                    realized_h = h * h_factor\n                    realized_r = r_factor\n\n                    next_N = state_dynamics(N, realized_h, realized_r)\n                    if next_N < 1:  # Ensure population doesn't go extinct\n                        continue\n\n                    # Use interpolation to get the value for next_N\n                    value = state_return(N, realized_h) + interpolator(next_N)\n                    expected_value += value * h_prob * r_prob\n\n            if expected_value > max_value:\n                max_value = expected_value\n                best_h = h\n\n        V[t, i] = max_value\n        policy[t, i] = best_h\n\n# Function to simulate the optimal policy using interpolation (stochastic version)\ndef simulate_optimal_policy(initial_N, T, num_simulations=100):\n    all_trajectories = []\n    all_harvests = []\n\n    for _ in range(num_simulations):\n        trajectory = [initial_N]\n        harvests = []\n\n        for t in range(T):\n            N = trajectory[-1]\n            \n            # Create interpolator for the policy at time t\n            policy_interpolator = interp1d(N_space, policy[t], kind='linear', bounds_error=False, fill_value=(policy[t][0], policy[t][-1]))\n            \n            intended_h = policy_interpolator(N)\n            \n            # Apply stochasticity\n            h_factor = np.random.choice(h_outcomes, p=h_probs)\n            r_factor = np.random.choice(r_outcomes, p=r_probs)\n            \n            realized_h = intended_h * h_factor\n            harvests.append(N * realized_h)\n\n            next_N = state_dynamics(N, realized_h, r_factor)\n            trajectory.append(next_N)\n\n        all_trajectories.append(trajectory)\n        all_harvests.append(harvests)\n\n    return all_trajectories, all_harvests\n\n# Example usage\ninitial_N = 50\ntrajectories, harvests = simulate_optimal_policy(initial_N, T)\n\n# Calculate average trajectory and total harvest\navg_trajectory = np.mean(trajectories, axis=0)\navg_total_harvest = np.mean([sum(h) for h in harvests])\n\nprint(\"Optimal policy (first few rows):\")\nprint(policy[:5])\nprint(\"\\nAverage population trajectory:\", avg_trajectory)\nprint(\"Average total harvest:\", avg_total_harvest)\n\n# Plot results\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nplt.subplot(121)\nfor traj in trajectories[:20]:  # Plot first 20 trajectories\n    plt.plot(range(T+1), traj, alpha=0.3)\nplt.plot(range(T+1), avg_trajectory, 'r-', linewidth=2)\nplt.title('Population Trajectories')\nplt.xlabel('Time')\nplt.ylabel('Population')\n\nplt.subplot(122)\nplt.hist([sum(h) for h in harvests], bins=20)\nplt.title('Distribution of Total Harvest')\nplt.xlabel('Total Harvest')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n","type":"content","url":"/dp#example-stochastic-optimal-harvest-in-resource-management","position":21},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Linear Quadratic Regulator via Dynamic Programming","lvl2":"Stochastic Dynamic Programming and Markov Decision Processes"},"type":"lvl3","url":"/dp#linear-quadratic-regulator-via-dynamic-programming","position":22},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Linear Quadratic Regulator via Dynamic Programming","lvl2":"Stochastic Dynamic Programming and Markov Decision Processes"},"content":"We now examine a special case where the backward recursion admits a closed-form solution. When the system dynamics are linear and the cost function is quadratic, the optimization at each stage can be solved analytically. Moreover, the value function itself maintains a quadratic structure throughout the recursion, and the optimal policy reduces to a simple linear feedback law. This result eliminates the need for discretization, interpolation, or any function approximation. The infinite-dimensional problem collapses to tracking a finite set of matrices.\n\nConsider a discrete-time linear system:\\mathbf{x}_{t+1} = A_t\\mathbf{x}_t + B_t\\mathbf{u}_t\n\nwhere \\mathbf{x}_t \\in \\mathbb{R}^n is the state and \\mathbf{u}_t \\in \\mathbb{R}^m is the control input. The matrices A_t \\in \\mathbb{R}^{n \\times n} and B_t \\in \\mathbb{R}^{n \\times m} describe the system dynamics at time t.\n\nThe cost function to be minimized is quadratic:J = \\frac{1}{2}\\mathbf{x}_T^\\top Q_T \\mathbf{x}_T + \\frac{1}{2}\\sum_{t=0}^{T-1} \\left(\\mathbf{x}_t^\\top Q_t \\mathbf{x}_t + \\mathbf{u}_t^\\top R_t \\mathbf{u}_t\\right)\n\nwhere Q_T \\succeq 0 (positive semidefinite), Q_t \\succeq 0, and R_t \\succ 0 (positive definite) are symmetric matrices of appropriate dimensions. The positive definiteness of R_t ensures the minimization problem is well-posed.\n\nWhat we now have to observe is that if the terminal cost is quadratic, then the value function at every earlier stage remains quadratic. This is not immediately obvious, but it follows from the structure of Bellman’s equation combined with the linearity of the dynamics.\n\nWe claim that the optimal cost-to-go from any stage t takes the form:J_t^\\star(\\mathbf{x}_t) = \\frac{1}{2}\\mathbf{x}_t^\\top P_t \\mathbf{x}_t\n\nfor some positive semidefinite matrix P_t. At the terminal time, this is true by definition: P_T = Q_T.\n\nLet’s verify this structure and derive the recursion for P_t using backward induction. Suppose we’ve established that J_{t+1}^\\star(\\mathbf{x}_{t+1}) = \\frac{1}{2}\\mathbf{x}_{t+1}^\\top P_{t+1} \\mathbf{x}_{t+1}. Bellman’s equation at stage t states:J_t^\\star(\\mathbf{x}_t) = \\min_{\\mathbf{u}_t} \\left[ \\frac{1}{2}\\mathbf{x}_t^\\top Q_t \\mathbf{x}_t + \\frac{1}{2}\\mathbf{u}_t^\\top R_t \\mathbf{u}_t + J_{t+1}^\\star(\\mathbf{x}_{t+1}) \\right]\n\nSubstituting the dynamics \\mathbf{x}_{t+1} = A_t\\mathbf{x}_t + B_t\\mathbf{u}_t and the quadratic form for J_{t+1}^\\star:J_t^\\star(\\mathbf{x}_t) = \\min_{\\mathbf{u}_t} \\left[ \\frac{1}{2}\\mathbf{x}_t^\\top Q_t \\mathbf{x}_t + \\frac{1}{2}\\mathbf{u}_t^\\top R_t \\mathbf{u}_t + \\frac{1}{2}(A_t\\mathbf{x}_t + B_t\\mathbf{u}_t)^\\top P_{t+1} (A_t\\mathbf{x}_t + B_t\\mathbf{u}_t) \\right]\n\nExpanding the last term:(A_t\\mathbf{x}_t + B_t\\mathbf{u}_t)^\\top P_{t+1} (A_t\\mathbf{x}_t + B_t\\mathbf{u}_t) = \\mathbf{x}_t^\\top A_t^\\top P_{t+1} A_t \\mathbf{x}_t + 2\\mathbf{x}_t^\\top A_t^\\top P_{t+1} B_t \\mathbf{u}_t + \\mathbf{u}_t^\\top B_t^\\top P_{t+1} B_t \\mathbf{u}_t\n\nThe expression inside the minimization becomes:\\frac{1}{2}\\mathbf{x}_t^\\top Q_t \\mathbf{x}_t + \\frac{1}{2}\\mathbf{u}_t^\\top R_t \\mathbf{u}_t + \\frac{1}{2}\\mathbf{x}_t^\\top A_t^\\top P_{t+1} A_t \\mathbf{x}_t + \\mathbf{x}_t^\\top A_t^\\top P_{t+1} B_t \\mathbf{u}_t + \\frac{1}{2}\\mathbf{u}_t^\\top B_t^\\top P_{t+1} B_t \\mathbf{u}_t\n\nCollecting terms involving \\mathbf{u}_t:= \\frac{1}{2}\\mathbf{x}_t^\\top (Q_t + A_t^\\top P_{t+1} A_t) \\mathbf{x}_t + \\mathbf{x}_t^\\top A_t^\\top P_{t+1} B_t \\mathbf{u}_t + \\frac{1}{2}\\mathbf{u}_t^\\top (R_t + B_t^\\top P_{t+1} B_t) \\mathbf{u}_t\n\nThis is a quadratic function of \\mathbf{u}_t. To find the minimizer, we take the gradient with respect to \\mathbf{u}_t and set it to zero:\\frac{\\partial}{\\partial \\mathbf{u}_t} = (R_t + B_t^\\top P_{t+1} B_t) \\mathbf{u}_t + B_t^\\top P_{t+1} A_t \\mathbf{x}_t = 0\n\nSince R_t + B_t^\\top P_{t+1} B_t is positive definite (both R_t and P_{t+1} are positive semidefinite with R_t strictly positive), we can solve for the optimal control:\\mathbf{u}_t^\\star = -(R_t + B_t^\\top P_{t+1} B_t)^{-1} B_t^\\top P_{t+1} A_t \\mathbf{x}_t\n\nDefine the gain matrix:K_t = (R_t + B_t^\\top P_{t+1} B_t)^{-1} B_t^\\top P_{t+1} A_t\n\nso that \\mathbf{u}_t^\\star = -K_t\\mathbf{x}_t. This is a linear feedback policy: the optimal control is simply a linear function of the current state.\n\nSubstituting \\mathbf{u}_t^\\star back into the cost-to-go expression and simplifying (by completing the square), we obtain:J_t^\\star(\\mathbf{x}_t) = \\frac{1}{2}\\mathbf{x}_t^\\top P_t \\mathbf{x}_t\n\nwhere P_t satisfies the discrete-time Riccati equation:P_t = Q_t + A_t^\\top P_{t+1} A_t - A_t^\\top P_{t+1} B_t (R_t + B_t^\\top P_{t+1} B_t)^{-1} B_t^\\top P_{t+1} A_t\n\nPutting everything together, the backward induction procedure under the LQR setting then becomes:\n\nBackward Recursion for LQR\n\nInput: System matrices A_t, B_t, cost matrices Q_t, R_t, Q_T, time horizon T\n\nOutput: Cost matrices P_t and gain matrices K_t for t = 0, \\ldots, T-1\n\nInitialize: P_T = Q_T\n\nFor t = T-1, T-2, \\ldots, 0:\n\nCompute the gain matrix:K_t = (R_t + B_t^\\top P_{t+1} B_t)^{-1} B_t^\\top P_{t+1} A_t\n\nCompute the cost matrix via the Riccati equation:P_t = Q_t + A_t^\\top P_{t+1} A_t - A_t^\\top P_{t+1} B_t (R_t + B_t^\\top P_{t+1} B_t)^{-1} B_t^\\top P_{t+1} A_t\n\nEnd For\n\nReturn: \\{P_0, \\ldots, P_T\\} and \\{K_0, \\ldots, K_{T-1}\\}\n\nOptimal policy: \\mathbf{u}_t^\\star = -K_t\\mathbf{x}_t\n\nOptimal cost-to-go: J_t^\\star(\\mathbf{x}_t) = \\frac{1}{2}\\mathbf{x}_t^\\top P_t \\mathbf{x}_t","type":"content","url":"/dp#linear-quadratic-regulator-via-dynamic-programming","position":23},{"hierarchy":{"lvl1":"Dynamic Programming","lvl2":"Markov Decision Process Formulation"},"type":"lvl2","url":"/dp#markov-decision-process-formulation","position":24},{"hierarchy":{"lvl1":"Dynamic Programming","lvl2":"Markov Decision Process Formulation"},"content":"Rather than expressing the stochasticity in our system through a disturbance term as a parameter to a deterministic difference equation, we often work with an alternative representation (more common in operations research) which uses the Markov Decision Process formulation. The idea is that when we model our system in this way with the disturbance term being drawn indepently of the previous stages, the induced trajectory are those of a Markov chain. Hence, we can re-cast our control problem in that language, leading to the so-called Markov Decision Process framework in which we express the system dynamics in terms of transition probabilities rather than explicit state equations. In this framework, we express the probability that the system is in a given state using the transition probability function:p_t(\\mathbf{x}_{t+1} | \\mathbf{x}_t, \\mathbf{u}_t)\n\nThis function gives the probability of transitioning to state \\mathbf{x}_{t+1} at time t+1, given that the system is in state \\mathbf{x}_t and action \\mathbf{u}_t is taken at time t. Therefore, p_t specifies a conditional probability distribution over the next states: namely, the sum (for discrete state spaces) or integral over the next state should be 1.\n\nGiven the control theory formulation of our problem via a deterministic dynamics function and a noise term, we can derive the corresponding transition probability function through the following relationship:\\begin{aligned}\np_t(\\mathbf{x}_{t+1} | \\mathbf{x}_t, \\mathbf{u}_t) &= \\mathbb{P}(\\mathbf{W}_t \\in \\left\\{\\mathbf{w} \\in \\mathbf{W}: \\mathbf{x}_{t+1} = f_t(\\mathbf{x}_t, \\mathbf{u}_t, \\mathbf{w})\\right\\}) \\\\\n&= \\sum_{\\left\\{\\mathbf{w} \\in \\mathbf{W}: \\mathbf{x}_{t+1} = f_t(\\mathbf{x}_t, \\mathbf{u}_t, \\mathbf{w})\\right\\}} q_t(\\mathbf{w})\n\\end{aligned}\n\nHere, q_t(\\mathbf{w}) represents the probability density or mass function of the disturbance \\mathbf{W}_t (assuming discrete state spaces). When dealing with continuous spaces, the above expression simply contains an integral rather than a summation.\n\nFor a system with deterministic dynamics and no disturbance, the transition probabilities become much simpler and be expressed using the indicator function. Given a deterministic system with dynamics:\\mathbf{x}_{t+1} = f_t(\\mathbf{x}_t, \\mathbf{u}_t)\n\nThe transition probability function can be expressed as:p_t(\\mathbf{x}_{t+1} | \\mathbf{x}_t, \\mathbf{u}_t) = \\begin{cases}\n1 & \\text{if } \\mathbf{x}_{t+1} = f_t(\\mathbf{x}_t, \\mathbf{u}_t) \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\nWith this transition probability function, we can recast our Bellman optimality equation:J_t^\\star(\\mathbf{x}_t) = \\max_{\\mathbf{u}_t \\in \\mathbf{U}} \\left\\{ c_t(\\mathbf{x}_t, \\mathbf{u}_t) + \\sum_{\\mathbf{x}_{t+1}} p_t(\\mathbf{x}_{t+1} | \\mathbf{x}_t, \\mathbf{u}_t) J_{t+1}^\\star(\\mathbf{x}_{t+1}) \\right\\}\n\nHere, {c}(\\mathbf{x}_t, \\mathbf{u}_t) represents the expected immediate reward (or negative cost) when in state \\mathbf{x}_t and taking action \\mathbf{u}_t at time t. The summation term computes the expected optimal value for the future states, weighted by their transition probabilities.\n\nThis formulation offers several advantages:\n\nIt makes the Markovian nature of the problem explicit: the future state depends only on the current state and action, not on the history of states and actions.\n\nFor discrete-state problems, the entire system dynamics can be specified by a set of transition matrices, one for each possible action.\n\nIt allows us to bridge the gap with the wealth of methods in the field of probabilistic graphical models and statistical machine learning techniques for modelling and analysis.","type":"content","url":"/dp#markov-decision-process-formulation","position":25},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Notation in Operations Reseach","lvl2":"Markov Decision Process Formulation"},"type":"lvl3","url":"/dp#notation-in-operations-reseach","position":26},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Notation in Operations Reseach","lvl2":"Markov Decision Process Formulation"},"content":"The presentation above was intended to bridge the gap between the control-theoretic perspective and the world of closed-loop control through the idea of determining the value function of a parametric optimal control problem. We then saw how the backward induction procedure was applicable to both the deterministic and stochastic cases by taking the expectation over the disturbance variable. We then said that we can alternatively work with a representation of our system where instead of writing our model as a deterministic dynamics function taking a disturbance as an input, we would rather work directly via its transition probability function, which gives rise to the Markov chain interpretation of our system in simulation.\n\nNote that the notation used in control theory tends to differ from that found in operations research communities, in which the field of dynamic programming flourished. We summarize those (purely notational) differences in this section.\n\nIn operations research, the system state at each decision epoch is typically denoted by s \\in \\mathcal{S}, where S is the set of possible system states. When the system is in state s, the decision maker may choose an action a from the set of allowable actions \\mathcal{A}_s. The union of all action sets is denoted as \\mathcal{A} = \\bigcup_{s \\in \\mathcal{S}} \\mathcal{A}_s.\n\nThe dynamics of the system are described by a transition probability function p_t(j | s, a), which represents the probability of transitioning to state j \\in \\mathcal{S} at time t+1, given that the system is in state s at time t and action a \\in \\mathcal{A}_s is chosen. This transition probability function satisfies:\\sum_{j \\in \\mathcal{S}} p_t(j | s, a) = 1\n\nIt’s worth noting that in operations research, we typically work with reward maximization rather than cost minimization, which is more common in control theory. However, we can easily switch between these perspectives by simply negating the quantity. That is, maximizing a reward function is equivalent to minimizing its negative, which we would then call a cost function.\n\nThe reward function is denoted by r_t(s, a), representing the reward received at time t when the system is in state s and action a is taken. In some cases, the reward may also depend on the next state, in which case it is denoted as r_t(s, a, j). The expected reward can then be computed as:r_t(s, a) = \\sum_{j \\in \\mathcal{S}} r_t(s, a, j) p_t(j | s, a)\n\nCombined together, these elemetns specify a Markov decision process, which is fully described by the tuple:\\{T, S, \\mathcal{A}_s, p_t(\\cdot | s, a), r_t(s, a)\\}\n\nwhere \\mathrm{T} represents the set of decision epochs (the horizon).","type":"content","url":"/dp#notation-in-operations-reseach","position":27},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"What is an Optimal Policy?","lvl2":"Markov Decision Process Formulation"},"type":"lvl3","url":"/dp#what-is-an-optimal-policy","position":28},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"What is an Optimal Policy?","lvl2":"Markov Decision Process Formulation"},"content":"Let’s go back to the starting point and define what it means for a policy to be optimal in a Markov Decision Problem. For this, we will be considering different possible search spaces (policy classes) and compare policies based on the ordering of their value from any possible start state. The value of a policy \\boldsymbol{\\pi} (optimal or not) is defined as the expected total reward obtained by following that policy from a given starting state. Formally, for a finite-horizon MDP with N decision epochs, we define the value function v^{\\boldsymbol{\\pi}}(s, t) as:v^{\\boldsymbol{\\pi}}(s, t) \\triangleq \\mathbb{E}\\left[\\sum_{k=t}^{N-1} r_t(S_k, A_k) + r_N(S_N) \\mid S_t = s\\right]\n\nwhere S_t is the state at time t, A_t is the action taken at time t, and r_t is the reward function. For simplicity, we write v^{\\boldsymbol{\\pi}}(s) to denote v^{\\boldsymbol{\\pi}}(s, 1), the value of following policy \\boldsymbol{\\pi} from state s at the first stage over the entire horizon N.\n\nIn finite-horizon MDPs, our goal is to identify an optimal policy, denoted by \\boldsymbol{\\pi}^*, that maximizes total expected reward over the horizon N. Specifically:v^{\\boldsymbol{\\pi}^*}(s) \\geq v^{\\boldsymbol{\\pi}}(s), \\quad \\forall s \\in \\mathcal{S}, \\quad \\forall \\boldsymbol{\\pi} \\in \\Pi^{\\text{HR}}\n\nWe call \\boldsymbol{\\pi}^* an optimal policy because it yields the highest possible value across all states and all policies within the policy class \\Pi^{\\text{HR}}. We denote by v^* the maximum value achievable by any policy:v^*(s) = \\max_{\\boldsymbol{\\pi} \\in \\Pi^{\\text{HR}}} v^{\\boldsymbol{\\pi}}(s), \\quad \\forall s \\in \\mathcal{S}\n\nIn reinforcement learning literature, v^* is typically referred to as the “optimal value function,” while in some operations research references, it might be called the “value of an MDP.” An optimal policy \\boldsymbol{\\pi}^* is one for which its value function equals the optimal value function:v^{\\boldsymbol{\\pi}^*}(s) = v^*(s), \\quad \\forall s \\in \\mathcal{S}\n\nThis notion of optimality applies to every state. Policies optimal in this sense are sometimes called “uniformly optimal policies.” A weaker notion of optimality, often encountered in reinforcement learning practice, is optimality with respect to an initial distribution of states. In this case, we seek a policy \\boldsymbol{\\pi} \\in \\Pi^{\\text{HR}} that maximizes:\\sum_{s \\in \\mathcal{S}} v^{\\boldsymbol{\\pi}}(s) P_1(S_1 = s)\n\nwhere P_1(S_1 = s) is the probability of starting in state s.\n\nThe maximum value can be achieved by searching over the space of deterministic Markovian Policies. Consequently:v^*(s) = \\max_{\\boldsymbol{\\pi} \\in \\Pi^{\\mathrm{HR}}} v^{\\boldsymbol{\\pi}}(s) = \\max _{\\boldsymbol{\\pi} \\in \\Pi^{M D}} v^{\\boldsymbol{\\pi}}(s), \\quad s \\in S\n\nThis equality significantly simplifies the computational complexity of our algorithms, as the search problem can now be decomposed into N sub-problems in which we only have to search over the set of possible actions. This is the backward induction algorithm, which we present a second time, but departing this time from the control-theoretic notation and using the MDP formalism:\n\nBackward Induction\n\nInput: State space S, Action space A, Transition probabilities p_t, Reward function r_t, Time horizon N\n\nOutput: Optimal value functions v^*\n\nInitialize:\n\nSet t = N\n\nFor all s_N \\in S:v^*(s_N, N) = r_N(s_N)\n\nFor t = N-1 to 1:\n\nFor each s_t \\in S:\na. Compute the optimal value function:v^*(s_t, t) = \\max_{a \\in A_{s_t}} \\left\\{r_t(s_t, a) + \\sum_{j \\in S} p_t(j | s_t, a) v^*(j, t+1)\\right\\}\n\nb. Determine the set of optimal actions:A_{s_t,t}^* = \\arg\\max_{a \\in A_{s_t}} \\left\\{r_t(s_t, a) + \\sum_{j \\in S} p_t(j | s_t, a) v^*(j, t+1)\\right\\}\n\nReturn the optimal value functions u_t^* and optimal action sets A_{s_t,t}^* for all t and s_t\n\nNote that the same procedure can also be used for finding the value of a policy with minor changes;\n\nPolicy Evaluation\n\nInput:\n\nState space S\n\nAction space A\n\nTransition probabilities p_t\n\nReward function r_t\n\nTime horizon N\n\nA markovian deterministic policy \\boldsymbol{\\pi} = (\\pi_1, \\ldots, \\pi_{N-1})\n\nOutput: Value function v^{\\boldsymbol{\\pi}} for policy \\boldsymbol{\\pi}\n\nInitialize:\n\nSet t = N\n\nFor all s_N \\in S:v^{\\boldsymbol{\\pi}}(s_N, N) = r_N(s_N)\n\nFor t = N-1 to 1:\n\nFor each s_t \\in S:\na. Compute the value function for the given policy:v^{\\boldsymbol{\\pi}}(s_t, t) = r_t(s_t, \\pi_t(s_t)) + \\sum_{j \\in S} p_t(j | s_t, \\pi_t(s_t)) v^{\\boldsymbol{\\pi}}(j, t+1)\n\nReturn the value function v^{\\boldsymbol{\\pi}}(s_t, t) for all t and s_t\n\nThis code could also finally be adapted to support randomized policies using:v^{\\boldsymbol{\\pi}}(s_t, t) = \\sum_{a_t \\in \\mathcal{A}_{s_t}} \\pi_t(a_t \\mid s_t) \\left( r_t(s_t, a_t) + \\sum_{j \\in S} p_t(j | s_t, a_t) v^{\\boldsymbol{\\pi}}(j, t+1) \\right)","type":"content","url":"/dp#what-is-an-optimal-policy","position":29},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Example: Sample Size Determination in Pharmaceutical Development","lvl2":"Markov Decision Process Formulation"},"type":"lvl3","url":"/dp#example-sample-size-determination-in-pharmaceutical-development","position":30},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Example: Sample Size Determination in Pharmaceutical Development","lvl2":"Markov Decision Process Formulation"},"content":"Pharmaceutical development is the process of bringing a new drug from initial discovery to market availability. This process is lengthy, expensive, and risky, typically involving several stages:\n\nDrug Discovery: Identifying a compound that could potentially treat a disease.\n\nPreclinical Testing: Laboratory and animal testing to assess safety and efficacy.\n. Clinical Trials: Testing the drug in humans, divided into phases:\n\nPhase I: Testing for safety in a small group of healthy volunteers.\n\nPhase II: Testing for efficacy and side effects in a larger group with the target condition.\n\nPhase III: Large-scale testing to confirm efficacy and monitor side effects.\n\nRegulatory Review: Submitting a New Drug Application (NDA) for approval.\n\nPost-Market Safety Monitoring: Continuing to monitor the drug’s effects after market release.\n\nThis process can take 10-15 years and cost over $1 billion \n\nAdams & Brantner (2009). The high costs and risks involved call for a principled approach to decision making. We’ll focus on the clinical trial phases and NDA approval, per the MDP model presented by \n\nChang (2010):\n\nStates (S): Our state space is S = \\{s_1, s_2, s_3, s_4\\}, where:\n\ns_1: Phase I clinical trial\n\ns_2: Phase II clinical trial\n\ns_3: Phase III clinical trial\n\ns_4: NDA approval\n\nActions (A): At each state, the action is choosing the sample size n_i for the corresponding clinical trial. The action space is A = \\{10, 11, ..., 1000\\}, representing possible sample sizes.\n\nTransition Probabilities (P): The probability of moving from one state to the next depends on the chosen sample size and the inherent properties of the drug.\nWe define:\n\nP(s_2|s_1, n_1) = p_{12}(n_1) = \\sum_{i=0}^{\\lfloor\\eta_1 n_1\\rfloor} \\binom{n_1}{i} p_0^i (1-p_0)^{n_1-i}\nwhere p_0 is the true toxicity rate and \\eta_1 is the toxicity threshold for Phase I.\n\nOf particular interest is the transition from Phase II to Phase III which we model as:\n\nP(s_3|s_2, n_2) = p_{23}(n_2) = \\Phi\\left(\\frac{\\sqrt{n_2}}{2}\\delta - z_{1-\\eta_2}\\right)\n\nwhere \\Phi is the cumulative distribution function (CDF) of the standard normal distribution:\n\n\\Phi(x) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^x e^{-t^2/2} dt\n\nThis is giving us the probability that we would observe a treatment effect this large or larger if the null hypothesis (no treatment effect) were true. A higher probability indicates stronger evidence of a treatment effect, making it more likely that the drug will progress to Phase III.\n\nIn this expression, \\delta is called the “normalized treatment effect”. In clinical trials, we’re often interested in the difference between the treatment and control groups. The “normalized” part means we’ve adjusted this difference for the variability in the data. Specifically \\delta = \\frac{\\mu_t - \\mu_c}{\\sigma} where \\mu_t is the mean outcome in the treatment group, \\mu_c is the mean outcome in the control group, and \\sigma is the standard deviation of the outcome. A larger \\delta indicates a stronger treatment effect.\n\nFurthermore, the term z_{1-\\eta_2} is the (1-\\eta_2)-quantile of the standard normal distribution. In other words, it’s the value where the probability of a standard normal random variable being greater than this value is \\eta_2. For example, if \\eta_2 = 0.05, then z_{1-\\eta_2} \\approx 1.645. A smaller \\eta_2 makes the trial more conservative, requiring stronger evidence to proceed to Phase III.\n\nFinally, n_2 is the sample size for Phase II. The \\sqrt{n_2} term reflects that the precision of our estimate of the treatment effect improves with the square root of the sample size.\n\nP(s_4|s_3, n_3) = p_{34}(n_3) = \\Phi\\left(\\frac{\\sqrt{n_3}}{2}\\delta - z_{1-\\eta_3}\\right)\nwhere \\eta_3 is the significance level for Phase III.\n\nRewards (R): The reward function captures the costs of running trials and the potential profit from a successful drug:\n\nr(s_i, n_i) = -c_i(n_i) for i = 1, 2, 3, where c_i(n_i) is the cost of running a trial with sample size n_i.\n\nr(s_4) = g_4, where g_4 is the expected profit from a successful drug.\n\nDiscount Factor (\\gamma): We use a discount factor 0 < \\gamma \\leq 1 to account for the time value of money and risk preferences.\n\nimport numpy as np\nfrom scipy.stats import binom\nfrom scipy.stats import norm\n\ndef binomial_pmf(k, n, p):\n    return binom.pmf(k, n, p)\n\ndef transition_prob_phase1(n1, eta1, p0):\n    return np.sum([binomial_pmf(i, n1, p0) for i in range(int(eta1 * n1) + 1)])\n\ndef transition_prob_phase2(n2, eta2, delta):\n    return norm.cdf((np.sqrt(n2) / 2) * delta - norm.ppf(1 - eta2))\n\ndef transition_prob_phase3(n3, eta3, delta):\n    return norm.cdf((np.sqrt(n3) / 2) * delta - norm.ppf(1 - eta3))\n\ndef immediate_reward(n):\n    return -n  # Negative to represent cost\n\ndef backward_induction(S, A, gamma, g4, p0, delta, eta1, eta2, eta3):\n    V = np.zeros(len(S))\n    V[3] = g4  # Value for NDA approval state\n    optimal_n = [None] * 3  # Store optimal n for each phase\n\n    # Backward induction\n    for i in range(2, -1, -1):  # Iterate backwards from Phase III to Phase I\n        max_value = -np.inf\n        for n in A:\n            if i == 0:  # Phase I\n                p = transition_prob_phase1(n, eta1, p0)\n            elif i == 1:  # Phase II\n                p = transition_prob_phase2(n, eta2, delta)\n            else:  # Phase III\n                p = transition_prob_phase3(n, eta3, delta)\n            value = immediate_reward(n) + gamma * p * V[i+1]\n            if value > max_value:\n                max_value = value\n                optimal_n[i] = n\n        V[i] = max_value\n\n    return V, optimal_n\n\n# Set up the problem parameters\nS = ['Phase I', 'Phase II', 'Phase III', 'NDA approval']\nA = range(10, 1001)\ngamma = 0.95\ng4 = 10000\np0 = 0.1  # Example toxicity rate for Phase I\ndelta = 0.5  # Example normalized treatment difference\neta1, eta2, eta3 = 0.2, 0.1, 0.025\n\n# Run the backward induction algorithm\nV, optimal_n = backward_induction(S, A, gamma, g4, p0, delta, eta1, eta2, eta3)\n\n# Print results\nfor i, state in enumerate(S):\n    print(f\"Value for {state}: {V[i]:.2f}\")\nprint(f\"Optimal sample sizes: Phase I: {optimal_n[0]}, Phase II: {optimal_n[1]}, Phase III: {optimal_n[2]}\")\n\n# Sanity checks\nprint(\"\\nSanity checks:\")\nprint(f\"1. NDA approval value: {V[3]}\")\nprint(f\"2. All values non-positive and <= NDA value: {all(v <= V[3] for v in V)}\")\nprint(f\"3. Optimal sample sizes in range: {all(10 <= n <= 1000 for n in optimal_n if n is not None)}\")\n\n","type":"content","url":"/dp#example-sample-size-determination-in-pharmaceutical-development","position":31},{"hierarchy":{"lvl1":"Dynamic Programming","lvl2":"Infinite-Horizon MDPs"},"type":"lvl2","url":"/dp#infinite-horizon-mdps","position":32},{"hierarchy":{"lvl1":"Dynamic Programming","lvl2":"Infinite-Horizon MDPs"},"content":"It often makes sense to model control problems over infinite horizons. We extend the previous setting and define the expected total reward of policy \\boldsymbol{\\pi} \\in \\Pi^{\\mathrm{HR}}, v^{\\boldsymbol{\\pi}} as:v^{\\boldsymbol{\\pi}}(s) = \\mathbb{E}\\left[\\sum_{t=1}^{\\infty} r(S_t, A_t)\\right]\n\nOne drawback of this model is that we could easily encounter values that are +\\infty or -\\infty, even in a setting as simple as a single-state MDP which loops back into itself and where the accrued reward is nonzero.\n\nTherefore, it is often more convenient to work with an alternative formulation which guarantees the existence of a limit: the expected total discounted reward of policy \\boldsymbol{\\pi} \\in \\Pi^{\\mathrm{HR}} is defined to be:v_\\gamma^{\\boldsymbol{\\pi}}(s) \\equiv \\lim_{N \\rightarrow \\infty} \\mathbb{E}\\left[\\sum_{t=1}^N \\gamma^{t-1} r(S_t, A_t)\\right]\n\nfor 0 \\leq \\gamma < 1 and when \\max_{s \\in \\mathcal{S}} \\max_{a \\in \\mathcal{A}_s}|r(s, a)| = R_{\\max} < \\infty, in which case, |v_\\gamma^{\\boldsymbol{\\pi}}(s)| \\leq (1-\\gamma)^{-1} R_{\\max}.\n\nFinally, another possibility for the infinite-horizon setting is the so-called average reward or gain of policy \\boldsymbol{\\pi} \\in \\Pi^{\\mathrm{HR}} defined as:g^{\\boldsymbol{\\pi}}(s) \\equiv \\lim_{N \\rightarrow \\infty} \\frac{1}{N} \\mathbb{E}\\left[\\sum_{t=1}^N r(S_t, A_t)\\right]\n\nWe won’t be working with this formulation in this course due to its inherent practical and theoretical complexities.\n\nExtending the previous notion of optimality from finite-horizon models, a policy \\boldsymbol{\\pi}^* is said to be discount optimal for a given \\gamma if:v_\\gamma^{\\boldsymbol{\\pi}^*}(s) \\geq v_\\gamma^{\\boldsymbol{\\pi}}(s) \\quad \\text { for each } s \\in S \\text { and all } \\boldsymbol{\\pi} \\in \\Pi^{\\mathrm{HR}}\n\nFurthermore, the value of a discounted MDP v_\\gamma^*(s), is defined by:v_\\gamma^*(s) \\equiv \\max _{\\boldsymbol{\\pi} \\in \\Pi^{\\mathrm{HR}}} v_\\gamma^{\\boldsymbol{\\pi}}(s)\n\nMore often, we refer to v_\\gamma by simply calling it the optimal value function.\n\nAs for the finite-horizon setting, the infinite horizon discounted model does not require history-dependent policies, since for any \\boldsymbol{\\pi} \\in \\Pi^{HR} there exists a \\boldsymbol{\\pi}^{\\prime} \\in \\Pi^{MR} with identical total discounted reward:v_\\gamma^*(s) \\equiv \\max_{\\boldsymbol{\\pi} \\in \\Pi^{HR}} v_\\gamma^{\\boldsymbol{\\pi}}(s)=\\max_{\\boldsymbol{\\pi} \\in \\Pi^{MR}} v_\\gamma^{\\boldsymbol{\\pi}}(s) .","type":"content","url":"/dp#infinite-horizon-mdps","position":33},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Random Horizon Interpretation of Discounting","lvl2":"Infinite-Horizon MDPs"},"type":"lvl3","url":"/dp#random-horizon-interpretation-of-discounting","position":34},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Random Horizon Interpretation of Discounting","lvl2":"Infinite-Horizon MDPs"},"content":"The use of discounting can be motivated both from a modeling perspective and as a means to ensure that the total reward remains bounded. From the modeling perspective, we can view discounting as a way to weight more or less importance on the immediate rewards vs. the long-term consequences. There is also another interpretation which stems from that of a finite horizon model but with an uncertain end time. More precisely:\n\nLet v_\\nu^{\\boldsymbol{\\pi}}(s) denote the expected total reward obtained by using policy \\boldsymbol{\\pi} when the horizon length \\nu is random. We define it by:v_\\nu^{\\boldsymbol{\\pi}}(s) \\equiv \\mathbb{E}_s^{\\boldsymbol{\\pi}}\\left[\\mathbb{E}_\\nu\\left\\{\\sum_{t=1}^\\nu r(S_t, A_t)\\right\\}\\right]\n\nRandom horizon interpretation of discounting\n\nSuppose that the horizon \\nu follows a geometric distribution with parameter \\gamma, 0 \\leq \\gamma < 1, independent of the policy such that\nP(\\nu=n) = (1-\\gamma) \\gamma^{n-1}, \\, n=1,2, \\ldots, then v_\\nu^{\\boldsymbol{\\pi}}(s) = v_\\gamma^{\\boldsymbol{\\pi}}(s) for all s \\in \\mathcal{S} .\n\nSee proposition 5.3.1 in \n\nPuterman (1994).\n\nBy definition of the finite-horizon value function and the law of total expectation:v_\\nu^{\\boldsymbol{\\pi}}(s) = \\sum_{n=1}^{\\infty} P(\\nu=n) \\cdot v_n^{\\boldsymbol{\\pi}}(s) = \\sum_{n=1}^{\\infty} (1-\\gamma) \\gamma^{n-1} \\cdot E_s^{\\boldsymbol{\\pi}} \\left\\{\\sum_{t=1}^n r(S_t, A_t)\\right\\}.\n\nCombining the expectation with the sum over n:v_\\nu^{\\boldsymbol{\\pi}}(s) = E_s^{\\boldsymbol{\\pi}} \\left\\{\\sum_{n=1}^{\\infty} (1-\\gamma) \\gamma^{n-1} \\sum_{t=1}^n r(S_t, A_t)\\right\\}.\n\nReordering the summations: Under the bounded reward assumption |r(s,a)| \\leq R_{\\max} and \\gamma < 1, we haveE_s^{\\boldsymbol{\\pi}} \\left\\{\\sum_{n=1}^{\\infty} \\sum_{t=1}^n |r(S_t, A_t)| \\cdot (1-\\gamma) \\gamma^{n-1}\\right\\} \\leq R_{\\max} \\sum_{n=1}^{\\infty} n (1-\\gamma) \\gamma^{n-1} = \\frac{R_{\\max}}{1-\\gamma} < \\infty,\n\nwhich justifies exchanging the order of summation by Fubini’s theorem.\n\nTo reverse the order, note that the pair (n,t) with 1 \\leq t \\leq n can be reindexed by fixing t first and letting n range from t to \\infty:\\sum_{n=1}^{\\infty} \\sum_{t=1}^n = \\sum_{t=1}^{\\infty} \\sum_{n=t}^{\\infty}.\n\nTherefore:\\begin{align*}\nv_\\nu^{\\boldsymbol{\\pi}}(s) &= E_s^{\\boldsymbol{\\pi}} \\left\\{\\sum_{t=1}^{\\infty} r(S_t, A_t) \\sum_{n=t}^{\\infty} (1-\\gamma) \\gamma^{n-1}\\right\\}.\n\\end{align*}\n\nEvaluating the inner sum: Using the substitution m = n - t + 1 (so n = m + t - 1):\\begin{align*}\n\\sum_{n=t}^{\\infty} (1-\\gamma) \\gamma^{n-1} &= \\sum_{m=1}^{\\infty} (1-\\gamma) \\gamma^{m+t-2} \\\\\n&= \\gamma^{t-1} (1-\\gamma) \\sum_{m=1}^{\\infty} \\gamma^{m-1} \\\\\n&= \\gamma^{t-1} (1-\\gamma) \\cdot \\frac{1}{1-\\gamma} = \\gamma^{t-1}.\n\\end{align*}\n\nSubstituting back:v_\\nu^{\\boldsymbol{\\pi}}(s) = E_s^{\\boldsymbol{\\pi}} \\left\\{\\sum_{t=1}^{\\infty} \\gamma^{t-1} r(S_t, A_t)\\right\\} = v_\\gamma^{\\boldsymbol{\\pi}}(s).","type":"content","url":"/dp#random-horizon-interpretation-of-discounting","position":35},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Vector Representation in Markov Decision Processes","lvl2":"Infinite-Horizon MDPs"},"type":"lvl3","url":"/dp#vector-representation-in-markov-decision-processes","position":36},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Vector Representation in Markov Decision Processes","lvl2":"Infinite-Horizon MDPs"},"content":"Let V be the set of bounded real-valued functions on a discrete state space S. This means any function  f \\in V  satisfies the condition:\\|f\\| = \\max_{s \\in S} |f(s)| < \\infty.\n\nwhere notation  \\|f\\|  represents the sup-norm (or  \\ell_\\infty -norm) of the function  f .\n\nWhen working with discrete state spaces, we can interpret elements of V as vectors and linear operators on V as matrices, allowing us to leverage tools from linear algebra. The sup-norm (\\ell_\\infty norm) of matrix \\mathbf{H} is defined as:\\|\\mathbf{H}\\| \\equiv \\max_{s \\in S} \\sum_{j \\in S} |\\mathbf{H}_{s,j}|\n\nwhere \\mathbf{H}_{s,j} represents the (s, j)-th component of the matrix \\mathbf{H}.\n\nFor a Markovian decision rule \\pi \\in \\Pi^{MD}, we define:\\begin{align*}\n\\mathbf{r}_\\pi(s) &\\equiv r(s, \\pi(s)), \\quad \\mathbf{r}_\\pi \\in \\mathbb{R}^{|S|}, \\\\\n[\\mathbf{P}_\\pi]_{s,j} &\\equiv p(j \\mid s, \\pi(s)), \\quad \\mathbf{P}_\\pi \\in \\mathbb{R}^{|S| \\times |S|}.\n\\end{align*}\n\nFor a randomized decision rule \\pi \\in \\Pi^{MR}, these definitions extend to:\\begin{align*}\n\\mathbf{r}_\\pi(s) &\\equiv \\sum_{a \\in A_s} \\pi(a \\mid s) \\, r(s, a), \\\\\n[\\mathbf{P}_\\pi]_{s,j} &\\equiv \\sum_{a \\in A_s} \\pi(a \\mid s) \\, p(j \\mid s, a).\n\\end{align*}\n\nIn both cases, \\mathbf{r}_\\pi denotes a reward vector in \\mathbb{R}^{|S|}, with each component \\mathbf{r}_\\pi(s) representing the reward associated with state s. Similarly, \\mathbf{P}_\\pi is a transition probability matrix in \\mathbb{R}^{|S| \\times |S|}, capturing the transition probabilities under decision rule \\pi.\n\nFor a nonstationary Markovian policy \\boldsymbol{\\pi} = (\\pi_1, \\pi_2, \\ldots) \\in \\Pi^{MR}, the expected total discounted reward is given by:\\mathbf{v}_\\gamma^{\\boldsymbol{\\pi}}(s)=\\mathbb{E}\\left[\\sum_{t=1}^{\\infty} \\gamma^{t-1} r\\left(S_t, A_t\\right) \\,\\middle|\\, S_1 = s\\right].\n\nUsing vector notation, this can be expressed as:\\begin{aligned}\n\\mathbf{v}_\\gamma^{\\boldsymbol{\\pi}} &= \\sum_{t=1}^{\\infty} \\gamma^{t-1} \\mathbf{P}_{\\boldsymbol{\\pi}}^{t-1} \\mathbf{r}_{\\pi_1} \\\\\n&= \\mathbf{r}_{\\pi_1} + \\gamma \\mathbf{P}_{\\pi_1} \\mathbf{r}_{\\pi_2} + \\gamma^2 \\mathbf{P}_{\\pi_1} \\mathbf{P}_{\\pi_2} \\mathbf{r}_{\\pi_3} + \\cdots \\\\\n&= \\mathbf{r}_{\\pi_1} + \\gamma \\mathbf{P}_{\\pi_1} \\left( \\mathbf{r}_{\\pi_2} + \\gamma \\mathbf{P}_{\\pi_2} \\mathbf{r}_{\\pi_3} + \\gamma^2 \\mathbf{P}_{\\pi_2} \\mathbf{P}_{\\pi_3} \\mathbf{r}_{\\pi_4} + \\cdots \\right).\n\\end{aligned}\n\nThis formulation leads to a recursive relationship:\\begin{align*}\n\\mathbf{v}_\\gamma^{\\boldsymbol{\\pi}} &= \\mathbf{r}_{\\pi_1} + \\gamma \\mathbf{P}_{\\pi_1} \\mathbf{v}_\\gamma^{\\boldsymbol{\\pi}^{\\prime}}\\\\\n&=\\sum_{t=1}^{\\infty} \\gamma^{t-1} \\mathbf{P}_{\\boldsymbol{\\pi}}^{t-1} \\mathbf{r}_{\\pi_t}\n\\end{align*}\n\nwhere \\boldsymbol{\\pi}^{\\prime} = (\\pi_2, \\pi_3, \\ldots).\n\nFor a stationary policy \\boldsymbol{\\pi} = \\mathrm{const}(\\pi) with constant decision rule \\pi, the total expected reward simplifies to:\\begin{align*}\n\\mathbf{v}_\\gamma^{\\pi} &= \\mathbf{r}_\\pi+ \\gamma \\mathbf{P}_\\pi \\mathbf{v}_\\gamma^{\\pi} \\\\\n&=\\sum_{t=1}^{\\infty} \\gamma^{t-1} \\mathbf{P}_\\pi^{t-1} \\mathbf{r}_{\\pi}\n\\end{align*}\n\nThis last expression is called a Neumann series expansion, and it’s guaranteed to exists under the assumptions of bounded reward and discount factor strictly less than one.\n\nNeumann Series and Invertibility\n\nThe spectral radius of a matrix \\mathbf{H} is defined as:\\rho(\\mathbf{H}) \\equiv \\max_{i} |\\lambda_i(\\mathbf{H})|\n\nwhere \\lambda_i(\\mathbf{H}) are the eigenvalues of \\mathbf{H}.\n\nNeumann Series Existence: For any matrix \\mathbf{H}, the Neumann series\\sum_{t=0}^{\\infty} \\mathbf{H}^t = \\mathbf{I} + \\mathbf{H} + \\mathbf{H}^2 + \\cdots\n\nconverges if and only if \\rho(\\mathbf{H}) < 1. When this condition holds, the matrix (\\mathbf{I} - \\mathbf{H}) is invertible and(\\mathbf{I} - \\mathbf{H})^{-1} = \\sum_{t=0}^{\\infty} \\mathbf{H}^t.\n\nNote that for any induced matrix norm \\|\\cdot\\| (i.e., a norm satisfying \\|\\mathbf{H}\\mathbf{v}\\| \\leq \\|\\mathbf{H}\\| \\cdot \\|\\mathbf{v}\\| for all vectors \\mathbf{v}) and any matrix \\mathbf{H}, the spectral radius is bounded by:\\rho(\\mathbf{H}) \\leq \\|\\mathbf{H}\\|.\n\nThis inequality provides a practical way to verify the convergence condition \\rho(\\mathbf{H}) < 1 by checking the simpler condition \\|\\mathbf{H}\\| < 1 rather than trying to compute the eigenvalues directly.\n\nWe can now verify that (\\mathbf{I} - \\gamma \\mathbf{P}_d) is invertible and the Neumann series converges.\n\nNorm of the transition matrix: Since \\mathbf{P}_d is a stochastic matrix (each row sums to 1 and all entries are non-negative), its \\ell_\\infty-norm is:\\|\\mathbf{P}_d\\| = \\max_{s \\in S} \\sum_{j \\in S} [\\mathbf{P}_d]_{s,j} = \\max_{s \\in S} 1 = 1.\n\nNorm of the scaled matrix: Using the homogeneity property of norms, we have:\\|\\gamma \\mathbf{P}_d\\| = |\\gamma| \\cdot \\|\\mathbf{P}_d\\| = |\\gamma| \\cdot 1 = |\\gamma|.\n\nBounding the spectral radius: Since the spectral radius is bounded by the matrix norm:\\rho(\\gamma \\mathbf{P}_d) \\leq \\|\\gamma \\mathbf{P}_d\\| = |\\gamma|.\n\nVerifying convergence: Since 0 \\leq \\gamma < 1 by assumption, we have:\\rho(\\gamma \\mathbf{P}_d) \\leq |\\gamma| < 1.\n\nThis strict inequality guarantees that (\\mathbf{I} - \\gamma \\mathbf{P}_d) is invertible and the Neumann series converges.\n\nTherefore, the Neumann series expansion converges and yields:\\mathbf{v}_\\gamma^{d^\\infty} = (\\mathbf{I} - \\gamma \\mathbf{P}_d)^{-1} \\mathbf{r}_d = \\sum_{t=0}^{\\infty} (\\gamma \\mathbf{P}_d)^t \\mathbf{r}_d = \\sum_{t=1}^{\\infty} \\gamma^{t-1} \\mathbf{P}_d^{t-1} \\mathbf{r}_d.\n\nConsequently, for a stationary policy, \\mathbf{v}_\\gamma^{d^\\infty} can be determined as the solution to the linear equation:\\mathbf{v} = \\mathbf{r}_d+ \\gamma \\mathbf{P}_d\\mathbf{v},\n\nwhich can be rearranged to:(\\mathbf{I} - \\gamma \\mathbf{P}_d) \\mathbf{v} = \\mathbf{r}_d.\n\nWe can also characterize \\mathbf{v}_\\gamma^{d^\\infty} as the solution to an operator equation. More specifically, define the transformation \\mathrm{L}_d by\\mathrm{L}_d \\mathbf{v} \\equiv \\mathbf{r}_d+\\gamma \\mathbf{P}_d\\mathbf{v}\n\nfor any \\mathbf{v} \\in V. Intuitively, \\mathrm{L}_d takes a value function \\mathbf{v} as input and returns a new value function that combines immediate rewards (\\mathbf{r}_d) with discounted future values (\\gamma \\mathbf{P}_d\\mathbf{v}).\n\nNote\n\nWhile we often refer to \\mathrm{L}_d as a “linear operator” in the RL literature, it is technically an affine operator (or affine transformation), not a linear operator in the strict sense. To see why, recall that a linear operator \\mathcal{T} must satisfy:\n\nAdditivity: \\mathcal{T}(\\mathbf{v}_1 + \\mathbf{v}_2) = \\mathcal{T}(\\mathbf{v}_1) + \\mathcal{T}(\\mathbf{v}_2)\n\nHomogeneity: \\mathcal{T}(\\alpha \\mathbf{v}) = \\alpha \\mathcal{T}(\\mathbf{v}) for all scalars \\alpha\n\nHowever, \\mathrm{L}_d fails the additivity test:\\mathrm{L}_d(\\mathbf{v}_1 + \\mathbf{v}_2) = \\mathbf{r}_d + \\gamma \\mathbf{P}_d(\\mathbf{v}_1 + \\mathbf{v}_2) = \\mathbf{r}_d + \\gamma \\mathbf{P}_d\\mathbf{v}_1 + \\gamma \\mathbf{P}_d\\mathbf{v}_2\n\nwhile\\mathrm{L}_d(\\mathbf{v}_1) + \\mathrm{L}_d(\\mathbf{v}_2) = (\\mathbf{r}_d + \\gamma \\mathbf{P}_d\\mathbf{v}_1) + (\\mathbf{r}_d + \\gamma \\mathbf{P}_d\\mathbf{v}_2) = 2\\mathbf{r}_d + \\gamma \\mathbf{P}_d\\mathbf{v}_1 + \\gamma \\mathbf{P}_d\\mathbf{v}_2.\n\nThe presence of the constant term \\mathbf{r}_d makes \\mathrm{L}_d affine rather than linear. An affine operator has the form \\mathcal{A}(\\mathbf{v}) = \\mathbf{b} + \\mathcal{T}(\\mathbf{v}), where \\mathbf{b} is a constant vector and \\mathcal{T} is a linear operator. In our case, \\mathbf{b} = \\mathbf{r}_d and \\mathcal{T}(\\mathbf{v}) = \\gamma \\mathbf{P}_d\\mathbf{v}.\n\nDespite this technical distinction, the term “linear operator” is commonly used in the reinforcement learning literature when referring to \\mathrm{L}_d, following a slight abuse of terminology.\n\nTherefore, we view \\mathrm{L}_d as an operator mapping elements of V to V: i.e., \\mathrm{L}_d: V \\rightarrow V. The fact that the value function of a policy is the solution to a fixed-point equation can then be expressed with the statement:\\mathbf{v}_\\gamma^{d^\\infty}=\\mathrm{L}_d \\mathbf{v}_\\gamma^{d^\\infty}.\n\nThis is a fixed-point equation: the value function \\mathbf{v}_\\gamma^{d^\\infty} is a fixed point of the operator \\mathrm{L}_d.","type":"content","url":"/dp#vector-representation-in-markov-decision-processes","position":37},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Solving Operator Equations","lvl2":"Infinite-Horizon MDPs"},"type":"lvl3","url":"/dp#solving-operator-equations","position":38},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Solving Operator Equations","lvl2":"Infinite-Horizon MDPs"},"content":"The operator equation we encountered in MDPs, \\mathbf{v}_\\gamma^{d^\\infty} = \\mathrm{L}_d \\mathbf{v}_\\gamma^{d^\\infty}, is a specific instance of a more general class of problems known as operator equations. These equations appear in various fields of mathematics and applied sciences, ranging from differential equations to functional analysis.\n\nOperator equations can take several forms, each with its own characteristics and solution methods:\n\nFixed Point Form: x = \\mathrm{T}(x), where \\mathrm{T}: X \\rightarrow X.\nCommon in fixed-point problems, such as our MDP equation, we seek a fixed point x^* such that x^* = \\mathrm{T}(x^*).\n\nGeneral Operator Equation: \\mathrm{T}(x) = y, where \\mathrm{T}: X \\rightarrow Y.\nHere, X and Y can be different spaces. We seek an x \\in X that satisfies the equation for a given y \\in Y.\n\nNonlinear Equation: \\mathrm{T}(x) = 0, where \\mathrm{T}: X \\rightarrow Y.\nA special case of the general operator equation where we seek roots or zeros of the operator.\n\nVariational Inequality: Find x^* \\in K such that \\langle \\mathrm{T}(x^*), x - x^* \\rangle \\geq 0 for all x \\in K.\nHere, K is a closed convex subset of X, and \\mathrm{T}: K \\rightarrow X^* (the dual space of X). These problems often arise in optimization, game theory, and partial differential equations.","type":"content","url":"/dp#solving-operator-equations","position":39},{"hierarchy":{"lvl1":"Dynamic Programming","lvl4":"Successive Approximation Method","lvl3":"Solving Operator Equations","lvl2":"Infinite-Horizon MDPs"},"type":"lvl4","url":"/dp#successive-approximation-method","position":40},{"hierarchy":{"lvl1":"Dynamic Programming","lvl4":"Successive Approximation Method","lvl3":"Solving Operator Equations","lvl2":"Infinite-Horizon MDPs"},"content":"For equations in fixed point form, a common numerical solution method is successive approximation, also known as fixed-point iteration:\n\nSuccessive Approximation\n\nInput: An operator \\mathrm{T}: X \\rightarrow X, an initial guess x_0 \\in X, and a tolerance \\epsilon > 0Output: An approximate fixed point x^* such that \\|x^* - \\mathrm{T}(x^*)\\| < \\epsilon\n\nInitialize n = 0\n\nrepeat3. Compute x_{n+1} = \\mathrm{T}(x_n)4. If \\|x_{n+1} - x_n\\| < \\epsilon, return x_{n+1}5. Set n = n + 1\n\nuntil convergence or maximum iterations reached\n\nThe convergence of successive approximation depends on the properties of the operator \\mathrm{T}. In the simplest and most common setting, we assume \\mathrm{T} is a contraction mapping. The Banach Fixed-Point Theorem then guarantees that \\mathrm{T} has a unique fixed point, and the successive approximation method will converge to this fixed point from any starting point. Specifically, \\mathrm{T} is a contraction if there exists a constant q \\in [0,1) such that for all x,y \\in X:d(\\mathrm{T}(x), \\mathrm{T}(y)) \\leq q \\cdot d(x,y)\n\nwhere d is the metric on X. In this case, the rate of convergence is linear, with error bound:d(x_n, x^*) \\leq \\frac{q^n}{1-q} d(x_1, x_0)\n\nHowever, the contraction mapping condition is not the only one that can lead to convergence. For instance, if \\mathrm{T} is nonexpansive (i.e., Lipschitz continuous with Lipschitz constant 1) and X is a Banach space with certain geometrical properties (e.g., uniformly convex), then under additional conditions (e.g., \\mathrm{T} has at least one fixed point), the successive approximation method can still converge, albeit potentially more slowly than in the contraction case.\n\nIn practice, when dealing with specific problems like MDPs or differential equations, the properties of the operator often naturally align with one of these convergence conditions. For example, in discounted MDPs, the Bellman operator is a contraction in the supremum norm, which guarantees the convergence of value iteration.","type":"content","url":"/dp#successive-approximation-method","position":41},{"hierarchy":{"lvl1":"Dynamic Programming","lvl4":"Newton-Kantorovich Method","lvl3":"Solving Operator Equations","lvl2":"Infinite-Horizon MDPs"},"type":"lvl4","url":"/dp#newton-kantorovich-method","position":42},{"hierarchy":{"lvl1":"Dynamic Programming","lvl4":"Newton-Kantorovich Method","lvl3":"Solving Operator Equations","lvl2":"Infinite-Horizon MDPs"},"content":"The Newton-Kantorovich method is a generalization of Newton’s method from finite dimensional vector spaces to infinite dimensional function spaces: rather than iterating in the space of vectors, we are iterating in the space of functions.\n\nNewton’s method is often written as the familiar update:x_{k+1} = x_k - [DF(x_k)]^{-1} F(x_k),\n\nwhich makes it look as though the essence of the method is “take a derivative and invert it.” But the real workhorse behind Newton’s method (both in finite and infinite dimensions) is linearization.\n\nAt each step, the idea is to replace the nonlinear operator F:X \\to Y by a local surrogate model of the formF(x+h) \\approx F(x) + Lh,\n\nwhere L is a linear map capturing how small perturbations in the input propagate to changes in the output. This is a Taylor-like expansion in Banach spaces: the role of the derivative is precisely to provide the correct notion of such a linear operator.\n\nTo find a root of F, we impose the condition that the surrogate vanishes at the next iterate:0 = F(x+h) \\approx F(x) + Lh.\n\nSolving this linear equation gives the increment h. In finite dimensions, L is the Jacobian matrix; in Banach spaces, it must be the Fréchet derivative.\n\nBut what exactly is a Fréchet derivative in infinite dimensions? To understand this, we need to generalize the concept of derivative from finite-dimensional calculus. In infinite-dimensional spaces, there are several notions of differentiability, each with different strengths and requirements:\n\n1. Gâteaux (Directional) Derivative\n\nWe say that the Gâteaux derivative of F at x in a specific direction h is:F'(x; h) = \\lim_{t \\to 0} \\frac{F(x + th) - F(x)}{t}\n\nThis quantity measures how the function F changes along the ray x + th. While this limit may exist for each direction h separately, it doesn’t guarantee that the derivative is linear in h. This is a key limitation: the Gâteaux derivative can exist in all directions but still fail to provide a good linear approximation.\n\n2. Hadamard Directional Derivative\n\nRather than considering a single direction of perturbation, we now consider a bundle of perturbations around h. We ask how the function changes as we approach the target direction from nearby directions. We say that F has a Hadamard directional derivative if:F'(x; h) = \\lim_{\\substack{t \\downarrow 0 \\\\ h' \\to h}} \\frac{F(x + t h') - F(x)}{t}\n\nThis is a stronger condition than Gâteaux differentiability because it requires the limit to be uniform over nearby directions. However, it still doesn’t guarantee linearity in h.\n\n3. Fréchet Derivative\n\nThe strongest and most natural notion: F is Fréchet differentiable at x if there exists a bounded linear operator L such that:\\lim_{h \\to 0} \\frac{\\|F(x + h) - F(x) - Lh\\|}{\\|h\\|} = 0\n\nThis definition directly addresses the inadequacy of the previous notions. Unlike Gâteaux and Hadamard derivatives, the Fréchet derivative explicitly requires the existence of a linear operator L that provides a good approximation. Key properties:\n\nL must be linear in h (unlike the directional derivatives above)\n\nThe approximation error is o(\\|h\\|), uniform in all directions\n\nThis is the “true” derivative: it generalizes the Jacobian matrix to infinite dimensions\n\nNotation: L = F'(x) or DF(x)\n\nRelationship:\\text{Fréchet differentiable} \\Rightarrow \\text{Hadamard directionally diff.} \\Rightarrow \\text{Gâteaux directionally diff.}\n\nIn the context of the Newton-Kantorovich method, we work with an operator F: X \\to Y where both X and Y are Banach spaces. The Fréchet derivative F'(x) is the best linear approximation of F near x, and it’s exactly this linear operator L that we use in our linearization F(x+h) \\approx F(x) + F'(x)h.\n\nNow apart from those mathematical technicalities, Newton-Kantorovich has in essence the same structure as that of the original Newton’s method. That is, it applies the following sequence of steps:\n\nLinearize the Operator:\nGiven an approximation  x_n , we consider the Fréchet derivative of  F , denoted by  F'(x_n) . This derivative is a linear operator that provides a local approximation of  F  near  x_n .\n\nSet Up the Newton Step:\nThe method then solves the linearized equation for a correction  h_n :F'(x_n) h_n = -F(x_n).\n\nThis equation represents a linear system where  h_n  is chosen so that the linearized operator  F(x_n) + F'(x_n)h_n  equals zero.\n\nUpdate the Solution:\nThe new approximation  x_{n+1}  is then given by:x_{n+1} = x_n + h_n.\n\nThis correction step refines  x_n , bringing it closer to the true solution.\n\nRepeat Until Convergence:\nWe repeat the linearization and update steps until the solution  x_n  converges to the desired tolerance, which can be verified by checking that  \\|F(x_n)\\|  is sufficiently small, or by monitoring the norm  \\|x_{n+1} - x_n\\| .\n\nThe convergence of Newton-Kantorovich does not hinge on  F  being a contraction over the entire domain (as it could be the case for successive approximation). The convergence properties of the Newton-Kantorovich method are as follows:\n\nLocal Convergence: Under mild conditions (e.g., F is Fréchet differentiable and F'(x) is invertible near the solution), the method converges locally. This means that if the initial guess is sufficiently close to the true solution, the method will converge.\n\nGlobal Convergence: Global convergence is not guaranteed in general. However, under stronger conditions (e.g., F is analytic and satisfies certain bounds), the method can converge globally.\n\nRate of Convergence: When the method converges, it typically exhibits quadratic convergence. This means that the error at each step is proportional to the square of the error at the previous step:\\|x_{n+1} - x^*\\| \\leq C\\|x_n - x^*\\|^2\n\nwhere x^* is the true solution and C is some constant. This quadratic convergence is significantly faster than the linear convergence typically seen in methods like successive approximation.","type":"content","url":"/dp#newton-kantorovich-method","position":43},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Optimality Equations for Infinite-Horizon MDPs","lvl2":"Infinite-Horizon MDPs"},"type":"lvl3","url":"/dp#optimality-equations-for-infinite-horizon-mdps","position":44},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Optimality Equations for Infinite-Horizon MDPs","lvl2":"Infinite-Horizon MDPs"},"content":"Recall that in the finite-horizon setting, the optimality equations are:v_n(s) = \\max_{a \\in A_s} \\left\\{r(s, a) + \\gamma \\sum_{j \\in S} p(j | s, a) v_{n+1}(j)\\right\\}\n\nwhere v_n(s) is the value function at time step n for state s, A_s is the set of actions available in state s, r(s, a) is the reward function, \\gamma is the discount factor, and p(j | s, a) is the transition probability from state s to state j given action a.\n\nIntuitively, we would expect that by taking the limit of n to infinity, we might get the nonlinear equations:v(s) = \\max_{a \\in A_s} \\left\\{r(s, a) + \\gamma \\sum_{j \\in S} p(j | s, a) v(j)\\right\\}\n\nwhich are called the optimality equations or Bellman equations for infinite-horizon MDPs.\n\nWe can adopt an operator-theoretic perspective by defining operators on the space V of bounded real-valued functions on the state space S. For a deterministic Markov rule \\pi \\in \\Pi^{MD}, define the policy-evaluation operator:(\\mathrm{L}_\\pi v)(s) = r(s,\\pi(s)) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,\\pi(s)) v(j)\n\nThe Bellman optimality operator is then:\\mathrm{L} \\mathbf{v} \\equiv \\max_{\\pi \\in \\Pi^{MD}} \\left\\{\\mathbf{r}_\\pi + \\gamma \\mathbf{P}_\\pi \\mathbf{v}\\right\\}\n\nwhere \\Pi^{MD} is the set of Markov deterministic decision rules, \\mathbf{r}_\\pi is the reward vector under decision rule \\pi, and \\mathbf{P}_\\pi is the transition probability matrix under decision rule \\pi.\n\nNote that while we write \\max_{\\pi \\in \\Pi^{MD}}, we do not implement the above operator by enumerating all decision rules. Rather, the fact that we compare policies based on their value functions in a componentwise fashion means that maximizing over the space of Markovian deterministic rules reduces to the following update in component form:(\\mathrm{L} \\mathbf{v})(s) = \\max_{a \\in \\mathcal{A}_s} \\left\\{r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j)\\right\\}\n\nFor convenience, we define the greedy selector \\mathrm{Greedy}(v) \\in \\Pi^{MD} that extracts an optimal decision rule from a value function:\\mathrm{Greedy}(v)(s) \\in \\arg\\max_{a \\in \\mathcal{A}_s} \\left\\{r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j)\\right\\}\n\nIn Puterman’s terminology, such a greedy selector is called v-improving (or conserving when it achieves the maximum). This operator will be useful for expressing algorithms succinctly:\n\nValue iteration: v_{k+1} = \\mathrm{L}v_k, then extract \\pi = \\mathrm{Greedy}(v^*)\n\nPolicy iteration: \\pi_{k+1} = \\mathrm{Greedy}(v^{\\pi_k}) with v^{\\pi_k} solving v = \\mathrm{L}_{\\pi_k}v\n\nThe equivalence between these two forms can be shown mathematically, as demonstrated in the following proposition and proof.\n\nThe operator \\mathrm{L} defined as a maximization over Markov deterministic decision rules:(\\mathrm{L} \\mathbf{v})(s) = \\max_{\\pi \\in \\Pi^{MD}} \\left\\{r(s,\\pi(s)) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,\\pi(s)) v(j)\\right\\}\n\nis equivalent to the componentwise maximization over actions:(\\mathrm{L} \\mathbf{v})(s) = \\max_{a \\in \\mathcal{A}_s} \\left\\{r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j)\\right\\}\n\nFix s. LetQ_v(s,a) \\triangleq r(s,a)+\\gamma\\sum_{j}p(j\\mid s,a)\\,v(j).\n\nFor any rule \\pi \\in \\Pi^{MD}, we have (\\mathrm{L}_\\pi v)(s)=Q_v(s,\\pi(s))\\le \\max_{a\\in\\mathcal{A}_s}Q_v(s,a).\n\nTaking the maximum over \\pi gives\\max_{\\pi\\in\\Pi^{MD}}(\\mathrm{L}_\\pi v)(s) \\le \\max_{a\\in\\mathcal{A}_s}Q_v(s,a).\n\nConversely, choose a greedy selector \\pi^v\\in\\Pi^{MD} such that for each s,\\pi^v(s)\\in\\arg\\max_{a\\in\\mathcal{A}_s}Q_v(s,a)\n\n(possible since \\mathcal{A}_s is finite; otherwise use a measurable \\varepsilon-greedy selector). Then(\\mathrm{L}_{\\pi^v}v)(s)=Q_v(s,\\pi^v(s))=\\max_{a\\in\\mathcal{A}_s}Q_v(s,a),\n\nso \\max_{\\pi}(\\mathrm{L}_\\pi v)(s)\\ge \\max_{a}Q_v(s,a). Combining both inequalities yields equality.","type":"content","url":"/dp#optimality-equations-for-infinite-horizon-mdps","position":45},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Algorithms for Solving the Optimality Equations","lvl2":"Infinite-Horizon MDPs"},"type":"lvl3","url":"/dp#algorithms-for-solving-the-optimality-equations","position":46},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Algorithms for Solving the Optimality Equations","lvl2":"Infinite-Horizon MDPs"},"content":"The optimality equations are operator equations. Therefore, we can apply general numerical methods to solve them. Applying the successive approximation method to the Bellman optimality equation yields a method known as “value iteration” in dynamic programming. A direct application of the blueprint for successive approximation yields the following algorithm:\n\nValue Iteration\n\nInput Given an MDP (S, A, P, R, \\gamma) and tolerance \\varepsilon > 0\n\nOutput Compute an \\varepsilon-optimal value function v and policy \\pi\n\nInitialize v_0(s) = 0 for all s \\in S\n\nn \\leftarrow 0\n\nrepeat\n\nFor each s \\in S:\n\nv_{n+1}(s) \\leftarrow (\\mathrm{L}v_n)(s) = \\max_{a \\in A} \\left\\{r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a)v_n(j)\\right\\}\n\n\\delta \\leftarrow \\|v_{n+1} - v_n\\|_\\infty\n\nn \\leftarrow n + 1\n\nuntil \\delta < \\frac{\\varepsilon(1-\\gamma)}{2\\gamma}\n\nExtract greedy policy: \\pi \\leftarrow \\mathrm{Greedy}(v_n) where\\mathrm{Greedy}(v)(s) \\in \\arg\\max_{a \\in \\mathcal{A}_s} \\left\\{r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a)v(j)\\right\\}\n\nreturn v_n, \\pi\n\nThe termination criterion in this algorithm is based on a specific bound that provides guarantees on the quality of the solution. This is in contrast to supervised learning, where we often use arbitrary termination criteria based on computational budget or early stopping when the learning curve flattens. This is because establishing implementable generalization bounds in supervised learning is challenging.\n\nHowever, in the dynamic programming context, we can derive various bounds that can be implemented in practice. These bounds help us terminate our procedure with a guarantee on the precision of our value function and, correspondingly, on the optimality of the resulting policy.\n\nConvergence of Value Iteration\n\n(Adapted from \n\nPuterman (1994) theorem 6.3.1)\n\nLet v_0 be any initial value function, \\varepsilon > 0 a desired accuracy, and let \\{v_n\\} be the sequence of value functions generated by value iteration, i.e., v_{n+1} = \\mathrm{L}v_n for n \\geq 0, where \\mathrm{L} is the Bellman optimality operator. Then:\n\nv_n converges to the optimal value function v^*_\\gamma,\n\nThe algorithm terminates in finite time,\n\nThe resulting policy \\pi_\\varepsilon is \\varepsilon-optimal, and\n\nWhen the algorithm terminates, v_{n+1} is within \\varepsilon/2 of v^*_\\gamma.\n\nParts 1 and 2 follow directly from the fact that \\mathrm{L} is a contraction mapping. Hence, by Banach’s fixed-point theorem, it has a unique fixed point (which is v^*_\\gamma), and repeated application of \\mathrm{L} will converge to this fixed point. Moreover, this convergence happens at a geometric rate, which ensures that we reach the termination condition in finite time.\n\nTo show that the Bellman optimality operator \\mathrm{L} is a contraction mapping, we need to prove that for any two value functions v and u:\\|\\mathrm{L}v - \\mathrm{L}u\\|_\\infty \\leq \\gamma \\|v - u\\|_\\infty\n\nwhere \\gamma \\in [0,1) is the discount factor and \\|\\cdot\\|_\\infty is the supremum norm.\n\nLet’s start by writing out the definition of \\mathrm{L}v and \\mathrm{L}u:\\begin{align*}\n(\\mathrm{L}v)(s) &= \\max_{a \\in A} \\left\\{r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a)v(j)\\right\\}\\\\\n(\\mathrm{L}u)(s) &= \\max_{a \\in A} \\left\\{r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a)u(j)\\right\\}\n\\end{align*}\n\nFor any state s, let a_v be the action that achieves the maximum for (\\mathrm{L}v)(s), and a_u be the action that achieves the maximum for (\\mathrm{L}u)(s). By the definition of these maximizers:\\begin{align*}\n(\\mathrm{L}v)(s) &\\geq r(s,a_u) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a_u)v(j)\\\\\n(\\mathrm{L}u)(s) &\\geq r(s,a_v) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a_v)u(j)\n\\end{align*}\n\nSubtracting these inequalities:\\begin{align*}\n(\\mathrm{L}v)(s) - (\\mathrm{L}u)(s) &\\leq \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a_v)(v(j) - u(j))\\\\\n(\\mathrm{L}u)(s) - (\\mathrm{L}v)(s) &\\leq \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a_u)(u(j) - v(j))\n\\end{align*}\n\nTaking the absolute value and using the fact that \\sum_{j \\in \\mathcal{S}} p(j|s,a) = 1:|(\\mathrm{L}v)(s) - (\\mathrm{L}u)(s)| \\leq \\gamma \\max_{j \\in \\mathcal{S}} |v(j) - u(j)| = \\gamma \\|v - u\\|_\\infty\n\nSince this holds for all s \\in \\mathcal{S}, taking the supremum over s gives:\\|\\mathrm{L}v - \\mathrm{L}u\\|_\\infty \\leq \\gamma \\|v - u\\|_\\infty\n\nThus, \\mathrm{L} is a contraction mapping with contraction factor \\gamma.\n\nNow, let’s prove parts 3 and 4. Suppose the algorithm has just terminated, i.e., \\|v_{n+1} - v_n\\|_\\infty < \\frac{\\varepsilon(1-\\gamma)}{2\\gamma} for some n. We want to show that our current value function v_{n+1} and the policy \\pi_\\varepsilon derived from it are close to optimal.\n\nBy the triangle inequality:\\|v^{\\pi_\\varepsilon}_\\gamma - v^*_\\gamma\\|_\\infty \\leq \\|v^{\\pi_\\varepsilon}_\\gamma - v_{n+1}\\|_\\infty + \\|v_{n+1} - v^*_\\gamma\\|_\\infty\n\nFor the first term, since v^{\\pi_\\varepsilon}_\\gamma is the fixed point of \\mathrm{L}_{\\pi_\\varepsilon} and \\pi_\\varepsilon is greedy with respect to v_{n+1} (i.e., \\mathrm{L}_{\\pi_\\varepsilon}v_{n+1} = \\mathrm{L}v_{n+1}):\\begin{aligned}\n\\|v^{\\pi_\\varepsilon}_\\gamma - v_{n+1}\\|_\\infty &= \\|\\mathrm{L}_{\\pi_\\varepsilon}v^{\\pi_\\varepsilon}_\\gamma - v_{n+1}\\|_\\infty \\\\\n&\\leq \\|\\mathrm{L}_{\\pi_\\varepsilon}v^{\\pi_\\varepsilon}_\\gamma - \\mathrm{L}_{\\pi_\\varepsilon}v_{n+1}\\|_\\infty + \\|\\mathrm{L}_{\\pi_\\varepsilon}v_{n+1} - v_{n+1}\\|_\\infty \\\\\n&= \\|\\mathrm{L}_{\\pi_\\varepsilon}v^{\\pi_\\varepsilon}_\\gamma - \\mathrm{L}_{\\pi_\\varepsilon}v_{n+1}\\|_\\infty + \\|\\mathrm{L}v_{n+1} - v_{n+1}\\|_\\infty \\\\\n&\\leq \\gamma\\|v^{\\pi_\\varepsilon}_\\gamma - v_{n+1}\\|_\\infty + \\gamma\\|v_{n+1} - v_n\\|_\\infty\n\\end{aligned}\n\nwhere we used that both \\mathrm{L} and \\mathrm{L}_{\\pi_\\varepsilon} are contractions with factor \\gamma, and that v_{n+1} = \\mathrm{L}v_n.\n\nRearranging:\\|v^{\\pi_\\varepsilon}_\\gamma - v_{n+1}\\|_\\infty \\leq \\frac{\\gamma}{1-\\gamma}\\|v_{n+1} - v_n\\|_\\infty\n\nSimilarly, since v^*_\\gamma is the fixed point of \\mathrm{L}:\\|v_{n+1} - v^*_\\gamma\\|_\\infty = \\|\\mathrm{L}v_n - \\mathrm{L}v^*_\\gamma\\|_\\infty \\leq \\gamma\\|v_n - v^*_\\gamma\\|_\\infty \\leq \\frac{\\gamma}{1-\\gamma}\\|v_{n+1} - v_n\\|_\\infty\n\nSince \\|v_{n+1} - v_n\\|_\\infty < \\frac{\\varepsilon(1-\\gamma)}{2\\gamma}:\\|v^{\\pi_\\varepsilon}_\\gamma - v_{n+1}\\|_\\infty \\leq \\frac{\\gamma}{1-\\gamma} \\cdot \\frac{\\varepsilon(1-\\gamma)}{2\\gamma} = \\frac{\\varepsilon}{2}\\|v_{n+1} - v^*_\\gamma\\|_\\infty \\leq \\frac{\\gamma}{1-\\gamma} \\cdot \\frac{\\varepsilon(1-\\gamma)}{2\\gamma} = \\frac{\\varepsilon}{2}\n\nCombining these:\\|v^{\\pi_\\varepsilon}_\\gamma - v^*_\\gamma\\|_\\infty \\leq \\frac{\\varepsilon}{2} + \\frac{\\varepsilon}{2} = \\varepsilon\n\nThis completes the proof, showing that v_{n+1} is within \\varepsilon/2 of v^*_\\gamma (part 4) and \\pi_\\varepsilon is \\varepsilon-optimal (part 3).","type":"content","url":"/dp#algorithms-for-solving-the-optimality-equations","position":47},{"hierarchy":{"lvl1":"Dynamic Programming","lvl4":"Newton-Kantorovich Applied to Bellman Optimality","lvl3":"Algorithms for Solving the Optimality Equations","lvl2":"Infinite-Horizon MDPs"},"type":"lvl4","url":"/dp#newton-kantorovich-applied-to-bellman-optimality","position":48},{"hierarchy":{"lvl1":"Dynamic Programming","lvl4":"Newton-Kantorovich Applied to Bellman Optimality","lvl3":"Algorithms for Solving the Optimality Equations","lvl2":"Infinite-Horizon MDPs"},"content":"We now apply the Newton-Kantorovich framework to the Bellman optimality equation. Let(\\mathrm{L}v)(s) = \\max_{a \\in A(s)} \\left\\{ r(s,a) + \\gamma \\sum_{s'} p(s' \\mid s,a) v(s') \\right\\}.\n\nThe problem is to find v such that \\mathrm{L}v = v, or equivalently \\mathrm{B}(v) := \\mathrm{L}v - v = 0. The operator \\mathrm{L} is piecewise affine, hence not globally differentiable, but it is directionally differentiable everywhere in the Hadamard sense and Fréchet differentiable at points where the maximizer is unique.\n\nWe consider three complementary perspectives for understanding and computing its derivative.","type":"content","url":"/dp#newton-kantorovich-applied-to-bellman-optimality","position":49},{"hierarchy":{"lvl1":"Dynamic Programming","lvl5":"Perspective 1: Max of Affine Maps","lvl4":"Newton-Kantorovich Applied to Bellman Optimality","lvl3":"Algorithms for Solving the Optimality Equations","lvl2":"Infinite-Horizon MDPs"},"type":"lvl5","url":"/dp#perspective-1-max-of-affine-maps","position":50},{"hierarchy":{"lvl1":"Dynamic Programming","lvl5":"Perspective 1: Max of Affine Maps","lvl4":"Newton-Kantorovich Applied to Bellman Optimality","lvl3":"Algorithms for Solving the Optimality Equations","lvl2":"Infinite-Horizon MDPs"},"content":"In tabular form, for finite state and action spaces, the Bellman operator can be written as a pointwise maximum of affine maps:(\\mathrm{L}v)(s) = \\max_{a \\in A(s)} \\left\\{ r(s,a) + \\gamma (P_a v)(s) \\right\\},\n\nwhere P_a \\in \\mathbb{R}^{|S| \\times |S|} is the transition matrix associated with action a. Each Q_a v := r^a + \\gamma P_a v is affine in v. The operator \\mathrm{L} therefore computes the upper envelope of a finite set of affine functions at each state.\n\nAt any v, let the active set at state s be\\mathcal{A}^*(s; v) := \\arg\\max_{a \\in A(s)} (Q_a v)(s).\n\nThen the Hadamard directional derivative exists and is given by(\\mathrm{L}'(v; h))(s) = \\max_{a \\in \\mathcal{A}^*(s; v)} \\gamma (P_a h)(s).\n\nIf the active set is a singleton, this expression becomes linear in h, and \\mathrm{L} is Fréchet differentiable at v, with\\mathrm{L}'(v) = \\gamma P_{\\pi_v},\n\nwhere \\pi_v(s) := a^*(s) is the greedy policy at v. In the presence of ties, the derivative becomes set-valued: the Clarke subdifferential consists of stochastic matrices whose rows are convex combinations of the $\\gamma P_a$ over $a \\in \\mathcal{A}^*(s; v)$. ","type":"content","url":"/dp#perspective-1-max-of-affine-maps","position":51},{"hierarchy":{"lvl1":"Dynamic Programming","lvl5":"Perspective 2: Envelope Theorem","lvl4":"Newton-Kantorovich Applied to Bellman Optimality","lvl3":"Algorithms for Solving the Optimality Equations","lvl2":"Infinite-Horizon MDPs"},"type":"lvl5","url":"/dp#perspective-2-envelope-theorem","position":52},{"hierarchy":{"lvl1":"Dynamic Programming","lvl5":"Perspective 2: Envelope Theorem","lvl4":"Newton-Kantorovich Applied to Bellman Optimality","lvl3":"Algorithms for Solving the Optimality Equations","lvl2":"Infinite-Horizon MDPs"},"content":"Consider now a value function approximated as a linear combination of basis functions:v_c(s) = \\sum_j c_j \\phi_j(s).\n\nAt a node s_i, define the parametric maximizationv_i(c) := (\\mathrm{L}v_c)(s_i) = \\max_{a \\in A(s_i)} \\left\\{ r(s_i,a) + \\gamma \\sum_j c_j \\mathbb{E}_{s' \\mid s_i, a}[\\phi_j(s')] \\right\\}.\n\nDefineF_i(a, c) := r(s_i,a) + \\gamma \\sum_j c_j \\mathbb{E}_{s' \\mid s_i, a}[\\phi_j(s')],\n\nso that v_i(c) = \\max_a F_i(a, c). Since F_i is linear in c, we can apply the envelope theorem (Danskin’s theorem): if the optimizer a_i^*(c) is unique or selected measurably, then\\frac{\\partial v_i}{\\partial c_j}(c) = \\gamma \\mathbb{E}_{s' \\mid s_i, a_i^*(c)}[\\phi_j(s')].\n\nWe do not need to differentiate the optimizer a_i^*(c) itself. The result extends to the subdifferential case when ties occur, where the Jacobian becomes set-valued.\n\nThis result is useful when solving the collocation equation \\Phi c = v(c). Newton’s method requires the Jacobian v'(c), and this expression allows us to compute it without involving any derivatives of the optimal action.","type":"content","url":"/dp#perspective-2-envelope-theorem","position":53},{"hierarchy":{"lvl1":"Dynamic Programming","lvl5":"Perspective 3: The Implicit Function Theorem","lvl4":"Newton-Kantorovich Applied to Bellman Optimality","lvl3":"Algorithms for Solving the Optimality Equations","lvl2":"Infinite-Horizon MDPs"},"type":"lvl5","url":"/dp#perspective-3-the-implicit-function-theorem","position":54},{"hierarchy":{"lvl1":"Dynamic Programming","lvl5":"Perspective 3: The Implicit Function Theorem","lvl4":"Newton-Kantorovich Applied to Bellman Optimality","lvl3":"Algorithms for Solving the Optimality Equations","lvl2":"Infinite-Horizon MDPs"},"content":"The third perspective applies the implicit function theorem to understand when the Bellman operator is differentiable despite containing a max operator. The maximization problem defines an implicit relationship between the value function and the optimal action, and the implicit function theorem tells us when this relationship is smooth enough to differentiate through.\n\nThe Bellman operator is defined as(\\mathrm{L}v)(s) = \\max_{a} \\left\\{ r(s,a) + \\gamma \\sum_j p(j \\mid s,a) v(j) \\right\\}.\n\nThe difficulty is that the max operator encodes a discrete selection: which action achieves the maximum. To apply the implicit function theorem, we reformulate this as follows. For each action a, define the action-value function:Q_a(v, s) := r(s,a) + \\gamma \\sum_j p(j \\mid s,a) v(j).\n\nThe optimal action at v satisfies the optimality condition:Q_{a^*(s)}(v, s) \\geq Q_a(v, s) \\quad \\text{for all } a.\n\nNow suppose that at a particular v, action a^*(s) is a strict local maximizer in the sense that there exists \\delta > 0 such thatQ_{a^*(s)}(v, s) > Q_a(v, s) + \\delta \\quad \\text{for all } a \\neq a^*(s).\n\nThis strict inequality is the regularity condition needed for the implicit function theorem. It ensures that the optimal action is unique at v and remains so in a neighborhood of v.\n\nTo see why, consider any perturbation v + h with \\|h\\| small. Since Q_a is linear in v, we have:Q_a(v+h, s) = Q_a(v, s) + \\gamma \\sum_j p(j \\mid s,a) h(j).\n\nThe perturbation term is bounded: |\\gamma \\sum_j p(j \\mid s,a) h(j)| \\leq \\gamma \\|h\\|. Therefore, for \\|h\\| < \\delta/\\gamma, the strict gap ensures thatQ_{a^*(s)}(v+h, s) > Q_a(v+h, s) \\quad \\text{for all } a \\neq a^*(s).\n\nThus a^*(s) remains the unique maximizer throughout the neighborhood \\{v + h : \\|h\\| < \\delta/\\gamma\\}.\n\nThe implicit function theorem now applies: in this neighborhood, the mapping v \\mapsto a^*(s; v) is constant (and hence smooth), taking the value a^*(s). This allows us to write(\\mathrm{L}v)(s) = Q_{a^*(s)}(v, s) = r(s,a^*(s)) + \\gamma \\sum_j p(j \\mid s,a^*(s)) v(j)\n\nas an explicit formula that holds throughout the neighborhood. Since Q_{a^*(s)}(\\cdot, s) is an affine (hence smooth) function of v, we can differentiate it:\\frac{d}{dv} (\\mathrm{L}v)(s) = \\gamma P_{a^*(s)}.\n\nMore precisely, for any perturbation h:(\\mathrm{L}(v+h))(s) = (\\mathrm{L}v)(s) + \\gamma \\sum_j p(j \\mid s,a^*(s)) h(j) + o(\\|h\\|).\n\nThis is the Fréchet derivative:\\mathrm{L}'(v) = \\gamma P_{\\pi_v},\n\nwhere \\pi_v(s) = a^*(s) is the greedy policy.\n\nThe role of the implicit function theorem: It guarantees that when the maximizer is unique with a strict gap (the regularity condition), the argmax function v \\mapsto a^*(s; v) is locally constant, which removes the non-differentiability of the max operator. Without this regularity condition (specifically, at points where multiple actions tie for optimality), the implicit function theorem does not apply, and the operator is not Fréchet differentiable. The active set perspective (Perspective 1) and the envelope theorem (Perspective 2) provide the tools to handle these non-smooth points.","type":"content","url":"/dp#perspective-3-the-implicit-function-theorem","position":55},{"hierarchy":{"lvl1":"Dynamic Programming","lvl4":"Connection to Policy Iteration","lvl3":"Algorithms for Solving the Optimality Equations","lvl2":"Infinite-Horizon MDPs"},"type":"lvl4","url":"/dp#connection-to-policy-iteration","position":56},{"hierarchy":{"lvl1":"Dynamic Programming","lvl4":"Connection to Policy Iteration","lvl3":"Algorithms for Solving the Optimality Equations","lvl2":"Infinite-Horizon MDPs"},"content":"We return to the Newton-Kantorovich step:(I - \\mathrm{L}'(v_n)) h_n = v_n - \\mathrm{L}v_n,\n\\quad\nv_{n+1} = v_n - h_n.\n\nSuppose \\mathrm{L}'(v_n) = \\gamma P_{\\pi_{v_n}} for the greedy policy \\pi_{v_n}. Then(I - \\gamma P_{\\pi_{v_n}}) v_{n+1} = r^{\\pi_{v_n}},\n\nwhich is exactly policy evaluation for \\pi_{v_n}. Recomputing the greedy policy from v_{n+1} yields the next iterate.\n\nThus, policy iteration is Newton-Kantorovich applied to the Bellman optimality equation. At points of nondifferentiability (when ties occur), the operator is still semismooth, and policy iteration corresponds to a semismooth Newton method. The envelope theorem is what justifies the simplification of the Jacobian to \\gamma P_{\\pi_v}, bypassing the need to differentiate through the optimizer. This completes the equivalence.","type":"content","url":"/dp#connection-to-policy-iteration","position":57},{"hierarchy":{"lvl1":"Dynamic Programming","lvl4":"The Semismooth Newton Perspective","lvl3":"Algorithms for Solving the Optimality Equations","lvl2":"Infinite-Horizon MDPs"},"type":"lvl4","url":"/dp#the-semismooth-newton-perspective","position":58},{"hierarchy":{"lvl1":"Dynamic Programming","lvl4":"The Semismooth Newton Perspective","lvl3":"Algorithms for Solving the Optimality Equations","lvl2":"Infinite-Horizon MDPs"},"content":"The three perspectives we developed above (the active set view, the envelope theorem, and the implicit function theorem) all point toward a deeper framework for understanding Newton-type methods on non-smooth operators. This framework, known as semismooth Newton methods, was developed precisely to handle operators like the Bellman operator that are piecewise smooth but not globally differentiable. The connection between policy iteration and semismooth Newton methods has been rigorously developed in recent work \n\nGargiani et al. (2022).\n\nThe classical Newton-Kantorovich method assumes the operator is Fréchet differentiable everywhere. The derivative exists, is unique, and varies continuously with the base point. But the Bellman operator \\mathrm{L} violates this assumption at any value function where multiple actions tie for optimality at some state. At such points, the implicit function theorem fails, and there is no unique Fréchet derivative.\n\nSemismooth Newton methods address this by replacing the notion of a single Jacobian with a generalized derivative that captures the behavior of the operator near non-smooth points. The most commonly used generalized derivative is the Clarke subdifferential, which we can think of as the convex hull of all possible “candidate Jacobians” that arise from limits approaching the non-smooth point from different directions.\n\nFor the Bellman residual \\mathrm{B}(v) = \\mathrm{L}v - v, the Clarke subdifferential at a point v can be characterized explicitly using our first perspective. Recall that at each state s, we defined the active set \\mathcal{A}^*(s; v) = \\arg\\max_a Q_a(v, s). When this set contains multiple actions, the operator is not Fréchet differentiable. However, it remains directionally differentiable in all directions, and the Clarke subdifferential consists of all matrices of the form\\partial \\mathrm{B}(v) = \\left\\{ I - \\gamma P_\\pi : \\pi(s) \\in \\mathcal{A}^*(s; v) \\text{ for all } s \\right\\}.\n\nIn words, the generalized Jacobian is the set of all matrices I - \\gamma P_\\pi where \\pi is any policy that selects an action from the active set at each state. When the maximizer is unique everywhere, this set reduces to a singleton, and we recover the classical Fréchet derivative. When ties occur, the set has multiple elements: precisely the convex combinations mentioned in Perspective 1.\n\nThe semismooth Newton method for solving \\mathrm{B}(v) = 0 proceeds by selecting an element J_k \\in \\partial \\mathrm{B}(v_k) at each iteration and solvingJ_k h_k = -\\mathrm{B}(v_k), \\quad v_{k+1} = v_k + h_k.\n\nWhat this tells us is that any choice from the Clarke subdifferential yields a valid Newton-like update. In the context of the Bellman equation, choosing J_k = I - \\gamma P_{\\pi_k} where \\pi_k is any greedy policy corresponds exactly to the policy evaluation step in policy iteration. The freedom in selecting which action to choose when ties occur translates to the freedom in selecting which element of the subdifferential to use.\n\nUnder appropriate regularity conditions (specifically, when the residual function is BD-regular or CD-regular), the semismooth Newton method converges locally at a quadratic rate \n\nGargiani et al. (2022). This means that near the solution, the error decreases quadratically:\\|v_{k+1} - v^*\\| \\leq C \\|v_k - v^*\\|^2.\n\nThis theoretical result explains an empirical observation that has long been noted in practice: policy iteration typically converges in very few iterations, often just a handful, even when the state and action spaces are enormous and the space of possible policies is exponentially large.\n\nThe semismooth Newton framework also suggests a spectrum of methods interpolating between value iteration and policy iteration. Value iteration can be interpreted as a Newton-like method where we choose J_k = I at every iteration, ignoring the dependence of \\mathrm{L} on v entirely. This choice guarantees global convergence through the contraction property but sacrifices the quadratic local convergence rate. Policy iteration, at the other extreme, uses the full generalized Jacobian J_k = I - \\gamma P_{\\pi_k}, achieving quadratic convergence but at the cost of solving a linear system at each iteration.\n\nBetween these extremes lie methods that use approximate Jacobians. One natural variant is to choose J_k = \\alpha I for some scalar \\alpha > 1. This leads to the updatev_{k+1} = \\frac{\\alpha - 1}{\\alpha} v_k + \\frac{1}{\\alpha} \\mathrm{L}v_k.\n\nThis is known as \\alpha-value iteration or successive over-relaxation when \\alpha > 1. For appropriate choices of \\alpha, this method retains global convergence while achieving better local rates than standard value iteration, and it requires only pointwise operations rather than solving a linear system. The Newton perspective thus unifies existing algorithms and generates new ones by systematically exploring different approximations to the generalized Jacobian.\n\nThe connection to semismooth Newton methods places policy iteration within a broader mathematical framework that extends far beyond dynamic programming. Semismooth Newton methods are used in optimization (for complementarity problems and variational inequalities), in PDE-constrained optimization (for problems with control constraints), and in economics (for equilibrium problems). The Bellman equation, viewed through this lens, is simply one instance of a piecewise smooth equation, and the tools developed for such equations apply directly.","type":"content","url":"/dp#the-semismooth-newton-perspective","position":59},{"hierarchy":{"lvl1":"Dynamic Programming","lvl4":"Policy Iteration","lvl3":"Algorithms for Solving the Optimality Equations","lvl2":"Infinite-Horizon MDPs"},"type":"lvl4","url":"/dp#policy-iteration","position":60},{"hierarchy":{"lvl1":"Dynamic Programming","lvl4":"Policy Iteration","lvl3":"Algorithms for Solving the Optimality Equations","lvl2":"Infinite-Horizon MDPs"},"content":"While we derived policy iteration-like steps from the Newton-Kantorovich method, it’s worth examining policy iteration as a standalone algorithm, as it has been traditionally presented in the field of dynamic programming.\n\nThe policy iteration algorithm for discounted Markov decision problems is as follows:\n\nPolicy Iteration\n\nInput: MDP (S, A, P, R, \\gamma)\nOutput: Optimal policy \\pi^*\n\nInitialize: n = 0, select an arbitrary decision rule \\pi_0 \\in \\Pi^{MD}\n\nrepeat\n3. (Policy evaluation) Obtain \\mathbf{v}^n by solving:(\\mathbf{I}-\\gamma \\mathbf{P}_{\\pi_n}) \\mathbf{v} = \\mathbf{r}_{\\pi_n}\n\n(Policy improvement) Choose \\pi_{n+1} = \\mathrm{Greedy}(\\mathbf{v}^n) where:\\pi_{n+1} \\in \\arg\\max_{\\pi \\in \\Pi^{MD}}\\left\\{\\mathbf{r}_\\pi+\\gamma \\mathbf{P}_\\pi \\mathbf{v}^n\\right\\}\n\nequivalently, for each s:\\pi_{n+1}(s) \\in \\arg\\max_{a \\in \\mathcal{A}_s}\\left\\{r(s,a)+\\gamma \\sum_j p(j|s,a) \\mathbf{v}^n(j)\\right\\}\n\nSet \\pi_{n+1} = \\pi_n if possible.\n\nIf \\pi_{n+1} = \\pi_n, return \\pi^* = \\pi_n\n\nn = n + 1\n\nuntil convergence\n\nAs opposed to value iteration, this algorithm produces a sequence of both deterministic Markovian decision rules \\{\\pi_n\\} and value functions \\{\\mathbf{v}^n\\}. We recognize in this algorithm the linearization step of the Newton-Kantorovich procedure, which takes place here in the policy evaluation step 3 where we solve the linear system (\\mathbf{I}-\\gamma \\mathbf{P}_{\\pi_n}) \\mathbf{v} = \\mathbf{r}_{\\pi_n}. In practice, this linear system could be solved either using direct methods (eg. Gaussian elimination), using simple iterative methods such as the successive approximation method for policy evaluation, or more sophisticated methods such as GMRES.","type":"content","url":"/dp#policy-iteration","position":61},{"hierarchy":{"lvl1":"Why This Book?"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Why This Book?"},"content":"Reinforcement learning often captures headlines with breakthroughs: AlphaGo defeating world champions, agents mastering Atari, chatbots engaging millions in conversation. Yet compared to other branches of machine learning—especially supervised learning, which has become routine in industry and research—RL has not yet achieved the same widespread adoption in everyday decision-making and operations.\n\nFor perspective, consider supervised learning. Standardized tools like scikit-learn, TensorFlow, or PyTorch have made it straightforward for data scientists to integrate classification and regression models into production workflows. In medicine, hundreds of FDA-approved devices now incorporate supervised machine learning. Randomized controlled trials regularly evaluate diagnostic and predictive models built with supervised learning techniques.\n\nBy contrast, reinforcement learning’s adoption is still in earlier stages. Among more than a thousand FDA-cleared AI-enabled medical devices as of 2025, none explicitly reference reinforcement learning in their public summaries \n\nU.S. Food and Drug Administration, 2025. A recent Lancet Digital Health review of 86 clinical trials involving AI identified only two studies that tested RL-based decision rules \n\nKleijnen & others, 2024. Most AI solutions in healthcare remain supervised learning models trained on labeled datasets.\n\nOutside healthcare, promising results exist, but scale remains modest. In building automation, a 2025 survey identified many field demonstrations of reinforcement learning and Model Predictive Control (MPC) for HVAC control. Yet, fewer than a third met basic methodological criteria. Among reliable studies, average cost reductions were around 13–16% \n\nChen & others, 2025. This is encouraging, yet still short of broad, industry-wide adoption.\n\nEven in high-profile reinforcement learning deployments, adoption at scale is sometimes uncertain or difficult to confirm. Google DeepMind famously reported significant cooling-energy reductions in data centers using RL back in 2018 \n\nEvans & DeepMind, 2018. However, more recent public confirmation of widespread autonomous use has been lacking. Meta, in a 2024 engineering blog, provided clear evidence that an RL-based airflow controller was achieving meaningful reductions in energy and water usage, and stated that broader deployment was underway, though specifics on scale were not disclosed \n\nMeta AI, 2024.\n\nNevertheless, notable successes continue to emerge. Uber has successfully integrated reinforcement learning into its ride-matching system, with measurable improvements across hundreds of cities \n\nUber AI Labs, 2025. ANYbotics has commercially deployed quadruped robots whose locomotion policies, trained entirely via RL in simulation, now reliably perform complex industrial inspections \n\nANYbotics, 2023.\n\nThese examples illustrate genuine progress. Yet reinforcement learning has not yet reached the “plug-and-play” status enjoyed by supervised learning. This is largely due to fundamental differences in problem structure. Supervised learning tasks typically involve well-defined inputs, outputs, and objective metrics. Reinforcement learning, by contrast, requires explicit problem formulation, exploration, interactive data collection, and a nuanced understanding of the environment’s structure.\n\nAs \n\nIskhakov et al. (2020) notes in econometrics, a primary challenge facing adoption of any sequential decision-making tool is:\n\n“The difficulty of learning about the objective function and environment facing real-world decision-makers.”\n\nIn reinforcement learning, this difficulty is integral. We cannot sidestep defining the problem, the objective, and the constraints; these are not incidental but central. Supervised learning allows practitioners to abstract away most of these concerns into standardized data formats and evaluation metrics. Reinforcement learning practitioners do not have this luxury.\n\nThat is where this book begins.\n\nI did not fully appreciate this difference as a PhD student. Like many others trained in machine learning, I focused on tuning algorithms, chasing benchmarks, and climbing leaderboards. Problem definition was abstracted away, often assumed or left as someone else’s responsibility.\n\nWorking in industry and consulting changed that. Real problems rarely fit neatly into a predefined framework. Sensors produce noisy data; constraints are non-negotiable; objectives may shift or conflict. I discovered firsthand that most effort goes into formulating the decision problem, long before selecting an algorithm.\n\nJohn Rust captures this precisely when reflecting on dynamic programming in practice:\n\n“The range of known real-world applications of Dynamic Programming seems disappointingly small, given the immense computer power and decades of research that have produced a myriad of alternative solution methods for DP problems. I believe the biggest constraint on progress is not limited computer power, but instead the difficulty of learning the underlying structure of the decision problem.” \n\nRust (1996)\n\nIn other words, solving a real-world decision problem starts with formulating it correctly. What are we optimizing? What can we observe and control? How does information flow? These questions are not secondary details; they define the problem itself.\n\nThe chapters that follow address these challenges explicitly. They offer strategies to bridge the gap from theoretical reinforcement learning formulations to practically useful systems. By carefully structuring decision problems, we can help reinforcement learning achieve broader impact, just as supervised learning already has.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Why This Book?","lvl2":"What Problem Are We Solving?"},"type":"lvl2","url":"/#what-problem-are-we-solving","position":2},{"hierarchy":{"lvl1":"Why This Book?","lvl2":"What Problem Are We Solving?"},"content":"The term reinforcement learning gets used in many different ways. In the formal sense defined by \n\nSutton & Barto (2018), RL is a problem: learning to act through interaction with an environment. But in common usage, reinforcement learning can mean a family of algorithms, a research community, or even a long-term scientific agenda. For some, it is part of an effort to “solve intelligence.” For others, it is a toolbox for solving control problems with data.\n\nThis book takes the latter view.\n\nWe are not in the business of solving intelligence. Our concern is more immediate: helping systems make good decisions using the data they already produce. That means we treat reinforcement learning not as an end in itself, but as a vocabulary for reasoning about decisions under uncertainty. When optimization, feedback, and data intersect, we are in the territory of reinforcement learning, whether we use temporal-difference learning, model-based planning, or simple policy rules. What unifies these approaches is not a specific algorithm, but a shared structure: decision-making through experience.\n\nBeing clear about the problem matters. If the goal is to understand cognition, then abstraction and simulation are appropriate tools. But if the goal is to improve real-world systems, whether in energy, logistics, health, or agriculture, then the hard part is not choosing an algorithm. It is defining the task. What are we optimizing? What constraints apply? What information is available, and when? These are modeling questions, and they sit upstream of any learning method.\n\nThis book begins with the problem, not the solution. We use reinforcement learning in its broadest sense: as a perspective on how to improve decision-making from data, rather than as a collection of algorithms.\n\nIn doing so, we take inspiration from Sutton’s philosophy, while shifting its emphasis. Sutton famously advises: “Approximate the solution, not the problem.” In his view, the reinforcement learning agent should be shaped by experience, not by handcrafted structure or strong priors. That framing has led to influential ideas and a high degree of generality.\n\nBut it also reflects a particular philosophy. For Sutton, “the problem” is the world itself—complex, unknown, and handed to us as-is. We do not design it; we simply confront it. Reinforcement learning, in this view, is a path toward understanding intelligence through interaction, not engineering through modeling.\n\nThis book takes a more pragmatic stance. In practice, the problem is rarely just “given.” It must be defined: what are the goals, what decisions are available, what feedback is observable, and under what constraints? Before we can solve a problem, we have to formulate it—and that formulation shapes everything that follows.","type":"content","url":"/#what-problem-are-we-solving","position":3},{"hierarchy":{"lvl1":"Why This Book?","lvl2":"What Does It Mean to Model a Decision Problem?"},"type":"lvl2","url":"/#what-does-it-mean-to-model-a-decision-problem","position":4},{"hierarchy":{"lvl1":"Why This Book?","lvl2":"What Does It Mean to Model a Decision Problem?"},"content":"Modeling is not about feeding data into a black box. It is about deciding what matters. It involves structuring a problem: defining objectives, specifying constraints, clarifying what is observable, and determining how decisions unfold over time.\n\nTake an HVAC system. The goal might be “maximize comfort while minimizing energy.” But what does comfort mean? A fixed temperature? Acceptable humidity? Rate of change? Occupant preferences? Is comfort linearly traded against energy, or are there thresholds? And how do you ensure safety and respect equipment limitations?\n\nIn irrigation, similar questions arise. Should irrigation be based on soil dryness, weather forecasts, plant health, or electricity prices? Should we water now or wait? How often should we revisit this choice? The answers depend on sensor availability, environmental dynamics, and risk tolerance.\n\nEven time plays a central role. Are we planning for the next few minutes, or the next growing season? Should we model time in discrete steps or continuously? These are not afterthoughts. They shape what is learnable, controllable, and feasible.\n\nReal-world systems come with hard constraints: physical limits, budgets, safety regulations, human expectations. Ignoring them may simplify the math, but it makes any solution irrelevant in practice. Good modeling incorporates these constraints from the beginning.\n\nThis kind of modeling is what Operations Research (OR) has long emphasized. By the 1970s, the foundational theory of dynamic programming was already in place. Today’s OR community often focuses on solving concrete decision problems, drawing on tools like mixed-integer linear programming when appropriate.\n\nThrough consulting, I came to appreciate OR’s pragmatism. My colleagues built decision-support systems connected to real-time data. In practice, they were doing reinforcement learning, just without Temporal Difference methods. They could not rely on massive data streams. Instead, they had to wrestle directly with business logic, real-world variability, and engineering constraints.\n\nThat does not mean OR has all the answers, or that reinforcement learning has been misguided. On the contrary, general-purpose solutions, even on toy problems, have advanced theory in meaningful ways. Simplified settings enable validation without full domain understanding: Did the pole balance? Did the agent reach the goal? Did it beat the Atari score? These abstractions follow good software engineering principles: separation of concerns, clear interfaces, and rapid iteration.\n\nBut abstraction is only one part of the equation. It is useful until it is not. A strong framework offers clarity at first, but eventually gets in the way, layering on complexity, edge cases, and configuration knobs. This mirrors the lifecycle of many software tools, where the initial elegance gives way to accumulated mess. I have come to treat modeling the same way: start small, surface the hard constraints early, and only add structure when it is required by the data, physics, or policy context.\n\nWe should not try to cram every control task into the discounted Markov Decision Process (MDP) format just because it is the default interface. Instead, we should keep a lean toolbox and reach for what the problem demands. This mindset, start simple, avoid premature generality, do not confuse abstraction with robustness, is well known to engineers. If that means choosing a basic model-predictive controller over a trendy reinforcement learning library, or the reverse, so be it.\n\nOver time, this habit of zooming in when needed and zooming out when possible reshaped how I approached research. Once I started questioning the abstractions, I began to see what they were hiding.\n\nPeeling back the layers revealed dynamics not captured by benchmarks, constraints ignored by rewards, and theory that only emerged in contact with the real world. This convinced me that some of the most meaningful discoveries still lie beneath the surface.\n\nReinforcement learning is a framework for learning to make decisions through experience. But experience is only useful if we have posed the right problem. Modeling determines both how learning proceeds and what we can learn at all.\n\nAt first, this might sound easy, just specify a reward and let the agent learn. That is the promise behind the “reward is enough” hypothesis. But in the real world, rewards are not handed to us. They must be constructed, inferred, or negotiated. And even when we manage that, rewards only express part of what matters. They do not tell us what information is available, what tradeoffs are acceptable, or how to handle ambiguity, delay, or disagreement.\n\nIn short, posing the right problem is itself a hard problem, and one for which we have few systematic tools.\n\nOften, the only place we can turn is to people. Domain experts act, react, and judge, even when they cannot explain their reasoning explicitly. Their preferences show up in behavior, in corrections, in choices they make under pressure. If we cannot write down what we want, perhaps we can learn it indirectly from them.","type":"content","url":"/#what-does-it-mean-to-model-a-decision-problem","position":5},{"hierarchy":{"lvl1":"Why This Book?","lvl2":"Learning From Humans"},"type":"lvl2","url":"/#learning-from-humans","position":6},{"hierarchy":{"lvl1":"Why This Book?","lvl2":"Learning From Humans"},"content":"When we cannot write down the right behavior, we often try to learn it from examples. This is the idea behind imitation learning: watch the expert, then generalize. But in practice, good demonstrations are hard to collect and rarely cover the full range of relevant situations, especially the rare or risky ones.\n\nThat is why many real-world applications require more than direct imitation. If we cannot show what to do in every case, we must express what we want. This is where reward design, cost functions, and preference modeling come in. These tools attempt to capture the underlying objective by observing both what people do and what they seem to value.\n\nPreference elicitation offers one route. Rather than specifying the optimal solution directly, we infer it from comparisons, rankings, or feedback. Under mild assumptions, the von Neumann–Morgenstern theorem tells us that such preferences correspond to a utility function. This principle forms the basis of Reinforcement Learning from Human Feedback (RLHF), now central to training large models.\n\nBut here, too, we face limits. Once we have inferred preferences or objectives from humans, what comes next? In many systems, the default answer is to treat this as a standard supervised learning problem: fit a black-box model to human-labeled data, then optimize the resulting predictions.\n\nThis approach can go surprisingly far. Recent work, such as Decision Transformers, has shown that supervised learning can recover policies that perform competitively, sometimes even state-of-the-art. But these successes are often built on vast datasets, carefully curated environments, and tight control over evaluation. In the real world, we rarely have that luxury.\n\nSupervised learning assumes that if we show enough examples, the system will generalize appropriately. But generalization is fragile when data is limited, feedback is partial, or the stakes are high. Without the right structure in place, we risk building policies that extrapolate poorly, violate constraints, or break in unexpected ways.\n\nThis is where modeling matters again.\n\nBy modeling the decision process, which includes the constraints, objectives, time structure, and information flows, we introduce the right inductive biases into the learning system. These biases are not arbitrary. They reflect how the world works and what the agent can and cannot do. They make learning tractable even when data is scarce and help ensure that the resulting decisions behave reasonably under uncertainty.\n\nSo while supervised learning plays a role, it is not the full story. The point is to embed what we have learned into a framework where decision-making remains accountable, robust, and grounded in structure.\n\nThat is the aim of this book: to take what we can learn from humans and from data, and combine it with modeling discipline to build systems that act for the right reasons.","type":"content","url":"/#learning-from-humans","position":7},{"hierarchy":{"lvl1":"Why This Book?","lvl2":"The Path Forward"},"type":"lvl2","url":"/#the-path-forward","position":8},{"hierarchy":{"lvl1":"Why This Book?","lvl2":"The Path Forward"},"content":"Rust’s critique remains timely. After decades of algorithmic progress, we still struggle to help people make better decisions in the settings that matter most. Reinforcement learning has pushed the boundaries of simulation. But in practice, its reach remains limited, not because the tools are broken, but because we have struggled to formulate problems in ways that connect to the real world.\n\nIt is not that decision problems cannot be solved. It is that we often fail to pose them in solvable form.\n\nThis is not a limitation of algorithms. It is a limitation in how we frame our goals.\n\nBut that may be changing.\n\nRust once wrote:\n\n“Humans are learning to replicate the type of subconscious model building that goes on inside their brains and bring it to the conscious, formal level, but they are doing this modeling themselves, since it is not clear how to teach computers how to model.”\n\nThat might have been true then. But today, we are beginning to see what it might look like to teach machines to model, or at least assist us in doing so.\n\nOne school of thought, inspired by Sutton’s long-term vision, imagines that we will not need to model at all. Instead, we build general-purpose agents trained on vast, unstructured experience. We do not hand them objectives or constraints. We do not define environments. We let them learn everything from scratch. The hope is that once such an agent is sufficiently broad and capable, it can generalize everywhere, even to tasks we have not yet imagined.\n\nThat is a bold bet. But it is still a bet. And if your goal is to solve meaningful problems now, in healthcare, infrastructure, climate, or logistics, then the challenge of formulation does not disappear. Even the most flexible agent cannot act reliably in a domain where the goals, tradeoffs, and structure remain unclear.\n\nWhat is more, our current models of generality, especially large language models, are not agents in the classical sense. They produce text, but they do not act in the world. They reason in language, but they do not optimize over time. Despite growing trends to describe these systems as “agents,” they are better understood as modeling tools: systems trained on enormous corpora of human knowledge, capable of reflecting, translating, and helping us express complex ideas.\n\nAnd that might be their greatest strength.\n\nWe may not hand full control to a language model anytime soon, but we will use these systems to help us model. They will assist in articulating objectives, surfacing hidden assumptions, identifying constraints, and mapping informal goals into structured forms. They will not replace modeling; they will augment it.\n\nAnd what takes action, what makes real decisions in the world, will still rely on explicit optimization, grounded in formal structure, and designed to behave predictably. The language model may help us design that system, but it will not be the one executing it.\n\nThat is where I believe the near future lies: general-purpose models as modeling assistants, paired with optimization and control systems that retain structure, constraints, and accountability.\n\nThis book is about building that bridge: from goals to models, from data to decisions, from abstraction to action.\n\nThis is the modeling mindset. And it is what turns reinforcement learning into a practical tool for solving real problems.","type":"content","url":"/#the-path-forward","position":9},{"hierarchy":{"lvl1":"Why Build a Model? For Whom?"},"type":"lvl1","url":"/modeling","position":0},{"hierarchy":{"lvl1":"Why Build a Model? For Whom?"},"content":"","type":"content","url":"/modeling","position":1},{"hierarchy":{"lvl1":"Why Build a Model? For Whom?"},"type":"lvl1","url":"/modeling#why-build-a-model-for-whom","position":2},{"hierarchy":{"lvl1":"Why Build a Model? For Whom?"},"content":"“The sciences do not try to explain, they hardly even try to interpret, they mainly make models. By a model is meant a mathematical construct which, with the addition of certain verbal interpretations, describes observed phenomena. The justification of such a mathematical construct is solely and precisely that it is expected to work.”\n— John von Neumann\n\nThe word model means different things depending on who you ask.\n\nIn machine learning, it typically refers to a parameterized function (often a neural network) fit to data. When we say “we trained a model,” we usually mean adjusting parameters so it makes good predictions. But that’s a narrow view.\n\nIn control, operations research, or structural economics, a model refers more broadly to a formal specification of a decision problem. It includes how a system evolves over time, what parts of the world we choose to represent, what decisions are available, what can be observed or measured, and how outcomes are evaluated. It also encodes assumptions about time (discrete or continuous, finite or infinite horizon), uncertainty, and information structure.\n\nTo clarify terminology, I’ll use the term decision-making model to refer to this broader object: one that includes not just system dynamics, but also a specification of state, control, observations, objectives, time structure, and information assumptions. In this sense, the model defines the structure of the decision problem. It’s the formal scaffold on which we build optimization or learning procedures.\n\nDepending on the setting, we may ask different things from a decision-making model. Sometimes we want a model that supports counterfactual reasoning or policy evaluation, and are willing to bake in more assumptions to get there. Other times, we just need a model that supports prediction or simulation, even if it remains agnostic about internal mechanisms.\n\nThis mirrors a interesting distinction in econometrics between structural and reduced-form approaches. Structural models aim to capture the underlying process that generates behavior, enabling reasoning about what would happen under alternative policies or conditions. Reduced-form models, by contrast, focus on capturing statistical regularities (often to estimate causal effects) without necessarily modeling the mechanisms that generate them. Both are forms of modeling, just with different goals. The same applies in control and RL: some models are built to support simulation and optimization, while others serve more diagnostic or predictive roles, with fewer assumptions about how the system works internally.\n\nThis chapter steps back from algorithms to focus on the modeling side. What kinds of models do we need to support decision-making from data? What are their assumptions? What do they let us express or ignore? And how do they shape what learning and optimization can even mean?","type":"content","url":"/modeling#why-build-a-model-for-whom","position":3},{"hierarchy":{"lvl1":"Modeling, Realism, and Control"},"type":"lvl1","url":"/modeling#modeling-realism-and-control","position":4},{"hierarchy":{"lvl1":"Modeling, Realism, and Control"},"content":"Realism is only one way to assess a model. When the purpose of modeling is to support control or decision making, accuracy in reproducing every detail of the system is not always necessary. What matters more is whether the model leads to decisions that perform well when applied in practice. A model may simplify the physics, ignore some variables, or group complex interactions into a disturbance term. As long as it retains the core feedback structure relevant to the control task, it can still be effective.\n\nIn some cases, high-fidelity models can be counterproductive. Their complexity makes them harder to understand, slower to simulate, and more difficult to tune. Worse, they may include uncertain parameters that do not affect the control decisions but still influence the outcome of optimization. The resulting decisions can become fragile or overfitted to details that are not stable across different operating conditions.\n\nA useful model for control is one that focuses on the variables, dynamics, and constraints that shape the decisions to be made. It should capture the key trade-offs without trying to account for every effect. In traditional control design, this principle appears through model simplification: engineers reduce the system to a manageable form, then use feedback to absorb remaining uncertainty. Reinforcement learning adopts a similar mindset, though often implicitly. It allows for model error and evaluates success based on the quality of the policy when deployed, rather than on the accuracy of the model itself.","type":"content","url":"/modeling#modeling-realism-and-control","position":5},{"hierarchy":{"lvl1":"Modeling, Realism, and Control","lvl2":"Example: A simple model that supports better decisions"},"type":"lvl2","url":"/modeling#example-a-simple-model-that-supports-better-decisions","position":6},{"hierarchy":{"lvl1":"Modeling, Realism, and Control","lvl2":"Example: A simple model that supports better decisions"},"content":"Researchers at the U.S. National Renewable Energy Laboratory investigated how to reduce cooling costs in a typical home in Austin, Texas \n\nCole et al., 2014. They had access to a detailed EnergyPlus simulation of the building, which included thousands of internal variables: layered wall models, HVAC cycling behavior, occupancy schedules, and detailed weather inputs.\n\nAlthough this simulator could closely reproduce indoor temperatures, it was too slow and too complex to use as a planning tool. Instead, the researchers constructed a much simpler model using just two parameters: an effective thermal resistance and an effective thermal capacitance. This reduced model did not capture short-term temperature fluctuations and could be off by as much as two degrees on hot afternoons.\n\nDespite these inaccuracies, the simplified model proved useful for testing different cooling strategies. One such strategy involved cooling the house early in the morning when electricity prices were low, letting the temperature rise slowly during the expensive late-afternoon period, and reheating only slightly overnight. When this strategy was simulated in the full EnergyPlus model, it reduced peak compressor power by approximately 70 percent and lowered total cooling cost by about 60 percent compared to a standard thermostat schedule.\n\nThe reason this worked is that the simple model captured the most important structural feature of the system: the thermal mass of the building acts as a buffer that allows load shifting over time. That was enough to discover a control strategy that exploited this property. The many other effects present in the full simulation did not change the main conclusions and could be treated as part of the background variability.\n\nThis example shows that a model can be inaccurate in detail but still highly effective in guiding decisions. For control, what matters is not whether the model matches reality in every respect, but whether it helps identify actions that perform well under real-world conditions.","type":"content","url":"/modeling#example-a-simple-model-that-supports-better-decisions","position":7},{"hierarchy":{"lvl1":"Model Predictive Control"},"type":"lvl1","url":"/mpc","position":0},{"hierarchy":{"lvl1":"Model Predictive Control"},"content":"The trajectory optimization methods presented so far compute a complete control trajectory from an initial state to a final time or state. Once computed, this trajectory is executed without modification, making these methods fundamentally open-loop. The control function, \\mathbf{u}[k] in discrete time or \\mathbf{u}(t) in continuous time, depends only on the clock, reading off precomputed values from memory or interpolating between them. This approach assumes perfect models and no disturbances. Under these idealized conditions, repeating the same control sequence from the same initial state would always produce identical results.\n\nReal systems face modeling errors, external disturbances, and measurement noise that accumulate over time. A precomputed trajectory becomes increasingly irrelevant as these perturbations push the actual system state away from the predicted path. The solution is to incorporate feedback, making control decisions that respond to the current state rather than blindly following a predetermined schedule. While dynamic programming provides the theoretical framework for deriving feedback policies through value functions and Bellman equations, there exists a more direct approach that leverages the trajectory optimization methods already developed.","type":"content","url":"/mpc","position":1},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Closing the Loop by Replanning"},"type":"lvl3","url":"/mpc#closing-the-loop-by-replanning","position":2},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Closing the Loop by Replanning"},"content":"Model Predictive Control creates a feedback controller by repeatedly solving trajectory optimization problems. Rather than computing a single trajectory for the entire task duration, MPC solves a finite-horizon problem at each time step, starting from the current measured state. The controller then applies only the first control action from this solution before repeating the entire process. This strategy transforms any trajectory optimization method into a feedback controller.","type":"content","url":"/mpc#closing-the-loop-by-replanning","position":3},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"The Receding Horizon Principle","lvl3":"Closing the Loop by Replanning"},"type":"lvl4","url":"/mpc#the-receding-horizon-principle","position":4},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"The Receding Horizon Principle","lvl3":"Closing the Loop by Replanning"},"content":"The defining characteristic of MPC is its receding horizon strategy. At each time step, the controller solves an optimization problem looking a fixed duration into the future, but this prediction window constantly moves forward in time. The horizon “recedes” because it always starts from the current time and extends forward by the same amount.\n\nConsider the discrete-time optimal control problem in Bolza form:\\begin{aligned}\n\\text{minimize} \\quad & c_T(\\mathbf{x}_N) + \\sum_{k=0}^{N-1} c(\\mathbf{x}_k, \\mathbf{u}_k) \\\\\n\\text{subject to} \\quad & \\mathbf{x}_{k+1} = \\mathbf{f}(\\mathbf{x}_k, \\mathbf{u}_k) \\\\\n& \\mathbf{g}(\\mathbf{x}_k, \\mathbf{u}_k) \\leq \\mathbf{0} \\\\\n& \\mathbf{u}_{\\text{min}} \\leq \\mathbf{u}_k \\leq \\mathbf{u}_{\\text{max}} \\\\\n\\text{given} \\quad & \\mathbf{x}_0 = \\mathbf{x}_{\\text{current}}\n\\end{aligned}\n\nAt time step t, this problem optimizes over the interval [t, t+N]. At the next time step t+1, the horizon shifts to [t+1, t+N+1]. What makes this work is that only the first control \\mathbf{u}_0^* from each optimization is applied. The remaining controls \\mathbf{u}_1^*, \\ldots, \\mathbf{u}_{N-1}^* are discarded, though they may initialize the next optimization through warm-starting.\n\nThis receding horizon principle enables feedback without computing an explicit policy. By constantly updating predictions based on current measurements, MPC naturally corrects for disturbances and model errors. The apparent waste of computing but not using most of the trajectory is actually the mechanism that provides robustness.","type":"content","url":"/mpc#the-receding-horizon-principle","position":5},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"Horizon Selection and Problem Formulation","lvl3":"Closing the Loop by Replanning"},"type":"lvl4","url":"/mpc#horizon-selection-and-problem-formulation","position":6},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"Horizon Selection and Problem Formulation","lvl3":"Closing the Loop by Replanning"},"content":"The choice of prediction horizon depends on the control objective. We distinguish between three cases, each requiring different mathematical formulations.","type":"content","url":"/mpc#horizon-selection-and-problem-formulation","position":7},{"hierarchy":{"lvl1":"Model Predictive Control","lvl5":"Infinite-Horizon Regulation","lvl4":"Horizon Selection and Problem Formulation","lvl3":"Closing the Loop by Replanning"},"type":"lvl5","url":"/mpc#infinite-horizon-regulation","position":8},{"hierarchy":{"lvl1":"Model Predictive Control","lvl5":"Infinite-Horizon Regulation","lvl4":"Horizon Selection and Problem Formulation","lvl3":"Closing the Loop by Replanning"},"content":"For stabilization problems where the system must operate indefinitely around an equilibrium, the true objective is:J_\\infty = \\sum_{k=0}^{\\infty} c(\\mathbf{x}_k, \\mathbf{u}_k)\n\nSince this cannot be solved directly, MPC approximates it with:\\begin{aligned}\n\\text{minimize} \\quad & V_f(\\mathbf{x}_N) + \\sum_{k=0}^{N-1} c(\\mathbf{x}_k, \\mathbf{u}_k) \\\\\n\\text{subject to} \\quad & \\mathbf{x}_{k+1} = \\mathbf{f}(\\mathbf{x}_k, \\mathbf{u}_k) \\\\\n& \\mathbf{x}_N \\in \\mathcal{X}_f \\\\\n& \\text{other constraints}\n\\end{aligned}\n\nThe terminal cost V_f(\\mathbf{x}_N) approximates \\sum_{k=N}^{\\infty} c(\\mathbf{x}_k, \\mathbf{u}_k), the cost-to-go beyond the horizon. The terminal constraint \\mathbf{x}_N \\in \\mathcal{X}_f ensures the state reaches a region where a known stabilizing controller exists. Without these terminal ingredients, the finite-horizon approximation may produce unstable behavior, as the controller ignores consequences beyond the horizon.","type":"content","url":"/mpc#infinite-horizon-regulation","position":9},{"hierarchy":{"lvl1":"Model Predictive Control","lvl5":"Finite-Duration Tasks","lvl4":"Horizon Selection and Problem Formulation","lvl3":"Closing the Loop by Replanning"},"type":"lvl5","url":"/mpc#finite-duration-tasks","position":10},{"hierarchy":{"lvl1":"Model Predictive Control","lvl5":"Finite-Duration Tasks","lvl4":"Horizon Selection and Problem Formulation","lvl3":"Closing the Loop by Replanning"},"content":"For tasks ending at time t_f, the true objective spans from current time t to t_f:J_{[t, t_f]} = c_f(\\mathbf{x}(t_f)) + \\sum_{k=t}^{t_f-1} c(\\mathbf{x}_k, \\mathbf{u}_k)\n\nThe MPC formulation must adapt as time progresses:\\begin{aligned}\n\\text{minimize} \\quad & c_{T,k}(\\mathbf{x}_{N_k}) + \\sum_{j=0}^{N_k-1} c(\\mathbf{x}_j, \\mathbf{u}_j) \\\\\n\\text{where} \\quad & N_k = \\min(N, t_f - t_k) \\\\\n& c_{T,k} = \\begin{cases}\nc_f & \\text{if } t_k + N_k = t_f \\\\\nc_T & \\text{otherwise}\n\\end{cases}\n\\end{aligned}\n\nAs the task approaches completion, the horizon shrinks and the terminal cost switches from the approximation c_T to the true final cost c_f. This prevents the controller from optimizing beyond task completion, which would produce meaningless or aggressive control actions.","type":"content","url":"/mpc#finite-duration-tasks","position":11},{"hierarchy":{"lvl1":"Model Predictive Control","lvl5":"Periodic Tasks","lvl4":"Horizon Selection and Problem Formulation","lvl3":"Closing the Loop by Replanning"},"type":"lvl5","url":"/mpc#periodic-tasks","position":12},{"hierarchy":{"lvl1":"Model Predictive Control","lvl5":"Periodic Tasks","lvl4":"Horizon Selection and Problem Formulation","lvl3":"Closing the Loop by Replanning"},"content":"Some systems operate on repeating cycles where the optimal behavior depends on the time of day, week, or season. Consider a commercial building where heating costs are higher at night, electricity prices vary hourly, and occupancy patterns repeat daily. The MPC controller must account for these periodic patterns while planning over a finite horizon.\n\nFor tasks with period T_p, such as daily building operations, the formulation accounts for transitions across period boundaries:\\begin{aligned}\n\\text{minimize} \\quad & \\sum_{k=0}^{N-1} c_k(\\mathbf{x}_k, \\mathbf{u}_k, \\phi_k) \\\\\n\\text{where} \\quad & \\phi_k = (t + k) \\mod T_p \\\\\n& c_k(\\cdot, \\cdot, \\phi) = \\begin{cases}\nc_{\\text{day}}(\\cdot, \\cdot) & \\text{if } \\phi \\in [6\\text{am}, 6\\text{pm}] \\\\\nc_{\\text{night}}(\\cdot, \\cdot) & \\text{otherwise}\n\\end{cases}\n\\end{aligned}\n\nThe cost function changes based on the phase \\phi within the period. Constraints may similarly depend on the phase, reflecting different operational requirements at different times.","type":"content","url":"/mpc#periodic-tasks","position":13},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"The MPC Algorithm","lvl3":"Closing the Loop by Replanning"},"type":"lvl4","url":"/mpc#the-mpc-algorithm","position":14},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"The MPC Algorithm","lvl3":"Closing the Loop by Replanning"},"content":"The complete MPC procedure implements the receding horizon principle through repeated optimization:\n\nModel Predictive Control with Horizon Management\n\nInput:\n\nNominal prediction horizon N\n\nSampling period \\Delta t\n\nTask type: {infinite, finite with duration t_f, periodic with period T_p}\n\nCost functions and dynamics\n\nConstraints\n\nProcedure:\n\nInitialize time t \\leftarrow 0\n\nMeasure initial state \\mathbf{x}_{\\text{current}} \\leftarrow \\mathbf{x}(t)\n\nWhile task continues:\n\nDetermine effective horizon and costs:\n\nIf infinite task:\n\nN_{\\text{eff}} \\leftarrow N\n\nUse terminal cost V_f and constraint \\mathcal{X}_f\n\nIf finite task:\n\nN_{\\text{eff}} \\leftarrow \\min(N, \\lfloor(t_f - t)/\\Delta t\\rfloor)\n\nIf t + N_{\\text{eff}}\\Delta t = t_f: use final cost c_f\n\nOtherwise: use approximation c_T\n\nIf periodic task:\n\nN_{\\text{eff}} \\leftarrow N\n\nAdjust costs/constraints based on phase\n\nSolve optimization:\nMinimize over \\mathbf{u}_{0:N_{\\text{eff}}-1} subject to dynamics, constraints, and \\mathbf{x}_0 = \\mathbf{x}_{\\text{current}}\n\nApply receding horizon control:\n\nExtract \\mathbf{u}^*_0 from solution\n\nApply to system for duration \\Delta t\n\nMeasure new state\n\nAdvance time: t \\leftarrow t + \\Delta t\n\nEnd While \n### Computational Considerations\n\nThe receding horizon principle requires solving optimization problems in real-time, placing stringent demands on the solver. Each problem must be solved within the sampling period $\\Delta t$. If the solver requires more time, the system operates without new control updates, potentially degrading performance or stability.\n\nFortunately, successive MPC problems differ only in their initial conditions and possibly their horizons. This similarity enables warm-starting strategies where the previous solution initializes the current optimization. The standard approach shifts the previous trajectory forward by one time step and appends a nominal control at the end. This initialization typically lies close to the new optimum, dramatically reducing iteration counts.\n\nThe computational burden also depends on the horizon length $N$. Longer horizons provide better approximations to infinite-horizon problems and enable more sophisticated maneuvers, but increase problem size. The choice of $N$ balances solution quality against computational resources. For linear systems with quadratic costs, horizons of 10-50 steps often suffice. Nonlinear systems may require longer horizons to capture essential dynamics, though move-blocking and other parameterization techniques can reduce the effective number of decision variables.  ### Connection to Dynamic Programming\n\nThe receding horizon principle connects MPC to the dynamic programming framework covered in the next chapter. Each MPC optimization implicitly computes the optimal cost-to-go $V_N(\\mathbf{x})$ from the current state over the horizon. This finite-horizon value function approximates the true infinite-horizon value function that dynamic programming seeks globally.\n\nThe connection becomes clearer when we consider what MPC actually does: it solves a finite-horizon optimization problem and extracts only the first control action. The remaining $N-1$ steps of the optimal trajectory are discarded, but the terminal cost $V_f$ approximates the value function at the horizon boundary. This suggests hybrid approaches where approximate value functions from dynamic programming provide terminal costs for MPC, combining global optimality properties with local constraint handling capabilities.\n\nThis idea is what we would refer to as **bootstrapping** when working with temporal difference learning methods in reinforcement learning. In temporal difference methods like Q-learning or SARSA, bootstrapping occurs when we use our current estimate of the value function to update itself—essentially \"pulling ourselves up by our bootstraps.\" Similarly, MPC bootstraps by using its finite-horizon value function approximation (computed through optimization) to make decisions, even though this approximation may not be perfect. The terminal cost $V_f$ acts as a bootstrap target, providing a value estimate for states beyond the horizon that guides the optimization process. \n ","type":"content","url":"/mpc#the-mpc-algorithm","position":15},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"Successive Linearization and Quadratic Approximations","lvl3":"Closing the Loop by Replanning"},"type":"lvl4","url":"/mpc#successive-linearization-and-quadratic-approximations","position":16},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"Successive Linearization and Quadratic Approximations","lvl3":"Closing the Loop by Replanning"},"content":"For many regulation and tracking problems, the nonlinear dynamics and costs we encounter can be approximated locally by linear and quadratic functions. The basic idea is to linearize the system around the current operating point and approximate the cost with a quadratic form. This reduces each MPC subproblem to a quadratic program (QP), which can be solved reliably and very quickly using standard solvers.\n\nSuppose the true dynamics are nonlinear,\\mathbf{x}_{k+1} = f(\\mathbf{x}_k,\\mathbf{u}_k).\n\nAround a nominal trajectory (\\bar{\\mathbf{x}}_k,\\bar{\\mathbf{u}}_k), we take a first-order expansion:\\mathbf{x}_{k+1} \\approx f(\\bar{\\mathbf{x}}_k,\\bar{\\mathbf{u}}_k) \n+ \\mathbf{A}_k(\\mathbf{x}_k - \\bar{\\mathbf{x}}_k) \n+ \\mathbf{B}_k(\\mathbf{u}_k - \\bar{\\mathbf{u}}_k),\n\nwith Jacobians\\mathbf{A}_k = \\frac{\\partial f}{\\partial \\mathbf{x}}(\\bar{\\mathbf{x}}_k,\\bar{\\mathbf{u}}_k), \n\\qquad\n\\mathbf{B}_k = \\frac{\\partial f}{\\partial \\mathbf{u}}(\\bar{\\mathbf{x}}_k,\\bar{\\mathbf{u}}_k).\n\nSimilarly, if the stage cost is nonlinear,c(\\mathbf{x}_k,\\mathbf{u}_k),\n\nwe approximate it quadratically near the nominal point:c(\\mathbf{x}_k,\\mathbf{u}_k) \\;\\approx\\; \n\\|\\mathbf{x}_k - \\mathbf{x}_k^{\\text{ref}}\\|_{\\mathbf{Q}_k}^2 \n+ \\|\\mathbf{u}_k - \\mathbf{u}_k^{\\text{ref}}\\|_{\\mathbf{R}_k}^2,\n\nwith positive semidefinite weighting matrices \\mathbf{Q}_k and \\mathbf{R}_k.\n\nThe resulting MPC subproblem has the form\\begin{aligned}\n\\min_{\\mathbf{x}_{0:N},\\mathbf{u}_{0:N-1}} \\quad &\n\\|\\mathbf{x}_N - \\mathbf{x}_N^{\\text{ref}}\\|_{\\mathbf{P}}^2\n+ \\sum_{k=0}^{N-1} \n\\left(\n\\|\\mathbf{x}_k - \\mathbf{x}_k^{\\text{ref}}\\|_{\\mathbf{Q}_k}^2\n+ \\|\\mathbf{u}_k - \\mathbf{u}_k^{\\text{ref}}\\|_{\\mathbf{R}_k}^2\n\\right) \\\\\n\\text{s.t.} \\quad &\n\\mathbf{x}_{k+1} = \\mathbf{A}_k \\mathbf{x}_k + \\mathbf{B}_k \\mathbf{u}_k + \\mathbf{d}_k, \\\\\n& \\mathbf{u}_{\\min} \\leq \\mathbf{u}_k \\leq \\mathbf{u}_{\\max}, \\\\\n& \\mathbf{x}_{\\min} \\leq \\mathbf{x}_k \\leq \\mathbf{x}_{\\max}, \\\\\n& \\mathbf{x}_0 = \\mathbf{x}_{\\text{current}} ,\n\\end{aligned}\n\nwhere \\mathbf{d}_k = f(\\bar{\\mathbf{x}}_k,\\bar{\\mathbf{u}}_k) - \\mathbf{A}_k \\bar{\\mathbf{x}}_k - \\mathbf{B}_k \\bar{\\mathbf{u}}_k captures the local affine offset.\n\nBecause the dynamics are now linear and the cost quadratic, this optimization problem is a convex quadratic program. Quadratic programs are attractive in practice: they can be solved at kilohertz rates with mature numerical methods, making them the backbone of many real-time MPC implementations.\n\nAt each MPC step, the controller updates its linearization around the new operating point, constructs the local QP, and solves it. The process repeats, with the linear model and quadratic cost refreshed at every reoptimization. Despite the approximation, this yields a closed-loop controller that inherits the fast computation of QPs while retaining the ability to track trajectories of the underlying nonlinear system.","type":"content","url":"/mpc#successive-linearization-and-quadratic-approximations","position":17},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Theoretical Guarantees"},"type":"lvl3","url":"/mpc#theoretical-guarantees","position":18},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Theoretical Guarantees"},"content":"The finite-horizon approximation in MPC brings a new challenge: the controller cannot see consequences beyond the horizon. Without proper design, this myopia can destabilize even simple systems. The solution is to carefully encode information about the infinite-horizon problem into the finite-horizon optimization through its terminal conditions.\n\nBefore diving into the mathematics, we should first establish what “stability” means and which tasks these theoretical guarantees address, as the notion of stability varies significantly across different control objectives.","type":"content","url":"/mpc#theoretical-guarantees","position":19},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"Stability Notions Across Control Tasks","lvl3":"Theoretical Guarantees"},"type":"lvl4","url":"/mpc#stability-notions-across-control-tasks","position":20},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"Stability Notions Across Control Tasks","lvl3":"Theoretical Guarantees"},"content":"The terminal conditions provide different types of guarantees depending on the control objective. For regulation problems, where the task is to drive the state to a fixed equilibrium (\\mathbf{x}_\\mathrm{eq}, \\mathbf{u}_\\mathrm{eq}) (often shifted to the origin), the stability guarantee is asymptotic stability: starting sufficiently close to the equilibrium, we have \\mathbf{x}_k \\to \\mathbf{x}_\\mathrm{eq} while constraints remain satisfied throughout the trajectory (recursive feasibility). This requires the stage cost \\ell(\\mathbf{x},\\mathbf{u}) to be positive definite in the deviation from equilibrium.\n\nWhen tracking a constant setpoint, the task becomes following a constant reference (\\mathbf{x}_\\mathrm{ref},\\mathbf{u}_\\mathrm{ref}) that solves the steady-state equations. This problem is handled by working in error coordinates \\tilde{\\mathbf{x}}=\\mathbf{x}-\\mathbf{x}_\\mathrm{ref} and \\tilde{\\mathbf{u}}=\\mathbf{u}-\\mathbf{u}_\\mathrm{ref}, transforming the tracking problem into a regulation problem for the error system. The stability guarantee becomes asymptotic tracking, meaning \\tilde{\\mathbf{x}}_k \\to 0, again with recursive feasibility.\n\nThe terminal conditions we discuss below primarily address regulation and constant reference tracking. Time-varying tracking and economic MPC require additional techniques such as tube MPC and dissipativity theory.","type":"content","url":"/mpc#stability-notions-across-control-tasks","position":21},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"MPC with Stability Guarantees","lvl3":"Theoretical Guarantees"},"type":"lvl4","url":"/mpc#mpc-with-stability-guarantees","position":22},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"MPC with Stability Guarantees","lvl3":"Theoretical Guarantees"},"content":"To provide theoretical guarantees, the finite-horizon MPC problem is augmented with three interconnected components. The terminal cost V_f(\\mathbf{x}) approximates the cost-to-go beyond the horizon, providing a surrogate for the infinite-horizon tail that cannot be explicitly optimized. The terminal constraint set \\mathcal{X}_f defines a region where we have local knowledge of how to stabilize the system. Finally, the terminal controller \\kappa_f(\\mathbf{x}) provides a local stabilizing control law that remains valid within \\mathcal{X}_f.\n\nThese components must satisfy specific compatibility conditions to provide theoretical guarantees:\n\nRecursive Feasibility and Asymptotic Stability\n\nConsider the MPC problem with terminal cost V_f, terminal set \\mathcal{X}_f, and local controller \\kappa_f. If the following conditions hold:\n\nControl invariance: For all \\mathbf{x} \\in \\mathcal{X}_f, we have \\mathbf{f}(\\mathbf{x}, \\kappa_f(\\mathbf{x})) \\in \\mathcal{X}_f (the set is invariant) and \\mathbf{g}(\\mathbf{x}, \\kappa_f(\\mathbf{x})) \\leq \\mathbf{0} (constraints remain satisfied).\n\nLyapunov decrease: For all \\mathbf{x} \\in \\mathcal{X}_f:V_f(\\mathbf{f}(\\mathbf{x}, \\kappa_f(\\mathbf{x}))) - V_f(\\mathbf{x}) \\leq -\\ell(\\mathbf{x}, \\kappa_f(\\mathbf{x}))\n\nwhere \\ell is the stage cost.\n\nThen the MPC controller achieves recursive feasibility (if the problem is feasible at time k, it remains feasible at time k+1), asymptotic stability to the target equilibrium for regulation problems, and monotonic cost decrease along trajectories until the target is reached. \n### Why Terminal Conditions Work\n\nUnderstanding why the terminal conditions guarantee recursive feasibility and asymptotic stability requires examining what the controller actually does from one step to the next. Suppose at time $k$ the MPC optimizer finds an optimal sequence of controls $(\\mathbf{u}_0^*, \\ldots, \\mathbf{u}_{N-1}^*)$ and states $(\\mathbf{x}_0^*, \\ldots, \\mathbf{x}_N^*)$, where $\\mathbf{x}_N^* \\in \\mathcal{X}_f$. The first control $\\mathbf{u}_0^*$ is applied to the system, and the remaining plan is discarded according to the receding horizon principle.\n\nAt time $k+1$, we need a new plan starting from the updated state $\\mathbf{x}_{\\text{new}} = \\mathbf{x}_1^*$. A natural fallback strategy is to **shift** the previous plan forward by one step and **append the terminal controller** $\\boldsymbol{\\kappa}_f$ at the end, yielding controls $(\\mathbf{u}_1^*, \\ldots, \\mathbf{u}_{N-1}^*, \\boldsymbol{\\kappa}_f(\\mathbf{x}_N^*))$ with states recomputed from the dynamics starting from $\\mathbf{x}_1^*$.\n\nThis shifted plan may no longer be optimal, but the **Lyapunov decrease condition** ensures it remains feasible and leads to progress. The condition\n\n$$\nV_f(\\mathbf{f}(\\mathbf{x}, \\kappa_f(\\mathbf{x}))) - V_f(\\mathbf{x}) \\leq -\\ell(\\mathbf{x}, \\kappa_f(\\mathbf{x}))\n\\quad \\forall\\, \\mathbf{x} \\in \\mathcal{X}_f\n$$\n\nrequires that the terminal cost $V_f$ decrease faster than the stage cost accumulates when following $\\boldsymbol{\\kappa}_f$ inside $\\mathcal{X}_f$. This means the controller makes progress in both state evolution and predicted cost-to-go.\n\nThis \"one-step contractiveness\" property ensures that applying $\\kappa_f$ within the terminal set leads to value decrease. When used at the horizon's tail, this guarantees that even a non-optimal shifted trajectory results in lower overall cost, making $V_N$ behave like a Lyapunov function for regulation and tracking tasks.  ### Computing Terminal Conditions in Practice\n\nFor linear systems with quadratic costs, the terminal conditions follow naturally from LQR theory. The process begins by solving the infinite-horizon LQR problem:\n\n$$\\mathbf{P} = \\mathbf{Q} + \\mathbf{A}^T \\mathbf{P} \\mathbf{A} - \\mathbf{A}^T \\mathbf{P} \\mathbf{B}(\\mathbf{R} + \\mathbf{B}^T \\mathbf{P} \\mathbf{B})^{-1} \\mathbf{B}^T \\mathbf{P} \\mathbf{A}$$\n$$\\mathbf{K} = -(\\mathbf{R} + \\mathbf{B}^T \\mathbf{P} \\mathbf{B})^{-1} \\mathbf{B}^T \\mathbf{P} \\mathbf{A}$$\n\nThe terminal cost and controller then follow directly: $V_f(\\mathbf{x}) = \\mathbf{x}^T \\mathbf{P} \\mathbf{x}$ and $\\kappa_f(\\mathbf{x}) = \\mathbf{K}\\mathbf{x}$. \n\nConstructing the terminal set $\\mathcal{X}_f$ presents more options with varying computational complexity. The most powerful but computationally intensive approach computes the **maximal control-invariant set**: the largest set where $\\mathbf{u} = \\mathbf{K}\\mathbf{x}$ keeps the state feasible indefinitely. This involves fixed-point iterations on polytopes. A more tractable alternative uses **ellipsoidal approximations**, finding the largest $\\alpha$ such that $\\mathcal{X}_f = \\{\\mathbf{x} : \\mathbf{x}^T \\mathbf{P} \\mathbf{x} \\leq \\alpha\\}$ satisfies all constraints under $\\mathbf{u} = \\mathbf{K}\\mathbf{x}$. The most conservative but always feasible approach starts with a small safe set where constraints are satisfied and grows it until hitting constraint boundaries.\n\nFor nonlinear systems, we linearize around the equilibrium to compute $\\mathbf{P}$ and $\\mathbf{K}$, then verify the decrease condition holds locally. The terminal set becomes a neighborhood where the linear approximation remains valid.  \n### Performance Implications\n\nThe terminal conditions create inherent tradeoffs between conservatism and computational burden. Larger terminal sets $\\mathcal{X}_f$ provide greater regions of attraction and impose fewer restrictions on trajectories, but require more intensive computation. Smaller terminal sets may necessitate longer horizons to reach from typical initial conditions. Similarly, more accurate terminal costs $V_f$ provide tighter approximations of infinite-horizon costs, enabling effective control with shorter horizons.\n\nThe importance of terminal conditions diminishes as the horizon length increases. With $N \\to \\infty$, any stabilizing terminal controller suffices for stability. In practice, short horizons (N = 10-20) make terminal conditions crucial for stability, medium horizons (N = 20-50) benefit from but don't critically depend on them, while long horizons (N > 50) often omit them entirely, relying solely on horizon length for stability. ","type":"content","url":"/mpc#mpc-with-stability-guarantees","position":23},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"Suboptimality Bounds","lvl3":"Theoretical Guarantees"},"type":"lvl4","url":"/mpc#suboptimality-bounds","position":24},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"Suboptimality Bounds","lvl3":"Theoretical Guarantees"},"content":"The finite-horizon MPC value V_N(\\mathbf{x}) provides an upper bound approximation of the true infinite-horizon value V_\\infty(\\mathbf{x}). Understanding how close this approximation can be tells us about the effectiveness of short-horizon MPC.\n\nThe upper bound V_N(\\mathbf{x}) \\geq V_\\infty(\\mathbf{x}) follows immediately from the fact that MPC considers fewer control choices. The infinite-horizon controller can choose any sequence (\\mathbf{u}_0, \\mathbf{u}_1, \\mathbf{u}_2, \\ldots), while the N-horizon controller is restricted to sequences of the form (\\mathbf{u}_0, \\ldots, \\mathbf{u}_{N-1}, \\kappa_f(\\mathbf{x}_N), \\kappa_f(\\mathbf{x}_{N+1}), \\ldots) where the tail follows the fixed terminal controller. Since the infinite-horizon problem optimizes over a larger feasible set, its optimal value cannot exceed that of the finite-horizon problem.","type":"content","url":"/mpc#suboptimality-bounds","position":25},{"hierarchy":{"lvl1":"Model Predictive Control","lvl5":"Deriving the Approximation Error","lvl4":"Suboptimality Bounds","lvl3":"Theoretical Guarantees"},"type":"lvl5","url":"/mpc#deriving-the-approximation-error","position":26},{"hierarchy":{"lvl1":"Model Predictive Control","lvl5":"Deriving the Approximation Error","lvl4":"Suboptimality Bounds","lvl3":"Theoretical Guarantees"},"content":"The interesting question is bounding the approximation error \\varepsilon_N = V_N(\\mathbf{x}) - V_\\infty(\\mathbf{x}). This error represents the cost of being forced to use \\kappa_f beyond the horizon rather than continuing to optimize.\n\nLet (\\mathbf{u}_0^*, \\mathbf{u}_1^*, \\ldots) denote the infinite-horizon optimal control sequence with corresponding state trajectory (\\mathbf{x}_0^*, \\mathbf{x}_1^*, \\ldots) where \\mathbf{x}_0^* = \\mathbf{x}. The infinite-horizon cost is:V_\\infty(\\mathbf{x}) = \\sum_{k=0}^{\\infty} \\ell(\\mathbf{x}_k^*, \\mathbf{u}_k^*)\n\nNow consider what happens when we truncate this optimal sequence at horizon N and continue with the terminal controller. The cost becomes:\\tilde{V}_N(\\mathbf{x}) = \\sum_{k=0}^{N-1} \\ell(\\mathbf{x}_k^*, \\mathbf{u}_k^*) + V_f(\\mathbf{x}_N^*)\n\nwhere V_f(\\mathbf{x}_N^*) approximates the tail cost \\sum_{k=N}^{\\infty} \\ell(\\mathbf{x}_k^*, \\mathbf{u}_k^*).\n\nSince V_N(\\mathbf{x}) is the optimal N-horizon cost (which may do better than this particular truncated sequence), we have V_N(\\mathbf{x}) \\leq \\tilde{V}_N(\\mathbf{x}). The approximation error therefore satisfies:\\varepsilon_N \\leq \\tilde{V}_N(\\mathbf{x}) - V_\\infty(\\mathbf{x}) = V_f(\\mathbf{x}_N^*) - \\sum_{k=N}^{\\infty} \\ell(\\mathbf{x}_k^*, \\mathbf{u}_k^*)\n\nThis bound shows that the approximation error depends on how well the terminal cost V_f approximates the true tail cost along the infinite-horizon optimal trajectory. \n#### Exponential Convergence in the Linear-Quadratic Case\n\nFor linear systems $\\mathbf{x}_{k+1} = \\mathbf{A}\\mathbf{x}_k + \\mathbf{B}\\mathbf{u}_k$ with quadratic costs $\\ell(\\mathbf{x}, \\mathbf{u}) = \\mathbf{x}^T\\mathbf{Q}\\mathbf{x} + \\mathbf{u}^T\\mathbf{R}\\mathbf{u}$, we can compute this error exactly. When the terminal cost is the LQR cost-to-go $V_f(\\mathbf{x}) = \\mathbf{x}^T\\mathbf{P}\\mathbf{x}$, the infinite-horizon optimal trajectory satisfies $\\mathbf{x}_k^* = \\mathbf{A}_{cl}^k \\mathbf{x}$ where $\\mathbf{A}_{cl} = \\mathbf{A} + \\mathbf{B}\\mathbf{K}$ is the closed-loop matrix.\n\nThe tail cost from time $N$ onward becomes:\n$\\sum_{k=N}^{\\infty} \\ell(\\mathbf{x}_k^*, \\mathbf{u}_k^*) = \\sum_{k=N}^{\\infty} (\\mathbf{x}_k^*)^T \\mathbf{Q}_{cl} \\mathbf{x}_k^* = (\\mathbf{x}_N^*)^T \\left(\\sum_{k=0}^{\\infty} (\\mathbf{A}_{cl}^T)^k \\mathbf{Q}_{cl} \\mathbf{A}_{cl}^k\\right) \\mathbf{x}_N^*$\n\nwhere $\\mathbf{Q}_{cl} = \\mathbf{Q} + \\mathbf{K}^T\\mathbf{R}\\mathbf{K}$ captures the quadratic cost under the optimal controller $\\mathbf{u} = \\mathbf{K}\\mathbf{x}$.\n\nThe infinite sum equals $\\mathbf{P}$ by definition of the LQR solution, so the terminal cost $V_f(\\mathbf{x}_N^*) = (\\mathbf{x}_N^*)^T \\mathbf{P} \\mathbf{x}_N^*$ exactly matches the true tail cost when computed along the infinite-horizon optimal trajectory. This gives $\\varepsilon_N = 0$ when following the infinite-horizon optimal path exactly!\n\nHowever, the finite-horizon optimizer typically finds a different trajectory for the first $N$ steps, leading to a different $\\mathbf{x}_N$. The approximation error then depends on how much the finite-horizon trajectory deviates from the infinite-horizon one. Since both are optimal for their respective problems and the terminal cost provides the correct tail approximation, this deviation shrinks exponentially with horizon length at a rate determined by the eigenvalues of $\\mathbf{A}_{cl}$.\n\nSpecifically, if $\\rho(\\mathbf{A}_{cl})$ denotes the spectral radius of the closed-loop matrix, then:\n\n$\\varepsilon_N = O(\\rho(\\mathbf{A}_{cl})^N)$\n\nFor stable systems, $\\rho(\\mathbf{A}_{cl}) < 1$, yielding exponential convergence. This explains why short horizons (N = 10-30) often achieve near-optimal performance: the approximation error decreases exponentially fast, making even modest horizons highly effective for regulation and constant reference tracking tasks.\n\n#### Implications for Horizon Selection\n\nThese bounds provide practical guidance for choosing prediction horizons. The exponential convergence means that beyond a certain horizon length, further increases yield diminishing returns. The optimal horizon balances approximation accuracy against computational cost, with the break-even point typically occurring when $\\rho(\\mathbf{A}_{cl})^N$ drops below the desired tolerance level.\n\nFor systems with slow dynamics (eigenvalues close to one), longer horizons may be necessary, while systems with fast dynamics achieve good approximations with surprisingly short horizons. This analysis also explains why terminal conditions become less critical as horizons increase: the exponential decay ensures that the tail beyond any reasonable horizon contributes negligibly to the total cost.  \n### When Terminal Constraints Cause Infeasibility\n\nThe terminal constraint $\\mathbf{x}_N \\in \\mathcal{X}_f$ can make the optimization infeasible, especially for:\n- Large disturbances pushing the state far from equilibrium\n- Short horizons that cannot reach $\\mathcal{X}_f$ in time\n- Conservative terminal sets that are unnecessarily small\n\nCommon remedies:\n\n1. **Soft terminal constraints**: Replace hard constraint with penalty\n   $$\\text{minimize} \\quad V_f(\\mathbf{x}_N) + \\rho \\cdot d(\\mathbf{x}_N, \\mathcal{X}_f) + \\ldots$$\n   where $d(\\cdot, \\mathcal{X}_f)$ measures distance to the set\n\n2. **Adaptive horizons**: Extend horizon when far from $\\mathcal{X}_f$\n\n3. **Backup strategy**: If infeasible, switch to unconstrained MPC or a fallback controller, then re-enable terminal constraints once feasible\n\nThe choice depends on whether theoretical guarantees or practical performance takes priority. Many industrial implementations omit terminal constraints entirely, relying on well-tuned horizons and costs to ensure stability.\n  \n# The Landscape of MPC Variants\n\nOnce the basic idea of receding horizon control is clear, it is helpful to see how the same backbone accommodates many variations. In every case, we transcribe the continuous problem to a nonlinear program of the form\n\n$$\n\\begin{aligned}\n\\text{minimize}\\quad & c_T(\\mathbf{x}_N)+\\sum_{k=0}^{N-1} w_k\\,c(\\mathbf{x}_k,\\mathbf{u}_k) \\\\\n\\text{subject to}\\quad & \\mathbf{x}_{k+1}=\\mathbf{F}_k(\\mathbf{x}_k,\\mathbf{u}_k) \\\\\n& \\mathbf{g}(\\mathbf{x}_k,\\mathbf{u}_k)\\leq \\mathbf{0}, \\\\\n& \\mathbf{x}_{\\min}\\leq \\mathbf{x}_k\\leq \\mathbf{x}_{\\max},\\quad \\mathbf{u}_{\\min}\\leq \\mathbf{u}_k\\leq \\mathbf{u}_{\\max}, \\\\\n& \\mathbf{x}_0=\\mathbf{x}_{\\text{current}} ,\n\\end{aligned}\n$$\n\nwith \\$\\mathbf{F}\\_k\\$ the chosen discretization of the dynamics and \\$w\\_k\\$ the quadrature weights. From this skeleton, several families of MPC emerge.\n\nIn **tracking MPC**, the stage and terminal costs are quadratic penalties on deviation from a reference trajectory,\n\n$$\nc(\\mathbf{x}_k,\\mathbf{u}_k)=\\|\\mathbf{x}_k-\\mathbf{x}_k^{\\text{ref}}\\|_{\\mathbf{Q}}^2+\\|\\mathbf{u}_k-\\mathbf{u}_k^{\\text{ref}}\\|_{\\mathbf{R}}^2 , \\qquad c_T(\\mathbf{x}_N)=\\|\\mathbf{x}_N-\\mathbf{x}_N^{\\text{ref}}\\|_{\\mathbf{P}}^2 .\n$$\n\nThis is the industrial workhorse, ensuring that the system follows a desired profile within limits.\n\nIn **regulatory MPC**, the reference is fixed at an equilibrium \\$(\\mathbf{x}^e,\\mathbf{u}^e)\\$, and the quadratic penalty encourages return to this point. Terminal constraints are often added so that stability can be guaranteed.\n\nIn **economic MPC**, the quadratic structure disappears altogether. Instead, the cost encodes economic performance,\n\n$$\nc(\\mathbf{x}_k,\\mathbf{u}_k)=c_{\\text{econ}}(\\mathbf{x}_k,\\mathbf{u}_k),\n$$\n\nfor instance energy cost, profit, or resource efficiency. The optimization then steers the system not toward a setpoint but toward economically optimal regimes.\n\nWhen uncertainty is represented by bounded sets, one arrives at **robust MPC**, which seeks controls that satisfy the constraints for all admissible disturbances. The resulting NLP has a min–max structure. A tractable alternative is tube MPC, where the nominal optimization is carried out with tightened constraints to guarantee feasibility of the true system under a disturbance feedback law.\n\nIf uncertainty is stochastic, the formulation turns into **stochastic MPC**, where the objective is the expected cost and the constraints are imposed with high probability,\n\n$$\n\\mathbb{P}\\!\\left[\\mathbf{g}(\\mathbf{x}_k,\\mathbf{u}_k)\\leq \\mathbf{0}\\right]\\geq 1-\\varepsilon .\n$$\n\nScenario-based versions replace expectations by a sampled, deterministic problem.\n\nSome systems require discrete choices, such as switching devices on or off. **Hybrid MPC** introduces integer variables into the transcription, producing a mixed-integer NLP that can handle such logic.\n\nFor large networks of subsystems, **distributed MPC** coordinates several local predictive controllers that optimize their own subsystems while communicating through coupling constraints.\n\nFinally, in settings where models are uncertain or slowly drifting, one finds **adaptive or learning-based MPC**, which uses parameter estimation or machine learning to update the model \\$\\mathbf{F}\\_k(\\cdot;\\theta\\_t)\\$ and possibly the cost function. The optimization step remains the same, but the model evolves as more data are collected.\n\nThese formulations illustrate that MPC is less a single algorithm than a recipe. The scaffolding is always the same: finite horizon prediction, state and input constraints, and receding horizon application of the control. What changes from one variant to another is the cost function, the way uncertainty is treated, the presence of discrete decisions, or the architecture across multiple agents. ","type":"content","url":"/mpc#deriving-the-approximation-error","position":27},{"hierarchy":{"lvl1":"Model Predictive Control","lvl2":"The Landscape of MPC Variants"},"type":"lvl2","url":"/mpc#the-landscape-of-mpc-variants","position":28},{"hierarchy":{"lvl1":"Model Predictive Control","lvl2":"The Landscape of MPC Variants"},"content":"Once the basic idea of receding-horizon control is clear, it is helpful to see how the same backbone accommodates many variations. In every case, we transcribe the continuous-time optimal control problem into a nonlinear program of the form\\begin{aligned}\n    \\text{minimize} \\quad & c(\\mathbf{x}_N) + \\sum_{k=0}^{N-1} w_k\\,c(\\mathbf{x}_k, \\mathbf{u}_k) \\\\\n    \\text{subject to} \\quad & \\mathbf{x}_{k+1} = \\mathbf{F}_k(\\mathbf{x}_k, \\mathbf{u}_k) \\\\\n                            & \\mathbf{g}(\\mathbf{x}_k, \\mathbf{u}_k) \\leq \\mathbf{0} \\\\\n                            & \\mathbf{x}_{\\min} \\leq \\mathbf{x}_k \\leq \\mathbf{x}_{\\max} \\\\\n                            & \\mathbf{u}_{\\min} \\leq \\mathbf{u}_k \\leq \\mathbf{u}_{\\max} \\\\\n    \\text{given} \\quad & \\mathbf{x}_0 = \\hat{\\mathbf{x}}(t) \\enspace .\n\\end{aligned}\n\nThe components in this NLP come from discretizing the continuous-time problem with a fixed horizon [t, t+T] and step size \\Delta t. The stage weights w_k and discrete dynamics \\mathbf{F}_k are determined by the choice of quadrature and integration scheme. With this blueprint in place, the rest is a matter of interpretation: how we define the cost, how we handle uncertainty, how we treat constraints, and what structure we exploit.","type":"content","url":"/mpc#the-landscape-of-mpc-variants","position":29},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Tracking MPC","lvl2":"The Landscape of MPC Variants"},"type":"lvl3","url":"/mpc#tracking-mpc","position":30},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Tracking MPC","lvl2":"The Landscape of MPC Variants"},"content":"The most common setup is reference tracking. Here, we are given time-varying target trajectories (\\mathbf{x}_k^{\\text{ref}}, \\mathbf{u}_k^{\\text{ref}}), and the controller’s job is to keep the system close to these. The cost is typically quadratic:\\begin{aligned}\n    c(\\mathbf{x}_k, \\mathbf{u}_k) &= \\| \\mathbf{x}_k - \\mathbf{x}_k^{\\text{ref}} \\|_{\\mathbf{Q}}^2 + \\| \\mathbf{u}_k - \\mathbf{u}_k^{\\text{ref}} \\|_{\\mathbf{R}}^2 \\\\\n    c(\\mathbf{x}_N) &= \\| \\mathbf{x}_N - \\mathbf{x}_N^{\\text{ref}} \\|_{\\mathbf{P}}^2 \\enspace .\n\\end{aligned}\n\nWhen dynamics are linear and constraints are polyhedral, this yields a convex quadratic program at each time step.","type":"content","url":"/mpc#tracking-mpc","position":31},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Regulatory MPC","lvl2":"The Landscape of MPC Variants"},"type":"lvl3","url":"/mpc#regulatory-mpc","position":32},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Regulatory MPC","lvl2":"The Landscape of MPC Variants"},"content":"In regulation tasks, we aim to bring the system back to an equilibrium point (\\mathbf{x}^e, \\mathbf{u}^e), typically in the presence of disturbances. This is simply tracking MPC with constant references:\\begin{aligned}\n    c(\\mathbf{x}_k, \\mathbf{u}_k) &= \\| \\mathbf{x}_k - \\mathbf{x}^e \\|_{\\mathbf{Q}}^2 + \\| \\mathbf{u}_k - \\mathbf{u}^e \\|_{\\mathbf{R}}^2 \\\\\n    c(\\mathbf{x}_N) &= \\| \\mathbf{x}_N - \\mathbf{x}^e \\|_{\\mathbf{P}}^2 \\enspace .\n\\end{aligned}\n\nTo guarantee stability, it is common to include a terminal constraint \\mathbf{x}_N \\in \\mathcal{X}_f, where \\mathcal{X}_f is a control-invariant set under a known feedback law.","type":"content","url":"/mpc#regulatory-mpc","position":33},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Economic MPC","lvl2":"The Landscape of MPC Variants"},"type":"lvl3","url":"/mpc#economic-mpc","position":34},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Economic MPC","lvl2":"The Landscape of MPC Variants"},"content":"Not all systems operate around a reference. Sometimes the goal is to optimize a true economic objective (eg. energy cost, revenue, efficiency) directly. This gives rise to economic MPC, where the cost functions reflect real operational performance:c(\\mathbf{x}_k, \\mathbf{u}_k) = c_{\\text{op}}(\\mathbf{x}_k, \\mathbf{u}_k), \\qquad\nc(\\mathbf{x}_N) = c_{\\text{op},T}(\\mathbf{x}_N) \\enspace .\n\nThere is no reference trajectory here. The optimal behavior emerges from the cost itself. In this setting, standard stability arguments no longer apply automatically, and one must be careful to add terminal penalties or constraints that ensure the closed-loop system remains well-behaved.","type":"content","url":"/mpc#economic-mpc","position":35},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Robust MPC","lvl2":"The Landscape of MPC Variants"},"type":"lvl3","url":"/mpc#robust-mpc","position":36},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Robust MPC","lvl2":"The Landscape of MPC Variants"},"content":"Some systems are exposed to external disturbances or small errors in the model. In those cases, we want the controller to make decisions that will still work no matter what happens, as long as the disturbances stay within some known bounds. This is the idea behind robust MPC.\n\nInstead of planning a single trajectory, the controller plans a “nominal” path (what would happen in the absence of any disturbance) and then adds a feedback correction to react to whatever disturbances actually occur. This looks like:\\mathbf{u}_k = \\bar{\\mathbf{u}}_k + \\mathbf{K} (\\mathbf{x}_k - \\bar{\\mathbf{x}}_k) \\enspace ,\n\nwhere \\bar{\\mathbf{u}}_k is the planned input and \\mathbf{K} is a feedback gain that pulls the system back toward the nominal path if it deviates.\n\nBecause we know the worst-case size of the disturbance, we can estimate how far the real state might drift from the plan, and “shrink” the constraints accordingly. The result is that the nominal plan is kept safely away from constraint boundaries, so even if the system gets pushed around, it stays inside limits. This is often called tube MPC because the true trajectory stays inside a tube around the nominal one.\n\nThe main benefit is that we can handle uncertainty without solving a complicated worst-case optimization at every time step. All the uncertainty is accounted for in the design of the feedback \\mathbf{K} and the tightened constraints.","type":"content","url":"/mpc#robust-mpc","position":37},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Stochastic MPC","lvl2":"The Landscape of MPC Variants"},"type":"lvl3","url":"/mpc#stochastic-mpc","position":38},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Stochastic MPC","lvl2":"The Landscape of MPC Variants"},"content":"If disturbances are random rather than adversarial, a natural goal is to optimize expected cost while enforcing constraints probabilistically. This gives rise to stochastic MPC, in which:\n\nThe cost becomes an expectation:\\mathbb{E} \\left[ c(\\mathbf{x}_N) + \\sum_{k=0}^{N-1} w_k\\, c(\\mathbf{x}_k, \\mathbf{u}_k) \\right]\n\nConstraints are allowed to be violated with small probability:\\mathbb{P}[\\mathbf{g}(\\mathbf{x}_k, \\mathbf{u}_k) \\leq \\mathbf{0}] \\geq 1 - \\varepsilon\n\nIn practice, expectations are approximated using a finite set of disturbance scenarios drawn ahead of time. For each scenario, the system dynamics are simulated forward using the same control inputs \\mathbf{u}_k, which are shared across all scenarios to respect non-anticipativity. The result is a single deterministic optimization problem with multiple parallel copies of the dynamics, one per sampled future. This retains the standard MPC structure, with only moderate growth in problem size.\n\nDespite appearances, this is not dynamic programming. There is no value function or tree of all possible paths. There is only a finite set of futures chosen a priori, and optimized over directly. This scenario-based approach is common in energy systems such as hydro scheduling, where inflows are uncertain but sample trajectories can be generated from forecasts.\n\nRisk constraints are typically enforced across all scenarios or encoded using risk measures like CVaR. For example, one might penalize violations that occur in the worst (1 - \\alpha)\\% of samples, while still optimizing expected performance overall.","type":"content","url":"/mpc#stochastic-mpc","position":39},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Hybrid and Mixed-Integer MPC","lvl2":"The Landscape of MPC Variants"},"type":"lvl3","url":"/mpc#hybrid-and-mixed-integer-mpc","position":40},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Hybrid and Mixed-Integer MPC","lvl2":"The Landscape of MPC Variants"},"content":"When systems involve discrete switches  (eg. on/off valves, mode selection, or combinatorial logic) the MPC problem must include integer or binary variables. These show up in constraints like\\boldsymbol{\\delta}_k \\in \\{0,1\\}^m, \\qquad \\mathbf{u}_k \\in \\mathcal{U}(\\boldsymbol{\\delta}_k)\n\nalong with mode-dependent dynamics and costs. The resulting formulation is a mixed-integer nonlinear program (MINLP). The receding-horizon idea is the same, but each solve is more expensive due to the combinatorial nature of the decision space.","type":"content","url":"/mpc#hybrid-and-mixed-integer-mpc","position":41},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Distributed and Decentralized MPC","lvl2":"The Landscape of MPC Variants"},"type":"lvl3","url":"/mpc#distributed-and-decentralized-mpc","position":42},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Distributed and Decentralized MPC","lvl2":"The Landscape of MPC Variants"},"content":"Large-scale systems often consist of interacting subsystems. Distributed MPC decomposes the global NLP into smaller ones that run in parallel, with coordination constraints enforcing consistency across shared variables:\\sum_{i} \\mathbf{H}^i \\mathbf{z}^i_k = \\mathbf{0} \\qquad \\text{(coupling constraint)}\n\nEach subsystem solves a local problem over its own state and input variables, then exchanges information with neighbors. Coordination can be done via primal–dual methods, ADMM, or consensus schemes, but each local block looks like a standard MPC problem.","type":"content","url":"/mpc#distributed-and-decentralized-mpc","position":43},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Adaptive and Learning-Based MPC","lvl2":"The Landscape of MPC Variants"},"type":"lvl3","url":"/mpc#adaptive-and-learning-based-mpc","position":44},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Adaptive and Learning-Based MPC","lvl2":"The Landscape of MPC Variants"},"content":"In practice, we may not know the true model \\mathbf{F}_k or cost function c precisely. In adaptive MPC, these are updated online from data:\\mathbf{x}_{k+1} = \\mathbf{F}_k(\\mathbf{x}_k, \\mathbf{u}_k; \\boldsymbol{\\theta}_t), \\qquad\nc(\\mathbf{x}_k, \\mathbf{u}_k) = c(\\mathbf{x}_k, \\mathbf{u}_k; \\boldsymbol{\\phi}_t)\n\nThe parameters \\boldsymbol{\\theta}_t and \\boldsymbol{\\phi}_t are learned in real time. When combined with policy distillation, value approximation, or trajectory imitation, this leads to overlaps with reinforcement learning where the MPC solutions act as supervision for a reactive policy.","type":"content","url":"/mpc#adaptive-and-learning-based-mpc","position":45},{"hierarchy":{"lvl1":"Model Predictive Control","lvl2":"Robustness in Real-Time MPC"},"type":"lvl2","url":"/mpc#robustness-in-real-time-mpc","position":46},{"hierarchy":{"lvl1":"Model Predictive Control","lvl2":"Robustness in Real-Time MPC"},"content":"The trajectory optimization methods we have studied assume perfect models and deterministic dynamics. In practice, however, MPC controllers must operate in environments where models are approximate, disturbances are unpredictable, and computational resources are limited. The mathematical elegance of optimal control must always yield to the engineering reality of robust operation as perfect optimality is less important than reliable operation. This philosophy permeates industrial MPC applications. A controller that achieves 95% performance 100% of the time is superior to one that achieves 100% performance 95% of the time and fails catastrophically the remaining 5%. Airlines accept suboptimal fuel consumption over missed approaches, power grids tolerate efficiency losses to prevent blackouts, and chemical plants sacrifice yield for safety. By designing for failure, we want to to create MPC systems that degrade gracefully rather than fail catastrophically, maintaining safety and stability even when the impossible is asked of them.","type":"content","url":"/mpc#robustness-in-real-time-mpc","position":47},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Example: Wind Farm Yield Optimization","lvl2":"Robustness in Real-Time MPC"},"type":"lvl3","url":"/mpc#example-wind-farm-yield-optimization","position":48},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Example: Wind Farm Yield Optimization","lvl2":"Robustness in Real-Time MPC"},"content":"Consider a wind farm where MPC controllers coordinate individual turbines to maximize overall power production while minimizing wake interference. Each turbine can adjust both its thrust coefficient (through blade pitch) and yaw angle to redirect its wake away from downstream turbines. At time t_k, the MPC controller solves the optimization problem:\\begin{aligned}\n\\min_{\\mathbf{u}_{0:N-1}} \\quad & \\sum_{i=0}^{N-1} \\|\\mathbf{x}_i - \\mathbf{x}_i^{\\text{ref}}\\|_{\\mathbf{Q}}^2 + \\|\\mathbf{u}_i\\|_{\\mathbf{R}}^2 \\\\\n\\text{s.t.} \\quad & \\mathbf{x}_{i+1} = \\mathbf{f}(\\mathbf{x}_i, \\mathbf{u}_i) \\\\\n& \\mathbf{x}_i \\in \\mathcal{X}_{\\text{safe}} \\\\\n& \\|\\mathbf{u}_i\\|_\\infty \\leq u_{\\max} \\\\\n& \\mathbf{x}_0 = \\mathbf{x}_{\\text{current}}\n\\end{aligned}\n\nNow suppose an unexpected wind direction change occurs, shifting the incoming wind vector by 30 degrees. The current state \\mathbf{x}_{\\text{current}} reflects wake patterns that no longer align with the new wind direction, and the optimizer discovers that no feasible trajectory exists that can redirect all wakes appropriately within the physical limits of yaw rate and thrust adjustment. The solver reports infeasibility.\n\nThis scenario reveals the fundamental challenge of real-time MPC: constraint incompatibility. When disturbances push the system into states from which recovery appears impossible, or when reference trajectories demand physically impossible maneuvers, the intersection of all constraint sets becomes empty. Model mismatch compounds this problem as prediction errors accumulate over the horizon.\n\nEven when feasible solutions exist, computational constraints can prevent their discovery. A control loop running at 100 Hz allows only 10 milliseconds per iteration. If the solver requires 15 milliseconds to converge, we face an impossible choice: delay the control action and risk destabilizing the system, or apply an unconverged iterate that may violate critical constraints.\n\nA third failure mode involves numerical instabilities: ill-conditioned matrices, rank deficiency, or division by zero in the linear algebra routines. These failures are particularly problematic because they occur sporadically, triggered by specific state configurations that create near-singular conditions in the optimization problem.","type":"content","url":"/mpc#example-wind-farm-yield-optimization","position":49},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Softening Constraints Through Slack Variables","lvl2":"Robustness in Real-Time MPC"},"type":"lvl3","url":"/mpc#softening-constraints-through-slack-variables","position":50},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Softening Constraints Through Slack Variables","lvl2":"Robustness in Real-Time MPC"},"content":"The first approach to handling infeasibility recognizes that not all constraints carry equal importance. A chemical reactor’s temperature must never exceed the runaway threshold: this is a hard constraint that cannot be violated. However, maintaining temperature within an optimal efficiency band is merely desirable. This can be treated as a soft constraint that we prefer to satisfy but can relax when necessary.\n\nThis hierarchy motivates reformulating the optimization problem using slack variables:\\begin{aligned}\n\\min_{\\mathbf{u}, \\boldsymbol{\\epsilon}} \\quad & \\sum_{i=0}^{N-1} \\|\\mathbf{x}_i - \\mathbf{x}_i^{\\text{ref}}\\|_{\\mathbf{Q}}^2 + \\|\\mathbf{u}_i\\|_{\\mathbf{R}}^2 + \\boldsymbol{\\rho}^T \\boldsymbol{\\epsilon}_i \\\\\n\\text{s.t.} \\quad & \\mathbf{x}_{i+1} = \\mathbf{f}(\\mathbf{x}_i, \\mathbf{u}_i) \\\\\n& \\mathbf{g}_{\\text{hard}}(\\mathbf{x}_i, \\mathbf{u}_i) \\leq \\mathbf{0} \\\\\n& \\mathbf{g}_{\\text{soft}}(\\mathbf{x}_i, \\mathbf{u}_i) \\leq \\boldsymbol{\\epsilon}_i \\\\\n& \\boldsymbol{\\epsilon}_i \\geq \\mathbf{0}\n\\end{aligned}\n\nThe penalty weights \\boldsymbol{\\rho} encode our priorities. Safety constraints might use \\rho_j = 10^6, while comfort constraints use \\rho_j = 1. This reformulated problem is always feasible as long as the hard constraints alone admit a solution. That is: we can always make the slack variables \\boldsymbol{\\epsilon} sufficiently large to satisfy the soft constraints.\n\nRather than treating constraints as binary hard/soft categories, we can establish a constraint hierarchy that enables graceful degradation:\\begin{aligned}\n\\text{Safety:} \\quad & T_{\\text{reactor}} \\leq T_{\\text{runaway}} - 10 \\quad & \\rho = \\infty \\text{ (hard)} \\\\\n\\text{Equipment:} \\quad & 0 \\leq u_{\\text{valve}} \\leq 100 \\quad & \\rho = 10^4 \\\\\n\\text{Efficiency:} \\quad & T_{\\text{optimal}} - 5 \\leq T \\leq T_{\\text{optimal}} + 5 \\quad & \\rho = 10^2 \\\\\n\\text{Comfort:} \\quad & |T - T_{\\text{setpoint}}| \\leq 1 \\quad & \\rho = 1\n\\end{aligned}\n\nAs conditions deteriorate, the controller abandons objectives in reverse priority order, maintaining safety even when optimality becomes impossible.","type":"content","url":"/mpc#softening-constraints-through-slack-variables","position":51},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Feasibility Restoration","lvl2":"Robustness in Real-Time MPC"},"type":"lvl3","url":"/mpc#feasibility-restoration","position":52},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Feasibility Restoration","lvl2":"Robustness in Real-Time MPC"},"content":"When even soft constraints prove insufficient (perhaps due to catastrophic solver failure or corrupted problem structure) we need feasibility restoration that finds any feasible point regardless of optimality:\\begin{aligned}\n\\min_{\\mathbf{u}, \\mathbf{s}} \\quad & \\|\\mathbf{s}\\|_1 \\\\\n\\text{s.t.} \\quad & \\mathbf{x}_{i+1} = \\mathbf{f}(\\mathbf{x}_i, \\mathbf{u}_i) + \\mathbf{s}_i \\\\\n& \\mathbf{x}_{\\min} - \\mathbf{s}_{x,i} \\leq \\mathbf{x}_i \\leq \\mathbf{x}_{\\max} + \\mathbf{s}_{x,i} \\\\\n& \\mathbf{u}_{\\min} \\leq \\mathbf{u}_i \\leq \\mathbf{u}_{\\max} \\\\\n& \\mathbf{s} \\geq \\mathbf{0}\n\\end{aligned}\n\nThis formulation temporarily relaxes even the dynamics constraints, finding the “least infeasible” solution. It answers the question: if we must violate something, what is the minimal violation required? Once feasibility is restored, we can warm-start the original problem from this point.","type":"content","url":"/mpc#feasibility-restoration","position":53},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Reference Governors","lvl2":"Robustness in Real-Time MPC"},"type":"lvl3","url":"/mpc#reference-governors","position":54},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Reference Governors","lvl2":"Robustness in Real-Time MPC"},"content":"Rather than reacting to infeasibility after it occurs, we can prevent it by filtering references through a reference governor. Consider an aircraft following waypoints. Instead of passing waypoints directly to the MPC, the governor asks: what is the closest approachable reference from our current state?\\mathbf{r}_{\\text{filtered}} = \\arg\\max_{\\kappa \\in [0,1]} \\kappa \\quad \\text{s.t. MPC}(\\mathbf{x}_{\\text{current}}, \\kappa \\mathbf{r}_{\\text{desired}} + (1-\\kappa)\\mathbf{x}_{\\text{current}}) \\text{ is feasible}\n\nThe governor performs a line search between the current state (always feasible since staying put requires no action) and the desired reference (potentially infeasible). This guarantees the MPC always receives feasible problems while making maximum progress toward the goal.\n\nFor computational efficiency, we can pre-compute the maximal output admissible set:\\mathcal{O}_\\infty = \\{\\mathbf{r} : \\exists \\text{ feasible trajectory from } \\mathbf{x} \\text{ to } \\mathbf{r} \\text{ respecting all constraints}\\}\n\nOnline, the governor simply projects the desired reference onto \\mathcal{O}_\\infty.","type":"content","url":"/mpc#reference-governors","position":55},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Backup Controllers","lvl2":"Robustness in Real-Time MPC"},"type":"lvl3","url":"/mpc#backup-controllers","position":56},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Backup Controllers","lvl2":"Robustness in Real-Time MPC"},"content":"When MPC fails entirely (due to solver crashes, timeouts, or numerical failures) we need backup controllers that require minimal computation while guaranteeing stability and keeping the system away from dangerous regions.\n\nThe standard approach uses a pre-computed local LQR controller around the equilibrium:\\mathbf{K}_{\\text{LQR}}, \\mathbf{P} = \\text{LQR}(\\mathbf{A}, \\mathbf{B}, \\mathbf{Q}, \\mathbf{R})\n\nwhere (\\mathbf{A}, \\mathbf{B}) are the linearized dynamics at equilibrium. When MPC fails:\\mathbf{u}_{\\text{backup}} = \\begin{cases}\n\\mathbf{K}_{\\text{LQR}}(\\mathbf{x} - \\mathbf{x}_{\\text{eq}}) & \\text{if } \\mathbf{x} \\in \\mathcal{X}_{\\text{LQR}} \\\\\n\\mathbf{u}_{\\text{safe}} & \\text{otherwise}\n\\end{cases}\n\nThe region \\mathcal{X}_{\\text{LQR}} = \\{\\mathbf{x} : (\\mathbf{x} - \\mathbf{x}_{\\text{eq}})^T \\mathbf{P} (\\mathbf{x} - \\mathbf{x}_{\\text{eq}}) \\leq \\alpha\\} represents the largest invariant set where LQR is guaranteed to work.","type":"content","url":"/mpc#backup-controllers","position":57},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Cascade Architectures","lvl2":"Robustness in Real-Time MPC"},"type":"lvl3","url":"/mpc#cascade-architectures","position":58},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Cascade Architectures","lvl2":"Robustness in Real-Time MPC"},"content":"Production MPC systems rarely rely on a single solver. Instead, they implement a cascade of increasingly conservative controllers that trade optimality for reliability:def get_control(self, x, time_budget):\n    \"\"\"\n    Multi-level cascade for robust real-time control\n    \"\"\"\n    time_remaining = time_budget\n    \n    # Level 1: Full nonlinear MPC\n    if time_remaining > 5e-3:  # 5ms minimum\n        try:\n            u, solve_time = self.solve_nmpc(x, time_remaining)\n            if converged:\n                return u\n        except:\n            pass\n        time_remaining -= solve_time\n    \n    # Level 2: Simplified linear MPC\n    if time_remaining > 1e-3:  # 1ms minimum\n        try:\n            # Linearize around current state\n            A, B = self.linearize_dynamics(x)\n            u, solve_time = self.solve_lmpc(x, A, B, time_remaining)\n            return u\n        except:\n            pass\n        time_remaining -= solve_time\n    \n    # Level 3: Explicit MPC lookup\n    if time_remaining > 1e-4:  # 0.1ms minimum\n        region = self.find_critical_region(x)\n        if region is not None:\n            return self.explicit_control_law[region](x)\n    \n    # Level 4: LQR backup\n    if self.in_lqr_region(x):\n        return self.K_lqr @ (x - self.x_eq)\n    \n    # Level 5: Emergency safe mode\n    return self.emergency_stop(x)\n\nEach level trades optimality for reliability: Level 1 provides optimal but computationally expensive control, Level 2 offers suboptimal but faster solutions, Level 3 provides pre-computed instant evaluation, Level 4 ensures stabilizing control without tracking, and Level 5 implements safe shutdown.\n\nEven when using backup controllers, we can maintain solution continuity through persistent warm-starting:\\begin{aligned}\n\\mathbf{z}_{\\text{warm}}^{(k+1)} = \\begin{cases}\n\\text{shift}(\\mathbf{z}^{(k)}) & \\text{if MPC succeeded at time } k \\\\\n\\text{lift}(\\mathbf{u}_{\\text{backup}}^{(k)}) & \\text{if backup controller used} \\\\\n\\text{propagate}(\\mathbf{z}_{\\text{warm}}^{(k)}) & \\text{if maintaining virtual solution}\n\\end{cases}\n\\end{aligned}\n\nThe shift operation takes a successful MPC solution and moves it forward by one time step, appending a terminal action: [\\mathbf{u}_1^{(k)}, \\mathbf{u}_2^{(k)}, \\ldots, \\mathbf{u}_{N-1}^{(k)}, \\kappa_f(\\mathbf{x}_N^{(k)})]. This shifted sequence provides natural temporal continuity for the next optimization.\n\nWhen MPC fails and backup control is applied, the lift operation extends the single backup action \\mathbf{u}_{\\text{backup}}^{(k)} into a full horizon-length sequence, either by repetition or by simulating the backup controller forward. This creates a reasonable warm-start guess from limited information.\n\nThe propagate operation maintains a “virtual” trajectory by continuing to evolve the previous solution as if it were still being executed, even when the actual system follows backup control. This forward simulation keeps the warm-start temporally aligned and relevant for when MPC recovers.","type":"content","url":"/mpc#cascade-architectures","position":59},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Example: Chemical Reactor Control Under Failure","lvl2":"Robustness in Real-Time MPC"},"type":"lvl3","url":"/mpc#example-chemical-reactor-control-under-failure","position":60},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Example: Chemical Reactor Control Under Failure","lvl2":"Robustness in Real-Time MPC"},"content":"Consider a continuous stirred tank reactor (CSTR) where an exothermic reaction must be controlled:\\begin{aligned}\n\\dot{C}_A &= \\frac{q}{V}(C_{A,in} - C_A) - k_0 e^{-E/RT} C_A \\\\\n\\dot{T} &= \\frac{q}{V}(T_{in} - T) + \\frac{\\Delta H}{\\rho c_p} k_0 e^{-E/RT} C_A - \\frac{UA}{\\rho c_p V}(T - T_c)\n\\end{aligned}\n\nThe MPC must maintain temperature below the runaway threshold T_{\\text{runaway}} while maximizing conversion. Under normal operation, it solves:\\begin{aligned}\n\\min \\quad & -C_A(t_f) + \\int_0^{t_f} \\|T - T_{\\text{optimal}}\\|^2 dt \\\\\n\\text{s.t.} \\quad & T \\leq T_{\\text{runaway}} - \\Delta T_{\\text{safety}} \\\\\n& q_{\\min} \\leq q \\leq q_{\\max}\n\\end{aligned}\n\nWhen the cooling system partially fails, T_c suddenly increases. The MPC cannot maintain T_{\\text{optimal}} within safety limits. The cascade activates: soft constraints allow T to exceed T_{\\text{optimal}} with penalty, the reference governor reduces the production target C_{A,\\text{target}}, and if still infeasible, the backup controller switches to maximum cooling q = q_{\\max}. If temperature approaches runaway, emergency shutdown stops the feed with q = 0.","type":"content","url":"/mpc#example-chemical-reactor-control-under-failure","position":61},{"hierarchy":{"lvl1":"Model Predictive Control","lvl2":"Computational Efficiency via Parametric Programming"},"type":"lvl2","url":"/mpc#computational-efficiency-via-parametric-programming","position":62},{"hierarchy":{"lvl1":"Model Predictive Control","lvl2":"Computational Efficiency via Parametric Programming"},"content":"Real-time model predictive control places strict limits on computation. In applications such as adaptive optics, the controller must run at kilohertz rates. A sampling frequency of 1000 Hz allows only one millisecond per step to compute and apply a control input. This makes efficiency a first-class concern.\n\nThe structure of MPC lends itself naturally to optimization reuse. Each time step requires solving a problem with the same dynamics and constraints. Only the initial state, forecasts, or reference signals change. Instead of treating each instance as a new problem, we can frame MPC as a parametric optimization problem and focus on how the solution evolves with the parameter.","type":"content","url":"/mpc#computational-efficiency-via-parametric-programming","position":63},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"General Framework: Parametric Optimization","lvl2":"Computational Efficiency via Parametric Programming"},"type":"lvl3","url":"/mpc#general-framework-parametric-optimization","position":64},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"General Framework: Parametric Optimization","lvl2":"Computational Efficiency via Parametric Programming"},"content":"We begin with a general optimization problem indexed by a parameter \\boldsymbol{\\theta} \\in \\Theta \\subset \\mathbb{R}^p:\\begin{aligned}\n\\min_{\\mathbf{x} \\in \\mathbb{R}^n} \\quad & f(\\mathbf{x}; \\boldsymbol{\\theta}) \\\\\n\\text{s.t.} \\quad & \\mathbf{g}(\\mathbf{x}; \\boldsymbol{\\theta}) \\le \\mathbf{0}, \\\\\n& \\mathbf{h}(\\mathbf{x}; \\boldsymbol{\\theta}) = \\mathbf{0}.\n\\end{aligned}\n\nFor each value of \\boldsymbol{\\theta}, we obtain a concrete optimization problem. The goal is to understand how the optimizer \\mathbf{x}^\\star(\\boldsymbol{\\theta}) and value functionv(\\boldsymbol{\\theta}) := \\inf\\{\\, f(\\mathbf{x}; \\boldsymbol{\\theta}) : \\mathbf{x} \\text{ feasible at } \\boldsymbol{\\theta}\\,\\}\n\ndepend on \\boldsymbol{\\theta}.\n\nWhen the problem is smooth and regular, the Karush–Kuhn–Tucker (KKT) conditions characterize optimality:\\begin{aligned}\n\\nabla_{\\mathbf{x}} f(\\mathbf{x}; \\boldsymbol{\\theta})\n+ \\nabla_{\\mathbf{x}} \\mathbf{g}(\\mathbf{x}; \\boldsymbol{\\theta})^\\top \\boldsymbol{\\lambda}\n+ \\nabla_{\\mathbf{x}} \\mathbf{h}(\\mathbf{x}; \\boldsymbol{\\theta})^\\top \\boldsymbol{\\nu} &= 0, \\\\\n\\mathbf{g}(\\mathbf{x}; \\boldsymbol{\\theta}) \\le 0, \\quad\n\\boldsymbol{\\lambda} \\ge 0, \\quad\n\\lambda_i g_i(\\mathbf{x}; \\boldsymbol{\\theta}) &= 0, \\\\\n\\mathbf{h}(\\mathbf{x}; \\boldsymbol{\\theta}) &= 0.\n\\end{aligned}\n\nIf the active set remains fixed over changes in \\boldsymbol{\\theta}, the implicit function theorem ensures that the mappings\\boldsymbol{\\theta} \\mapsto \\mathbf{x}^\\star(\\boldsymbol{\\theta}), \\quad\n\\boldsymbol{\\theta} \\mapsto \\boldsymbol{\\lambda}^\\star(\\boldsymbol{\\theta}), \\quad\n\\boldsymbol{\\theta} \\mapsto \\boldsymbol{\\nu}^\\star(\\boldsymbol{\\theta})\n\nare differentiable.\n\nIn linear and quadratic programming, this structure becomes even more tractable. Consider a linear program with affine dependence on \\boldsymbol{\\theta}:\\min_{\\mathbf{x}} \\ \\mathbf{c}(\\boldsymbol{\\theta})^\\top \\mathbf{x}\n\\quad \\text{s.t.} \\quad \\mathbf{A}(\\boldsymbol{\\theta})\\mathbf{x} \\le \\mathbf{b}(\\boldsymbol{\\theta}).\n\nEach active set determines a basis and thus a region in \\Theta where the solution is affine in \\boldsymbol{\\theta}. The feasible parameter space is partitioned into polyhedral regions, each with its own affine law.\n\nSimilarly, in strictly convex quadratic programs\\min_{\\mathbf{x}} \\ \\tfrac{1}{2} \\mathbf{x}^\\top \\mathbf{H} \\mathbf{x} + \\mathbf{q}(\\boldsymbol{\\theta})^\\top \\mathbf{x}\n\\quad \\text{s.t.} \\quad \\mathbf{A}\\mathbf{x} \\le \\mathbf{b}(\\boldsymbol{\\theta}), \\qquad \\mathbf{H} \\succ 0,\n\neach active set again leads to an affine optimizer, with piecewise-affine global structure and a piecewise-quadratic value function.\n\nParametric programming focuses on the structure of the map \\boldsymbol{\\theta} \\mapsto \\mathbf{x}^\\star(\\boldsymbol{\\theta}), and the regions over which this map takes a simple form.","type":"content","url":"/mpc#general-framework-parametric-optimization","position":65},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"Solution Sensitivity via the Implicit Function Theorem","lvl3":"General Framework: Parametric Optimization","lvl2":"Computational Efficiency via Parametric Programming"},"type":"lvl4","url":"/mpc#solution-sensitivity-via-the-implicit-function-theorem","position":66},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"Solution Sensitivity via the Implicit Function Theorem","lvl3":"General Framework: Parametric Optimization","lvl2":"Computational Efficiency via Parametric Programming"},"content":"We often meet equations of the formF(y,\\boldsymbol{\\theta})=0,\n\nwhere y\\in\\mathbb{R}^m are unknowns and \\boldsymbol{\\theta}\\in\\mathbb{R}^p are parameters. The implicit function theorem says that, if F is smooth and the Jacobian with respect to y,\\frac{\\partial F}{\\partial y}(y^\\star,\\boldsymbol{\\theta}^\\star),\n\nis invertible at a solution (y^\\star,\\boldsymbol{\\theta}^\\star), then in a neighborhood of \\boldsymbol{\\theta}^\\star there exists a unique smooth mapping y(\\boldsymbol{\\theta}) with F(y(\\boldsymbol{\\theta}),\\boldsymbol{\\theta})=0 and y(\\boldsymbol{\\theta}^\\star)=y^\\star. Moreover, its derivative is\\frac{d y}{d\\boldsymbol{\\theta}}(\\boldsymbol{\\theta}^\\star)\n\\;=\\;\n-\\Big(\\tfrac{\\partial F}{\\partial y}(y^\\star,\\boldsymbol{\\theta}^\\star)\\Big)^{-1}\n\\;\\tfrac{\\partial F}{\\partial \\boldsymbol{\\theta}}(y^\\star,\\boldsymbol{\\theta}^\\star).\n\nIn words: if the square Jacobian in y is nonsingular, the solution varies smoothly with the parameter, and we can differentiate it by solving one linear system.\n\nReturn to (P_{\\theta}) and its KKT system. Collect the primal and dual variables intoy \\;:=\\; (\\mathbf{x},\\,\\boldsymbol{\\lambda},\\,\\boldsymbol{\\nu}),\n\nand write the KKT equations as a single residualF(y,\\boldsymbol{\\theta}) \\;=\\; \n\\begin{bmatrix}\n\\nabla_{\\mathbf{x}} f(\\mathbf{x};\\boldsymbol{\\theta})\n+ \\nabla_{\\mathbf{x}} \\mathbf{g}(\\mathbf{x};\\boldsymbol{\\theta})^\\top \\boldsymbol{\\lambda}\n+ \\nabla_{\\mathbf{x}} \\mathbf{h}(\\mathbf{x};\\boldsymbol{\\theta})^\\top \\boldsymbol{\\nu} \\\\\n\\mathbf{h}(\\mathbf{x};\\boldsymbol{\\theta}) \\\\\n\\mathbf{g}_\\mathcal{A}(\\mathbf{x};\\boldsymbol{\\theta})\n\\end{bmatrix}\n\\;=\\; \\mathbf{0}.\n\nHere \\mathcal{A} denotes the set of inequality constraints active at the solution (the complementarity part is encoded by keeping \\mathcal{A} fixed; see below).\n\nTo invoke IFT, we need the Jacobian \\partial F/\\partial y to be invertible at (y^\\star,\\boldsymbol{\\theta}^\\star). Standard regularity conditions that ensure this are:\n\nLICQ (Linear Independence Constraint Qualification) at (\\mathbf{x}^\\star,\\boldsymbol{\\theta}^\\star): the gradients of all active constraints are linearly independent.\n\nSecond-order sufficiency on the critical cone (the Lagrangian Hessian is positive definite on feasible directions).\n\nStrict complementarity (optional but convenient): each active inequality has strictly positive multiplier.\n\nUnder these, the KKT matrix,K \\;=\\;\n\\frac{\\partial F}{\\partial y}(y^\\star,\\boldsymbol{\\theta}^\\star)\n\\;=\\;\n\\begin{bmatrix}\n\\nabla^2_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}(\\mathbf{x}^\\star,\\boldsymbol{\\lambda}^\\star,\\boldsymbol{\\nu}^\\star;\\boldsymbol{\\theta}^\\star)\n& \\nabla_{\\mathbf{x}} \\mathbf{g}_\\mathcal{A}(\\mathbf{x}^\\star;\\boldsymbol{\\theta}^\\star)^\\top\n& \\nabla_{\\mathbf{x}} \\mathbf{h}(\\mathbf{x}^\\star;\\boldsymbol{\\theta}^\\star)^\\top \\\\\n\\nabla_{\\mathbf{x}} \\mathbf{g}_\\mathcal{A}(\\mathbf{x}^\\star;\\boldsymbol{\\theta}^\\star) & 0 & 0 \\\\\n\\nabla_{\\mathbf{x}} \\mathbf{h}(\\mathbf{x}^\\star;\\boldsymbol{\\theta}^\\star) & 0 & 0\n\\end{bmatrix},\n\nis nonsingular. Here \\mathcal{L}=f+\\boldsymbol{\\lambda}^\\top \\mathbf{g}+\\boldsymbol{\\nu}^\\top \\mathbf{h}.\n\nThe right-hand side sensitivity to parameters isG \\;=\\; \\frac{\\partial F}{\\partial \\boldsymbol{\\theta}}(y^\\star,\\boldsymbol{\\theta}^\\star)\n\\;=\\;\n\\begin{bmatrix}\n\\nabla_{\\boldsymbol{\\theta}}\\nabla_{\\mathbf{x}} f\n+ \\sum_{i\\in\\mathcal{A}} \\lambda_i^\\star \\nabla_{\\boldsymbol{\\theta}}\\nabla_{\\mathbf{x}} g_i\n+ \\sum_j \\nu_j^\\star \\nabla_{\\boldsymbol{\\theta}}\\nabla_{\\mathbf{x}} h_j \\\\\n\\nabla_{\\boldsymbol{\\theta}} \\mathbf{h} \\\\\n\\nabla_{\\boldsymbol{\\theta}} \\mathbf{g}_\\mathcal{A}\n\\end{bmatrix}_{(\\mathbf{x}^\\star,\\boldsymbol{\\theta}^\\star)} .\n\nIFT then gives local differentiability of the optimizer and multipliers:\\frac{d y^\\star}{d\\boldsymbol{\\theta}}(\\boldsymbol{\\theta}^\\star)\n\\;=\\; -\\,K^{-1} G.\n\nThe formula above is valid as long as the active set \\mathcal{A} does not change. If a constraint switches between active/inactive, the mapping remains piecewise smooth, but the derivative may jump. In MPC, this is exactly why warm-starts are very effective most of the time and occasionally require a refactorization when the active set flips.\n\nIn parametric MPC, \\boldsymbol{\\theta} gathers the current state, references, and forecasts. The IFT tells us that, under regularity and a stable active set, the optimal trajectory and first input vary smoothly with \\boldsymbol{\\theta}. The linear map -K^{-1}G is exactly the object used in sensitivity-based warm starts and real-time iterations: small changes in \\boldsymbol{\\theta} can be propagated through a single KKT solve to update the primal–dual guess before taking one or two Newton/SQP steps.","type":"content","url":"/mpc#solution-sensitivity-via-the-implicit-function-theorem","position":67},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"Predictor-Corrector MPC","lvl3":"General Framework: Parametric Optimization","lvl2":"Computational Efficiency via Parametric Programming"},"type":"lvl4","url":"/mpc#predictor-corrector-mpc","position":68},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"Predictor-Corrector MPC","lvl3":"General Framework: Parametric Optimization","lvl2":"Computational Efficiency via Parametric Programming"},"content":"We start with a smooth root-finding problemF(y)=0,\\qquad F:\\mathbb{R}^m\\to\\mathbb{R}^m.\n\nNewton’s method iteratesy^{(t+1)} \\;=\\; y^{(t)} - \\big[\\nabla F(y^{(t)})\\big]^{-1} F\\big(y^{(t)}\\big),\n\nor equivalently solves the linearized system\\nabla F(y^{(t)})\\,\\Delta y^{(t)} = -F\\big(y^{(t)}\\big),\\qquad y^{(t+1)}=y^{(t)}+\\Delta y^{(t)}.\n\nConvergence is local and fast when the Jacobian is nonsingular and the initial guess is close.\n\nNow suppose the root depends on a parameter:F\\big(y,\\theta\\big)=0,\\qquad \\theta\\in\\mathbb{R}.\n\nWe want the solution path \\theta\\mapsto y^\\star(\\theta). Numerical continuation advances \\theta in small steps and uses the previous solution as a warm start for the next Newton solve. This is the simplest and most effective way to “track” solutions of parametric systems.\n\nAt a known solution (y^\\star,\\theta^\\star), differentiate F(y^\\star(\\theta),\\theta)=0 with respect to \\theta:\\nabla_y F(y^\\star,\\theta^\\star)\\,\\frac{dy^\\star}{d\\theta}(\\theta^\\star) \\;+\\; \\nabla_\\theta F(y^\\star,\\theta^\\star) \\;=\\; 0.\n\nIf \\nabla_y F is invertible (IFT conditions), the tangent is\\frac{dy^\\star}{d\\theta}(\\theta^\\star) \\;=\\; -\\big[\\nabla_y F(y^\\star,\\theta^\\star)\\big]^{-1}\\,\\nabla_\\theta F(y^\\star,\\theta^\\star).\n\nThis is exactly the implicit differentiation formula. Continuation uses it as a predictor:y_{\\text{pred}} \\;=\\; y^\\star(\\theta^\\star) \\;+\\; \\Delta\\theta\\;\\frac{dy^\\star}{d\\theta}(\\theta^\\star).\n\nThen a few corrector steps apply Newton to F(\\,\\cdot\\,,\\theta^\\star+\\Delta\\theta)=0 starting from y_{\\text{pred}}. If Newton converges quickly, the step \\Delta\\theta was appropriate; otherwise reduce \\Delta\\theta and retry.\n\nFor parametric KKT systems, set y=(\\mathbf{x},\\boldsymbol{\\lambda},\\boldsymbol{\\nu}) where \\mathbf{x} stacks the primal decision variables (states and inputs), and F(y,\\theta)=0 the KKT residual with \\theta collecting state, references, forecasts. The KKT matrix K=\\partial F/\\partial y and parameter sensitivity G=\\partial F/\\partial \\theta give the tangent\\frac{dy^\\star}{d\\theta} \\;=\\; -\\,K^{-1}G.\n\nContinuation then becomes:\n\nPredictor: y_{\\text{pred}} = y^\\star + (\\Delta\\theta)\\,(-K^{-1}G).\n\nCorrector: a few Newton/SQP steps on the KKT equations at the new \\theta.\n\nIn MPC, this yields efficient warm starts across time: as the parameter \\theta_t (current state, references) changes slightly, we predict the new primal–dual point and correct with 1–2 iterations—often enough to hit tolerance in real time. \n## Application to MPC\n\nWe now specialize this idea to the structure of finite-horizon MPC. Fix a prediction horizon $N$. At each time step, we solve a problem with fixed structure and varying data. Define\n\n$$\n\\boldsymbol{\\theta} := (\\mathbf{x}_0,\\, \\mathbf{r},\\, \\mathbf{w}),\n$$\n\nwhich includes the current state $\\mathbf{x}_0$, reference signals $\\mathbf{r}$, and exogenous forecasts $\\mathbf{w}$.\n\nThe finite-horizon problem becomes\n\n$$\n\\begin{aligned}\n\\min_{z} \\quad & J(z;\\boldsymbol{\\theta}) \\\\\n\\text{s.t.} \\quad & c(z;\\boldsymbol{\\theta}) = 0 \\\\\n& d(z;\\boldsymbol{\\theta}) \\leq 0,\n\\end{aligned}\n$$\n\nwith decision variable $z = (\\mathbf{x}_{0:N}, \\mathbf{u}_{0:N-1})$. The equality constraints enforce dynamics and terminal conditions. The inequalities encode input and state bounds.\n\nSolving $(P_\\theta)$ produces an optimal trajectory $z^\\star(\\boldsymbol{\\theta})$. The control law is the first input:\n\n$$\n\\pi(\\boldsymbol{\\theta}) := \\mathbf{u}_0^\\star(\\boldsymbol{\\theta}).\n$$\n\nThis mapping from parameter to input defines the MPC policy. Parametric programming helps us understand and exploit its structure to speed up evaluation.\n\n## Two Approaches to Efficient MPC\n\nParametric structure can be used in two main ways: either to construct an explicit control law offline, or to warm-start the optimizer online using sensitivity information.\n\n### Explicit MPC\n\nWhen the problem is a linear or quadratic program with affine dependence on $\\boldsymbol{\\theta}$, we can work out the solution symbolically. The parameter space is partitioned into regions $\\mathcal{R}_1, \\dots, \\mathcal{R}_M$, each associated with a fixed active set. On each region:\n\n$$\nz^\\star(\\boldsymbol{\\theta}) = A_r\\,\\boldsymbol{\\theta} + b_r,\n\\qquad\n\\pi(\\boldsymbol{\\theta}) = K_r\\,\\boldsymbol{\\theta} + k_r.\n$$\n\nAt runtime, we identify which region contains $\\boldsymbol{\\theta}$, then apply the corresponding affine formula. This approach avoids optimization entirely during deployment.\n\nIt requires storing the region definitions and control laws. The number of regions grows with horizon length and constraint count, which limits this approach to systems with low state dimension and short horizons.\n\n### Sensitivity-Based MPC\n\nWhen symbolic enumeration is intractable, we can still track how the solution varies locally. Suppose we have a solution $\\bar{y} = (\\bar{z}, \\bar{\\lambda}, \\bar{\\nu})$ at parameter $\\bar{\\boldsymbol{\\theta}}$. The KKT system reads:\n\n$$\nF(y; \\boldsymbol{\\theta}) = 0.\n$$\n\nDifferentiating with respect to $\\boldsymbol{\\theta}$,\n\n$$\n\\frac{\\partial F}{\\partial y}(\\bar{y}; \\bar{\\boldsymbol{\\theta}})\\, \\mathrm{d}y\n= - \\frac{\\partial F}{\\partial \\boldsymbol{\\theta}}(\\bar{y}; \\bar{\\boldsymbol{\\theta}})\\, \\mathrm{d}\\boldsymbol{\\theta}.\n$$\n\nLet $K$ be the KKT matrix and $G$ the sensitivity of the residual. Then the sensitivity operator $T$ satisfies\n\n$$\nK T = -G \\quad \\Rightarrow \\quad \\mathrm{d}y = T\\, \\mathrm{d}\\boldsymbol{\\theta}.\n$$\n\nIf $\\boldsymbol{\\theta}$ changes slightly, we update the primal-dual pair:\n\n$$\ny^{(0)} \\leftarrow \\bar{y} + T\\,\\Delta\\boldsymbol{\\theta},\n$$\n\nand use it as the starting point for Newton or SQP.\n\nThis is the basis of real-time iteration schemes. When the active set is stable, the warm start is accurate to first order. When it changes, we refactorize and repeat, still with far less effort than solving from scratch. ","type":"content","url":"/mpc#predictor-corrector-mpc","position":69},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Amortized Optimization and Neural Approximation of Controllers","lvl2":"Computational Efficiency via Parametric Programming"},"type":"lvl3","url":"/mpc#amortized-optimization-and-neural-approximation-of-controllers","position":70},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Amortized Optimization and Neural Approximation of Controllers","lvl2":"Computational Efficiency via Parametric Programming"},"content":"The idea of reusing structure across similar optimization problems is not exclusive to parametric programming. In machine learning, a related concept known as amortized optimization aims to reduce the cost of repeated inference by replacing explicit optimization with a function that has been learned to approximate the solution map. This approach shifts the computational burden from online solving to offline training.\n\nThe goal is to construct a function \\hat{\\pi}_{\\phi}(\\boldsymbol{\\theta}), typically parameterized by a neural network, that maps the input \\boldsymbol{\\theta} to an approximate solution \\hat{z}^\\star(\\boldsymbol{\\theta}) or control action \\hat{\\mathbf{u}}_0^\\star(\\boldsymbol{\\theta}). Once trained, this map can be evaluated quickly at runtime, with no need to solve an optimization problem explicitly.\n\nAmortized optimization has emerged in several contexts:\n\nIn probabilistic inference, where variational autoencoders (VAEs) amortize the computation of posterior distributions across a dataset.\n\nIn meta-learning, where the objective is to learn a model that generalizes across tasks by internalizing how to adapt.\n\nIn hyperparameter optimization, where learning a surrogate model can guide the search over configuration space efficiently.\n\nThis perspective has also begun to influence control. Recent work investigates how to amortize nonlinear MPC (NMPC) policies into neural networks. The training data come from solving many instances of the underlying optimal control problem offline. The resulting neural policy \\hat{\\pi}_\\phi acts as a differentiable, low-latency controller that can generalize to new situations within the training distribution.\n\nCompared to explicit MPC, which partitions the parameter space and stores exact solutions region by region, amortized control smooths over the domain by learning an approximate policy globally. It is less precise, but scalable to high-dimensional problems where enumeration of regions is impossible.\n\nNeural network amortization is advantageous due to the expressivity of these models. However, the challenge is ensuring constraint satisfaction and safety, which are hard to guarantee with unconstrained neural approximators. Hybrid approaches attempt to address this by combining a neural warm-start policy with a final projection step, or by embedding the network within a constrained optimization layer. Other strategies include learning structured architectures that respect known physics or control symmetries.","type":"content","url":"/mpc#amortized-optimization-and-neural-approximation-of-controllers","position":71},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Imitation Learning Framework","lvl2":"Computational Efficiency via Parametric Programming"},"type":"lvl3","url":"/mpc#imitation-learning-framework","position":72},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Imitation Learning Framework","lvl2":"Computational Efficiency via Parametric Programming"},"content":"Consider a fixed horizon N and parameter vector \\boldsymbol{\\theta} encoding the current state, references, and forecasts. The oracle MPC controller solves\\begin{aligned}\nz^\\star(\\boldsymbol{\\theta}) \\in \\arg\\min_{z=(\\mathbf{x}_{0:N},\\mathbf{u}_{0:N-1})}\n&\\; J(z;\\boldsymbol{\\theta})\\\\\n\\text{s.t. }& \\mathbf{x}_{k+1}=f(\\mathbf{x}_k,\\mathbf{u}_k;\\boldsymbol{\\theta}),\\quad k=0..N-1,\\\\\n& g(\\mathbf{x}_k,\\mathbf{u}_k;\\boldsymbol{\\theta})\\le 0,\\; h(\\mathbf{x}_N;\\boldsymbol{\\theta})=0.\n\\end{aligned}\n\nThe applied action is \\pi^\\star(\\boldsymbol{\\theta}) := \\mathbf{u}_0^\\star(\\boldsymbol{\\theta}). Our goal is to learn a fast surrogate mapping \\hat{\\pi}_\\phi:\\boldsymbol{\\theta}\\mapsto \\hat{\\mathbf{u}}_0 \\approx \\pi^\\star(\\boldsymbol{\\theta}) that can be evaluated in microseconds, optionally followed by a safety projection layer.\n\nSupervised learning from oracle solutions.\nOne first samples parameters \\boldsymbol{\\theta}^{(i)} from the operational domain and solves the corresponding NMPC problems offline. The resulting dataset\\mathcal{D} = \\{ (\\boldsymbol{\\theta}^{(i)},\\, \\mathbf{u}_0^\\star(\\boldsymbol{\\theta}^{(i)})) \\}_{i=1}^M\n\nis then used to train a neural network \\hat{\\pi}_\\phi by minimizing\\min_\\phi \\; \\frac{1}{M}\\sum_{i=1}^M \\big\\|\\hat{\\pi}_\\phi(\\boldsymbol{\\theta}^{(i)}) - \\mathbf{u}_0^\\star(\\boldsymbol{\\theta}^{(i)})\\big\\|^2 .\n\nOnce trained, the network acts as a surrogate for the optimizer, providing instantaneous evaluations that approximate the MPC law.","type":"content","url":"/mpc#imitation-learning-framework","position":73},{"hierarchy":{"lvl1":"Model Predictive Control","lvl2":"Example: Propofol Infusion Control"},"type":"lvl2","url":"/mpc#example-propofol-infusion-control","position":74},{"hierarchy":{"lvl1":"Model Predictive Control","lvl2":"Example: Propofol Infusion Control"},"content":"This problem explores the control of propofol infusion in total intravenous anesthesia (TIVA). Our presentation follows the problem formulation developped by \n\nSawaguchi et al. (2008). The primary objective is to maintain the desired level of unconsciousness while minimizing adverse reactions and ensuring quick recovery after surgery.\n\nThe level of unconsciousness is measured by the Bispectral Index (BIS), which is obtained using an electroencephalography (EEG) device. The BIS ranges from 0 (complete suppression of brain activity) to 100 (fully awake), with the target range for general anesthesia typically between 40 and 60.\n\nThe goal is to design a control system that regulates the infusion rate of propofol to maintain the BIS within the target range. This can be formulated as an optimal control problem:\\begin{align*}\n\\min_{u(t)} & \\int_{0}^{T} \\left( BIS(t) - BIS_{\\text{target}} \\right)^2 + \\lambda\\, u(t)^2 \\, dt \\\\\n\\text{subject to:} \\\\\n\\dot{x}_1 &= -(k_{10} + k_{12} + k_{13})x_1 + k_{21}x_2 + k_{31}x_3 + \\frac{u(t)}{V_1} \\\\\n\\dot{x}_2 &= k_{12}x_1 - k_{21}x_2 \\\\\n\\dot{x}_3 &= k_{13}x_1 - k_{31}x_3 \\\\\n\\dot{x}_e &= k_{e0}(x_1 - x_e) \\\\\nBIS(t) &= E_0 - E_{\\text{max}}\\frac{x_e^\\gamma}{x_e^\\gamma + EC_{50}^\\gamma}\n\\end{align*}\n\nWhere:\n\nu(t) is the propofol infusion rate (mg/kg/h)\n\nx_1, x_2, and x_3 are the drug concentrations in different body compartments\n\nx_e is the effect-site concentration\n\nk_{ij} are rate constants for drug transfer between compartments\n\nBIS(t) is the Bispectral Index\n\n\\lambda is a regularization parameter penalizing excessive drug use\n\nE_0, E_{\\text{max}}, EC_{50}, and \\gamma are parameters of the pharmacodynamic model\n\nThe specific dynamics model used in this problem is so-called “Pharmacokinetic-Pharmacodynamic Model” and consists of three main components:\n\nPharmacokinetic Model, which describes how the drug distributes through the body over time. It’s based on a three-compartment model:\n\nCentral compartment (blood and well-perfused organs)\n\nShallow peripheral compartment (muscle and other tissues)\n\nDeep peripheral compartment (fat)\n\nEffect Site Model, which represents the delay between drug concentration in the blood and its effect on the brain.\n\nPharmacodynamic Model that relates the effect-site concentration to the observed BIS.\n\nThe propofol infusion control problem presents several interesting challenges from a research perspective.\nFirst, there is a delay in how fast the drug can reach a different compartments in addition to the BIS measurements which can lag. This could lead to instability if not properly addressed in the control design.\n\nFurthermore, every patient is different from another. Hence, we cannot simply learn a single controller offline and hope that it will generalize to an entire patient population. We will account for this variability through Model Predictive Control (MPC) and dynamically adapt to the model mismatch through replanning. How a patient will react to a given dose of drug also varies and must be carefully controlled to avoid overdoses. This adds an additional layer of complexity since we have to incorporate safety constraints. Finally, the patient might suddenly change state, for example due to surgical stimuli, and the controller must be able to adapt quickly to compensate for the disturbance to the system.\n\nimport numpy as np\nfrom scipy.optimize import minimize\nimport matplotlib.pyplot as plt\n\nclass Patient:\n    def __init__(self, age, weight):\n        self.age = age\n        self.weight = weight\n        self.set_pk_params()\n        self.set_pd_params()\n\n    def set_pk_params(self):\n        self.v1 = 4.27 * (self.weight / 70) ** 0.71 * (self.age / 30) ** (-0.39)\n        self.v2 = 18.9 * (self.weight / 70) ** 0.64 * (self.age / 30) ** (-0.62)\n        self.v3 = 238 * (self.weight / 70) ** 0.95\n        self.cl1 = 1.89 * (self.weight / 70) ** 0.75 * (self.age / 30) ** (-0.25)\n        self.cl2 = 1.29 * (self.weight / 70) ** 0.62\n        self.cl3 = 0.836 * (self.weight / 70) ** 0.77\n        self.k10 = self.cl1 / self.v1\n        self.k12 = self.cl2 / self.v1\n        self.k13 = self.cl3 / self.v1\n        self.k21 = self.cl2 / self.v2\n        self.k31 = self.cl3 / self.v3\n        self.ke0 = 0.456\n\n    def set_pd_params(self):\n        self.E0 = 100\n        self.Emax = 100\n        self.EC50 = 3.4\n        self.gamma = 3\n\ndef pk_model(x, u, patient):\n    x1, x2, x3, xe = x\n    dx1 = -(patient.k10 + patient.k12 + patient.k13) * x1 + patient.k21 * x2 + patient.k31 * x3 + u / patient.v1\n    dx2 = patient.k12 * x1 - patient.k21 * x2\n    dx3 = patient.k13 * x1 - patient.k31 * x3\n    dxe = patient.ke0 * (x1 - xe)\n    return np.array([dx1, dx2, dx3, dxe])\n\ndef pd_model(ce, patient):\n    return patient.E0 - patient.Emax * (ce ** patient.gamma) / (ce ** patient.gamma + patient.EC50 ** patient.gamma)\n\ndef simulate_step(x, u, patient, dt):\n    x_next = x + dt * pk_model(x, u, patient)\n    bis = pd_model(x_next[3], patient)\n    return x_next, bis\n\ndef objective(u, x0, patient, dt, N, target_bis):\n    x = x0.copy()\n    total_cost = 0\n    for i in range(N):\n        x, bis = simulate_step(x, u[i], patient, dt)\n        total_cost += (bis - target_bis)**2 + 0.1 * u[i]**2\n    return total_cost\n\ndef mpc_step(x0, patient, dt, N, target_bis):\n    u0 = 10 * np.ones(N)  # Initial guess\n    bounds = [(0, 20)] * N  # Infusion rate between 0 and 20 mg/kg/h\n    \n    result = minimize(objective, u0, args=(x0, patient, dt, N, target_bis),\n                      method='SLSQP', bounds=bounds)\n    \n    return result.x[0]  # Return only the first control input\n\ndef run_mpc_simulation(patient, T, dt, N, target_bis):\n    steps = int(T / dt)\n    x = np.zeros((steps+1, 4))\n    bis = np.zeros(steps+1)\n    u = np.zeros(steps)\n    \n    for i in range(steps):\n        # Add noise to the current state to simulate real-world uncertainty\n        x_noisy = x[i] + np.random.normal(0, 0.01, size=4)\n        \n        # Use noisy state for MPC planning\n        u[i] = mpc_step(x_noisy, patient, dt, N, target_bis)\n        \n        # Evolve the true state using the deterministic model\n        x[i+1], bis[i] = simulate_step(x[i], u[i], patient, dt)\n    \n    bis[-1] = pd_model(x[-1, 3], patient)\n    return x, bis, u\n\n# Set up the problem\npatient = Patient(age=40, weight=70)\nT = 120  # Total time in minutes\ndt = 0.5  # Time step in minutes\nN = 20  # Prediction horizon\ntarget_bis = 50  # Target BIS value\n\n# Run MPC simulation\nx, bis, u = run_mpc_simulation(patient, T, dt, N, target_bis)\n\n# Plot results\nt = np.arange(0, T+dt, dt)\nfig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 12), sharex=True)\n\nax1.plot(t, bis)\nax1.set_ylabel('BIS')\nax1.set_ylim(0, 100)\nax1.axhline(y=target_bis, color='r', linestyle='--')\n\nax2.plot(t[:-1], u)\nax2.set_ylabel('Infusion Rate (mg/kg/h)')\n\nax3.plot(t, x[:, 3])\nax3.set_ylabel('Effect-site Concentration (µg/mL)')\nax3.set_xlabel('Time (min)')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Initial BIS: {bis[0]:.2f}\")\nprint(f\"Final BIS: {bis[-1]:.2f}\")\nprint(f\"Mean infusion rate: {np.mean(u):.2f} mg/kg/h\")\nprint(f\"Final effect-site concentration: {x[-1, 3]:.2f} µg/mL\")\n\n ### Deployment Patterns\n\nThere are several ways to use an amortized controller once it has been trained. The simplest option is **direct amortization**, where the control input is taken to be $u = \\hat{\\pi}_\\phi(\\boldsymbol{\\theta})$. In this case, the neural network provides the control action directly, with no optimization performed during deployment.\n\nA second option is **amortization with projection**, where the network output $\\tilde u = \\hat{\\pi}_\\phi(\\boldsymbol{\\theta})$ is passed through a small optimization step, such as a quadratic program or barrier-function filter, in order to enforce constraints. This adds a negligible computational overhead but restores guarantees of feasibility and safety.\n\nWe could for example integrate a convex approximation of the MPC subproblem directly as a differentiable layer inside the network. The network proposes a candidate action $\\tilde u$, which is then corrected through a small quadratic program:\n\n$$\nu = \\arg\\min_v \\tfrac12\\|v-\\tilde u\\|^2 \\quad \\text{s.t. } g(x,v)\\le 0.\n$$\n\nGradients are propagated through this correction using implicit differentiation, allowing the network to be trained end-to-end while retaining constraint satisfaction. This hybrid keeps the fast evaluation of a learned map while preserving the structure of MPC.\n\nA third option is **amortized warm-starting**, where the neural network provides an initialization for one or two Newton or SQP iterations of the underlying NMPC problem. In this setting, the learned map delivers an excellent starting point, so the optimizer converges quickly and the cost of re-solving at each time step is greatly reduced.  ## Demo: Batch Bioreactor MPC with do-mpc\n\nWe illustrate nonlinear MPC on a fed-batch bioreactor. The process has four states: biomass concentration \\(X_s\\), substrate \\(S_s\\), product \\(P_s\\), and liquid volume \\(V_s\\). The manipulated feed flow \\(u_{\\text{inp}}\\) augments volume and changes concentrations. The dynamics are\n\n$$\n\\begin{aligned}\n\\dot X_s &= \\mu(S_s)X_s - \\tfrac{u_{\\text{inp}}}{V_s} X_s, \\\\\n\\dot S_s &= -\\tfrac{\\mu(S_s)X_s}{Y_x} - \\tfrac{v X_s}{Y_p} + \\tfrac{u_{\\text{inp}}}{V_s}(S_{\\text{in}} - S_s), \\\\\n\\dot P_s &= v X_s - \\tfrac{u_{\\text{inp}}}{V_s} P_s, \\\\\n\\dot V_s &= u_{\\text{inp}},\n\\end{aligned}\n$$\n\nwith inhibited Monod kinetics\n\n$$\n\\mu(S_s) = \\frac{\\mu_m S_s}{K_m + S_s + S_s^2/K_i}.\n$$\n\nWe impose bounds on states and input, e.g. \\(0 \\le X_s \\le 3.7\\), \\(0 \\le P_s \\le 3.0\\), and \\(0 \\le u_{\\text{inp}} \\le 0.2\\). Two parameters are uncertain: yield \\(Y_x\\) and inlet concentration \\(S_{\\text{in}}\\). We treat them via a small scenario set with non-anticipativity (a single input sequence is shared across scenarios).\n\nAt each MPC step, we solve a finite-horizon problem that encourages product formation while regularizing effort:\n\n$$\n\\min_{x_{0:N},u_{0:N-1}} \\; -\\,P_s(N) + \\sum_{k=0}^{N-1} \\big( -\\,P_s(k) + \\rho\\, u_k^2 \\big)\n$$\n\nsubject to the discretized dynamics and box constraints for all uncertainty scenarios, sharing the inputs across scenarios. The continuous-time ODEs are discretized by orthogonal collocation on finite elements, producing an NLP [orthogonal collocation](https://www.do-mpc.com/en/latest/theory_orthogonal_collocation.html). The resulting NMPC is re-solved in a receding-horizon loop [MPC basics](https://www.do-mpc.com/en/latest/theory_mpc.html).\n\nThe cell below runs the closed-loop simulation and plots the states and input. The script is adapted from the do-mpc Batch Bioreactor example.\n\n```{code-cell} ipython3\n:tags: [hide-input]\n\n\nimport numpy as np\nimport do_mpc\nfrom casadi import *  # noqa: F401 - do-mpc constructs CasADi symbols under the hood\n\n\ndef build_model():\n    model_type = 'continuous'\n    model = do_mpc.model.Model(model_type)\n\n    # States\n    X_s = model.set_variable('_x', 'X_s')  # biomass\n    S_s = model.set_variable('_x', 'S_s')  # substrate\n    P_s = model.set_variable('_x', 'P_s')  # product\n    V_s = model.set_variable('_x', 'V_s')  # volume\n\n    # Control input (feed flow)\n    inp = model.set_variable('_u', 'inp')\n\n    # Certain parameters\n    mu_m = 0.02\n    K_m = 0.05\n    K_i = 5.0\n    v_par = 0.004\n    Y_p = 1.2\n\n    # Uncertain parameters\n    Y_x = model.set_variable('_p', 'Y_x')\n    S_in = model.set_variable('_p', 'S_in')\n\n    # Specific growth rate\n    mu_S = mu_m * S_s / (K_m + S_s + (S_s**2 / K_i))\n\n    # Dynamics\n    model.set_rhs('X_s', mu_S * X_s - inp / V_s * X_s)\n    model.set_rhs('S_s', -mu_S * X_s / Y_x - v_par * X_s / Y_p + inp / V_s * (S_in - S_s))\n    model.set_rhs('P_s', v_par * X_s - inp / V_s * P_s)\n    model.set_rhs('V_s', inp)\n\n    model.setup()\n    return model\n\n\ndef build_mpc(model):\n    mpc = do_mpc.controller.MPC(model)\n    setup_mpc = {\n        'n_horizon': 30,\n        't_step': 1.0,\n        'n_robust': 1,\n        'store_full_solution': True,\n    }\n    mpc.set_param(**setup_mpc)\n\n    # Objective: encourage product formation and small inputs (economic-like MPC)\n    X_s = model.x['X_s']\n    S_s = model.x['S_s']\n    P_s = model.x['P_s']\n    V_s = model.x['V_s']\n    inp = model.u['inp']\n\n    mterm = -P_s  # maximize product at horizon end\n    lterm = -P_s + 1e-4 * inp**2  # small input penalty\n    mpc.set_objective(mterm=mterm, lterm=lterm)\n\n    # Box constraints\n    mpc.bounds['lower', '_x', 'X_s'] = 0.0\n    mpc.bounds['lower', '_x', 'S_s'] = -0.01\n    mpc.bounds['lower', '_x', 'P_s'] = 0.0\n    mpc.bounds['lower', '_x', 'V_s'] = 0.0\n    mpc.bounds['upper', '_x', 'X_s'] = 3.7\n    mpc.bounds['upper', '_x', 'P_s'] = 3.0\n    mpc.bounds['lower', '_u', 'inp'] = 0.0\n    mpc.bounds['upper', '_u', 'inp'] = 0.2\n\n    # Uncertainty scenarios (shared control across scenarios)\n    Y_x_values = np.array([0.5, 0.4, 0.3])\n    S_in_values = np.array([200.0, 220.0, 180.0])\n    mpc.set_uncertainty_values(Y_x=Y_x_values, S_in=S_in_values)\n\n    mpc.setup()\n    return mpc\n\n\ndef build_estimator(model):\n    return do_mpc.estimator.StateFeedback(model)\n\n\ndef build_simulator(model):\n    simulator = do_mpc.simulator.Simulator(model)\n    params_sim = {\n        'integration_tool': 'cvodes',\n        'abstol': 1e-10,\n        'reltol': 1e-10,\n        't_step': 1.0,\n    }\n    simulator.set_param(**params_sim)\n\n    # Realizations of uncertain parameters used by the simulator\n    p_num = simulator.get_p_template()\n    p_num['Y_x'] = 0.4\n    p_num['S_in'] = 200.0\n\n    def p_fun(t_now):\n        return p_num\n\n    simulator.set_p_fun(p_fun)\n    simulator.setup()\n    return simulator\n\n\ndef run_closed_loop():\n    model = build_model()\n    mpc = build_mpc(model)\n    estimator = build_estimator(model)\n    simulator = build_simulator(model)\n\n    # Initial state\n    x0 = np.array([1.0, 0.5, 0.0, 120.0])\n    mpc.x0 = x0\n    estimator.x0 = x0\n    simulator.x0 = x0\n    mpc.set_initial_guess()\n\n    # Closed-loop simulation\n    n_steps = 60\n    for _ in range(n_steps):\n        u0 = mpc.make_step(x0)\n        y_next = simulator.make_step(u0)\n        x0 = estimator.make_step(y_next)\n\n    # Visualization\n    import matplotlib.pyplot as plt\n\n    mpc_graphics = do_mpc.graphics.Graphics(mpc.data)\n    sim_graphics = do_mpc.graphics.Graphics(simulator.data)\n\n    fig, ax = plt.subplots(5, sharex=True, figsize=(12, 8))\n    fig.align_ylabels()\n\n    for g in [sim_graphics, mpc_graphics]:\n        g.add_line(var_type='_x', var_name='X_s', axis=ax[0], color='#1f77b4')\n        g.add_line(var_type='_x', var_name='S_s', axis=ax[1], color='#1f77b4')\n        g.add_line(var_type='_x', var_name='P_s', axis=ax[2], color='#1f77b4')\n        g.add_line(var_type='_x', var_name='V_s', axis=ax[3], color='#1f77b4')\n        g.add_line(var_type='_u', var_name='inp', axis=ax[4], color='#1f77b4')\n\n    ax[0].set_ylabel('X_s [mol/l]')\n    ax[1].set_ylabel('S_s [mol/l]')\n    ax[2].set_ylabel('P_s [mol/l]')\n    ax[3].set_ylabel('V_s [m^3]')\n    ax[4].set_ylabel('u_inp [m^3/min]')\n    ax[4].set_xlabel('t [min]')\n\n    # Plot full horizon results\n    sim_graphics.plot_results()\n    mpc_graphics.plot_predictions()\n    mpc_graphics.reset_axes()\n    plt.tight_layout()\n    plt.show()\n\n\n# Run the closed-loop simulation\nrun_closed_loop()\n```\n\n### Interactive Animation\n\nThe following cell creates an interactive animation of the batch bioreactor control process, showing the MPC predictions and the evolution of the system states in real-time. The visualization includes a tank representation with liquid level and biomass particles, along with time-series plots of all states and control inputs.\n\n```{code-cell} ipython3\n:tags: [hide-input]\n\n\nimport numpy as np\nimport do_mpc\nfrom casadi import *  # noqa: F401 - do-mpc constructs CasADi symbols under the hood\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom IPython.display import HTML, display\nimport matplotlib.patches as mpatches\nfrom contextlib import redirect_stdout, redirect_stderr\nimport io\n\n\ndef build_model():\n    model_type = 'continuous'\n    model = do_mpc.model.Model(model_type)\n\n    # States\n    X_s = model.set_variable('_x', 'X_s')  # biomass\n    S_s = model.set_variable('_x', 'S_s')  # substrate\n    P_s = model.set_variable('_x', 'P_s')  # product\n    V_s = model.set_variable('_x', 'V_s')  # volume\n\n    # Control input (feed flow)\n    inp = model.set_variable('_u', 'inp')\n\n    # Certain parameters\n    mu_m = 0.02\n    K_m = 0.05\n    K_i = 5.0\n    v_par = 0.004\n    Y_p = 1.2\n\n    # Uncertain parameters\n    Y_x = model.set_variable('_p', 'Y_x')\n    S_in = model.set_variable('_p', 'S_in')\n\n    # Auxiliary term for specific growth rate\n    mu = mu_m * S_s / (K_m + S_s + S_s**2 / K_i)\n    model.set_expression('mu', mu)\n\n    # Differential equations\n    model.set_rhs('X_s', mu * X_s - inp / V_s * X_s)\n    model.set_rhs('S_s', -mu * X_s / Y_x - v_par * X_s / Y_p + inp / V_s * (S_in - S_s))\n    model.set_rhs('P_s', v_par * X_s - inp / V_s * P_s)\n    model.set_rhs('V_s', inp)\n\n    model.setup()\n    return model\n\n\ndef setup_mpc(model):\n    mpc = do_mpc.controller.MPC(model)\n\n    setup_mpc = {\n        'n_horizon': 20,\n        't_step': 1.0,\n        'n_robust': 1,\n        'store_full_solution': True,\n        # Silence IPOPT/CasADi prints\n        'nlpsol_opts': {\n            'ipopt.print_level': 0,\n            'ipopt.sb': 'yes',\n            'print_time': 0,\n        },\n    }\n    mpc.set_param(**setup_mpc)\n\n    # Objective\n    mterm = model.aux['mu']  # terminal cost on growth\n    lterm = model.aux['mu']  # stage cost on growth\n    mpc.set_objective(mterm=mterm, lterm=lterm)\n\n    # Constraints\n    mpc.bounds['lower', '_u', 'inp'] = 0.0\n    mpc.bounds['upper', '_u', 'inp'] = 0.2\n    \n    mpc.bounds['lower', '_x', 'X_s'] = 0.0\n    mpc.bounds['lower', '_x', 'S_s'] = 0.0\n    mpc.bounds['lower', '_x', 'P_s'] = 0.0\n    mpc.bounds['lower', '_x', 'V_s'] = 0.0\n\n    # Uncertain parameters\n    Y_x_values = np.array([0.5])  # single scenario to avoid prediction dim issues\n    S_in_values = np.array([200.0])\n    mpc.set_uncertainty_values(Y_x=Y_x_values, S_in=S_in_values)\n\n    mpc.setup()\n    return mpc\n\n\ndef setup_simulator(model):\n    simulator = do_mpc.simulator.Simulator(model)\n    simulator.set_param(t_step=1.0)\n\n    # Set uncertain parameters for simulation\n    p_template = simulator.get_p_template()\n    p_template['Y_x'] = 0.5\n    p_template['S_in'] = 200.0\n    simulator.set_p_fun(lambda t_now: p_template)\n\n    simulator.setup()\n    return simulator\n\n\ndef setup_estimator(model):\n    estimator = do_mpc.estimator.StateFeedback(model)\n    return estimator\n\n\ndef run_closed_loop_simulation():\n    \"\"\"Run the closed-loop simulation and return results (silencing solver output).\"\"\"\n    # Build system\n    model = build_model()\n    mpc = setup_mpc(model)\n    simulator = setup_simulator(model)\n    estimator = setup_estimator(model)\n\n    # Initial state\n    x0 = np.array([1.0, 150.0, 0.0, 120.0]).reshape(-1, 1)\n    mpc.x0 = x0\n    simulator.x0 = x0\n    estimator.x0 = x0\n    mpc.set_initial_guess()\n\n    # Storage for results\n    results = {\n        't': [],\n        'x': [],\n        'u': [],\n    }\n\n    # Silence IPOPT/CasADi output during the loop\n    fnull = io.StringIO()\n    with redirect_stdout(fnull), redirect_stderr(fnull):\n        for k in range(60):\n            u0 = mpc.make_step(x0)\n            y_next = simulator.make_step(u0)\n            x0 = estimator.make_step(y_next)\n            \n            # Store results\n            results['t'].append(k)\n            results['x'].append(x0.flatten())\n            results['u'].append(u0.flatten())\n\n    # Convert to arrays\n    results['x'] = np.array(results['x'])\n    results['u'] = np.array(results['u'])\n    \n    return results\n\n\ndef create_animation(results):\n    \"\"\"Create an animated visualization of the batch bioreactor process.\"\"\"\n    \n    # Create figure with subplots\n    fig = plt.figure(figsize=(14, 10))\n    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n    \n    # Tank visualization (left side)\n    ax_tank = fig.add_subplot(gs[:, 0])\n    \n    # State plots (middle and right)\n    ax_biomass = fig.add_subplot(gs[0, 1])\n    ax_substrate = fig.add_subplot(gs[0, 2])\n    ax_product = fig.add_subplot(gs[1, 1])\n    ax_volume = fig.add_subplot(gs[1, 2])\n    ax_control = fig.add_subplot(gs[2, 1:])\n    \n    # Setup axes\n    n_steps = len(results['t'])\n    \n    # State axes setup\n    for ax, label, ylim in [\n        (ax_biomass, 'Biomass X_s [g/L]', (0, 6)),\n        (ax_substrate, 'Substrate S_s [g/L]', (0, 250)),\n        (ax_product, 'Product P_s [g/L]', (0, 50)),\n        (ax_volume, 'Volume V_s [L]', (100, 200))\n    ]:\n        ax.set_xlim(0, n_steps)\n        ax.set_ylim(ylim)\n        ax.set_xlabel('Time [h]')\n        ax.set_ylabel(label)\n        ax.grid(True, alpha=0.3)\n    \n    ax_control.set_xlim(0, n_steps)\n    ax_control.set_ylim(-0.01, 0.21)\n    ax_control.set_xlabel('Time [h]')\n    ax_control.set_ylabel('Feed Flow u [L/h]')\n    ax_control.grid(True, alpha=0.3)\n    \n    # Tank setup\n    ax_tank.set_xlim(-1.5, 1.5)\n    ax_tank.set_ylim(0, 3)\n    ax_tank.set_aspect('equal')\n    ax_tank.axis('off')\n    ax_tank.set_title('Batch Bioreactor', fontsize=14, fontweight='bold')\n    \n    # Initialize plot elements\n    lines = {\n        'biomass': ax_biomass.plot([], [], 'g-', lw=2, label='Biomass')[0],\n        'substrate': ax_substrate.plot([], [], 'b-', lw=2, label='Substrate')[0],\n        'product': ax_product.plot([], [], 'r-', lw=2, label='Product')[0],\n        'volume': ax_volume.plot([], [], 'm-', lw=2, label='Volume')[0],\n        'control': ax_control.plot([], [], 'k-', lw=2, label='Control')[0],\n    }\n    \n    # Current point markers\n    markers = {\n        'biomass': ax_biomass.plot([], [], 'go', markersize=8)[0],\n        'substrate': ax_substrate.plot([], [], 'bo', markersize=8)[0],\n        'product': ax_product.plot([], [], 'ro', markersize=8)[0],\n        'volume': ax_volume.plot([], [], 'mo', markersize=8)[0],\n        'control': ax_control.plot([], [], 'ko', markersize=8)[0],\n    }\n    \n    # Tank components\n    tank_outline = mpatches.FancyBboxPatch(\n        (-1, 0.2), 2, 2, boxstyle=\"round,pad=0.02\",\n        linewidth=3, edgecolor='black', facecolor='none'\n    )\n    ax_tank.add_patch(tank_outline)\n    \n    # Liquid in tank (will be updated)\n    liquid = mpatches.Rectangle((-0.95, 0.25), 1.9, 1.0, \n                                facecolor='lightblue', alpha=0.6)\n    ax_tank.add_patch(liquid)\n    \n    # Biomass particles (circles)\n    biomass_particles = []\n    for _ in range(10):\n        particle = plt.Circle((0, 1), 0.05, color='green', alpha=0.7)\n        ax_tank.add_patch(particle)\n        biomass_particles.append(particle)\n    \n    # Feed pipe and valve\n    feed_pipe = mpatches.Rectangle((0.8, 2.2), 0.1, 0.5, \n                                   facecolor='gray', edgecolor='black')\n    ax_tank.add_patch(feed_pipe)\n    \n    valve = mpatches.FancyBboxPatch(\n        (0.75, 2.15), 0.2, 0.1, boxstyle=\"round,pad=0.01\",\n        linewidth=2, edgecolor='black', facecolor='red', alpha=0.5\n    )\n    ax_tank.add_patch(valve)\n    \n    # Text displays\n    time_text = fig.text(0.02, 0.98, '', fontsize=12, fontweight='bold',\n                         transform=fig.transFigure)\n    \n    tank_text = ax_tank.text(0, 2.7, '', ha='center', fontsize=10)\n    \n    # Add legend to one subplot\n    ax_biomass.legend(loc='upper right')\n    \n    def init():\n        \"\"\"Initialize animation.\"\"\"\n        for line in lines.values():\n            line.set_data([], [])\n        for marker in markers.values():\n            marker.set_data([], [])\n        return list(lines.values()) + list(markers.values())\n    \n    def animate(frame):\n        \"\"\"Animation function.\"\"\"\n        # Update time text\n        time_text.set_text(f'Time: {frame:.0f} h')\n        \n        # Update history lines\n        t_data = results['t'][:frame+1]\n        x_data = results['x'][:frame+1]\n        u_data = results['u'][:frame+1]\n        \n        if frame > 0:\n            lines['biomass'].set_data(t_data, x_data[:, 0])\n            lines['substrate'].set_data(t_data, x_data[:, 1])\n            lines['product'].set_data(t_data, x_data[:, 2])\n            lines['volume'].set_data(t_data, x_data[:, 3])\n            lines['control'].set_data(t_data, u_data[:, 0])\n            \n            # Update current point markers\n            markers['biomass'].set_data([frame], [x_data[frame, 0]])\n            markers['substrate'].set_data([frame], [x_data[frame, 1]])\n            markers['product'].set_data([frame], [x_data[frame, 2]])\n            markers['volume'].set_data([frame], [x_data[frame, 3]])\n            markers['control'].set_data([frame], [u_data[frame, 0]])\n        \n        # Update tank visualization\n        if frame < len(x_data):\n            # Update liquid level based on volume\n            volume = x_data[frame, 3]\n            liquid_height = 1.5 * (volume / 200.0)  # Normalize to tank height\n            liquid.set_height(liquid_height)\n            \n            # Update biomass particles\n            biomass_conc = x_data[frame, 0]\n            n_visible = int(10 * min(biomass_conc / 5.0, 1.0))  # Scale particles\n            \n            for i, particle in enumerate(biomass_particles):\n                if i < n_visible:\n                    # Random position in liquid\n                    x = np.random.uniform(-0.8, 0.8)\n                    y = np.random.uniform(0.3, 0.25 + liquid_height * 0.9)\n                    particle.set_center((x, y))\n                    particle.set_alpha(0.7)\n                else:\n                    particle.set_alpha(0)\n            \n            # Update valve color based on control input\n            u_val = u_data[frame, 0] if frame < len(u_data) else 0\n            valve_color = plt.cm.RdYlGn_r(u_val / 0.2)  # Red=high flow, Green=low\n            valve.set_facecolor(valve_color)\n            valve.set_alpha(0.8 if u_val > 0.01 else 0.3)\n            \n            # Update tank text\n            tank_text.set_text(\n                f'V={volume:.1f}L, X={biomass_conc:.2f}g/L\\n'\n                f'S={x_data[frame, 1]:.1f}g/L, P={x_data[frame, 2]:.1f}g/L'\n            )\n        \n        return (list(lines.values()) + list(markers.values()) + \n                biomass_particles + [liquid, valve, tank_text, time_text])\n    \n    # Create animation\n    anim = FuncAnimation(fig, animate, init_func=init, \n                        frames=n_steps, interval=100, blit=False)\n    \n    plt.suptitle('Batch Bioreactor Control with do-mpc', fontsize=16, fontweight='bold')\n    \n    return fig, anim\n\n\n# Run simulation and create animation (no prints)\nresults = run_closed_loop_simulation()\nfig, anim = create_animation(results)\n\n# Render like the pendulum example: JS HTML animation and no extra prints\njs_anim = anim.to_jshtml()\nplt.close(fig)\ndisplay(HTML(js_anim))\n``` ","type":"content","url":"/mpc#example-propofol-infusion-control","position":75},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization"},"type":"lvl1","url":"/ocp","position":0},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization"},"content":"In the previous chapter, we examined different ways to represent dynamical systems: continuous versus discrete time, deterministic versus stochastic, fully versus partially observable, and even simulation-based views such as agent-based or programmatic models. Our focus was on the structure of models: how they capture evolution, uncertainty, and information.\n\nIn this chapter, we turn to what makes these models useful for decision-making. The goal is no longer just to describe how a system behaves, but to leverage that description to compute actions over time. This doesn’t mean the model prescribes actions on its own. Rather, it provides the scaffolding for optimization: given a model and an objective, we can derive the control inputs that make the modeled system behave well according to a chosen criterion.\n\nOur entry point will be trajectory optimization. By a trajectory, we mean the time-indexed sequence of states and controls that the system follows under a plan: the states (\\mathbf{x}_1, \\dots, \\mathbf{x}_T) together with the controls (\\mathbf{u}_1, \\dots, \\mathbf{u}_{T-1}). In this chapter, we focus on an open-loop viewpoint: starting from a known initial state, we compute the entire sequence of controls in advance and then apply it as-is. This is appealing because, for discrete-time problems, it yields a finite-dimensional optimization over a vector of decisions and cleanly exposes the structure of the constraints. In continuous time, the base formulation is infinite-dimensional; in this course we will rely on direct methods—time discretization and parameterization—to transform it into a finite-dimensional nonlinear program.\n\nOpen loop also has a clear limitation: if reality deviates from the model—due to disturbances, model mismatch, or unanticipated events—the state you actually reach may differ from the predicted one. The precomputed controls that were optimal for the nominal trajectory can then lead you further off course, and errors can compound over time.\n\nLater, we will study closed-loop (feedback) strategies, where the choice of action at time t can depend on the state observed at time t. Instead of a single sequence, we optimize a policy \\pi_t mapping states to controls, \\mathbf{u}_t = \\pi_t(\\mathbf{x}_t). Feedback makes plans resilient to unforeseen situations by adapting on the fly, but it leads to a more challenging problem class. We start with open-loop trajectory optimization to build core concepts and tools before tackling feedback design.","type":"content","url":"/ocp","position":1},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"Discrete-Time Optimal Control Problems (DOCPs)"},"type":"lvl3","url":"/ocp#discrete-time-optimal-control-problems-docps","position":2},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"Discrete-Time Optimal Control Problems (DOCPs)"},"content":"Consider a system described by a state \\mathbf{x}_t \\in \\mathbb{R}^n, summarizing everything needed to predict its evolution. At each stage t, we can influence the system through a control input \\mathbf{u}_t \\in \\mathbb{R}^m. The dynamics specify how the state evolves:\\mathbf{x}_{t+1} = \\mathbf{f}_t(\\mathbf{x}_t, \\mathbf{u}_t),\n\nwhere \\mathbf{f}_t may be nonlinear or time-varying. We assume the initial state \\mathbf{x}_1 is known.\n\nThe goal is to pick a sequence of controls \\mathbf{u}_1,\\dots,\\mathbf{u}_{T-1} that makes the trajectory desirable. But desirable in what sense? That depends on an objective function, which often includes two components:\\text{(i) stage cost: } c_t(\\mathbf{x}_t,\\mathbf{u}_t), \\qquad \\text{(ii) terminal cost: } c_T(\\mathbf{x}_T).\n\nThe stage cost reflects ongoing penalties—energy, delay, risk. The terminal cost measures the value (or cost) of ending in a particular state. Together, these give a discrete-time Bolza problem with path constraints and bounds:\\begin{aligned}\n    \\text{minimize} \\quad & c_T(\\mathbf{x}_T) + \\sum_{t=1}^{T-1} c_t(\\mathbf{x}_t,\\mathbf{u}_t) \\\\\n    \\text{subject to} \\quad & \\mathbf{x}_{t+1} = \\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t) \\\\\n                            & \\mathbf{g}_t(\\mathbf{x}_t,\\mathbf{u}_t) \\leq \\mathbf{0} \\\\\n                            & \\mathbf{x}_{\\text{min}} \\leq \\mathbf{x}_t \\leq \\mathbf{x}_{\\text{max}} \\\\\n                            & \\mathbf{u}_{\\text{min}} \\leq \\mathbf{u}_t \\leq \\mathbf{u}_{\\text{max}} \\\\\n    \\text{given} \\quad & \\mathbf{x}_1 = \\mathbf{x}_0 \\enspace .\n\\end{aligned}\n\nWritten this way, it may seem obvious that the decision variables are the controls \\mathbf{u}_t. After all, in most intuitive descriptions of control, we think of choosing inputs to influence the system. But notice that in the program above, the entire state trajectory also appears as a set of variables, linked to the controls by the dynamics constraints. This is intentional: it reflects one way of writing the problem that makes the constraints explicit.\n\nWhy introduce \\mathbf{x}_t as decision variables if they can be simulated forward from the controls? Many readers hesitate here, and the question is natural: If the model is deterministic and \\mathbf{x}_1 is known, why not pick \\mathbf{u}_{1:T-1} and compute \\mathbf{x}_{2:T} on the fly? That instinct leads to single shooting, a method we will return to shortly.\n\nAlready in this formulation, though, we see an important theme: the structure of the problem matters. Ignoring it can make our life much harder. The reason is twofold:\n\nDimensionality grows with the horizon. For a horizon of length T, the program has roughly (T-1)(m+n) decision variables.\n\nTemporal coupling. Each control affects all future states and costs. The feasible set is not a simple box but a narrow manifold defined by the dynamics.\n\nTogether, these features explain why specialized methods exist and why the way we write the problem influences the algorithms we can use. Whether we keep states explicit or eliminate them through forward simulation determines not just the problem size, but also its conditioning and the trade-offs between robustness and computational effort.","type":"content","url":"/ocp#discrete-time-optimal-control-problems-docps","position":3},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"Existence of Solutions and Optimality Conditions"},"type":"lvl3","url":"/ocp#existence-of-solutions-and-optimality-conditions","position":4},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"Existence of Solutions and Optimality Conditions"},"content":"Now that we have the optimization problem written down, a natural question arises: does this program always have a solution? And if it does, how can we recognize one when we see it? These questions bring us into the territory of feasibility and optimality conditions.","type":"content","url":"/ocp#existence-of-solutions-and-optimality-conditions","position":5},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"When does a solution exist?","lvl3":"Existence of Solutions and Optimality Conditions"},"type":"lvl4","url":"/ocp#when-does-a-solution-exist","position":6},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"When does a solution exist?","lvl3":"Existence of Solutions and Optimality Conditions"},"content":"Notice first that nothing in the problem statement required the dynamics\\mathbf{x}_{t+1} = \\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t)\n\nto be stable. In fact, many problems of interest involve unstable systems; think of balancing a pole or steering a spacecraft. What matters is that the dynamics are well defined: given a state–control pair, the rule \\mathbf{f}_t produces a valid next state.\n\nIn continuous time, one usually requires \\mathbf{f} to be continuous (often Lipschitz continuous) in \\mathbf{x} so that the ODE has a unique solution on the horizon of interest. In discrete time, the requirement is lighter—we only need the update map to be well posed.\n\nExistence also hinges on feasibility. A candidate control sequence must generate a trajectory that respects all constraints: the dynamics, any bounds on state and control, and any terminal requirements. If no such sequence exists, the feasible set is empty and the problem has no solution. This can happen if the constraints are overly strict, or if the system is uncontrollable from the given initial condition.","type":"content","url":"/ocp#when-does-a-solution-exist","position":7},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"What does optimality look like?","lvl3":"Existence of Solutions and Optimality Conditions"},"type":"lvl4","url":"/ocp#what-does-optimality-look-like","position":8},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"What does optimality look like?","lvl3":"Existence of Solutions and Optimality Conditions"},"content":"Assume the feasible set is nonempty. To characterize a point that is not only feasible but locally optimal, we use the Lagrange multiplier machinery from nonlinear programming. For a smooth problem\\begin{aligned}\n\\min_{\\mathbf{z}}\\quad & F(\\mathbf{z})\\\\\n\\text{s.t.}\\quad & G(\\mathbf{z})=\\mathbf{0},\\\\\n& H(\\mathbf{z})\\ge \\mathbf{0},\n\\end{aligned}\n\ndefine the Lagrangian\\mathcal{L}(\\mathbf{z},\\boldsymbol{\\lambda},\\boldsymbol{\\mu})\n= F(\\mathbf{z})+\\boldsymbol{\\lambda}^{\\top}G(\\mathbf{z})+\\boldsymbol{\\mu}^{\\top}H(\\mathbf{z}),\\qquad \\boldsymbol{\\mu}\\ge \\mathbf{0}.\n\nFor an inequality system H(\\mathbf{z})\\ge \\mathbf{0} and a candidate point \\mathbf{z}, the active set is\\mathcal{A}(\\mathbf{z}) \\;=\\; \\{\\, i \\;:\\; H_i(\\mathbf{z})=0 \\,\\},\n\nwhile indices with H_i(\\mathbf{z})>0 are inactive. Only active inequalities can carry positive multipliers.\n\nWe now make a constraint qualification assumption. In plain language, it says the constraints near the solution intersect in a regular way so that the feasible set has a well-defined tangent space and the multipliers exist. Algebraically, this amounts to a full row rank condition on the Jacobian of the equalities together with the active inequalities:\\text{rows of }\\big[\\nabla G(\\mathbf{z}^\\star);\\ \\nabla H_{\\mathcal{A}}(\\mathbf{z}^\\star)\\big]\\ \\text{are linearly independent.}\n\nThis is the LICQ (Linear Independence Constraint Qualification). In convex problems, Slater’s condition (existence of a strictly feasible point) plays a similar role. You can think of these as the assumptions that let the linearized KKT equations be solvable; we do not literally invert that Jacobian, but the full-rank property is the key ingredient that would make such an inversion possible in principle.\n\nUnder such a constraint qualification, any local minimizer \\mathbf{z}^\\star admits multipliers (\\boldsymbol{\\lambda}^\\star,\\boldsymbol{\\mu}^\\star) that satisfy the Karush–Kuhn–Tucker (KKT) conditions:\\begin{aligned}\n&\\text{stationarity:} && \\nabla_{\\mathbf{z}}\\mathcal{L}(\\mathbf{z}^\\star,\\boldsymbol{\\lambda}^\\star,\\boldsymbol{\\mu}^\\star)=\\mathbf{0},\\\\\n&\\text{primal feasibility:} && G(\\mathbf{z}^\\star)=\\mathbf{0},\\quad H(\\mathbf{z}^\\star)\\ge \\mathbf{0},\\\\\n&\\text{dual feasibility:} && \\boldsymbol{\\mu}^\\star\\ge \\mathbf{0},\\\\\n&\\text{complementarity:} && \\mu_i^\\star\\,H_i(\\mathbf{z}^\\star)=0\\quad \\text{for all } i.\n\\end{aligned}\n\nOnly constraints that are active at \\mathbf{z}^\\star can have \\mu_i^\\star>0; inactive ones have \\mu_i^\\star=0. The multipliers quantify marginal costs: \\lambda_j^\\star measures how the optimal value changes if the j-th equality is relaxed, and \\mu_i^\\star does the same for the i-th inequality. (If you prefer h(\\mathbf{z})\\le 0, signs flip accordingly.)\n\nIn our trajectory problems, \\mathbf{z} stacks state and control trajectories, G enforces the dynamics, and H collects bounds and path constraints. The equalities’ multipliers act as costates or shadow prices for the dynamics. Writing the KKT system stage by stage yields the discrete-time Pontryagin principle, derived next. For convex programs these conditions are also sufficient.\n\nWhat fails without a CQ? If the active gradients are dependent (for example duplicated or nearly parallel), the Jacobian loses rank; multipliers may then be nonunique or fail to exist, and the linearized equations become ill-posed. In transcribed trajectory problems this shows up as dependent dynamic constraints or redundant path constraints, which leads to fragile solver behavior.","type":"content","url":"/ocp#what-does-optimality-look-like","position":9},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"From KKT to algorithms","lvl3":"Existence of Solutions and Optimality Conditions"},"type":"lvl4","url":"/ocp#from-kkt-to-algorithms","position":10},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"From KKT to algorithms","lvl3":"Existence of Solutions and Optimality Conditions"},"content":"The KKT system can be read as the first-order optimality conditions of a saddle-point problem. With equalities G(\\mathbf{z})=\\mathbf{0} and inequalities H(\\mathbf{z})\\ge \\mathbf{0}, define the Lagrangian\\mathcal{L}(\\mathbf{z},\\boldsymbol{\\lambda},\\boldsymbol{\\mu})\n= F(\\mathbf{z})+\\boldsymbol{\\lambda}^{\\top}G(\\mathbf{z})+\\boldsymbol{\\mu}^{\\top}H(\\mathbf{z}),\\quad \\boldsymbol{\\mu}\\ge \\mathbf{0}.\n\nOptimality corresponds to a saddle: minimize in \\mathbf{z}, maximize in (\\boldsymbol{\\lambda},\\boldsymbol{\\mu}) (with \\boldsymbol{\\mu} constrained to the nonnegative orthant).","type":"content","url":"/ocp#from-kkt-to-algorithms","position":11},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl5":"Primal–dual gradient dynamics (Arrow–Hurwicz)","lvl4":"From KKT to algorithms","lvl3":"Existence of Solutions and Optimality Conditions"},"type":"lvl5","url":"/ocp#primal-dual-gradient-dynamics-arrow-hurwicz","position":12},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl5":"Primal–dual gradient dynamics (Arrow–Hurwicz)","lvl4":"From KKT to algorithms","lvl3":"Existence of Solutions and Optimality Conditions"},"content":"The simplest algorithm mirrors this saddle structure by descending in the primal variables and ascending in the dual variables, with a projection for the inequalities:\\begin{aligned}\n\\mathbf{z}^{k+1} &= \\mathbf{z}^{k}-\\alpha_k\\big(\\nabla F(\\mathbf{z}^{k})+\\nabla G(\\mathbf{z}^{k})^{\\top}\\boldsymbol{\\lambda}^{k}+\\nabla H(\\mathbf{z}^{k})^{\\top}\\boldsymbol{\\mu}^{k}\\big),\\\\[2mm]\n\\boldsymbol{\\lambda}^{k+1} &= \\boldsymbol{\\lambda}^{k}+\\beta_k\\,G(\\mathbf{z}^{k}),\\\\[1mm]\n\\boldsymbol{\\mu}^{k+1} &= \\Pi_{\\ge 0}\\!\\big(\\boldsymbol{\\mu}^{k}+\\beta_k\\,H(\\mathbf{z}^{k})\\big).\n\\end{aligned}\n\nHere \\Pi_{\\ge 0} is the projection onto \\{\\boldsymbol{\\mu}\\ge 0\\}. In convex settings and with suitable step sizes, these iterates converge to a saddle point. In nonconvex problems (our trajectory optimizations after transcription), these updates are often used inside augmented Lagrangian or penalty frameworks to improve robustness, for example by replacing \\mathcal{L} with\\mathcal{L}_\\rho(\\mathbf{z},\\boldsymbol{\\lambda},\\boldsymbol{\\mu})\n= \\mathcal{L}(\\mathbf{z},\\boldsymbol{\\lambda},\\boldsymbol{\\mu})\n+\\tfrac{\\rho}{2}\\|G(\\mathbf{z})\\|^2\n+\\tfrac{\\rho}{2}\\|\\min\\{0,H(\\mathbf{z})\\}\\|^2,\n\nwhich stabilizes the dual ascent when constraints are not yet well satisfied.","type":"content","url":"/ocp#primal-dual-gradient-dynamics-arrow-hurwicz","position":13},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl5":"SQP as Newton on the KKT system (equality case)","lvl4":"From KKT to algorithms","lvl3":"Existence of Solutions and Optimality Conditions"},"type":"lvl5","url":"/ocp#sqp-as-newton-on-the-kkt-system-equality-case","position":14},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl5":"SQP as Newton on the KKT system (equality case)","lvl4":"From KKT to algorithms","lvl3":"Existence of Solutions and Optimality Conditions"},"content":"With only equality constraints G(\\mathbf{z})=\\mathbf{0}, write first-order conditions\\nabla_{\\mathbf{z}}\\mathcal{L}(\\mathbf{z},\\boldsymbol{\\lambda})=\\mathbf{0},\n\\qquad\nG(\\mathbf{z})=\\mathbf{0},\n\\quad \\text{where }\\mathcal{L}=F+\\boldsymbol{\\lambda}^{\\top}G.\n\nApplying Newton’s method to this system gives the linear KKT solve\\begin{bmatrix}\n\\nabla_{\\mathbf{z}\\mathbf{z}}^2\\mathcal{L}(\\mathbf{z}^k,\\boldsymbol{\\lambda}^k) & \\nabla G(\\mathbf{z}^k)^{\\top}\\\\\n\\nabla G(\\mathbf{z}^k) & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\Delta \\mathbf{z}\\\\ \\Delta \\boldsymbol{\\lambda}\n\\end{bmatrix}\n=\n-\n\\begin{bmatrix}\n\\nabla_{\\mathbf{z}}\\mathcal{L}(\\mathbf{z}^k,\\boldsymbol{\\lambda}^k)\\\\\nG(\\mathbf{z}^k)\n\\end{bmatrix}.\n\nThis is exactly the step computed by Sequential Quadratic Programming (SQP) in the equality-constrained case: it is Newton’s method on the KKT equations. For general problems with inequalities, SQP forms a quadratic subproblem by quadratically modeling F with \\nabla_{\\mathbf{z}\\mathbf{z}}^2\\mathcal{L} and linearizing the constraints, then solves that QP with line search or trust region. In least-squares-like problems one often uses Gauss–Newton (or a Levenberg–Marquardt trust region) as a positive-definite approximation to the Lagrangian Hessian.\n\nIn trajectory optimization. After transcription, the KKT matrix inherits banded/sparse structure from the dynamics. Newton/SQP steps can be computed efficiently by exploiting this structure; in the special case of quadratic models and linearized dynamics, the QP reduces to an LQR solve along the horizon (this is the backbone of iLQR/DDP-style methods). Primal–dual updates provide simpler iterations and are easy to implement; augmented terms are typically needed to obtain stable progress when constraints couple stages.\n\nWhen to use which. Primal–dual gradients give lightweight iterations and are good for warm starts or as inner loops with penalties. SQP/Newton gives rapid local convergence when you are close to a solution and LICQ holds; use trust regions or line search to globalize.","type":"content","url":"/ocp#sqp-as-newton-on-the-kkt-system-equality-case","position":15},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"Examples of DOCPs"},"type":"lvl3","url":"/ocp#examples-of-docps","position":16},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"Examples of DOCPs"},"content":"To make things concrete, here are three problems that are naturally posed as discrete-time OCPs. In each case, we seek an optimal trajectory of states and controls over a finite horizon.","type":"content","url":"/ocp#examples-of-docps","position":17},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"Periodic Inventory Control","lvl3":"Examples of DOCPs"},"type":"lvl4","url":"/ocp#periodic-inventory-control","position":18},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"Periodic Inventory Control","lvl3":"Examples of DOCPs"},"content":"Decisions are made once per period: choose order quantity u_k \\ge 0 to meet forecast demand d_k. The state x_k is on-hand inventory with dynamics x_{k+1} = x_k + u_k - d_k. A typical stage cost is c_k(x_k,u_k) = h\\,[x_k]_+ + p\\,[-x_k]_+ + c\\,u_k, trading off holding, backorder, and ordering costs. The horizon objective is \\min \\sum_{k=0}^{T-1} c_k(x_k,u_k) subject to bounds.","type":"content","url":"/ocp#periodic-inventory-control","position":19},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"End-of-Day Portfolio Rebalancing","lvl3":"Examples of DOCPs"},"type":"lvl4","url":"/ocp#end-of-day-portfolio-rebalancing","position":20},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"End-of-Day Portfolio Rebalancing","lvl3":"Examples of DOCPs"},"content":"At each trading day k, choose trades u_k to adjust holdings h_k before next-day returns r_{k} realize. Deterministic planning uses predicted returns \\mu_k, with dynamics h_{k+1} = (h_k + u_k) \\odot (\\mathbf{1} + \\mu_k) and budget/box constraints. The stage cost can capture transaction costs and risk, e.g., c_k(h_k,u_k) = \\tau\\lVert u_k \\rVert_1 + \\tfrac{\\lambda}{2}\\,h_k^\\top \\Sigma_k h_k, with a terminal utility or wealth objective.","type":"content","url":"/ocp#end-of-day-portfolio-rebalancing","position":21},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"Daily Ad-Budget Allocation with Carryover","lvl3":"Examples of DOCPs"},"type":"lvl4","url":"/ocp#daily-ad-budget-allocation-with-carryover","position":22},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"Daily Ad-Budget Allocation with Carryover","lvl3":"Examples of DOCPs"},"content":"Allocate spend u_k \\in [0, U_{\\max}] to build awareness s_k with carryover dynamics s_{k+1} = \\alpha s_k + \\beta u_k. Conversions/revenue at day k follow a response curve g(s_k,u_k); the goal is \\max \\sum_{k=0}^{T-1} g(s_k,u_k) - c\\,u_k subject to spend limits. This is naturally discrete because decisions and measurements occur daily.","type":"content","url":"/ocp#daily-ad-budget-allocation-with-carryover","position":23},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"DOCPs Arising from the Discretization of Continuous-Time OCPs","lvl3":"Examples of DOCPs"},"type":"lvl4","url":"/ocp#docps-arising-from-the-discretization-of-continuous-time-ocps","position":24},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"DOCPs Arising from the Discretization of Continuous-Time OCPs","lvl3":"Examples of DOCPs"},"content":"Although many applications are natively discrete-time, it is also common to obtain a DOCP by discretizing a continuous-time formulation. Consider a system on [0, T_c] given by\\dot{\\mathbf{x}}(t) = \\mathbf{f}(t, \\mathbf{x}(t), \\mathbf{u}(t)), \\qquad \\mathbf{x}(0) = \\mathbf{x}_0.\n\nChoose a step size \\Delta > 0 and grid t_k = k\\,\\Delta. A one-step integration scheme induces a discrete map \\mathbf{F}_\\Delta so that\\mathbf{x}_{k+1} = \\mathbf{F}_\\Delta(\\mathbf{x}_k, \\mathbf{u}_k, t_k),\\qquad k=0,\\dots, T-1,\n\nwhere, for example, explicit Euler gives \\mathbf{F}_\\Delta(\\mathbf{x},\\mathbf{u},t) = \\mathbf{x} + \\Delta\\,\\mathbf{f}(t,\\mathbf{x},\\mathbf{u}). The resulting discrete-time optimal control problem takes the Bolza form with these induced dynamics:\\begin{aligned}\n\\min_{\\{\\mathbf{x}_k,\\mathbf{u}_k\\}}\\; & c_T(\\mathbf{x}_T) + \\sum_{k=0}^{T-1} c_k(\\mathbf{x}_k,\\mathbf{u}_k) \\\\\n\\text{s.t.}\\; & \\mathbf{x}_{k+1} - \\mathbf{F}_\\Delta(\\mathbf{x}_k,\\mathbf{u}_k, t_k) = 0,\\quad k=0,\\dots,T-1, \\\\\n& \\mathbf{x}_0 = \\mathbf{x}_\\mathrm{init}.\n\\end{aligned}","type":"content","url":"/ocp#docps-arising-from-the-discretization-of-continuous-time-ocps","position":25},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"Programs as DOCPs and Differentiable Programming","lvl3":"Examples of DOCPs"},"type":"lvl4","url":"/ocp#programs-as-docps-and-differentiable-programming","position":26},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"Programs as DOCPs and Differentiable Programming","lvl3":"Examples of DOCPs"},"content":"It is often useful to view a computer program itself as a discrete-time dynamical system. Let the program state collect memory, buffers, and intermediate variables, and let the control represent inputs or tunable decisions at each step. A single execution step defines a transition map\\mathbf{x}_{k+1}=\\Phi_k(\\mathbf{x}_k,\\mathbf{u}_k),\n\nand a scalar objective (e.g., loss, error, runtime, energy) yields a DOCP:\\min_{\\{\\mathbf{u}_k\\}} \\; c_T(\\mathbf{x}_T)+\\sum_{k=0}^{T-1} c_k(\\mathbf{x}_k,\\mathbf{u}_k)\n\\quad\\text{s.t.}\\quad \\mathbf{x}_{k+1}=\\Phi_k(\\mathbf{x}_k,\\mathbf{u}_k).\n\nIn differentiable programming (e.g., JAX, PyTorch), the composed map \\Phi_{T-1}\\circ\\cdots\\circ\\Phi_0 is differentiable, enabling reverse-mode automatic differentiation and efficient gradient-based trajectory optimization. When parts of the program are non-differentiable (discrete branches, simulators with events), DOCPs can still be solved using derivative-free or weak-gradient methods (eg. finite differences, SPSA, Nelder–Mead, CMA-ES, or evolutionary strategies) optionally combined with smoothing, relaxations, or stochastic estimators to navigate non-smooth regions.","type":"content","url":"/ocp#programs-as-docps-and-differentiable-programming","position":27},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl5":"Example: HTTP Retrier Optimization","lvl4":"Programs as DOCPs and Differentiable Programming","lvl3":"Examples of DOCPs"},"type":"lvl5","url":"/ocp#example-http-retrier-optimization","position":28},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl5":"Example: HTTP Retrier Optimization","lvl4":"Programs as DOCPs and Differentiable Programming","lvl3":"Examples of DOCPs"},"content":"As an example we cast the problem of optimizing a “HTTP retrier with backoff” as a DOCP where the state tracks wall-clock time, attempt index, success, last code, and jitter; the control is the chosen wait time before the next request (the backoff schedule); the transition encapsulates waiting and a probabilistic request outcome; and the objective penalizes latency and failure. We then optimize the schedule either directly (per-step SPSA) or via a two-parameter exponential policy using common random numbers for variance reduction.\n\nfrom dataclasses import dataclass\nimport math, random\n\n# ---------------------------\n# PROGRAM = \"HTTP retrier with backoff\"\n# ---------------------------\n\n@dataclass\nclass State:\n    t: float            # wall-clock time (s)\n    k: int              # attempt index\n    done: bool          # success flag\n    code: int | None    # last HTTP code or None\n    jitter: float       # per-run jitter (simulates clock/socket noise)\n\n# Controls (decision variables): per-step wait times (backoff schedule)\n# u[k] can be optimized; in a fixed policy you'd set u[k] = base * gamma**k\n# We'll keep them bounded for realism.\ndef clamp(x, lo, hi): return max(lo, min(hi, x))\n\n# Simulated environment: availability is time-varying (spiky outage)\ndef server_success_prob(t: float) -> float:\n    # Low availability for the first 2 seconds, then rebounds\n    base = 0.15 if t < 2.0 else 0.85\n    # Some diurnal-like wobble (toy)\n    wobble = 0.1 * math.sin(2 * math.pi * (t / 3.0))\n    return clamp(base + wobble, 0.01, 0.99)\n\ndef http_request():\n    # Just returns a code; success = 200, failure = 503\n    return 200 if random.random() < 0.5 else 503\n\n# -------- DOCP ingredients --------\n# State x_k = (t, k, done, code, jitter)\n# Control u_k = wait time before next attempt (our backoff schedule entry)\n# Transition Phi_k: one \"program step\" = (optional wait) + (one request) + (branch)\ndef Phi(state: State, u_k: float) -> State:\n    if state.done:\n        # No-ops after success (absorbing state)\n        return State(state.t, state.k, True, state.code, state.jitter)\n\n    # 1) Wait according to control (backoff schedule) + jitter\n    wait = clamp(u_k + 0.02 * state.jitter, 0.0, 3.0)\n    t = state.t + wait\n\n    # 2) Environment: success probability depends on time t\n    p = server_success_prob(t)\n\n    # 3) \"Perform request\": success with prob p; otherwise 503\n    code = 200 if random.random() < p else 503\n    done = (code == 200)\n\n    # 4) Advance attempt counter and wall clock\n    return State(t=t, k=state.k + 1, done=done, code=code, jitter=state.jitter)\n\n# Stage cost: latency penalty each step; heavy penalty if still failing late\ndef stage_cost(state: State, u_k: float) -> float:\n    # Latency/energy per unit wait + small per-step overhead when not done\n    return 0.20 * u_k + (0.00 if state.done else 0.002)\n\n# Terminal cost: if failed after horizon, big penalty; if succeeded, pay total time\ndef terminal_cost(state: State, max_attempts: int) -> float:\n    # Pay for elapsed time; fail late incurs extra penalty\n    return 0.3 * state.t + (5.0 if (not state.done and state.k >= max_attempts) else 0.0)\n\ndef rollout(u, max_attempts=8, seed=0):\n    random.seed(seed)\n    s = State(t=0.0, k=0, done=False, code=None, jitter=random.uniform(-1,1))\n    J = 0.0\n    for k in range(max_attempts):\n        J += stage_cost(s, u[k])\n        s = Phi(s, u[k])\n        if s.done:  # early stop like a real program\n            break\n    J += terminal_cost(s, max_attempts)\n    return J, s  # return final state for debugging if needed\n\n# ---------- helpers for SPSA with common random numbers ----------\ndef eval_policy(u, seeds, max_attempts=8):\n    # Average over a fixed set of seeds (CRN helps SPSA a lot)\n    Js = []\n    for sd in seeds:\n        J, _ = rollout(u, max_attempts=max_attempts, seed=sd)\n        Js.append(J)\n    return sum(Js) / len(Js)\n\ndef project_waits(u):\n    # Keep waits in [0, 3] for realism\n    return [max(0.0, min(3.0, x)) for x in u]\n\n# ---------- schedule parameterizations ----------\ndef schedule_exp(base, gamma, K):\n    # u[k] = base * gamma**k\n    return [base * (gamma ** k) for k in range(K)]\n\n# If you prefer per-step but monotone nonnegative waits, use softplus increments:\ndef schedule_softplus(z, K):\n    # z in R^K -> u monotone via cumulative softplus increments\n    def softplus(x):\n        return math.log1p(math.exp(-abs(x))) + max(x, 0.0)\n    inc = [softplus(zi) for zi in z]\n    u = []\n    s_accum = 0.0\n    for i in range(K):\n        s_accum += inc[i]\n        u.append(s_accum)\n    return u\n\n# ---------------------------\n# Black-box optimization (SPSA) of the schedule u[0:K]\n# ---------------------------\ndef spsa_optimize(K=8, iters=200, seed=0):\n    random.seed(seed)\n    # Initialize a conservative schedule (small linear backoff)\n    u = [0.05 + 0.1*k for k in range(K)]\n    alpha = 0.2      # learning rate\n    c0 = 0.1         # perturbation scale\n    for t in range(1, iters+1):\n        c = c0 / (t ** 0.101)\n        # Rademacher perturbation\n        delta = [1.0 if random.random() < 0.5 else -1.0 for _ in range(K)]\n        u_plus  = [clamp(u[i] + c * delta[i], 0.0, 3.0) for i in range(K)]\n        u_minus = [clamp(u[i] - c * delta[i], 0.0, 3.0) for i in range(K)]\n\n        Jp, _ = rollout(u_plus, seed=seed + 10*t + 1)\n        Jm, _ = rollout(u_minus, seed=seed + 10*t + 2)\n\n        # SPSA gradient estimate\n        g = [(Jp - Jm) / (2.0 * c * delta[i]) for i in range(K)]\n        # Update (project back to bounds)\n        u = [clamp(u[i] - alpha * g[i], 0.0, 3.0) for i in range(K)]\n    return u\n\n# ---------- SPSA over 2 parameters (base, gamma) with CRN ----------\ndef spsa_optimize_exp(K=8, iters=200, seed=0, Nmc=16):\n    random.seed(seed)\n    # fixed seeds reused every iteration (CRN)\n    seeds = [seed + 1000 + i for i in range(Nmc)]\n\n    # init: small base, mild growth\n    base, gamma = 0.05, 1.4\n    alpha0, c0 = 0.15, 0.2  # learning rate and perturbation scales\n\n    for t in range(1, iters + 1):\n        a_t = alpha0 / (t ** 0.602)   # standard SPSA decay\n        c_t = c0 / (t ** 0.101)\n\n        # Rademacher perturbations for 2 params\n        d_base = 1.0 if random.random() < 0.5 else -1.0\n        d_gamma = 1.0 if random.random() < 0.5 else -1.0\n\n        base_plus  = base  + c_t * d_base\n        base_minus = base  - c_t * d_base\n        gamma_plus  = gamma + c_t * d_gamma\n        gamma_minus = gamma - c_t * d_gamma\n\n        u_plus  = project_waits(schedule_exp(base_plus,  gamma_plus,  K))\n        u_minus = project_waits(schedule_exp(base_minus, gamma_minus, K))\n\n        Jp = eval_policy(u_plus, seeds, max_attempts=K)\n        Jm = eval_policy(u_minus, seeds, max_attempts=K)\n\n        # SPSA gradient estimate\n        g_base  = (Jp - Jm) / (2.0 * c_t * d_base)\n        g_gamma = (Jp - Jm) / (2.0 * c_t * d_gamma)\n\n        # Update\n        base  = max(0.0, base  - a_t * g_base)\n        gamma = max(0.5, gamma - a_t * g_gamma)  # keep reasonable\n\n    return base, gamma\n\nif __name__ == \"__main__\":\n    K = 8\n    # Baseline linear schedule\n    u0 = [0.05 + 0.1*k for k in range(K)]\n    J0, s0 = rollout(u0, seed=42)\n\n    # Optimize per-step waits (K-dim SPSA)\n    u_opt = spsa_optimize(K=K, iters=200, seed=123)\n    J1, s1 = rollout(u_opt, seed=999)\n\n    # Optimize exponential schedule parameters (2-dim SPSA with CRN)\n    base_opt, gamma_opt = spsa_optimize_exp(K=K, iters=200, seed=321, Nmc=16)\n    u_exp = project_waits(schedule_exp(base_opt, gamma_opt, K))\n    J2, s2 = rollout(u_exp, seed=777)\n\n    print(\"Initial schedule:\", [round(x,3) for x in u0], \"  Cost ≈\", round(J0,3))\n    print(\"Optimized (per-step SPSA):\", [round(x,3) for x in u_opt], \"  Cost ≈\", round(J1,3))\n    print(\"Optimized (exp base, gamma): base=\", round(base_opt,3), \" gamma=\", round(gamma_opt,3),\n          \"  schedule=\", [round(x,3) for x in u_exp], \"  Cost ≈\", round(J2,3))\n    print(\"Attempts (init → per-step → exp):\", s0.k, \"→\", s1.k, \"→\", s2.k,\n          \"  Success codes:\", s0.code, s1.code, s2.code)\n\n","type":"content","url":"/ocp#example-http-retrier-optimization","position":29},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl5":"Example: Gradient Descent with Momentum as DOCP","lvl4":"Programs as DOCPs and Differentiable Programming","lvl3":"Examples of DOCPs"},"type":"lvl5","url":"/ocp#example-gradient-descent-with-momentum-as-docp","position":30},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl5":"Example: Gradient Descent with Momentum as DOCP","lvl4":"Programs as DOCPs and Differentiable Programming","lvl3":"Examples of DOCPs"},"content":"To connect this lens to familiar practice—and to hyperparameter optimization—treat the learning rate and momentum (or their schedules) as controls. Rather than fixing them a priori, we can optimize them as part of a trajectory optimization. The optimizer itself becomes the dynamical system whose execution we shape to minimize final loss.\n\nProgram: gradient descent with momentum on a quadratic loss. We fit \\boldsymbol{\\theta}\\in\\mathbb{R}^p to data (\\mathbf{A},\\mathbf{b}) by minimizing\\ell(\\boldsymbol{\\theta})=\\tfrac{1}{2}\\,\\lVert\\mathbf{A}\\boldsymbol{\\theta}-\\mathbf{b}\\rVert_2^2.\n\nThe program maintains parameters \\boldsymbol{\\theta}_k and momentum \\mathbf{m}_k. Each iteration does:\n\ncompute gradient  \\mathbf{g}_k=\\nabla_{\\boldsymbol{\\theta}}\\ell(\\boldsymbol{\\theta}_k)=\\mathbf{A}^\\top(\\mathbf{A}\\boldsymbol{\\theta}_k-\\mathbf{b})\n\nupdate momentum  \\mathbf{m}_{k+1}=\\beta_k \\, \\mathbf{m}_k + \\mathbf{g}_k\n\nupdate parameters  \\boldsymbol{\\theta}_{k+1}=\\boldsymbol{\\theta}_k - \\alpha_k \\, \\mathbf{m}_{k+1}\n\nState, control, and transition. Define the state \\mathbf{x}_k=\\begin{bmatrix}\\boldsymbol{\\theta}_k\\\\ \\mathbf{m}_k\\end{bmatrix}\\in\\mathbb{R}^{2p} and the control \\mathbf{u}_k=\\begin{bmatrix}\\alpha_k\\\\ \\beta_k\\end{bmatrix}. One program step is\\Phi_k(\\mathbf{x}_k,\\mathbf{u}_k)=\n\\begin{bmatrix}\n\\boldsymbol{\\theta}_k - \\alpha_k\\!\\left(\\beta_k \\, \\mathbf{m}_k + \\mathbf{A}^\\top(\\mathbf{A}\\boldsymbol{\\theta}_k-\\mathbf{b})\\right)\\\\[2mm]\n\\beta_k \\, \\mathbf{m}_k + \\mathbf{A}^\\top(\\mathbf{A}\\boldsymbol{\\theta}_k-\\mathbf{b})\n\\end{bmatrix}.\n\nExecuting the program for T iterations gives the trajectory\\mathbf{x}_{k+1}=\\Phi_k(\\mathbf{x}_k,\\mathbf{u}_k),\\quad k=0,\\dots,T-1,\\qquad\n\\mathbf{x}_0=\\begin{bmatrix}\\boldsymbol{\\theta}_0\\\\ \\mathbf{m}_0\\end{bmatrix}.\n\nObjective as a DOCP. Choose terminal cost c_T(\\mathbf{x}_T)=\\ell(\\boldsymbol{\\theta}_T) and (optionally) stage costs c_k(\\mathbf{x}_k,\\mathbf{u}_k)=\\rho_\\alpha \\, \\alpha_k^2+\\rho_\\beta\\,(\\beta_k- \\bar\\beta)^2. The program-as-control problem is\\min_{\\{\\alpha_k,\\beta_k\\}} \\; \\ell(\\boldsymbol{\\theta}_T)+\\sum_{k=0}^{T-1}\\big(\\rho_\\alpha \\, \\alpha_k^2+\\rho_\\beta\\,(\\beta_k-\\bar\\beta)^2\\big)\n\\quad\\text{s.t.}\\quad \\mathbf{x}_{k+1}=\\Phi_k(\\mathbf{x}_k,\\mathbf{u}_k).\n\nBackpropagation = reverse-time costate recursion. Because \\Phi_k is differentiable, reverse-mode AD computes \\nabla_{\\mathbf{u}_{0:T-1}} \\big(c_T+\\sum c_k\\big) by propagating a costate \\boldsymbol{\\lambda}_k=\\partial \\mathcal{J}/\\partial \\mathbf{x}_k backward:\\boldsymbol{\\lambda}_T=\\nabla_{\\mathbf{x}_T} c_T,\\qquad\n\\boldsymbol{\\lambda}_k=\\nabla_{\\mathbf{x}_k} c_k + \\left(\\nabla_{\\mathbf{x}_k}\\Phi_k\\right)^\\top \\boldsymbol{\\lambda}_{k+1},\n\nand the gradients with respect to controls are\\nabla_{\\mathbf{u}_k}\\mathcal{J}=\\nabla_{\\mathbf{u}_k} c_k + \\left(\\nabla_{\\mathbf{u}_k}\\Phi_k\\right)^\\top \\boldsymbol{\\lambda}_{k+1}.\n\nUnrolling a tiny horizon (T=3) to see the composition:\\begin{aligned}\n\\mathbf{x}_1&=\\Phi_0(\\mathbf{x}_0,\\mathbf{u}_0),\\\\\n\\mathbf{x}_2&=\\Phi_1(\\mathbf{x}_1,\\mathbf{u}_1),\\\\\n\\mathbf{x}_3&=\\Phi_2(\\mathbf{x}_2,\\mathbf{u}_2),\\qquad\n\\mathcal{J}=c_T(\\mathbf{x}_3)+\\sum_{k=0}^{2} c_k(\\mathbf{x}_k,\\mathbf{u}_k).\n\\end{aligned}\n\nWhat if the program branches? Suppose we insert a “skip-small-gradients” branch\\boldsymbol{\\theta}_{k+1}=\\boldsymbol{\\theta}_k - \\alpha_k\\,\\mathbf{m}_{k+1}\\,\\mathbf{1}\\{ \\lVert\\mathbf{g}_k\\rVert>\\tau\\},\n\nwhich is non-differentiable because of the indicator. The DOCP view still applies, but gradients are unreliable. Two practical paths: smooth the branch (e.g., replace \\mathbf{1}\\{\\cdot\\} with \\sigma((\\lVert\\mathbf{g}_k\\rVert-\\tau)/\\epsilon) for small \\epsilon) and use autodiff; or go derivative-free on \\{\\alpha_k,\\beta_k,\\tau\\} (e.g., SPSA or CMA-ES) while keeping the inner dynamics exact.","type":"content","url":"/ocp#example-gradient-descent-with-momentum-as-docp","position":31},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"Variants: Lagrange and Mayer Problems"},"type":"lvl3","url":"/ocp#variants-lagrange-and-mayer-problems","position":32},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"Variants: Lagrange and Mayer Problems"},"content":"The Bolza form is general enough to cover most situations, but two common special cases are worth noting:\n\nLagrange problem (no terminal cost)\nIf the objective only accumulates stage costs:\\min_{\\mathbf{u}_{1:T-1}} \\sum_{t=1}^{T-1} c_t(\\mathbf{x}_t,\\mathbf{u}_t).\n\nExample: Energy minimization for a delivery drone. The concern is total battery use, regardless of the final position.\n\nMayer problem (terminal cost only)\nIf the objective depends only on the final state:\\min_{\\mathbf{u}_{1:T-1}} c_T(\\mathbf{x}_T).\n\nExample: Satellite orbital transfer. The only goal is to reach a specified orbit, no matter the fuel spent along the way.\n\nThese distinctions matter when deriving optimality conditions, but conceptually they fit in the same framework: the system evolves over time, and we choose controls to shape the trajectory.","type":"content","url":"/ocp#variants-lagrange-and-mayer-problems","position":33},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"Reducing to Mayer Form by State Augmentation","lvl3":"Variants: Lagrange and Mayer Problems"},"type":"lvl4","url":"/ocp#reducing-to-mayer-form-by-state-augmentation","position":34},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"Reducing to Mayer Form by State Augmentation","lvl3":"Variants: Lagrange and Mayer Problems"},"content":"Although Bolza, Lagrange, and Mayer problems look different, they are equivalent in expressive power. Any problem with running costs can be rewritten as a Mayer problem (one whose objective depends only on the final state) through a simple trick: augment the state with a running sum of costs.\n\nThe idea is straightforward. Introduce a new variable, y_t, that keeps track of the cumulative cost so far. At each step, we update this running sum along with the system state:\\tilde{\\mathbf{x}}_{t+1} =\n\\begin{pmatrix}\n\\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t) \\\\\ny_t + c_t(\\mathbf{x}_t,\\mathbf{u}_t)\n\\end{pmatrix},\n\nwhere \\tilde{\\mathbf{x}}_t = (\\mathbf{x}_t, y_t). The terminal cost then becomes:\\tilde{c}_T(\\tilde{\\mathbf{x}}_T) = c_T(\\mathbf{x}_T) + y_T.\n\nThe overall effect is that the explicit sum \\sum_{t=1}^{T-1} c_t(\\mathbf{x}_t,\\mathbf{u}_t) disappears from the objective and is captured implicitly by the augmented state. This lets us write every optimal control problem in Mayer form.\n\nWhy do this? Two reasons. First, it often simplifies mathematical derivations, as we will see later when deriving necessary conditions. Second, it can streamline algorithmic implementation: instead of writing separate code paths for Mayer, Lagrange, and Bolza problems, we can reduce everything to one canonical form. That said, this “one size fits all” approach isn’t always best in practice—specialized formulations can sometimes be more efficient computationally, especially when the running cost has simple structure.\n\nThe unifying theme is that a DOCP may look like a generic NLP on paper, but its structure matters. Ignoring that structure often leads to impractical solutions, whereas formulations that expose sparsity and respect temporal coupling allow modern solvers to scale effectively. In the following sections, we will examine how these choices play out in practice through single shooting, multiple shooting, and collocation methods, and why different formulations strike different trade-offs between robustness and computational effort.","type":"content","url":"/ocp#reducing-to-mayer-form-by-state-augmentation","position":35},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl2":"Numerical Methods for Solving DOCPs"},"type":"lvl2","url":"/ocp#numerical-methods-for-solving-docps","position":36},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl2":"Numerical Methods for Solving DOCPs"},"content":"Before we discuss specific algorithms, it is useful to clarify the goal: we want to recast a discrete-time optimal control problem as a standard nonlinear program (NLP). Collect all decision variables—states, controls, and any auxiliary variables—into a single vector \\mathbf{z}\\in\\mathbb{R}^{n_z} and write\\begin{aligned}\n\\min_{\\mathbf{z}\\in\\mathbb{R}^{n_z}} \\quad & F(\\mathbf{z}) \\\\\n\\text{s.t.} \\quad & G(\\mathbf{z}) = 0, \\\\\n& H(\\mathbf{z}) \\ge 0,\n\\end{aligned}\n\nwith maps F:\\mathbb{R}^{n_z}\\to\\mathbb{R}, G:\\mathbb{R}^{n_z}\\to\\mathbb{R}^{r_e}, and H:\\mathbb{R}^{n_z}\\to\\mathbb{R}^{r_h}. In optimal control, G typically encodes dynamics and boundary conditions, while H captures path and box constraints.\n\nThere are multiple ways to arrive at (and benefit from) this NLP:\n\nSimultaneous (direct transcription / full discretization): keep all states and controls as variables and impose the dynamics as equality constraints. This is straightforward and exposes sparsity, but the problem can be large unless solver-side techniques (e.g., condensing) are exploited.\n\nSequential (recursive elimination / single shooting): eliminate states by forward propagation from the initial condition, leaving controls as the main decision variables. This reduces dimension and constraints, but can be sensitive to initialization and longer horizons.\n\nMultiple shooting: introduce state variables at segment boundaries and enforce continuity between simulated segments. This compromises between size and conditioning and is often more robust than pure single shooting.\n\nThe next sections work through these formulations—starting with simultaneous methods, then sequential methods, and finally multiple shooting—before discussing how generic NLP solvers and specialized algorithms leverage the resulting structure in practice.","type":"content","url":"/ocp#numerical-methods-for-solving-docps","position":37},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"Simultaneous Methods","lvl2":"Numerical Methods for Solving DOCPs"},"type":"lvl3","url":"/ocp#simultaneous-methods","position":38},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"Simultaneous Methods","lvl2":"Numerical Methods for Solving DOCPs"},"content":"In the simultaneous (also called direct transcription or full discretization) approach, we keep the entire trajectory explicit and enforce the dynamics as equality constraints. Starting from the Bolza DOCP,\\min_{\\{\\mathbf{x}_t,\\mathbf{u}_t\\}}\\; c_T(\\mathbf{x}_T) + \\sum_{t=1}^{T-1} c_t(\\mathbf{x}_t,\\mathbf{u}_t)\n\\quad\\text{s.t.}\\quad \\mathbf{x}_{t+1} - \\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t) = 0,\\; t=1,\\dots,T-1,\n\ncollect all variables into a single vector\\mathbf{z} := \\begin{bmatrix}\n\\mathbf{x}_1^\\top & \\cdots & \\mathbf{x}_T^\\top & \\mathbf{u}_1^\\top & \\cdots & \\mathbf{u}_{T-1}^\\top\n\\end{bmatrix}^\\top \\in \\mathbb{R}^{n_z}.\n\nPath constraints typically apply only at selected times. Let \\mathscr{E} index additional equality constraints g_i and \\mathscr{I} index inequality constraints h_i. For each constraint i, define the set of time indices K_i \\subseteq \\{1,\\dots,T\\} where it is enforced (e.g., terminal constraints use K_i = \\{T\\}). The simultaneous transcription is the NLP\\begin{aligned}\n\\min_{\\mathbf{z}}\\quad & F(\\mathbf{z}) := c_T(\\mathbf{x}_T) + \\sum_{t=1}^{T-1} c_t(\\mathbf{x}_t,\\mathbf{u}_t) \\\\\n\\text{s.t.}\\quad & G(\\mathbf{z}) = \\begin{bmatrix}\n\\big[\\, g_i(\\mathbf{x}_k,\\mathbf{u}_k) \\big]_{i\\in\\mathscr{E},\\, k\\in K_i} \\\\\n\\big[\\, \\mathbf{x}_{t+1} - \\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t) \\big]_{t=1: T-1} \\\\\n\\mathbf{x}_1 - \\mathbf{x}_\\mathrm{init}\n\\end{bmatrix} = \\mathbf{0}, \\\\\n& H(\\mathbf{z}) = \\big[\\, h_i(\\mathbf{x}_k,\\mathbf{u}_k) \\big]_{i\\in\\mathscr{I},\\, k\\in K_i} \\; \\ge \\; \\mathbf{0},\n\\end{aligned}\n\noptionally with simple bounds \\mathbf{x}_{\\mathrm{lb}} \\le \\mathbf{x}_t \\le \\mathbf{x}_{\\mathrm{ub}} and \\mathbf{u}_{\\mathrm{lb}} \\le \\mathbf{u}_t \\le \\mathbf{u}_{\\mathrm{ub}} folded into H or provided to the solver separately. For notational convenience, some constraints may not depend on \\mathbf{u}_k at times in K_i; the indexing still helps specify when each condition is active.\n\nThis direct transcription is attractive because it is faithful to the model and exposes sparsity. The Jacobian of G has a block bi-diagonal structure induced by the dynamics, and the KKT matrix is sparse and structured—properties exploited by interior-point and SQP methods. The trade-off is size: with state dimension n and control dimension m, the decision vector has (T\\!\\cdot\\!n) + ((T\\!-\n1)\\cdot m) entries, and there are roughly (T\\!-\n1)\\cdot n dynamic equalities plus any path and boundary conditions. Techniques such as partial or full condensing eliminate state variables to reduce the equality set (at the cost of denser matrices), while keeping states explicit preserves sparsity and often improves robustness on long horizons and in the presence of state constraints.\n\nCompared to alternatives, simultaneous methods avoid the long nonlinear dependency chains of single shooting and make it easier to impose state/path constraints. They can, however, demand more memory and per-iteration linear algebra, so practical performance hinges on exploiting sparsity and good initialization.\n\nThe same logic applies when selecting an optimizer. For small-scale problems, it is common to rely on general-purpose routines such as those in scipy.optimize.minimize. Derivative-free methods like Nelder–Mead require no gradients but scale poorly as dimensionality increases. Quasi-Newton schemes such as BFGS work well for moderate dimensions and can approximate gradients by finite differences, while large-scale trajectory optimization often calls for gradient-based constrained solvers such as interior-point or sequential quadratic programming methods that can exploit sparse Jacobians and benefit from automatic differentiation. Stochastic techniques, including genetic algorithms, simulated annealing, or particle swarm optimization, occasionally appear when gradients are unavailable, but their cost grows rapidly with dimension and they are rarely competitive for structured optimal control problems. ### On the Choice of Optimizer\n\nAlthough the code example uses SLSQP, many alternatives exist. `scipy.optimize.minimize` provides a menu of options, and each has implications for speed, robustness, and scalability:\n\n* **Derivative-free methods** such as Nelder–Mead avoid gradients altogether. They are attractive when gradients are unavailable or noisy, but they scale poorly with dimension.\n* **Quasi-Newton methods** like BFGS approximate gradients by finite differences. They work well for moderate-scale problems and often outperform derivative-free schemes when the objective is smooth.\n* **Gradient-based constrained solvers** such as interior-point or SQP methods exploit derivatives—exact or automatic—and are typically the most efficient for large structured problems like trajectory optimization.\n\nBeyond these, **stochastic optimizers** occasionally appear in practice, especially when gradients are unreliable or the loss landscape is rugged. Random search is the simplest example, while genetic algorithms, simulated annealing, and particle swarm optimization introduce mechanisms for global exploration at the cost of significant computational effort.\n\nWhich method to choose depends on the context: problem size, availability of derivatives, and computational resources. When automatic differentiation is accessible, first-order methods like L-BFGS or Adam often dominate, particularly for single-shooting formulations where the objective is smooth and unconstrained except for simple bounds. This is why researchers with a machine learning background tend to gravitate toward these techniques: they integrate seamlessly with existing frameworks and run efficiently on GPUs.  \n### Example: Direct Solution to the Eco-cruise Problem\n\nMany modern vehicles include features that aim to improve energy efficiency without requiring extra effort from the driver. One such feature is Eco-Cruise. Unlike traditional cruise control, which keeps the car at a fixed speed regardless of conditions, Eco-Cruise adjusts speed within small margins to reduce energy consumption. The reasoning is straightforward: holding speed up a hill by applying full throttle uses more energy than allowing the car to slow slightly and regain speed later. Some systems go further by using map data, anticipating slopes and curves to plan ahead. These ideas are no longer experimental; several manufacturers already deploy predictive cruise systems based on navigation input.\n\nThe setup we will use is slightly idealized, but not unrealistic. It assumes that the driver provides a destination and an acceptable time target, something that most navigation systems already require. With that information, the controller can decide how fast to go and when to accelerate while ensuring the trip remains on schedule. Framing the problem in this way allows us to cast Eco-Cruise as a trajectory optimization exercise and to explore the structure of a discrete-time optimal control problem.\n\nConsider a 1 km segment of road that must be completed in exactly 60 seconds. We divide this horizon into 60 steps of one second each. At step $t$, the state consists of the cumulative distance $s_t$ and the speed $v_t$. The control input is the longitudinal acceleration $u_t$. With a time step of one second, the dynamics are written as\n\n$$\ns_{t+1} = s_t + v_t, \\qquad\nv_{t+1} = v_t + u_t.\n$$\n\nThe trip starts from rest, so $s_1 = 0$ and $v_1 = 0$, and it must end at $s_{T+1} = 1000$ m with $v_{T+1} = 0$.\n\nEnergy consumption depends on both acceleration and speed. Rather than model the details of rolling resistance, drivetrain losses, and aerodynamics, we adopt a simple quadratic approximation. Each stage incurs a cost\n\n$$\nc_t(v_t, u_t) = \\tfrac{1}{2}\\beta u_t^2 + \\tfrac{1}{2}\\gamma v_t^2,\n$$\n\nwhere the first term penalizes strong accelerations and the second discourages high cruising speed. Reasonable values are $\\beta = 1.0$ and $\\gamma = 0.1$. The objective is to minimize the sum of these stage costs across the horizon:\n\n$$\n\\min \\sum_{t=1}^{T} \\bigl( \\tfrac{\\beta}{2}u_t^2 + \\tfrac{\\gamma}{2}v_t^2 \\bigr).\n$$\n\nThe optimization must also respect physical limits. Speeds must remain between zero and $20\\ \\text{m/s}$ (about 72 km/h), and accelerations are bounded by $|u_t| \\le 3\\ \\text{m/s}^2$ for comfort and safety.\n\n\nThe complete formulation is\n\n$$\n\\begin{aligned}\n\\min_{\\{s_t,v_t,u_t\\}} \\ & \\sum_{t=1}^{T} \\bigl( \\tfrac{\\beta}{2}u_t^2 + \\tfrac{\\gamma}{2}v_t^2 \\bigr) \\\\\n\\text{subject to}\\ & s_{t+1}-s_t-v_t = 0,\\ \\ v_{t+1}-v_t-u_t = 0,\\ t=1,\\dots,T, \\\\\n& s_1 = 0,\\ v_1 = 0,\\ s_{T+1} = 1000,\\ v_{T+1} = 0, \\\\\n& 0 \\le v_t \\le 20,\\ \\ |u_t|\\le 3.\n\\end{aligned}\n$$\n\n#### Solution\n\nOnce the objective and constraints are expressed as Python functions, the problem can be passed to a generic optimizer with very little extra work. Here is a direct implementation using `scipy.optimize.minimize` with the SLSQP method:\n\n```{code-cell} ipython3\n:tags: [remove-input, remove-output]\n\nimport numpy as np\nimport json\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import minimize, Bounds\n\ndef solve_eco_cruise(beta=1.0, gamma=0.05, T=60, v_max=20.0, a_max=3.0, distance=1000.0):\n    \"\"\"Solve the eco-cruise optimization problem.\"\"\"\n    \n    n_state, n_control = T + 1, T\n    \n    def unpack(z):\n        s, v, u = z[:n_state], z[n_state:2*n_state], z[2*n_state:]\n        return s, v, u\n\n    def objective(z):\n        _, v, u = unpack(z)\n        return 0.5 * beta * np.sum(u**2) + 0.5 * gamma * np.sum(v[:-1]**2)\n\n    def dynamics(z):\n        s, v, u = unpack(z)\n        ceq = np.empty(2*T)\n        ceq[0::2] = s[1:] - s[:-1] - v[:-1]  # position dynamics\n        ceq[1::2] = v[1:] - v[:-1] - u        # velocity dynamics\n        return ceq\n\n    def boundary(z):\n        s, v, _ = unpack(z)\n        return np.array([s[0], v[0], s[-1]-distance, v[-1]])  # start/end conditions\n\n    # Optimization setup\n    cons = [{'type':'eq', 'fun': dynamics}, {'type':'eq', 'fun': boundary}]\n    bounds = Bounds(\n        lb=np.concatenate([np.full(n_state,-1e4), np.zeros(n_state), np.full(n_control,-a_max)]),\n        ub=np.concatenate([np.full(n_state,1e4), v_max*np.ones(n_state), np.full(n_control,a_max)])\n    )\n\n    # Initial guess: triangular velocity profile\n    accel_time = int(0.3 * T)\n    decel_time = int(0.3 * T)\n    cruise_time = T - accel_time - decel_time\n    peak_v = min(1.2 * distance/T, 0.8 * v_max)\n    \n    v0 = np.zeros(n_state)\n    v0[:accel_time+1] = np.linspace(0, peak_v, accel_time+1)\n    v0[accel_time:accel_time+cruise_time+1] = peak_v\n    v0[accel_time+cruise_time:] = np.linspace(peak_v, 0, decel_time+1)\n    \n    s0 = np.cumsum(np.concatenate([[0], v0[:-1]]))\n    scale = distance / s0[-1]\n    s0, v0 = s0 * scale, v0 * scale\n    u0 = np.diff(v0)\n    \n    z0 = np.concatenate([s0, v0, u0])\n    \n    # Solve optimization\n    print(f\"Solving eco-cruise optimization (β={beta}, γ={gamma})...\")\n    res = minimize(objective, z0, method=\"SLSQP\", bounds=bounds, constraints=cons,\n                   options={\"maxiter\": 1000, \"ftol\": 1e-9})\n    \n    if not res.success:\n        print(f\"Optimization failed: {res.message}\")\n        return None\n        \n    s_opt, v_opt, u_opt = unpack(res.x)\n    \n    # Create trajectory data\n    eco_trajectory = []\n    cumulative_energy = 0\n    \n    for t in range(T + 1):\n        if t < T:\n            stage_cost = 0.5 * beta * u_opt[t]**2 + 0.5 * gamma * v_opt[t]**2\n            cumulative_energy += stage_cost\n        else:\n            stage_cost = 0\n            \n        eco_trajectory.append({\n            \"time\": float(t), \"position\": float(s_opt[t]), \"velocity\": float(v_opt[t]),\n            \"acceleration\": float(u_opt[t]) if t < T else 0.0,\n            \"stageCost\": float(stage_cost), \"cumulativeEnergy\": float(cumulative_energy)\n        })\n    \n    return {\n        \"eco_trajectory\": eco_trajectory,\n        \"total_energy\": float(cumulative_energy),\n        \"optimization_success\": True,\n        \"parameters\": {\"beta\": beta, \"gamma\": gamma, \"T\": T, \"v_max\": v_max, \"a_max\": a_max, \"distance\": distance}\n    }\n\ndef generate_naive_trajectory(T=60, distance=1000.0, gamma=0.05):\n    \"\"\"Generate naive constant-speed trajectory for comparison.\"\"\"\n    \n    # Simple triangular profile: accelerate, cruise, decelerate\n    accel_time = decel_time = 4\n    cruise_time = T - accel_time - decel_time\n    cruise_speed = distance / (0.5 * accel_time + cruise_time + 0.5 * decel_time)\n    \n    naive_trajectory = []\n    cumulative_energy = 0\n    \n    for t in range(T + 1):\n        if t <= accel_time:\n            velocity = (cruise_speed / accel_time) * t\n            acceleration = cruise_speed / accel_time\n        elif t <= accel_time + cruise_time:\n            velocity = cruise_speed\n            acceleration = 0.0\n        else:\n            remaining_time = T - t\n            velocity = (cruise_speed / decel_time) * remaining_time\n            acceleration = -cruise_speed / decel_time\n        \n        # Calculate position by integration\n        position = 0 if t == 0 else naive_trajectory[t-1]['position'] + naive_trajectory[t-1]['velocity']\n        \n        # Calculate costs\n        if t < T:\n            stage_cost = 0.5 * 1.0 * acceleration**2 + 0.5 * gamma * velocity**2\n            cumulative_energy += stage_cost\n        else:\n            stage_cost = 0.0\n            \n        naive_trajectory.append({\n            \"time\": float(t), \"position\": float(position), \"velocity\": float(velocity),\n            \"acceleration\": float(acceleration), \"stageCost\": float(stage_cost),\n            \"cumulativeEnergy\": float(cumulative_energy)\n        })\n    \n    return {\"naive_trajectory\": naive_trajectory, \"total_energy\": float(cumulative_energy)}\n\ndef plot_comparison(eco_data, naive_data=None, save_plot=True):\n    \"\"\"Create visualization plots comparing eco-cruise and naive trajectories.\"\"\"\n    \n    eco_traj = eco_data['eco_trajectory']\n    times = [p['time'] for p in eco_traj]\n    positions = [p['position'] for p in eco_traj]\n    velocities = [p['velocity'] for p in eco_traj]\n    accelerations = [p['acceleration'] for p in eco_traj]\n    energy_costs = [p['stageCost'] for p in eco_traj]\n    cumulative_energy = [p['cumulativeEnergy'] for p in eco_traj]\n    \n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    fig.suptitle('Eco-Cruise vs Naive Trajectory Comparison', fontsize=16, fontweight='bold')\n    \n    # Plot 1: Position vs Time\n    axes[0, 0].plot(times, positions, 'b-', linewidth=2, label='Eco-Cruise')\n    if naive_data:\n        naive_traj = naive_data['naive_trajectory']\n        naive_times = [p['time'] for p in naive_traj]\n        naive_positions = [p['position'] for p in naive_traj]\n        axes[0, 0].plot(naive_times, naive_positions, 'r--', linewidth=2, label='Naive')\n    axes[0, 0].set_xlabel('Time (s)'); axes[0, 0].set_ylabel('Position (m)')\n    axes[0, 0].set_title('Position vs Time'); axes[0, 0].grid(True, alpha=0.3); axes[0, 0].legend()\n    \n    # Plot 2: Velocity vs Time\n    axes[0, 1].plot(times, velocities, 'b-', linewidth=2, label='Eco-Cruise')\n    if naive_data:\n        naive_velocities = [p['velocity'] for p in naive_traj]\n        axes[0, 1].plot(naive_times, naive_velocities, 'r--', linewidth=2, label='Naive')\n    axes[0, 1].set_xlabel('Time (s)'); axes[0, 1].set_ylabel('Velocity (m/s)')\n    axes[0, 1].set_title('Velocity vs Time'); axes[0, 1].grid(True, alpha=0.3); axes[0, 1].legend()\n    \n    # Plot 3: Acceleration vs Time\n    axes[0, 2].plot(times[:-1], accelerations[:-1], 'b-', linewidth=2, label='Eco-Cruise')\n    axes[0, 2].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n    axes[0, 2].set_xlabel('Time (s)'); axes[0, 2].set_ylabel('Acceleration (m/s²)')\n    axes[0, 2].set_title('Acceleration vs Time'); axes[0, 2].grid(True, alpha=0.3); axes[0, 2].legend()\n    \n    # Plot 4: Stage Cost vs Time\n    axes[1, 0].plot(times[:-1], energy_costs[:-1], 'b-', linewidth=2, label='Eco-Cruise')\n    if naive_data:\n        naive_costs = [p['stageCost'] for p in naive_traj[:-1]]\n        axes[1, 0].plot(naive_times[:-1], naive_costs, 'r--', linewidth=2, label='Naive')\n    axes[1, 0].set_xlabel('Time (s)'); axes[1, 0].set_ylabel('Stage Cost')\n    axes[1, 0].set_title('Stage Cost vs Time'); axes[1, 0].grid(True, alpha=0.3); axes[1, 0].legend()\n    \n    # Plot 5: Cumulative Energy vs Time\n    axes[1, 1].plot(times, cumulative_energy, 'b-', linewidth=2, label='Eco-Cruise')\n    if naive_data:\n        naive_cumulative = [p['cumulativeEnergy'] for p in naive_traj]\n        axes[1, 1].plot(naive_times, naive_cumulative, 'r--', linewidth=2, label='Naive')\n    axes[1, 1].set_xlabel('Time (s)'); axes[1, 1].set_ylabel('Cumulative Energy')\n    axes[1, 1].set_title('Cumulative Energy vs Time'); axes[1, 1].grid(True, alpha=0.3); axes[1, 1].legend()\n    \n    # Plot 6: Phase Space (Velocity vs Position)\n    axes[1, 2].plot(positions, velocities, 'b-', linewidth=2, label='Eco-Cruise')\n    if naive_data:\n        naive_positions = [p['position'] for p in naive_traj]\n        axes[1, 2].plot(naive_positions, naive_velocities, 'r--', linewidth=2, label='Naive')\n    axes[1, 2].set_xlabel('Position (m)'); axes[1, 2].set_ylabel('Velocity (m/s)')\n    axes[1, 2].set_title('Phase Space: Velocity vs Position'); axes[1, 2].grid(True, alpha=0.3); axes[1, 2].legend()\n    \n    plt.tight_layout()\n    \n    if save_plot:\n        plt.savefig('_static/eco_cruise_visualization.png', dpi=300, bbox_inches='tight')\n        print(\"Plot saved to _static/eco_cruise_visualization.png\")\n    \n    plt.show()\n    return fig\n\ndef demo():\n    \"\"\"Run complete eco-cruise demonstration with visualization.\"\"\"\n    \n    # Solve optimization\n    eco_data = solve_eco_cruise(beta=1.0, gamma=0.05, T=60, distance=1000.0)\n    if eco_data is None:\n        # Use Jupyter Book's gluing feature for error message\n        try:\n            from myst_nb import glue\n            glue(\"eco_cruise_output\", \"❌ Optimization failed!\", display=False)\n        except ImportError:\n            print(\"Optimization failed!\")\n        return None\n    \n    # Generate naive trajectory\n    naive_data = generate_naive_trajectory(T=60, distance=1000.0, gamma=0.05)\n    \n    # Create visualization\n    fig = plot_comparison(eco_data, naive_data, save_plot=True)\n    \n    # Use Jupyter Book's gluing feature to display the figure\n    try:\n        from myst_nb import glue\n        glue(\"eco_cruise_figure\", fig, display=False)\n    except ImportError:\n        # Fallback for when not running in Jupyter Book context\n        pass\n    \n    return eco_data, naive_data, fig\n\nif __name__ == \"__main__\":\n    demo()\n```\n\n```{glue:figure} eco_cruise_figure\n:figwidth: 100%\n:name: \"fig-eco-cruise\"\n\nEco-Cruise optimization results showing the comparison between energy-efficient and naive trajectory approaches.\n```\n\n``````{tab-set}\n:tags: [full-width]\n\n`````{tab-item} Visualization\n```{raw} html\n<script src=\"_static/iframe-modal.js\"></script>\n<div id=\"eco-cruise-container\"></div>\n<script>\ncreateIframeModal({\n  containerId: 'eco-cruise-container',\n  iframeSrc: '_static/eco-cruise-demo.html',\n  title: 'Eco-Cruise Optimization Visualization',\n  aspectRatio: '200%',\n  maxWidth: '1400px',\n  maxHeight: '900px'\n});\n</script>\n`````\n\n`````{tab-item} Code\n```{literalinclude} code/eco-cruise.py\n:language: python\n```\n`````\n``````\n\nThe function `scipy.optimize.minimize` expects three things: an objective function that returns a scalar cost, a set of constraints grouped as equality or inequality functions, and bounds on individual variables. Everything else is about bookkeeping.\n\nThe first step is to gather all decision variables—positions, speeds, and accelerations—into a single vector $\\mathbf{z}$. Helper routines like `unpack` then slice this vector back into its components so that the rest of the code reads naturally. The objective function mirrors the analytical form of the cost: it sums quadratic penalties on speeds and accelerations across the horizon.\n\nDynamics and boundary conditions appear as equality constraints. Each entry in `dynamics` enforces one of the discrete-time equations\n\n$$\ns_{t+1} - s_t - v_t = 0,\\qquad\nv_{t+1} - v_t - u_t = 0,\n$$\n\nwhile `boundary` pins down the start and end conditions. Together, these ensure that any candidate solution corresponds to a physically consistent trajectory.\n\nBounds serve two purposes: they impose physical limits on speed and acceleration and keep the otherwise unbounded position variables within a large but finite range. This prevents the optimizer from exploring meaningless regions of the search space during intermediate iterations.\n\nFinally, an initial guess is constructed by interpolating a straight line for the position, assigning a constant speed, and setting accelerations to zero. This is not intended to be optimal; it simply gives the solver a feasible starting point close enough to the constraint manifold to converge quickly.\n\nOnce these components are in place, the call to `minimize` does the rest. Internally, SLSQP linearizes the constraints, builds a quadratic subproblem, and iterates until both the Karush–Kuhn–Tucker conditions and the stopping tolerances are met. From the user's perspective, the heavy lifting reduces to providing functions that compute costs and residuals—everything else is handled by the solver. ","type":"content","url":"/ocp#simultaneous-methods","position":39},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"Sequential Methods","lvl2":"Numerical Methods for Solving DOCPs"},"type":"lvl3","url":"/ocp#sequential-methods","position":40},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"Sequential Methods","lvl2":"Numerical Methods for Solving DOCPs"},"content":"The previous section showed how a discrete-time optimal control problem can be solved by treating all states and controls as decision variables and enforcing the dynamics as equality constraints. This produces a nonlinear program that can be passed to solvers such as scipy.optimize.minimize with the SLSQP method. For short horizons, this approach is straightforward and works well; the code stays close to the mathematical formulation.\n\nIt also has a real advantage: by keeping the states explicit and imposing the dynamics through constraints, we anchor the trajectory at multiple points. This extra structure helps stabilize the optimization, especially for long horizons where small deviations in early steps can otherwise propagate and cause the optimizer to drift or diverge. In that sense, this formulation is better conditioned and more robust than approaches that treat the dynamics implicitly.\n\nThe drawback is scale. As the horizon grows, the number of variables and constraints grows with it, and all are coupled by the dynamics. Each iteration of a sequential quadratic programming (SQP) or interior-point method requires building and factorizing large Jacobians and Hessians. These methods have been embedded in reinforcement learning and differentiable programming pipelines—through implicit layers or differentiable convex solvers—but the cost is significant. They remain serial, rely on repeated linear algebra factorizations, and are difficult to parallelize efficiently. When thousands of such problems must be solved inside a learning loop, the overhead becomes prohibitive.\n\nThis motivates an alternative that aligns better with the computational model of machine learning. If the dynamics are deterministic and state constraints are absent (or reducible to simple bounds on controls), we can eliminate the equality constraints altogether by making the states implicit. Instead of solving for both states and controls, we fix the initial state and roll the system forward under a candidate control sequence. This is the essence of single shooting.\n\nThe term “shooting” comes from the idea of aiming and firing a trajectory from the initial state: you pick a control sequence, integrate (or step) the system forward, and see where it lands. If the final state misses the target, you adjust the controls and try again: like adjusting the angle of a shot until it hits the mark. It is called single shooting because we compute the entire trajectory in one pass from the starting point, without breaking it into segments. Later, we will contrast this with multiple shooting, where the horizon is divided into smaller arcs that are optimized jointly to improve stability and conditioning.\n\nThe analogy with deep learning is also immediate: the control sequence plays the role of parameters, the rollout is a forward pass, and the cost is a scalar loss. Gradients can be obtained with reverse-mode automatic differentiation. In the single shooting formulation of the DOCP, the constrained program\\min_{\\mathbf{x}_{1:T},\\,\\mathbf{u}_{1:T-1}} J(\\mathbf{x}_{1:T},\\mathbf{u}_{1:T-1})\n\\quad\\text{s.t.}\\quad \n\\mathbf{x}_{t+1}=\\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t)\n\ncollapses to\\min_{\\mathbf{u}_{1:T-1}}\\;\nc_T\\!\\bigl(\\boldsymbol{\\phi}_{T}(\\mathbf{u}, \\mathbf{x}_1)\\bigr)\n+\\sum_{t=1}^{T-1} c_t\\!\\bigl(\\boldsymbol{\\phi}_{t}(\\mathbf{u}, \\mathbf{x}_1), \\mathbf{u}_t\\bigr),\n\\qquad\n\\mathbf{u}_{\\mathrm{lb}}\\le\\mathbf{u}_{t}\\le\\mathbf{u}_{\\mathrm{ub}}.\n\nHere \\boldsymbol{\\phi}_t denotes the state reached at time t by recursively applying the dynamics to the previous state and current control. This recursion can be written as\\boldsymbol{\\phi}_{t+1}(\\mathbf{u},\\mathbf{x}_1)=\n\\mathbf{f}_{t}\\!\\bigl(\\boldsymbol{\\phi}_{t}(\\mathbf{u},\\mathbf{x}_1),\\mathbf{u}_t\\bigr),\\qquad\n\\boldsymbol{\\phi}_{1}=\\mathbf{x}_1.\n\nConcretely, here is JAX-style pseudocode for defining phi(u, x_0, t) using jax.lax.scan with a zero-based time index:def phi(u_seq, x0, t):\n    \"\"\"Return \\phi_t(u, x0) with 0-based t (\\phi_0 = x0).\n\n    u_seq: controls of length T (or T-1); only first t entries are used\n    x0: initial state at time 0\n    t: integer >= 0\n    \"\"\"\n    if t <= 0:\n        return x0\n\n    def step(carry, u):\n        x, t_idx = carry\n        x_next = f(x, u, t_idx)\n        return (x_next, t_idx + 1), None\n\n    (x_t, _), _ = lax.scan(step, (x0, 0), u_seq[:t])\n    return x_t\n\nThe pattern mirrors an RNN unroll: starting from an initial state (\\mathbf{x}^\\star_1) and a sequence of controls (\\mathbf{u}^*_{1:T-1}), we propagate forward through the dynamics, updating the state at each step and accumulating cost along the way. This structural similarity is why single shooting often feels natural to practitioners with a deep learning background: the rollout is a forward pass, and gradients propagate backward through time exactly as in backpropagation through an RNN.\n\nAlgorithmically:\n\nSingle Shooting: Forward Unroll\n\nInputs: Initial state \\mathbf{x}_1, horizon T, control bounds \\mathbf{u}_{\\mathrm{lb}}, \\mathbf{u}_{\\mathrm{ub}}, dynamics \\mathbf{f}_t, costs c_t\n\nOutput: Optimal control sequence \\mathbf{u}^*_{1:T-1}\n\nInitialize \\mathbf{u}_{1:T-1} within bounds\n\nDefine ComputeTrajectoryAndCost(\\mathbf{u}, \\mathbf{x}_1):\n\n\\mathbf{x} \\leftarrow \\mathbf{x}_1, J \\leftarrow 0\n\nFor t = 1 to T-1:\n\nJ \\leftarrow J + c_t(\\mathbf{x}, \\mathbf{u}_t)\n\n\\mathbf{x} \\leftarrow \\mathbf{f}_t(\\mathbf{x}, \\mathbf{u}_t)\n\nJ \\leftarrow J + c_T(\\mathbf{x})\n\nReturn J\n\nSolve \\min_{\\mathbf{u}} J(\\mathbf{u}) subject to \\mathbf{u}_{\\mathrm{lb}} \\le \\mathbf{u}_t \\le \\mathbf{u}_{\\mathrm{ub}}\n\nReturn \\mathbf{u}^*_{1:T-1}\n\nIn JAX or PyTorch, this loop can be JIT-compiled and differentiated automatically. Any gradient-based optimizer—L-BFGS, Adam, even SGD—can be applied, making the pipeline look very much like training a neural network. In effect, we are “backpropagating through the world model” when computing \\nabla J(\\mathbf{u}).\n\nSingle shooting is attractive for its simplicity and compatibility with differentiable programming, but it has limitations. The absence of intermediate constraints makes it sensitive to initialization and prone to numerical instability over long horizons. When state constraints or robustness matter, formulations that keep states explicit—such as multiple shooting or collocation—become preferable. These trade-offs are the focus of the next section. \n```{code-cell} ipython3\n:tags: [hide-cell]\n:mystnb:\n:  code_prompt_show: \"Show code demonstration\"\n:  code_prompt_hide: \"Hide code demonstration\"\n:load: code/single_shooting_unrolled.py\n``` ","type":"content","url":"/ocp#sequential-methods","position":41},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"In Between Sequential and Simultaneous","lvl2":"Numerical Methods for Solving DOCPs"},"type":"lvl3","url":"/ocp#in-between-sequential-and-simultaneous","position":42},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"In Between Sequential and Simultaneous","lvl2":"Numerical Methods for Solving DOCPs"},"content":"The two formulations we have seen so far lie at opposite ends. The full discretization approach keeps every state explicit and enforces the dynamics through equality constraints, which makes the structure clear but leads to a large optimization problem. At the other end, single shooting removes these constraints by simulating forward from the initial state, leaving only the controls as decision variables. That makes the problem smaller, but it also introduces a long and highly nonlinear dependency from the first control to the last state.\n\nMultiple shooting sits in between. Instead of simulating the entire horizon in one shot, we divide it into smaller segments. For each segment, we keep its starting state as a decision variable and propagate forward using the dynamics for that segment. At the end, we enforce continuity by requiring that the simulated end state of one segment matches the decision variable for the next.\n\nFormally, suppose the horizon of T steps is divided into K segments of length L (with T = K \\cdot L for simplicity). We introduce:\n\nThe controls for each step: \\mathbf{u}_{1:T-1}.\n\nThe state at the start of each segment: \\mathbf{x}_1,\\dots,\\mathbf{x}_K.\n\nGiven \\mathbf{x}_k and the controls in its segment, we compute the predicted terminal state by simulating forward:\\hat{\\mathbf{x}}_{k+1} = \\Phi(\\mathbf{x}_k,\\mathbf{u}_{\\text{segment }k}),\n\nwhere \\Phi represents L applications of the dynamics. Continuity constraints enforce:\\mathbf{x}_{k+1} - \\hat{\\mathbf{x}}_{k+1} = 0, \\qquad k=1,\\dots,K-1.\n\nThe resulting nonlinear program looks like this:\\begin{aligned}\n\\min_{\\{\\mathbf{x}_k,\\mathbf{u}_t\\}} \\quad &\nc_T(\\mathbf{x}_T) + \\sum_{t=1}^{T-1} c_t(\\mathbf{x}_t,\\mathbf{u}_t) \\\\\n\\text{subject to} \\quad &\n\\mathbf{x}_{k+1} - \\Phi(\\mathbf{x}_k,\\mathbf{u}_{\\text{segment }k}) = 0,\\quad k = 1,\\dots,K-1, \\\\\n& \\mathbf{u}_{\\mathrm{lb}} \\le \\mathbf{u}_t \\le \\mathbf{u}_{\\mathrm{ub}}, \\\\\n& \\text{boundary conditions on } \\mathbf{x}_1 \\text{ and } \\mathbf{x}_K.\n\\end{aligned}\n\nCompared to the full NLP, we no longer introduce every intermediate state as a variable—only the anchors at segment boundaries. Inside each segment, states are reconstructed by simulation. Compared to single shooting, these anchors break the long dependency chain that makes optimization unstable: gradients only have to travel across L steps before they hit a decision variable, rather than the entire horizon. This is the same reason why exploding or vanishing gradients appear in deep recurrent networks: when the chain is too long, information either dies out or blows up. Multiple shooting shortens the chain and improves conditioning.\n\nBy adjusting the number of segments K, we can interpolate between the two extremes: K = 1 gives single shooting, while K = T recovers the full direct NLP. In practice, a moderate number of segments often strikes a good balance between robustness and complexity.\n\n\"\"\"\nMultiple Shooting as a Boundary-Value Problem (BVP) for a Ballistic Trajectory\n-----------------------------------------------------------------------------\nWe solve for the initial velocities (and total flight time) so that the terminal\nposition hits a target, enforcing continuity between shooting segments.\n\"\"\"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.integrate import solve_ivp\nfrom scipy.optimize import minimize\nfrom IPython.display import HTML, display\n\n# -----------------------------\n# Physical parameters\n# -----------------------------\ng = 9.81          # gravity (m/s^2)\nm = 1.0           # mass (kg)\ndrag_coeff = 0.1  # quadratic drag coefficient\n\n\ndef dynamics(t, state):\n    \"\"\"Ballistic dynamics with quadratic drag. state = [x, y, vx, vy].\"\"\"\n    x, y, vx, vy = state\n    v = np.hypot(vx, vy)\n    drag_x = -drag_coeff * v * vx / m if v > 0 else 0.0\n    drag_y = -drag_coeff * v * vy / m if v > 0 else 0.0\n    dx  = vx\n    dy  = vy\n    dvx = drag_x\n    dvy = drag_y - g\n    return np.array([dx, dy, dvx, dvy])\n\n\ndef flow(y0, h):\n    \"\"\"One-segment flow map Φ(y0; h): integrate dynamics over duration h.\"\"\"\n    sol = solve_ivp(dynamics, (0.0, h), y0, method=\"RK45\", rtol=1e-7, atol=1e-9)\n    return sol.y[:, -1], sol\n\n# -----------------------------\n# Multiple-shooting BVP residuals\n# -----------------------------\n\ndef residuals(z, K, x_init, x_target):\n    \"\"\"\n    Unknowns z = [vx0, vy0, H, y1(4), y2(4), ..., y_{K-1}(4)]  (total len = 3 + 4*(K-1))\n    We define y0 from x_init and (vx0, vy0). Each segment has duration h = H/K.\n    Residual vector stacks:\n      - initial position constraints: y0[:2] - x_init[:2]\n      - continuity: y_{k+1} - Φ(y_k; h) for k=0..K-2\n      - terminal position constraint at end of last segment: Φ(y_{K-1}; h)[:2] - x_target[:2]\n    \"\"\"\n    n = 4\n    vx0, vy0, H = z[0], z[1], z[2]\n    if H <= 0:\n        # Strongly penalize nonpositive durations to keep solver away\n        return 1e6 * np.ones(2 + 4*(K-1) + 2)\n\n    h = H / K\n\n    # Build list of segment initial states y_0..y_{K-1}\n    ys = []\n    y0 = np.array([x_init[0], x_init[1], vx0, vy0], dtype=float)\n    ys.append(y0)\n    if K > 1:\n        rest = z[3:]\n        y_internals = rest.reshape(K-1, n)\n        ys.extend(list(y_internals))  # y1..y_{K-1}\n\n    res = []\n\n    # Initial position must match exactly\n    res.extend(ys[0][:2] - x_init[:2])\n\n    # Continuity across segments\n    for k in range(K-1):\n        yk = ys[k]\n        yk1_pred, _ = flow(yk, h)\n        res.extend(ys[k+1] - yk1_pred)\n\n    # Terminal position at the end of last segment equals target\n    y_last_end, _ = flow(ys[-1], h)\n    res.extend(y_last_end[:2] - x_target[:2])\n\n    # Optional soft \"stay above ground\" at knots (kept gentle)\n    # res.extend(np.minimum(0.0, np.array([y[1] for y in ys])).ravel())\n\n    return np.asarray(res)\n\n# -----------------------------\n# Solve BVP via optimization on 0.5*||residuals||^2\n# -----------------------------\n\ndef solve_bvp_multiple_shooting(K=5, x_init=np.array([0., 0.]), x_target=np.array([10., 0.])):\n    \"\"\"\n    K: number of shooting segments.\n    x_init: initial position (x0, y0). Initial velocities are unknown.\n    x_target: desired terminal position (xT, yT) at time H (unknown).\n    \"\"\"\n    # Heuristic initial guesses:\n    dx = x_target[0] - x_init[0]\n    dy = x_target[1] - x_init[1]\n    H0 = max(0.5, dx / 5.0)  # guess ~ 5 m/s horizontal\n    vx0_0 = dx / H0\n    vy0_0 = (dy + 0.5 * g * H0**2) / H0  # vacuum guess\n\n    # Intentionally disconnected internal knots to visualize defect shrinkage\n    internals = []\n    for k in range(1, K):  # y1..y_{K-1}\n        xk = x_init[0] + (dx * k) / K\n        yk = x_init[1] + (dy * k) / K + 2.0  # offset to create mismatch\n        internals.append(np.array([xk, yk, 0.0, 0.0]))\n    internals = np.array(internals) if K > 1 else np.array([])\n\n    z0 = np.concatenate(([vx0_0, vy0_0, H0], internals.ravel()))\n\n    # Variable bounds: H > 0, keep velocities within a reasonable range\n    # Use wide bounds to let the solver work; tune if needed.\n    lb = np.full_like(z0, -np.inf, dtype=float)\n    ub = np.full_like(z0,  np.inf, dtype=float)\n    lb[2] = 1e-2  # H lower bound\n    # Optional velocity bounds\n    lb[0], ub[0] = -50.0, 50.0\n    lb[1], ub[1] = -50.0, 50.0\n\n    # Objective and callback for L-BFGS-B\n    def objective(z):\n        r = residuals(z, K,\n                      np.array([x_init[0], x_init[1], 0., 0.]),\n                      np.array([x_target[0], x_target[1], 0., 0.]))\n        return 0.5 * np.dot(r, r)\n\n    iterate_history = []\n    def cb(z):\n        iterate_history.append(z.copy())\n\n    bounds = list(zip(lb.tolist(), ub.tolist()))\n    sol = minimize(objective, z0, method='L-BFGS-B', bounds=bounds,\n                   callback=cb, options={'maxiter': 300, 'ftol': 1e-12})\n\n    return sol, iterate_history\n\n# -----------------------------\n# Reconstruct and plot (optional static figure)\n# -----------------------------\n\ndef reconstruct_and_plot(sol, K, x_init, x_target):\n    n = 4\n    vx0, vy0, H = sol.x[0], sol.x[1], sol.x[2]\n    h = H / K\n\n    ys = []\n    y0 = np.array([x_init[0], x_init[1], vx0, vy0])\n    ys.append(y0)\n    if K > 1:\n        internals = sol.x[3:].reshape(K-1, n)\n        ys.extend(list(internals))\n\n    # Integrate each segment and stitch\n    traj_x, traj_y = [], []\n    for k in range(K):\n        yk = ys[k]\n        yend, seg = flow(yk, h)\n        traj_x.extend(seg.y[0, :].tolist() if k == 0 else seg.y[0, 1:].tolist())\n        traj_y.extend(seg.y[1, :].tolist() if k == 0 else seg.y[1, 1:].tolist())\n\n    # Plot\n    fig, ax = plt.subplots(figsize=(7, 4.2))\n    ax.plot(traj_x, traj_y, '-', label='Multiple-shooting solution')\n    ax.plot([x_init[0]], [x_init[1]], 'go', label='Start')\n    ax.plot([x_target[0]], [x_target[1]], 'r*', ms=12, label='Target')\n    total_pts = len(traj_x)\n    for k in range(1, K):\n        idx = int(k * total_pts / K)\n        ax.axvline(traj_x[idx], color='k', ls='--', alpha=0.3, lw=1)\n\n    ax.set_xlabel('x (m)')\n    ax.set_ylabel('y (m)')\n    ax.set_title(f'Multiple Shooting BVP (K={K})   H={H:.3f}s   v0=({vx0:.2f},{vy0:.2f}) m/s')\n    ax.grid(True, alpha=0.3)\n    ax.legend(loc='best')\n    plt.tight_layout()\n    plt.show()\n\n    # Report residual norms\n    res = residuals(sol.x, K, np.array([x_init[0], x_init[1], 0., 0.]), np.array([x_target[0], x_target[1], 0., 0.]))\n    print(f\"\\nFinal residual norm: {np.linalg.norm(res):.3e}\")\n    print(f\"vx0={vx0:.4f} m/s, vy0={vy0:.4f} m/s, H={H:.4f} s\")\n\n# -----------------------------\n# Create JS animation for notebooks\n# -----------------------------\n\ndef create_animation_progress(iter_history, K, x_init, x_target):\n    \"\"\"Return a JS animation (to_jshtml) showing defect shrinkage across segments.\"\"\"\n    import matplotlib.pyplot as plt\n    from matplotlib.animation import FuncAnimation\n\n    n = 4\n\n    def unpack(z):\n        vx0, vy0, H = z[0], z[1], z[2]\n        ys = [np.array([x_init[0], x_init[1], vx0, vy0])]\n        if K > 1 and len(z) > 3:\n            internals = z[3:].reshape(K-1, n)\n            ys.extend(list(internals))\n        return H, ys\n\n    fig, ax = plt.subplots(figsize=(7, 4.2))\n    ax.set_xlabel('Segment index (normalized time)')\n    ax.set_ylabel('y (m)')\n    ax.set_title('Multiple Shooting: Defect Shrinkage (Fixed Boundaries)')\n    ax.grid(True, alpha=0.3)\n\n    # Start/target markers at fixed indices\n    ax.plot([0], [x_init[1]], 'go', label='Start')\n    ax.plot([K], [x_target[1]], 'r*', ms=12, label='Target')\n    # Vertical dashed lines at boundaries\n    for k in range(1, K):\n        ax.axvline(k, color='k', ls='--', alpha=0.35, lw=1)\n    ax.legend(loc='best')\n\n    # Pre-create line artists\n    colors = plt.cm.plasma(np.linspace(0, 1, K))\n    segment_lines = [ax.plot([], [], '-', color=colors[k], lw=2, alpha=0.9)[0] for k in range(K)]\n    connector_lines = [ax.plot([], [], 'r-', lw=1.4, alpha=0.75)[0] for _ in range(K-1)]\n\n    text_iter = ax.text(0.02, 0.98, '', transform=ax.transAxes,\n                        va='top', fontsize=9,\n                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n\n    def animate(i):\n        idx = min(i, len(iter_history)-1)\n        z = iter_history[idx]\n        H, ys = unpack(z)\n        h = H / K\n\n        all_y = [x_init[1], x_target[1]]\n        total_defect = 0.0\n        for k in range(K):\n            yk = ys[k]\n            yend, seg = flow(yk, h)\n            # Map local time to [k, k+1]\n            t_local = seg.t\n            x_vals = k + (t_local / t_local[-1])\n            y_vals = seg.y[1, :]\n            segment_lines[k].set_data(x_vals, y_vals)\n            all_y.extend(y_vals.tolist())\n            if k < K-1:\n                y_next = ys[k+1]\n                # Vertical connector at boundary x=k+1\n                connector_lines[k].set_data([k+1, k+1], [yend[1], y_next[1]])\n                total_defect += abs(y_next[1] - yend[1])\n\n        # Fixed x-limits in index space\n        ax.set_xlim(-0.1, K + 0.1)\n        ymin, ymax = min(all_y), max(all_y)\n        margin_y = 0.10 * max(1.0, ymax - ymin)\n        ax.set_ylim(ymin - margin_y, ymax + margin_y)\n\n        text_iter.set_text(f'Iterate {idx+1}/{len(iter_history)}  |  Sum vertical defect: {total_defect:.3e}')\n        return segment_lines + connector_lines + [text_iter]\n\n    anim = FuncAnimation(fig, animate, frames=len(iter_history), interval=600, blit=False, repeat=True)\n    plt.tight_layout()\n    js_anim = anim.to_jshtml()\n    plt.close(fig)\n    return js_anim\n\n\ndef main():\n    # Problem definition\n    x_init = np.array([0.0, 0.0])      # start at origin\n    x_target = np.array([10.0, 0.0])   # hit ground at x=10 m\n    K = 6                               # number of shooting segments\n\n    sol, iter_hist = solve_bvp_multiple_shooting(K=K, x_init=x_init, x_target=x_target)\n    # Optionally show static reconstruction (commented for docs cleanliness)\n    # reconstruct_and_plot(sol, K, x_init, x_target)\n\n    # Animate progression (defect shrinkage across segments) and display as JS\n    js_anim = create_animation_progress(iter_hist, K, x_init, x_target)\n    display(HTML(js_anim))\n\n\nif __name__ == \"__main__\":\n    main()\n\n","type":"content","url":"/ocp#in-between-sequential-and-simultaneous","position":43},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl2":"The Discrete-Time Pontryagin Principle"},"type":"lvl2","url":"/ocp#the-discrete-time-pontryagin-principle","position":44},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl2":"The Discrete-Time Pontryagin Principle"},"content":"If we take the Bolza formulation of the DOCP and apply the KKT conditions directly, we obtain an optimization system with many multipliers and constraints. Written in raw form, it looks like any other nonlinear program. But in control, this structure has a long history and a name of its own: the Pontryagin principle. In fact, the discrete-time version can be seen as the structured KKT system that emerges once we introduce multipliers for the dynamics and collect terms stage by stage.\n\nWe work with the Bolza program\\begin{aligned}\n\\min_{\\{\\mathbf{x}_t,\\mathbf{u}_t\\}} \\quad & c_T(\\mathbf{x}_T)\\;+\\;\\sum_{t=1}^{T-1} c_t(\\mathbf{x}_t,\\mathbf{u}_t) \\\\\n\\text{s.t.}\\quad & \\mathbf{x}_{t+1}=\\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t),\\quad t=1,\\dots,T-1,\\\\\n& \\mathbf{g}_t(\\mathbf{x}_t,\\mathbf{u}_t)\\le \\mathbf{0},\\quad \\mathbf{u}_t\\in \\mathcal{U}_t,\\\\\n& \\mathbf{h}(\\mathbf{x}_T)=\\mathbf{0}\\quad\\text{(optional terminal equalities)}.\n\\end{aligned}\n\nIntroduce costates \\boldsymbol{\\lambda}_{t+1}\\in\\mathbb{R}^n for the dynamics, multipliers \\boldsymbol{\\mu}_t\\ge \\mathbf{0} for path inequalities, and \\boldsymbol{\\nu} for terminal equalities. The Lagrangian is\\mathcal{L}\n= c_T(\\mathbf{x}_T) + \\sum_{t=1}^{T-1} c_t(\\mathbf{x}_t,\\mathbf{u}_t)\n+ \\sum_{t=1}^{T-1} \\boldsymbol{\\lambda}_{t+1}^\\top\\!\\big(\\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t)-\\mathbf{x}_{t+1}\\big)\n+ \\sum_{t=1}^{T-1} \\boldsymbol{\\mu}_t^\\top \\mathbf{g}_t(\\mathbf{x}_t,\\mathbf{u}_t)\n+ \\boldsymbol{\\nu}^\\top \\mathbf{h}(\\mathbf{x}_T).\n\nIt is convenient to package the stagewise terms in a HamiltonianH_t(\\mathbf{x}_t,\\mathbf{u}_t,\\boldsymbol{\\lambda}_{t+1},\\boldsymbol{\\mu}_t)\n:= c_t(\\mathbf{x}_t,\\mathbf{u}_t)\n+ \\boldsymbol{\\lambda}_{t+1}^\\top \\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t)\n+ \\boldsymbol{\\mu}_t^\\top \\mathbf{g}_t(\\mathbf{x}_t,\\mathbf{u}_t).\n\nThen\\mathcal{L} = c_T(\\mathbf{x}_T)+\\boldsymbol{\\nu}^\\top \\mathbf{h}(\\mathbf{x}_T)\n+ \\sum_{t=1}^{T-1}\\Big[H_t(\\mathbf{x}_t,\\mathbf{u}_t,\\boldsymbol{\\lambda}_{t+1},\\boldsymbol{\\mu}_t)\n- \\boldsymbol{\\lambda}_{t+1}^\\top \\mathbf{x}_{t+1}\\Big].","type":"content","url":"/ocp#the-discrete-time-pontryagin-principle","position":45},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"Necessary conditions","lvl2":"The Discrete-Time Pontryagin Principle"},"type":"lvl3","url":"/ocp#necessary-conditions","position":46},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"Necessary conditions","lvl2":"The Discrete-Time Pontryagin Principle"},"content":"Gradient convention\n\nThroughout this section, we use the denominator layout (gradient layout) convention:\n\n\\nabla_{\\mathbf{x}} f(\\mathbf{x}) produces a column vector (gradient)\n\n\\frac{\\partial f}{\\partial \\mathbf{x}} produces the Jacobian matrix\n\nFor scalar functions: \\nabla_{\\mathbf{x}} f = \\left(\\frac{\\partial f}{\\partial \\mathbf{x}}\\right)^\\top\n\nThis is the standard convention in optimization and control theory.\n\nTaking first-order variations and collecting terms gives the discrete-time adjoint system, control stationarity, and complementarity. At a local minimum \\{\\mathbf{x}_t^\\star,\\mathbf{u}_t^\\star\\} with multipliers \\{\\boldsymbol{\\lambda}_t^\\star,\\boldsymbol{\\mu}_t^\\star,\\boldsymbol{\\nu}^\\star\\}:\n\nState dynamics (primal feasibility)\\mathbf{x}_{t+1}^\\star=\\mathbf{f}_t(\\mathbf{x}_t^\\star,\\mathbf{u}_t^\\star),\\quad t=1,\\dots,T-1.\n\nCostate recursion (backward “adjoint” equation)\\boldsymbol{\\lambda}_t^\\star\n= \\nabla_{\\mathbf{x}} H_t\\big(\\mathbf{x}_t^\\star,\\mathbf{u}_t^\\star,\\boldsymbol{\\lambda}_{t+1}^\\star,\\boldsymbol{\\mu}_t^\\star\\big)\n= \\nabla_{\\mathbf{x}} c_t(\\mathbf{x}_t^\\star,\\mathbf{u}_t^\\star)\n+ \\big[\\nabla_{\\mathbf{x}} \\mathbf{f}_t(\\mathbf{x}_t^\\star,\\mathbf{u}_t^\\star)\\big]^\\top \\boldsymbol{\\lambda}_{t+1}^\\star\n+ \\big[\\nabla_{\\mathbf{x}} \\mathbf{g}_t(\\mathbf{x}_t^\\star,\\mathbf{u}_t^\\star)\\big]^\\top \\boldsymbol{\\mu}_t^\\star,\n\nwith the terminal condition\\boldsymbol{\\lambda}_T^\\star\n= \\nabla_{\\mathbf{x}} c_T(\\mathbf{x}_T^\\star) + \\big[\\nabla_{\\mathbf{x}} \\mathbf{h}(\\mathbf{x}_T^\\star)\\big]^\\top \\boldsymbol{\\nu}^\\star\n\\quad\\text{(and \\(\\boldsymbol{\\nu}^\\star=\\mathbf{0}\\) if there are no terminal equalities).}\n\nControl stationarity (first-order optimality in \\mathbf{u}_t)\nIf \\mathcal{U}_t=\\mathbb{R}^m (no explicit set constraint), then\\nabla_{\\mathbf{u}} H_t\\big(\\mathbf{x}_t^\\star,\\mathbf{u}_t^\\star,\\boldsymbol{\\lambda}_{t+1}^\\star,\\boldsymbol{\\mu}_t^\\star\\big)=\\mathbf{0}.\n\nIf \\mathcal{U}_t imposes bounds or a convex set, the condition becomes the variational inequality\\mathbf{0}\\in \\nabla_{\\mathbf{u}} H_t(\\cdot)\\;+\\;N_{\\mathcal{U}_t}(\\mathbf{u}_t^\\star),\n\nwhere N_{\\mathcal{U}_t}(\\cdot) is the normal cone to \\mathcal{U}_t. For simple box bounds, this reduces to standard KKT sign and complementarity conditions on the components of \\mathbf{u}_t^\\star.\n\nPath-constraint multipliers (primal/dual feasibility and complementarity)\\mathbf{g}_t(\\mathbf{x}_t^\\star,\\mathbf{u}_t^\\star)\\le \\mathbf{0},\\quad\n\\boldsymbol{\\mu}_t^\\star\\ge \\mathbf{0},\\quad\n\\mu_{t,i}^\\star\\, g_{t,i}(\\mathbf{x}_t^\\star,\\mathbf{u}_t^\\star)=0\\quad \\text{for all }i,t.\n\nTerminal equalities (if present)\\mathbf{h}(\\mathbf{x}_T^\\star)=\\mathbf{0}.\n\nThe triplet “forward state, backward costate, control stationarity” is the discrete-time Euler–Lagrange system tailored to control with dynamics. It is the same KKT logic as before, but organized stagewise through the Hamiltonian.\n\nDiscrete-time Pontryagin necessary conditions (summary)\n\nAt a local minimum of the DOCP\\min_{\\{\\mathbf{x}_t,\\mathbf{u}_t\\}}\\ c_T(\\mathbf{x}_T)+\\sum_{t=1}^{T-1} c_t(\\mathbf{x}_t,\\mathbf{u}_t)\n\\quad\\text{s.t.}\\quad \\mathbf{x}_{t+1}=\\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t),\\ \\mathbf{g}_t(\\mathbf{x}_t,\\mathbf{u}_t)\\le \\mathbf{0},\\ \\mathbf{h}(\\mathbf{x}_T)=\\mathbf{0},\n\nthere exist multipliers \\{\\boldsymbol{\\lambda}_{t+1}\\}, \\{\\boldsymbol{\\mu}_t\\ge\\mathbf{0}\\}, and (if present) \\boldsymbol{\\nu} such that, for t=1,\\dots,T-1:\n\nState dynamics: \\ \\mathbf{x}_{t+1}=\\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t).\n\nBackward costate recursion:\\boldsymbol{\\lambda}_t = \\nabla_{\\mathbf{x}} c_t(\\mathbf{x}_t,\\mathbf{u}_t)\n  + \\big[\\nabla_{\\mathbf{x}} \\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t)\\big]^\\top \\boldsymbol{\\lambda}_{t+1}\n  + \\big[\\nabla_{\\mathbf{x}} \\mathbf{g}_t(\\mathbf{x}_t,\\mathbf{u}_t)\\big]^\\top \\boldsymbol{\\mu}_t.\n\nTerminal condition: \\ \\boldsymbol{\\lambda}_T = \\nabla_{\\mathbf{x}} c_T(\\mathbf{x}_T) + \\big[\\nabla_{\\mathbf{x}} \\mathbf{h}(\\mathbf{x}_T)\\big]^\\top \\boldsymbol{\\nu}.\n\nControl stationarity (unconstrained control): \\ \\nabla_{\\mathbf{u}} H_t(\\cdot)=\\mathbf{0}; with a convex control set \\mathcal{U}_t, \\ \\mathbf{0}\\in \\nabla_{\\mathbf{u}} H_t(\\cdot)+N_{\\mathcal{U}_t}(\\mathbf{u}_t).\n\nPath inequalities: \\ \\mathbf{g}_t(\\mathbf{x}_t,\\mathbf{u}_t)\\le \\mathbf{0}, \\ \\boldsymbol{\\mu}_t\\ge\\mathbf{0}, and complementarity \\ \\mu_{t,i}\\,g_{t,i}(\\mathbf{x}_t,\\mathbf{u}_t)=0 for all i.\n\nTerminal equalities (if present): \\ \\mathbf{h}(\\mathbf{x}_T)=\\mathbf{0}.\n\nHere H_t(\\mathbf{x}_t,\\mathbf{u}_t,\\boldsymbol{\\lambda}_{t+1},\\boldsymbol{\\mu}_t):=c_t(\\mathbf{x}_t,\\mathbf{u}_t)+\\boldsymbol{\\lambda}_{t+1}^\\top\\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t)+\\boldsymbol{\\mu}_t^\\top\\mathbf{g}_t(\\mathbf{x}_t,\\mathbf{u}_t) is the stage Hamiltonian.","type":"content","url":"/ocp#necessary-conditions","position":47},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"The adjoint equation as reverse accumulation","lvl2":"The Discrete-Time Pontryagin Principle"},"type":"lvl3","url":"/ocp#the-adjoint-equation-as-reverse-accumulation","position":48},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"The adjoint equation as reverse accumulation","lvl2":"The Discrete-Time Pontryagin Principle"},"content":"Optimization needs sensitivities. In trajectory problems we adjust decisions—controls or parameters—to reduce an objective while respecting dynamics and constraints. First‑order methods in the unconstrained case (e.g., gradient descent, L‑BFGS, Adam) require the gradient of the objective with respect to all controls, and constrained methods (SQP, interior‑point) require gradients of the Lagrangian, i.e., of costs and constraints. The discrete‑time adjoint equations provide these derivatives in a way that scales to long horizons and many decision variables.\n\nConsiderJ = c_T(\\mathbf{x}_T) + \\sum_{t=1}^{T-1} c_t(\\mathbf{x}_t,\\mathbf{u}_t),\n\\qquad \\mathbf{x}_{t+1}=\\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t).\n\nA single forward rollout computes and stores the trajectory \\mathbf{x}_{1:T}. A single backward sweep then applies the reverse‑mode chain rule stage by stage. Defining the costate by\\boldsymbol{\\lambda}_T = \\nabla_{\\mathbf{x}} c_T(\\mathbf{x}_T),\\qquad\n\\boldsymbol{\\lambda}_t = \\nabla_{\\mathbf{x}} c_t(\\mathbf{x}_t,\\mathbf{u}_t) + \\big[\\nabla_{\\mathbf{x}} \\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t)\\big]^\\top \\boldsymbol{\\lambda}_{t+1},\\quad t=T-1,\\dots,1,\n\nyields exactly the discrete‑time adjoint (PMP) recursion. The gradient with respect to each control follows from the same reverse pass:\\nabla_{\\mathbf{u}_t} J = \\nabla_{\\mathbf{u}} c_t(\\mathbf{x}_t,\\mathbf{u}_t) + \\big[\\nabla_{\\mathbf{u}} \\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t)\\big]^\\top \\boldsymbol{\\lambda}_{t+1}.\n\nTwo points are worth emphasizing. Computationally, this reverse accumulation produces all control gradients with one forward rollout and one backward adjoint pass; its cost is essentially a small constant multiple of simulating the system once. Conceptually, the costate \\boldsymbol{\\lambda}_t is the marginal effect of perturbing the state at time t on the total objective; the control gradient combines a direct contribution from c_t and an indirect contribution through how \\mathbf{u}_t changes the next state. This is the same structure that underlies backpropagation, expressed for dynamical systems.\n\nIt is instructive to contrast this with alternatives. Black‑box finite differences perturb one decision at a time and re‑roll the system, requiring on the order of p rollouts for p decision variables and suffering from step‑size and noise issues—prohibitive when p=(T-1)m for an m‑dimensional control over T steps. Forward‑mode (tangent) sensitivities propagate Jacobian–vector products for each parameter direction; their work also scales with p. Reverse‑mode (the adjoint) instead propagates a single vector \\boldsymbol{\\lambda}_t backward and then reads off all partial derivatives \\nabla_{\\mathbf{u}_t} J at once. For a scalar objective, its cost is effectively independent of p, at the price of storing (or checkpointing) the forward trajectory. This scalability is why the adjoint is the method of choice for gradient‑based trajectory optimization and for constrained transcriptions via the Hamiltonian.","type":"content","url":"/ocp#the-adjoint-equation-as-reverse-accumulation","position":49},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations"},"type":"lvl1","url":"/projdp","position":0},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations"},"content":"The Bellman optimality equation \\mathrm{L}v = v is a functional equation: an equation where the unknown is an entire function rather than a finite-dimensional vector. When the state space is continuous or very large, we cannot represent the value function exactly on a computer. We must instead work with finite-dimensional approximations. This motivates projection methods, a general framework for transforming infinite-dimensional problems into tractable finite-dimensional ones.","type":"content","url":"/projdp","position":1},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl2":"What Does It Mean for a Residual to Be Zero?"},"type":"lvl2","url":"/projdp#what-does-it-mean-for-a-residual-to-be-zero","position":2},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl2":"What Does It Mean for a Residual to Be Zero?"},"content":"Suppose we have found a candidate approximate solution \\hat{v} to the Bellman equation. To verify it satisfies \\mathrm{L}\\hat{v} = \\hat{v}, we compute the residual function R(s) = \\mathrm{L}\\hat{v}(s) - \\hat{v}(s). For a true solution, this residual should be the zero function: R(s) = 0 for every state s. But what does it really mean for a function to equal zero?\n\nIn finite dimensions, a vector \\mathbf{r} \\in \\mathbb{R}^n equals zero if and only if \\langle \\mathbf{r}, \\mathbf{y} \\rangle = 0 for every vector \\mathbf{y} \\in \\mathbb{R}^n. This follows because if \\mathbf{r} \\neq \\mathbf{0}, we can always choose \\mathbf{y} = \\mathbf{r}, giving \\langle \\mathbf{r}, \\mathbf{r} \\rangle = \\|\\mathbf{r}\\|^2 > 0. Conversely, if \\mathbf{r} = \\mathbf{0}, then \\langle \\mathbf{r}, \\mathbf{y} \\rangle = 0 trivially for all \\mathbf{y}.\n\nInner products can distinguish the zero vector from any nonzero vector: for any \\mathbf{r} \\neq \\mathbf{0}, there exists some test vector \\mathbf{y} that “witnesses” the fact that \\mathbf{r} is nonzero by producing \\langle \\mathbf{r}, \\mathbf{y} \\rangle \\neq 0. This property, that we can tell apart (separate) different vectors by testing them with inner products, is what makes inner products so useful for verification.\n\nThe same principle extends to functions. A function R equals the zero function if and only if its “inner product” with every “test function” p vanishes:R = 0 \\quad \\text{if and only if} \\quad \\langle R, p \\rangle = \\int_{\\mathcal{S}} R(s) p(s) w(s) ds = 0 \\quad \\text{for all test functions } p,\n\nwhere w(s) is a weight function (often chosen to emphasize certain regions of the state space). Why does this work? For the same reason as in finite dimensions: if R is not the zero function, there must be some region where R(s) \\neq 0. We can then choose a test function p that is nonzero in that same region (for instance, p(s) = R(s) itself), which will produce \\langle R, p \\rangle = \\int R(s) p(s) w(s) ds > 0, witnessing that R is nonzero. Conversely, if R is the zero function, then \\langle R, p \\rangle = 0 for any test function p.\n\nThis ability to distinguish between different functions using inner products is a fundamental principle from functional analysis. Just as we can test a vector by taking inner products with other vectors, we can test a function by taking inner products with other functions.\n\nConnection to Functional Analysis\n\nThe principle that “a function equals zero if and only if it has zero inner product with all test functions” is a consequence of the Hahn-Banach theorem, one of the cornerstones of functional analysis. The theorem guarantees that for any nonzero function R in a suitable function space, there exists a continuous linear functional (which can be represented as an inner product with some test function p) that produces a nonzero value when applied to R. This is often phrased as “the dual space separates points.”\n\nWhile you don’t need to know the Hahn-Banach theorem to use projection methods, it provides the rigorous mathematical foundation ensuring that our inner product tests are theoretically sound. The constructive argument we gave above (choosing p = R) works in simple cases with well-behaved functions, but the Hahn-Banach theorem extends this guarantee to much more general settings.\n\nWhy is this useful? It transforms the pointwise condition “R(s) = 0 for all s” (infinitely many conditions, one per state) into an equivalent condition about inner products. We still cannot test against all possible test functions, since there are infinitely many of those too. But the inner product perspective suggests a natural computational strategy: choose a finite collection of test functions \\{p_1, \\ldots, p_n\\} and require\\langle R, p_i \\rangle = 0, \\quad i = 1, \\ldots, n.\n\nThis gives us exactly n conditions that we can actually compute. This approach defines what are called weighted residual methods: we make the residual “small” by requiring it to satisfy certain weighted integral conditions.\n\nWithin weighted residual methods, there are two main families:\n\nProjection methods (also called orthogonal projection methods) directly require the residual to have zero inner product with chosen test functions:\\langle R, p_i \\rangle = 0, \\quad i = 1, \\ldots, n.\n\nWe are “projecting” the residual to be orthogonal to the span of the test functions. Different choices of test functions give different projection methods:\n\nGalerkin: Test against the basis functions used to represent \\hat{v}, so p_i = \\varphi_i\n\nCollocation: Test against delta functions p_i = \\delta(s - s_i), which reduces to pointwise evaluation R(s_i) = 0\n\nMethod of moments: Test against polynomials p_i = s^{i-1}, ensuring low-order moments of the residual vanish\n\nSubdomain method: Test against indicator functions p_i = I_{D_i} for subregions D_i, requiring zero average residual in each subdomain","type":"content","url":"/projdp#what-does-it-mean-for-a-residual-to-be-zero","position":3},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl4":"The Special Role of Spectral Methods","lvl2":"What Does It Mean for a Residual to Be Zero?"},"type":"lvl4","url":"/projdp#the-special-role-of-spectral-methods","position":4},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl4":"The Special Role of Spectral Methods","lvl2":"What Does It Mean for a Residual to Be Zero?"},"content":"You may encounter the term spectral methods in the literature. This doesn’t refer to a different choice of test functions, but rather to a choice of basis functions \\varphi_i. Spectral methods use basis functions from families of orthogonal polynomials (like Chebyshev, Legendre, or Hermite polynomials) or trigonometric functions (Fourier series). The “spectral” name comes from the decomposition of the solution into these orthogonal components, analogous to decomposing a signal into frequency components.\n\nWhat makes spectral bases special is their approximation properties: for smooth problems (functions with many continuous derivatives), spectral approximations achieve exponential convergence. As you add more basis functions, the approximation error decreases exponentially rather than polynomially. A function with k continuous derivatives approximated by piecewise polynomials of degree p has error O(h^{p+1}) where h is the grid spacing. But the same function approximated by a spectral method with n terms has error that decreases like O(e^{-cn}) for some constant c > 0. This dramatic difference makes spectral methods extremely efficient for smooth problems.\n\nNow, spectral bases can be combined with any projection method. When we use a spectral basis with Galerkin projection (testing against the basis functions themselves), we get a spectral Galerkin method. The orthogonality of the basis functions often simplifies the resulting linear systems. When we use a spectral basis with collocation, we get what’s often called a pseudospectral method or spectral collocation method.","type":"content","url":"/projdp#the-special-role-of-spectral-methods","position":5},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl4":"Orthogonal Collocation","lvl2":"What Does It Mean for a Residual to Be Zero?"},"type":"lvl4","url":"/projdp#orthogonal-collocation","position":6},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl4":"Orthogonal Collocation","lvl2":"What Does It Mean for a Residual to Be Zero?"},"content":"Orthogonal collocation exploits a useful connection between collocation and quadrature. The idea is to:\n\nChoose basis functions from an orthogonal polynomial family (say, Chebyshev polynomials T_0, T_1, \\ldots, T_{n-1})\n\nChoose collocation points at the zeros of the n-th polynomial in that family\n\nWhy is this clever? Because these same points are also the optimal nodes for Gauss quadrature using the weight function associated with that polynomial family. For example, the zeros of the Chebyshev polynomial T_n(s) are also the Chebyshev-Gauss quadrature nodes. This means:\n\nWe get the computational simplicity of collocation: the projection conditions are just R(s_i) = 0 at the collocation points (no integrals to evaluate)\n\nWhen we do need to compute integrals (say, inside the operator \\mathscr{N} itself), we can use the collocation points as quadrature nodes and the resulting quadrature is exact for polynomials up to degree 2n-1\n\nFor smooth problems, we inherit the exponential convergence of spectral approximations\n\nThis coordination between approximation and integration is why orthogonal collocation is so effective. You’ll sometimes see it called a “pseudospectral method,” though different authors use these terms with slight variations. The key point is that by carefully coordinating our choice of basis, test functions (collocation points), and quadrature nodes, we can achieve excellent accuracy with computational efficiency.\n\nIn summary, “spectral” describes the basis choice (orthogonal polynomials or Fourier), while “Galerkin,” “collocation,” etc. describe the projection choice (which test functions). Orthogonal collocation represents an optimal marriage of these choices for smooth problems.\n\nLeast squares methods take a different approach: instead of requiring orthogonality to specific test functions, we minimize the overall size of the residual measured in a weighted norm:\\min_{\\theta} \\|R(\\cdot; \\theta)\\|^2 = \\min_{\\theta} \\int_{\\mathcal{S}} R(s; \\theta)^2 w(s) ds.\n\nThis seeks the coefficients \\theta that make the residual as small as possible in the least squares sense. The first-order optimality conditions for this minimization problem turn out to be equivalent to a projection method with test functions p_i = \\partial R / \\partial \\theta_i (the derivatives of the residual with respect to the coefficients). So least squares can be viewed as a projection method with data-dependent test functions.\n\nBoth families aim to make the residual “close to zero,” but projection methods do this by requiring orthogonality to chosen directions, while least squares does this by directly minimizing the norm of the residual. The term “projection methods” as used in the approximate dynamic programming literature often refers to both families, since they share the same computational framework of restricting the search to a finite-dimensional subspace and solving for coefficients that satisfy certain residual conditions.\n\nIn summary, we have transformed the impossible task of verifying “R(s) = 0 for all s” into a finite-dimensional problem: find coefficients \\theta = (\\theta_1, \\ldots, \\theta_n) in our approximation \\hat{v}(s) = \\sum_{i=1}^n \\theta_i \\varphi_i(s) such that either:\n\nThe residual is orthogonal to n chosen test functions (projection methods), or\n\nThe residual has minimum norm (least squares methods)\n\nThis is a major conceptual step forward: instead of infinitely many pointwise conditions, we have n conditions. However, these n conditions are not yet fully “feasible” computationally. Each projection condition \\langle R, p_i \\rangle = \\int_{\\mathcal{S}} R(s) p_i(s) w(s) ds = 0 still involves an integral that may need to be approximated numerically.\n\nThe computational cost hierarchy. Different methods have different computational burdens:\n\nCollocation is the cheapest: since \\langle R, \\delta(\\cdot - s_i) \\rangle = R(s_i), we only evaluate the residual pointwise. No integration is needed in the projection conditions themselves.\n\nOrthogonal collocation shares this advantage (projection conditions are just pointwise evaluations), but adds a bonus: if integrals appear elsewhere, say inside the operator \\mathscr{N}, the collocation points double as optimal quadrature nodes. This synergy between approximation and integration is particularly valuable for smooth problems.\n\nGalerkin methods require evaluating integrals \\int R(s) \\varphi_i(s) w(s) ds for each basis function. When using orthogonal polynomial bases (spectral Galerkin), these integrals can sometimes be simplified by orthogonality, but numerical quadrature is still typically needed.\n\nMethod of moments and subdomain methods similarly require numerical quadrature to evaluate weighted integrals of the residual.\n\nLeast squares requires computing \\int R(s)^2 w(s) ds, which involves integrating the squared residual. This is potentially expensive, though the first-order conditions reduce this to a system similar to Galerkin.\n\nThe general pattern: collocation methods avoid integration in the projection step by testing at points rather than against functions, while methods that test against smooth functions (Galerkin, moments, subdomain) must pay the computational cost of numerical integration.\n\nThe rest of this chapter develops this framework systematically, showing how to choose bases, select test functions, evaluate or approximate the necessary integrals, and solve the resulting finite-dimensional problems.","type":"content","url":"/projdp#orthogonal-collocation","position":7},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl2":"The General Framework"},"type":"lvl2","url":"/projdp#the-general-framework","position":8},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl2":"The General Framework"},"content":"Consider an operator equation of the form\\mathscr{N}(f) = 0,\n\nwhere \\mathscr{N}: B_1 \\to B_2 is a continuous operator between complete normed vector spaces B_1 and B_2. For the Bellman equation, we have \\mathscr{N}(v) = \\mathrm{L}v - v, so that solving \\mathscr{N}(v) = 0 is equivalent to finding the fixed point v = \\mathrm{L}v.\n\nThe projection method approach consists of several conceptual steps that transform this infinite-dimensional problem into a finite-dimensional one.","type":"content","url":"/projdp#the-general-framework","position":9},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl3":"Step 1: Choose a Finite-Dimensional Approximation Space","lvl2":"The General Framework"},"type":"lvl3","url":"/projdp#step-1-choose-a-finite-dimensional-approximation-space","position":10},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl3":"Step 1: Choose a Finite-Dimensional Approximation Space","lvl2":"The General Framework"},"content":"We begin by selecting a basis \\Phi = \\{\\varphi_1, \\varphi_2, \\ldots, \\varphi_n\\} and approximating the unknown function as a linear combination:\\hat{f}(x) = \\sum_{i=1}^n \\theta_i \\varphi_i(x).\n\nThe choice of basis functions \\varphi_i is problem-dependent. Common choices include:\n\nPolynomials: For smooth problems, we might use Chebyshev polynomials or other orthogonal polynomial families\n\nSplines: For problems where we expect the solution to have regions of different smoothness\n\nRadial basis functions: For high-dimensional problems where tensor product methods become intractable\n\nThe number of basis functions n determines the flexibility of our approximation. In practice, we start with small n and increase it until the approximation quality is satisfactory. The only unknowns now are the coefficients \\theta = (\\theta_1, \\ldots, \\theta_n).\n\nWhile the classical presentation of projection methods focuses on polynomial bases, the framework applies equally well to other function classes. Neural networks, for instance, can be viewed through this lens: a neural network \\hat{f}(x; \\theta) with parameters \\theta defines a flexible function class, and many training procedures can be interpreted as projection methods with specific choices of test functions or residual norms. The distinction is that classical methods typically use predetermined basis functions with linear coefficients, while neural networks use adaptive nonlinear features. Throughout this chapter, we focus on the classical setting to develop the core concepts, but the principles extend naturally to modern function approximators.","type":"content","url":"/projdp#step-1-choose-a-finite-dimensional-approximation-space","position":11},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl3":"Step 2: Define the Residual Function","lvl2":"The General Framework"},"type":"lvl3","url":"/projdp#step-2-define-the-residual-function","position":12},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl3":"Step 2: Define the Residual Function","lvl2":"The General Framework"},"content":"Since we are approximating f with \\hat{f}, the operator \\mathscr{N} will generally not vanish exactly. Instead, we obtain a residual function:R(x; \\theta) = \\mathscr{N}(\\hat{f}(\\cdot; \\theta))(x).\n\nThis residual measures how far our candidate solution is from satisfying the equation at each point x. As we discussed in the introduction, we will assess whether this residual is “close to zero” by testing its inner products against chosen test functions.","type":"content","url":"/projdp#step-2-define-the-residual-function","position":13},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl3":"Step 3: Choose Weighted Residual Conditions","lvl2":"The General Framework"},"type":"lvl3","url":"/projdp#step-3-choose-weighted-residual-conditions","position":14},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl3":"Step 3: Choose Weighted Residual Conditions","lvl2":"The General Framework"},"content":"Having chosen our basis and defined the residual, we must decide how to make the residual “close to zero.” As discussed in the introduction, we can either:\n\nUse projection conditions: Select n test functions \\{p_1, \\ldots, p_n\\} and require:\\langle R(\\cdot; \\theta), p_i \\rangle = \\int_{\\mathcal{S}} R(x; \\theta) p_i(x) w(x) dx = 0, \\quad i = 1, \\ldots, n,\n\nfor some weight function w(x). This yields n equations to determine the n coefficients in \\theta.\n\nUse a least squares condition: Minimize the norm of the residual directly:\\min_\\theta \\int_{\\mathcal{S}} R(x; \\theta)^2 w(x) dx.\n\nWe begin by examining the main projection methods, distinguished entirely by their choice of test functions p_i, then discuss least squares as an alternative approach.\n\nLet us examine the standard choices of test functions and what they tell us about the residual:","type":"content","url":"/projdp#step-3-choose-weighted-residual-conditions","position":15},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl4":"Galerkin Method: Test Against the Basis","lvl3":"Step 3: Choose Weighted Residual Conditions","lvl2":"The General Framework"},"type":"lvl4","url":"/projdp#galerkin-method-test-against-the-basis","position":16},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl4":"Galerkin Method: Test Against the Basis","lvl3":"Step 3: Choose Weighted Residual Conditions","lvl2":"The General Framework"},"content":"The Galerkin method chooses test functions p_i = \\varphi_i, the same basis functions used to approximate \\hat{f}:\\langle R(\\cdot; \\theta), \\varphi_i \\rangle = 0, \\quad i = 1, \\ldots, n.\n\nTo understand what this means, recall that in finite dimensions, two vectors are orthogonal when their inner product is zero. For functions, \\langle R, \\varphi_i \\rangle = \\int R(x) \\varphi_i(x) w(x) dx = 0 expresses the same concept: R and \\varphi_i are orthogonal as functions. But there’s more to this than just testing against individual basis functions.\n\nConsider our approximation space \\text{span}\\{\\varphi_1, \\ldots, \\varphi_n\\} as an n-dimensional subspace within the infinite-dimensional space of all functions. Any function g in this space can be written as g = \\sum_{i=1}^n c_i \\varphi_i for some coefficients c_i. If the residual R is orthogonal to all basis functions \\varphi_i, then by linearity of the inner product, for any such function g:\\langle R, g \\rangle = \\left\\langle R, \\sum_{i=1}^n c_i \\varphi_i \\right\\rangle = \\sum_{i=1}^n c_i \\langle R, \\varphi_i \\rangle = 0.\n\nThis shows that R is orthogonal to every function we can represent with our basis. The residual has “zero overlap” with our approximation space: we cannot express any part of it using our basis functions. In this sense, the residual is as “invisible” to our approximation as possible.\n\nThis condition is the defining property of optimality. By choosing our approximation \\hat{f} so that the residual R = \\mathscr{N}(\\hat{f}) is orthogonal to the entire approximation space, we ensure that \\hat{f} is the orthogonal projection of the true solution onto \\text{span}{\\varphi_1, \\ldots, \\varphi_n}. Within this n-dimensional space, no better choice is possible: any other coefficients would yield a residual with a nonzero component inside the space, and therefore a larger norm.\n\nThe finite-dimensional analogy makes this concrete. Suppose you want to approximate a vector \\mathbf{v} \\in \\mathbb{R}^3 using only the xy-plane (a 2D subspace). The best approximation is to project \\mathbf{v} onto the plane, giving \\hat{\\mathbf{v}} = (v_1, v_2, 0). The error is \\mathbf{r} = \\mathbf{v} - \\hat{\\mathbf{v}} = (0, 0, v_3), which points purely in the z-direction, orthogonal to the entire xy-plane. We see the Galerkin condition in action: the error is orthogonal to the approximation space.","type":"content","url":"/projdp#galerkin-method-test-against-the-basis","position":17},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl4":"Method of Moments: Test Against Monomials","lvl3":"Step 3: Choose Weighted Residual Conditions","lvl2":"The General Framework"},"type":"lvl4","url":"/projdp#method-of-moments-test-against-monomials","position":18},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl4":"Method of Moments: Test Against Monomials","lvl3":"Step 3: Choose Weighted Residual Conditions","lvl2":"The General Framework"},"content":"The method of moments, for problems on D \\subset \\mathbb{R}, chooses test functions p_i(x) = x^{i-1} for i = 1, \\ldots, n:\\langle R(\\cdot; \\theta), x^{i-1} \\rangle = 0, \\quad i = 1, \\ldots, n.\n\nThis requires the first n moments of the residual function to vanish, ensuring the residual is “balanced” in the sense that it has no systematic trend captured by low-order polynomials. The moments \\int x^k R(x; \\theta) w(x) dx measure weighted averages of the residual, with increasing powers of x giving more weight to larger values. Setting these to zero ensures the residual doesn’t grow systematically with x. This approach is particularly useful when w(x) is chosen as a probability measure, making the conditions natural moment restrictions familiar from statistics and econometrics.","type":"content","url":"/projdp#method-of-moments-test-against-monomials","position":19},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl4":"Collocation Method: Test Against Delta Functions","lvl3":"Step 3: Choose Weighted Residual Conditions","lvl2":"The General Framework"},"type":"lvl4","url":"/projdp#collocation-method-test-against-delta-functions","position":20},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl4":"Collocation Method: Test Against Delta Functions","lvl3":"Step 3: Choose Weighted Residual Conditions","lvl2":"The General Framework"},"content":"The collocation method chooses test functions p_i(x) = \\delta(x - x_i), the Dirac delta functions at points \\{x_1, \\ldots, x_n\\}:\\langle R(\\cdot; \\theta), \\delta(\\cdot - x_i) \\rangle = R(x_i; \\theta) = 0, \\quad i = 1, \\ldots, n.\n\nThis is projection against the most localized test functions possible: delta functions that “sample” the residual at specific points, requiring the residual to vanish exactly where we test it. When using orthogonal polynomials with collocation points at the zeros of the n-th polynomial, the Chebyshev interpolation theorem guarantees that forcing R(x_i; \\theta) = 0 at these specific points makes R(x; \\theta) small everywhere. Using the zeros of orthogonal polynomials as collocation points produces well-conditioned systems and near-optimal interpolation error. The computational advantage is significant. Collocation avoids numerical integration entirely, requiring only pointwise evaluation of R.","type":"content","url":"/projdp#collocation-method-test-against-delta-functions","position":21},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl4":"Subdomain Method: Test Against Indicator Functions","lvl3":"Step 3: Choose Weighted Residual Conditions","lvl2":"The General Framework"},"type":"lvl4","url":"/projdp#subdomain-method-test-against-indicator-functions","position":22},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl4":"Subdomain Method: Test Against Indicator Functions","lvl3":"Step 3: Choose Weighted Residual Conditions","lvl2":"The General Framework"},"content":"The subdomain method partitions the domain into n subregions \\{D_1, \\ldots, D_n\\} and chooses test functions p_i = I_{D_i}, the indicator functions:\\langle R(\\cdot; \\theta), I_{D_i} \\rangle = \\int_{D_i} R(x; \\theta) w(x) dx = 0, \\quad i = 1, \\ldots, n.\n\nThis requires the residual to have zero average over each subdomain, ensuring the approximation is good “on average” over each piece of the domain. This approach is particularly natural for finite element methods where the domain is divided into elements, ensuring local balance of the residual within each element.","type":"content","url":"/projdp#subdomain-method-test-against-indicator-functions","position":23},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl4":"Least Squares","lvl3":"Step 3: Choose Weighted Residual Conditions","lvl2":"The General Framework"},"type":"lvl4","url":"/projdp#least-squares","position":24},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl4":"Least Squares","lvl3":"Step 3: Choose Weighted Residual Conditions","lvl2":"The General Framework"},"content":"The least squares approach appears different at first glance, but it also fits the test function framework. We minimize:\\min_\\theta \\int_{\\mathcal{S}} R(x; \\theta)^2 w(x) dx = \\min_\\theta \\langle R(\\cdot; \\theta), R(\\cdot; \\theta) \\rangle.\n\nThe first-order conditions for this minimization problem are:\\left\\langle R(\\cdot; \\theta), \\frac{\\partial R(\\cdot; \\theta)}{\\partial \\theta_i} \\right\\rangle = 0, \\quad i = 1, \\ldots, n.\n\nThus least squares implicitly uses test functions p_i = \\partial R / \\partial \\theta_i, the gradients of the residual with respect to parameters. Unlike other methods where test functions are chosen a priori, here they depend on the current guess for \\theta and on the structure of our approximation.\n\nWe can now see the unifying structure of weighted residual methods: whether we use projection conditions or least squares minimization, all these methods follow the same template of restricting the search to an n-dimensional function space and imposing n conditions on the residual. For projection methods specifically, we pick n test functions and require \\langle R, p_i \\rangle = 0. They differ only in their philosophy about which test functions best detect whether the residual is “nearly zero.” Galerkin tests against the approximation basis itself (natural for orthogonal bases), the method of moments tests against monomials (ensuring polynomial balance), collocation tests against delta functions (pointwise satisfaction), subdomain tests against indicators (local average satisfaction), and least squares tests against residual gradients (global norm minimization). Each choice reflects different priorities: computational efficiency, theoretical optimality, ease of implementation, or sensitivity to errors in different regions of the domain.","type":"content","url":"/projdp#least-squares","position":25},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl3":"Step 4: Solve the Finite-Dimensional Problem","lvl2":"The General Framework"},"type":"lvl3","url":"/projdp#step-4-solve-the-finite-dimensional-problem","position":26},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl3":"Step 4: Solve the Finite-Dimensional Problem","lvl2":"The General Framework"},"content":"The projection conditions give us a system to solve for the coefficients \\theta. For test function methods (Galerkin, collocation, moments, subdomain), we solve:P_i(\\theta) \\equiv \\langle R(\\cdot; \\theta), p_i \\rangle = 0, \\quad i = 1, \\ldots, n.\n\nThis is a system of n (generally nonlinear) equations in n unknowns. For least squares, we solve the optimization problem \\min_\\theta \\langle R(\\cdot; \\theta), R(\\cdot; \\theta) \\rangle.\n\nThe conditioning of the system depends on the choice of test functions. The Jacobian matrix has entries:J_{ij} = \\frac{\\partial P_i}{\\partial \\theta_j} = \\left\\langle \\frac{\\partial R(\\cdot; \\theta)}{\\partial \\theta_j}, p_i \\right\\rangle.\n\nWhen test functions are orthogonal (or nearly so), the Jacobian tends to be well-conditioned. This is why orthogonal polynomial bases are preferred in Galerkin methods: they produce Jacobians with controlled condition numbers.\n\nThe computational cost per iteration varies significantly:\n\nCollocation: Cheapest to evaluate since P_i(\\theta) = R(x_i; \\theta) requires only pointwise evaluation (no integration). The Jacobian is also cheap: J_{ij} = \\frac{\\partial R(x_i; \\theta)}{\\partial \\theta_j}.\n\nGalerkin and moments: More expensive due to integration. Computing P_i(\\theta) = \\int R(x; \\theta) p_i(x) w(x) dx requires numerical quadrature. Each Jacobian entry requires integrating \\frac{\\partial R}{\\partial \\theta_j} p_i.\n\nLeast squares: Most expensive when done via the objective function, which requires integrating R^2. However, the first-order conditions reduce it to a system like Galerkin, with test functions p_i = \\partial R / \\partial \\theta_i.\n\nFor methods requiring integration, the choice of quadrature rule should match the basis. Gaussian quadrature with nodes at orthogonal polynomial zeros is efficient. When combined with collocation at those same points, the quadrature is exact for polynomials up to a certain degree. This coordination between quadrature and collocation makes orthogonal collocation effective.\n\nThe choice of solver depends on whether the finite-dimensional approximation preserves the structural properties of the original infinite-dimensional problem. This matters for the Bellman equation, where the original operator \\mathrm{L} is a contraction.\n\nSuccessive approximation (fixed-point iteration) is the natural choice when the original operator is a contraction, as it preserves the global convergence guarantees. However, the finite-dimensional approximation \\hat{\\mathrm{L}} may not inherit the contraction property of \\mathrm{L}. The approximation can introduce spurious fixed points or destroy the contraction constant, leading to divergence or slow convergence. This is especially problematic when using high-order polynomial approximations, which can create artificial oscillations that destabilize the iteration.\n\nNewton’s method is often the default choice for projection methods because it doesn’t rely on the contraction property. Instead, it exploits the smoothness of the residual function. When the original problem is smooth and the approximation preserves this smoothness, Newton’s method provides quadratic convergence near the solution. However, Newton’s method requires good initial guesses and may converge to spurious solutions if the finite-dimensional problem has multiple fixed points that the original problem lacks.\n\nThe choice of basis and projection method affects which algorithm is most appropriate. For example:\n\nLinear interpolation often preserves contraction properties, making successive approximation reliable\n\nHigh-order polynomials may destroy contraction but provide smooth approximations suitable for Newton’s method\n\nShape-preserving splines can maintain both smoothness and structural properties\n\nIn practice, which algorithm should we use? When the operator equation can be written as a fixed-point problem f = \\mathscr{T}f and the operator \\mathscr{T} is known to be a contraction, successive approximation is often the best starting point: it is computationally cheap and globally convergent. However, not all equations \\mathscr{N}(f) = 0 admit a natural fixed-point reformulation, and even when they do (e.g., f = f - \\alpha \\mathscr{N}(f) for some \\alpha > 0), the resulting operator may not be a contraction in the finite-dimensional approximation space. In such cases, Newton’s method becomes the primary option despite its requirement for good initial guesses and higher computational cost per iteration. A hybrid approach often works well: use successive approximation when applicable to generate an initial guess, then switch to Newton’s method for refinement.\n\nAnother consideration is the conditioning of the resulting system. Poorly chosen basis functions or collocation points can lead to nearly singular Jacobians, causing numerical instability. Orthogonal bases and carefully chosen collocation points (like Chebyshev nodes) are preferred because they tend to produce well-conditioned systems.","type":"content","url":"/projdp#step-4-solve-the-finite-dimensional-problem","position":27},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl3":"Step 5: Verify the Solution","lvl2":"The General Framework"},"type":"lvl3","url":"/projdp#step-5-verify-the-solution","position":28},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl3":"Step 5: Verify the Solution","lvl2":"The General Framework"},"content":"Once we have computed a candidate solution \\hat{f}, we must verify its quality. Projection methods optimize \\hat{f} with respect to specific criteria (specific test functions or collocation points), but we should check that the residual is small everywhere, including directions or points we did not optimize over.\n\nTypical diagnostic checks include:\n\nComputing \\|R(\\cdot; \\theta)\\| using a more accurate quadrature rule than was used in the optimization\n\nEvaluating R(x; \\theta) at many points not used in the fitting process\n\nIf using Galerkin with the first n basis functions, checking orthogonality against higher-order basis functions","type":"content","url":"/projdp#step-5-verify-the-solution","position":29},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl2":"Application to the Bellman Equation"},"type":"lvl2","url":"/projdp#application-to-the-bellman-equation","position":30},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl2":"Application to the Bellman Equation"},"content":"We now apply the projection method framework to the Bellman optimality equation. Recall that we seek a function v satisfyingv(s) = \\mathrm{L}v(s) = \\max_{a \\in \\mathcal{A}_s} \\left\\{ r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j) \\right\\}.\n\nWriting this as an operator equation \\mathscr{N}(v) = 0 with \\mathscr{N}(v) = \\mathrm{L}v - v, the residual function for a candidate approximation \\hat{v}(s) = \\sum_{i=1}^n \\theta_i \\varphi_i(s) is:R(s; \\theta) = \\mathrm{L}\\hat{v}(s) - \\hat{v}(s) = \\max_{a \\in \\mathcal{A}_s} \\left\\{ r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) \\hat{v}(j) \\right\\} - \\sum_{i=1}^n \\theta_i \\varphi_i(s).\n\nAny of the projection methods we discussed (Galerkin, method of moments, collocation, subdomain, or least squares) can be applied here. Each would give us n conditions to determine the n coefficients in our approximation. For instance:\n\nGalerkin would require \\langle R(\\cdot; \\theta), \\varphi_i \\rangle = 0 for i = 1, \\ldots, n, involving integration of the residual weighted by basis functions\n\nMethod of moments would require \\langle R(\\cdot; \\theta), s^{i-1} \\rangle = 0, setting the first n moments of the residual to zero\n\nCollocation would require R(s_i; \\theta) = 0 at n chosen states, forcing the residual to vanish pointwise\n\nIn practice, collocation is the most commonly used projection method for the Bellman equation. The reason is computational: collocation avoids the numerical integration required by Galerkin and method of moments. Since the Bellman operator already involves integration (or summation) over next states, adding another layer of integration for the projection conditions would be computationally expensive. Collocation sidesteps this by requiring the equation to hold exactly at specific points.\n\nWe focus on collocation in detail, though the principles extend to other projection methods.","type":"content","url":"/projdp#application-to-the-bellman-equation","position":31},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl3":"Collocation for the Bellman Equation","lvl2":"Application to the Bellman Equation"},"type":"lvl3","url":"/projdp#collocation-for-the-bellman-equation","position":32},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl3":"Collocation for the Bellman Equation","lvl2":"Application to the Bellman Equation"},"content":"The collocation approach chooses n states \\{s_1, \\ldots, s_n\\} (the collocation points) and requires:R(s_i; \\theta) = 0, \\quad i = 1, \\ldots, n.\n\nThis gives us a system of n nonlinear equations in n unknowns:\\sum_{j=1}^n \\theta_j \\varphi_j(s_i) = \\max_{a \\in \\mathcal{A}_{s_i}} \\left\\{ r(s_i,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s_i,a) \\hat{v}(j) \\right\\}, \\quad i = 1, \\ldots, n.\n\nThe right-hand side requires evaluating the Bellman operator at the collocation points. For each collocation point s_i, we must:\n\nFor each action a \\in \\mathcal{A}_{s_i}, compute the expected continuation value \\sum_{j \\in \\mathcal{S}} p(j|s_i,a) \\hat{v}(j)\n\nTake the maximum over actions\n\nWhen the state space is continuous, the expectation involves integration, which typically requires numerical quadrature. When the state space is discrete but large, this is a straightforward (though potentially expensive) summation.","type":"content","url":"/projdp#collocation-for-the-bellman-equation","position":33},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl4":"Solving the Collocation System","lvl3":"Collocation for the Bellman Equation","lvl2":"Application to the Bellman Equation"},"type":"lvl4","url":"/projdp#solving-the-collocation-system","position":34},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl4":"Solving the Collocation System","lvl3":"Collocation for the Bellman Equation","lvl2":"Application to the Bellman Equation"},"content":"To organize our thinking about solution methods, it helps to introduce the collocation target operator \\mathcal{T}: \\mathbb{R}^n \\to \\mathbb{R}^n, which maps coefficient vectors to target values at the collocation points. Given a coefficient vector \\theta = (\\theta_1, \\ldots, \\theta_n), the i-th component is defined as:[\\mathcal{T}(\\theta)]_i = \\max_{a \\in \\mathcal{A}_{s_i}} \\left\\{ r(s_i, a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s_i, a) \\sum_{\\ell=1}^n \\theta_\\ell \\varphi_\\ell(j) \\right\\}.\n\nIn words: [\\mathcal{T}(\\theta)]_i is the value obtained by applying the Bellman operator at collocation point s_i, using the approximation \\hat{v}(s; \\theta) = \\sum_{\\ell=1}^n \\theta_\\ell \\varphi_\\ell(s) in place of the true value function. This is equivalently written as [\\mathcal{T}(\\theta)]_i = [\\mathrm{L}\\hat{v}(\\cdot; \\theta)](s_i), emphasizing that we evaluate the Bellman operator \\mathrm{L} applied to the parametric approximation at the collocation points. The operator \\mathcal{T} evaluates the right-hand side of the Bellman equation at all collocation points, given a current guess for the coefficients.\n\nLet \\boldsymbol{\\Phi} denote the n \\times n collocation matrix with entries \\Phi_{ij} = \\varphi_j(s_i). The left-hand side of the collocation equations is \\boldsymbol{\\Phi} \\theta, which gives the values of \\hat{v}(s_i; \\theta) at the collocation points. The collocation equation requires these to match:\\boldsymbol{\\Phi} \\theta = \\mathcal{T}(\\theta).\n\nThis is the fixed-point problem we need to solve. We can approach it in two fundamentally different ways: as a fixed-point iteration (function iteration) or as a rootfinding problem (Newton’s method).","type":"content","url":"/projdp#solving-the-collocation-system","position":35},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl3":"Method 1: Function Iteration (Successive Approximation)","lvl2":"Application to the Bellman Equation"},"type":"lvl3","url":"/projdp#method-1-function-iteration-successive-approximation","position":36},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl3":"Method 1: Function Iteration (Successive Approximation)","lvl2":"Application to the Bellman Equation"},"content":"We can rewrite the collocation equation as \\theta = \\boldsymbol{\\Phi}^{-1} \\mathcal{T}(\\theta) (assuming \\boldsymbol{\\Phi} is invertible, which holds when the basis functions are linearly independent at the collocation points). This suggests the function iteration scheme:\\theta^{(k+1)} = \\boldsymbol{\\Phi}^{-1} \\mathcal{T}(\\theta^{(k)}).\n\nThis iteration has an intuitive interpretation when we break it into two steps:\n\nStep 1 (Apply Bellman operator): For the current coefficient guess \\theta^{(k)}, compute the Bellman operator values at all collocation points:t_i^{(k)} = [\\mathcal{T}(\\theta^{(k)})]_i = \\max_{a \\in \\mathcal{A}_{s_i}} \\left\\{ r(s_i, a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s_i, a) \\sum_{\\ell=1}^n \\theta^{(k)}_\\ell \\varphi_\\ell(j) \\right\\}, \\quad i = 1, \\ldots, n.\n\nWe now have a vector of targets t^{(k)} = (t_1^{(k)}, \\ldots, t_n^{(k)}) = \\mathcal{T}(\\theta^{(k)}).\n\nStep 2 (Fit to targets): Find new coefficients \\theta^{(k+1)} such that the approximation matches the targets at the collocation points:\\sum_{\\ell=1}^n \\theta^{(k+1)}_\\ell \\varphi_\\ell(s_i) = t_i^{(k)}, \\quad i = 1, \\ldots, n.\n\nIn matrix form: \\boldsymbol{\\Phi} \\theta^{(k+1)} = t^{(k)}, which gives \\theta^{(k+1)} = \\boldsymbol{\\Phi}^{-1} t^{(k)} = \\boldsymbol{\\Phi}^{-1} \\mathcal{T}(\\theta^{(k)}).\n\nThis is parametric value iteration or projection-based value iteration: we iterate the Bellman operator in the finite-dimensional coefficient space, projecting back onto the span of the basis functions at each step. The method:\n\nSeparates the nonlinear optimization (maximization in the Bellman operator) from the linear fitting problem\n\nIs globally convergent when the finite-dimensional approximation preserves the contraction property\n\nRequires only solving a linear system \\boldsymbol{\\Phi} \\theta^{(k+1)} = t^{(k)} at each iteration\n\nHandling Stochastic Expectations\n\nWhen the model includes a continuous random variable (e.g., s' = g(s, a, \\epsilon) where \\epsilon is a random disturbance, often called a “shock” in economics and econometrics), we must approximate the expectation using numerical quadrature. We replace the continuous \\epsilon with a discrete approximation taking values \\{\\epsilon_1, \\ldots, \\epsilon_m\\} with probabilities \\{w_1, \\ldots, w_m\\}. The collocation target operator becomes:[\\mathcal{T}(\\theta)]_i = \\max_{a \\in \\mathcal{A}_{s_i}} \\left\\{ r(s_i, a) + \\gamma \\sum_{k=1}^m w_k \\sum_{\\ell=1}^n \\theta_\\ell \\varphi_\\ell(g(s_i, a, \\epsilon_k)) \\right\\}.\n\nCommon quadrature schemes include Gauss-Hermite (for normal random variables), Gauss-Legendre (for uniform random variables), or sparse grids for high-dimensional random variables.","type":"content","url":"/projdp#method-1-function-iteration-successive-approximation","position":37},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl3":"Method 2: Newton’s Method with the Envelope Theorem","lvl2":"Application to the Bellman Equation"},"type":"lvl3","url":"/projdp#method-2-newtons-method-with-the-envelope-theorem","position":38},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl3":"Method 2: Newton’s Method with the Envelope Theorem","lvl2":"Application to the Bellman Equation"},"content":"Alternatively, we can write the collocation equation as a rootfinding problem:F(\\theta) \\equiv \\boldsymbol{\\Phi} \\theta - \\mathcal{T}(\\theta) = 0.\n\nNewton’s method for this system uses the update:\\theta^{(k+1)} = \\theta^{(k)} - J_F(\\theta^{(k)})^{-1} F(\\theta^{(k)}),\n\nwhere J_F(\\theta) is the Jacobian of F at \\theta. Since J_F = \\boldsymbol{\\Phi} - J_{\\mathcal{T}} where J_{\\mathcal{T}} is the Jacobian of the collocation target operator \\mathcal{T}, we can rewrite this as:\\theta^{(k+1)} = \\theta^{(k)} - [\\boldsymbol{\\Phi} - J_{\\mathcal{T}}(\\theta^{(k)})]^{-1} [\\boldsymbol{\\Phi} \\theta^{(k)} - \\mathcal{T}(\\theta^{(k)})].\n\nThe main challenge now is computing the Jacobian J_{\\mathcal{T}}(\\theta). The (i,j)-th entry is:[J_{\\mathcal{T}}]_{ij} = \\frac{\\partial [\\mathcal{T}(\\theta)]_i}{\\partial \\theta_j} = \\frac{\\partial}{\\partial \\theta_j} \\left[ \\max_{a \\in \\mathcal{A}_{s_i}} \\left\\{ r(s_i, a) + \\gamma \\sum_{k \\in \\mathcal{S}} p(k|s_i, a) \\sum_{\\ell=1}^n \\theta_\\ell \\varphi_\\ell(k) \\right\\} \\right].\n\nAt first glance, this appears problematic because the max operator is not differentiable. However, we can apply the Envelope Theorem to compute this derivative without dealing with the non-differentiability of the max operator.\n\nThe Envelope Theorem\n\nSetup: Consider a smooth objective function f(\\mathbf{x}, \\boldsymbol{\\theta}) and define the optimal value:v(\\boldsymbol{\\theta}) = \\max_{\\mathbf{x}} f(\\mathbf{x}, \\boldsymbol{\\theta}).\n\nLet \\mathbf{x}(\\boldsymbol{\\theta}) denote the maximizer, satisfying the first-order condition:\\nabla_{\\mathbf{x}} f(\\mathbf{x}(\\boldsymbol{\\theta}), \\boldsymbol{\\theta}) = \\mathbf{0}.\n\nThe Result: To find how the optimal value changes with \\boldsymbol{\\theta}, write v(\\boldsymbol{\\theta}) = f(\\mathbf{x}(\\boldsymbol{\\theta}), \\boldsymbol{\\theta}) and apply the chain rule:\\nabla_{\\boldsymbol{\\theta}} v(\\boldsymbol{\\theta}) = \\underbrace{\\nabla_{\\boldsymbol{\\theta}} f(\\mathbf{x}(\\boldsymbol{\\theta}), \\boldsymbol{\\theta})}_{\\text{direct effect}} + \\underbrace{\\nabla_{\\mathbf{x}} f(\\mathbf{x}(\\boldsymbol{\\theta}), \\boldsymbol{\\theta})^{\\top}}_{\\mathbf{0} \\text{ at optimum}} \\frac{\\partial \\mathbf{x}}{\\partial \\boldsymbol{\\theta}}.\n\nSince \\nabla_{\\mathbf{x}} f = \\mathbf{0} at the optimum, the second term vanishes:\\boxed{\\nabla_{\\boldsymbol{\\theta}} v(\\boldsymbol{\\theta}) = \\nabla_{\\boldsymbol{\\theta}} f(\\mathbf{x}(\\boldsymbol{\\theta}), \\boldsymbol{\\theta})}.\n\nThis results tells us that you can compute the derivative of the optimal value by treating the maximizer as constant. You don’t need to compute \\frac{\\partial \\mathbf{x}}{\\partial \\boldsymbol{\\theta}}.\n\nIn our Bellman collocation problem, [\\mathcal{T}(\\theta)]_i = \\max_a \\{r(s_i, a) + \\gamma \\mathbb{E}[\\sum_\\ell \\theta_\\ell \\varphi_\\ell(s')]\\}. To compute \\frac{\\partial [\\mathcal{T}(\\theta)]_i}{\\partial \\theta_j}, we don’t need to figure out how a_i^*(\\theta) changes with \\theta. We just evaluate the gradient at the optimal action:\\frac{\\partial [\\mathcal{T}(\\theta)]_i}{\\partial \\theta_j} = \\gamma \\sum_{s'} p(s'|s_i, a_i^*(\\theta)) \\varphi_j(s').\n\nImportant assumptions: f is smooth, the maximizer is unique and in the interior (or constraints are smooth with stable active sets), and the first-order condition holds.\n\nSpecifically, let a_i^*(\\theta) denote the optimal action at collocation point s_i given coefficients \\theta:a_i^*(\\theta) \\in \\operatorname{argmax}_{a \\in \\mathcal{A}_{s_i}} \\left\\{ r(s_i, a) + \\gamma \\sum_{k \\in \\mathcal{S}} p(k|s_i, a) \\sum_{\\ell=1}^n \\theta_\\ell \\varphi_\\ell(k) \\right\\}.\n\nBy the Envelope Theorem, we can compute the derivative by treating a_i^* as constant:\\frac{\\partial [\\mathcal{B}(\\theta)]_i}{\\partial \\theta_j} = \\gamma \\sum_{k \\in \\mathcal{S}} p(k|s_i, a_i^*(\\theta)) \\varphi_j(k).\n\nThis is simply the expected value of the j-th basis function at the next state, evaluated at the optimal action for the current coefficient vector.\n\nWhy the Envelope Theorem Works Here\n\nThe Envelope Theorem applies to value functions of optimization problems. For a problem V(\\theta) = \\max_x f(x, \\theta), the derivative with respect to the parameter \\theta is:\\frac{dV}{d\\theta} = \\frac{\\partial f}{\\partial \\theta}(x^*(\\theta), \\theta),\n\nwhere x^*(\\theta) is the optimal choice. We don’t need to account for \\frac{\\partial x^*}{\\partial \\theta} because at the optimum, the first-order condition \\frac{\\partial f}{\\partial x} = 0 makes that term vanish.\n\nIn our case, [\\mathcal{B}(\\theta)]_i is the value of maximizing over actions a, with \\theta playing the role of the parameter vector. The Envelope Theorem lets us compute \\frac{\\partial [\\mathcal{B}(\\theta)]_i}{\\partial \\theta_j} by differentiating the objective (the Bellman right-hand side) with respect to \\theta_j while holding the optimal action a_i^* fixed. Since \\theta_j only appears in the continuation value \\sum_{\\ell} \\theta_\\ell \\varphi_\\ell(k), the derivative picks out the coefficient of \\theta_j, which is the expected basis function value.\n\nThis approach is equivalent to the semi-smooth Newton method mentioned earlier: we’re computing a generalized Jacobian by treating the optimal action as locally constant. As we converge to the solution, the optimal actions stabilize, and Newton’s method achieves superlinear convergence.\n\nNewton’s method algorithm:\n\nStart with initial guess \\theta^{(0)}\n\nFor iteration k = 0, 1, 2, \\ldots:\n\nCompute \\mathcal{B}(\\theta^{(k)}) and optimal actions a_i^*(\\theta^{(k)}) for all collocation points\n\nCompute Jacobian entries: [J_{\\mathcal{B}}]_{ij} = \\gamma \\sum_{s' \\in \\mathcal{S}} p(s'|s_i, a_i^*(\\theta^{(k)})) \\varphi_j(s')\n\nUpdate: \\theta^{(k+1)} = \\theta^{(k)} - [\\boldsymbol{\\Phi} - J_{\\mathcal{B}}(\\theta^{(k)})]^{-1} [\\boldsymbol{\\Phi} \\theta^{(k)} - \\mathcal{B}(\\theta^{(k)})]\n\nCheck convergence\n\nThis method offers quadratic convergence near the solution but requires a good initial guess. The Jacobian computation via the Envelope Theorem is typically cheaper than explicit semi-smooth calculus and has a clear economic interpretation: it tracks how the value propagates through the optimal decisions.","type":"content","url":"/projdp#method-2-newtons-method-with-the-envelope-theorem","position":39},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl4":"Comparison of Solution Methods","lvl3":"Method 2: Newton’s Method with the Envelope Theorem","lvl2":"Application to the Bellman Equation"},"type":"lvl4","url":"/projdp#comparison-of-solution-methods","position":40},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl4":"Comparison of Solution Methods","lvl3":"Method 2: Newton’s Method with the Envelope Theorem","lvl2":"Application to the Bellman Equation"},"content":"We now have two approaches to solving the collocation fixed-point equation \\boldsymbol{\\Phi} \\theta = \\mathcal{B}(\\theta):\n\nMethod\n\nFormulation\n\nConvergence\n\nPer-iteration cost\n\nInitial guess sensitivity\n\nFunction iteration\n\n\\theta^{(k+1)} = \\boldsymbol{\\Phi}^{-1} \\mathcal{B}(\\theta^{(k)})\n\nLinear (when contraction holds)\n\nLow (one linear solve)\n\nRobust\n\nNewton’s method\n\n\\theta^{(k+1)} = \\theta^{(k)} - [\\boldsymbol{\\Phi} - J_{\\mathcal{B}}]^{-1}[\\boldsymbol{\\Phi} \\theta^{(k)} - \\mathcal{B}(\\theta^{(k)})]\n\nQuadratic (near solution)\n\nModerate (Jacobian + linear solve)\n\nRequires good initial guess\n\nFunction iteration exploits the fixed-point structure directly, treating the collocation problem as value iteration in coefficient space. When the finite-dimensional approximation preserves the contraction property of the Bellman operator, it converges globally from any initial guess. Each iteration is cheap: evaluate the Bellman operator at n points (the collocation nodes) and solve one linear system \\boldsymbol{\\Phi} \\theta^{(k+1)} = \\mathcal{B}(\\theta^{(k)}). However, convergence can be slow, especially when \\gamma is close to 1.\n\nNewton’s method treats the problem as rootfinding, exploiting smoothness (via the Envelope Theorem) to achieve fast local convergence. Once close to the solution, Newton’s method typically converges in just a few iterations. The per-iteration cost is higher: in addition to evaluating \\mathcal{B}(\\theta^{(k)}), we must compute the Jacobian J_{\\mathcal{B}}(\\theta^{(k)}), which requires evaluating expected basis function values at all collocation points. However, Newton’s method is sensitive to initial conditions and may diverge or converge to spurious solutions when started far from the true fixed point.\n\nConnection to Policy Iteration\n\nThe Newton update \\theta^{(k+1)} = [\\boldsymbol{\\Phi} - J_{\\mathcal{B}}(\\theta^{(k)})]^{-1} \\mathcal{B}(\\theta^{(k)}) is equivalent to the policy iteration algorithm commonly used in discrete-state dynamic programming. To see this, note that at the optimal coefficients \\theta^*, we have \\boldsymbol{\\Phi} \\theta^* = \\mathcal{B}(\\theta^*). The Newton step finds the coefficients that would be optimal if the policy (the optimal actions at each collocation point) were held fixed at the current iteration’s policy. This connection explains why Newton’s method often converges rapidly: like policy iteration, it implicitly performs policy evaluation and policy improvement, which can converge in just a few iterations for well-behaved problems.\n\nPractical recommendations:\n\nFor problems with strong contraction (small \\gamma, well-conditioned \\boldsymbol{\\Phi}, shape-preserving bases): Start with function iteration. It’s simple, robust, and often converges adequately in 20-50 iterations.\n\nFor problems with weak contraction (large \\gamma, high-order polynomial bases): Use a hybrid approach:\n\nRun function iteration for 5-10 iterations to get into the basin of attraction\n\nSwitch to Newton’s method for fast convergence to high accuracy\n\nFor problems where contraction fails (non-monotone bases, approximation destroys contraction): Newton’s method may be necessary from the start, but requires careful initialization (e.g., from a coarser approximation).\n\nQuasi-Newton methods (like BFGS or Broyden) offer a middle ground: they approximate the Jacobian using function evaluations only, avoiding the Envelope Theorem calculation. This can be useful when computing J_{\\mathcal{B}} is expensive or when the Jacobian approximation is acceptable.\n\nThe choice often depends on the application domain. In economic models where the value function is guaranteed to be concave and monotone, simple bases (linear interpolation, shape-preserving splines) combined with function iteration are reliable. In control problems with complex dynamics, high-order approximations combined with Newton’s method may be necessary for accuracy. ### Shape-Preserving Considerations\n\nIn dynamic programming, the value function typically has specific structural properties that we want our approximation to preserve. For instance:\n- **Monotonicity**: If having more of a resource is better, the value function should be increasing\n- **Concavity**: Diminishing returns often imply concave value functions\n- **Boundedness**: The value function is bounded when rewards are bounded\n\nStandard polynomial approximation does not automatically preserve these properties. A polynomial fit to increasing, concave data points can produce a function with non-monotonic or convex regions between the data points. This can destabilize the iterative algorithm: artificially high values at non-collocation points can lead to poor decisions in the maximization step, which feeds back into even worse approximations.\n\n**Shape-preserving approximation methods** address this issue. For one-dimensional problems, Schumaker's shape-preserving quadratic splines maintain monotonicity and concavity while providing continuously differentiable approximations. For multidimensional problems, linear interpolation on simplices preserves monotonicity and convex combinations (though not concavity or smoothness).\n\nThe trade-off is between smoothness and shape preservation. Smooth approximations (high-order polynomials or splines) enable efficient optimization in the maximization step through gradient-based methods, but risk introducing spurious features. Simple approximations (linear interpolation) guarantee shape preservation but introduce kinks that complicate optimization and may produce discontinuous policies when the true policy is continuous. ","type":"content","url":"/projdp#comparison-of-solution-methods","position":41},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl2":"Monotone Projection and the Preservation of Contraction"},"type":"lvl2","url":"/projdp#monotone-projection-and-the-preservation-of-contraction","position":42},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl2":"Monotone Projection and the Preservation of Contraction"},"content":"The informal discussion of shape preservation hints at a deeper theoretical question: when does the function iteration method converge? Recall from our discussion of collocation that function iteration proceeds in two steps:\n\nApply the Bellman operator at collocation points: t^{(k)} = v(\\theta^{(k)}) where t_i^{(k)} = \\mathrm{L}\\hat{v}^{(k)}(s_i)\n\nFit new coefficients to match these targets: \\boldsymbol{\\Phi} \\theta^{(k+1)} = t^{(k)}, giving \\theta^{(k+1)} = \\boldsymbol{\\Phi}^{-1} v(\\theta^{(k)})\n\nWe can reinterpret this iteration in function space rather than coefficient space. Let \\Psi be the projection operator that takes any function f and returns its approximation in \\text{span}\\{\\varphi_1, \\ldots, \\varphi_n\\}. For collocation, \\Psi is the interpolation operator: (\\Psi f)(s) is the unique linear combination of basis functions that matches f at the collocation points. Then Step 2 can be written as: fit \\hat{v}^{(k+1)} so that \\hat{v}^{(k+1)}(s_i) = \\mathrm{L}\\hat{v}^{(k)}(s_i) for all collocation points, which means \\hat{v}^{(k+1)} = \\Psi(\\mathrm{L}\\hat{v}^{(k)}).\n\nIn other words, function iteration is equivalent to projected value iteration in function space:\\hat{v}^{(k+1)} = \\Psi \\mathrm{L} \\hat{v}^{(k)}.\n\nWe know that standard value iteration v_{k+1} = \\mathrm{L} v_k converges because \\mathrm{L} is a \\gamma-contraction in the sup norm. But now we’re iterating with the composed operator \\Psi\\mathrm{L} instead of \\mathrm{L} alone.\n\nThis \\Psi\\mathrm{L} structure is not specific to collocation. It is inherent in all projection methods. The general pattern is always the same: apply the Bellman operator to get a target function \\mathrm{L}\\hat{v}^{(k)}, then project it back onto our approximation space to get \\hat{v}^{(k+1)}. The projection step defines an operator \\Psi that depends on our choice of test functions:\n\nFor collocation, \\Psi interpolates values at collocation points\n\nFor Galerkin, \\Psi is orthogonal projection with respect to \\langle \\cdot, \\cdot \\rangle_w\n\nFor least squares, \\Psi minimizes the weighted residual norm\n\nBut regardless of which projection method we use, iteration takes the form \\hat{v}^{(k+1)} = \\Psi\\mathrm{L}\\hat{v}^{(k)}.\n\nThe critical question is: does the composition \\Psi \\mathrm{L} inherit the contraction property of \\mathrm{L}? If not, the iteration may diverge, oscillate, or converge to a spurious fixed point even though the original problem is well-posed.","type":"content","url":"/projdp#monotone-projection-and-the-preservation-of-contraction","position":43},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl3":"Monotone Approximators and Stability","lvl2":"Monotone Projection and the Preservation of Contraction"},"type":"lvl3","url":"/projdp#monotone-approximators-and-stability","position":44},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl3":"Monotone Approximators and Stability","lvl2":"Monotone Projection and the Preservation of Contraction"},"content":"The answer turns out to depend on specific properties of the approximation operator \\Psi. This theory was developed independently across multiple research communities—computational economics \n\nJudd (1992)\n\nJudd (1996)\n\nSantos & Vigo-Aguiar (1998), economic dynamics \n\nStachurski (2009), and reinforcement learning \n\nGordon (1995)\n\nGordon (1999)—arriving at essentially the same mathematical conditions.","type":"content","url":"/projdp#monotone-approximators-and-stability","position":45},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl4":"Monotonicity Implies Nonexpansiveness","lvl3":"Monotone Approximators and Stability","lvl2":"Monotone Projection and the Preservation of Contraction"},"type":"lvl4","url":"/projdp#monotonicity-implies-nonexpansiveness","position":46},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl4":"Monotonicity Implies Nonexpansiveness","lvl3":"Monotone Approximators and Stability","lvl2":"Monotone Projection and the Preservation of Contraction"},"content":"It turns out that approximation operators satisfying simple structural properties automatically preserve contraction.\n\nMonotone operators are nonexpansive (Stachurski)\n\nLet \\Psi: B(\\mathcal{S}) \\to B(\\mathcal{S}) be a linear operator on the space of bounded functions. If \\Psi satisfies:\n\nMonotonicity: f \\leq g pointwise implies \\Psi f \\leq \\Psi g\n\nConstant preservation: \\Psi\\mathbf{1} = \\mathbf{1} where \\mathbf{1} is the constant function equal to 1\n\nThen \\Psi is nonexpansive in the sup norm: \\|\\Psi f - \\Psi g\\|_\\infty \\leq \\|f - g\\|_\\infty for all f, g \\in B(\\mathcal{S}).\n\nLet M = \\|f - g\\|_\\infty. Then -M \\leq f(s) - g(s) \\leq M for all s, which can be written as g - M\\mathbf{1} \\leq f \\leq g + M\\mathbf{1}. By monotonicity, \\Psi(g - M\\mathbf{1}) \\leq \\Psi f \\leq \\Psi(g + M\\mathbf{1}). By linearity and constant preservation, \\Psi g - M\\mathbf{1} \\leq \\Psi f \\leq \\Psi g + M\\mathbf{1}, which means |\\Psi f(s) - \\Psi g(s)| \\leq M for all s. Therefore \\|\\Psi f - \\Psi g\\|_\\infty \\leq \\|f - g\\|_\\infty.\n\nThis proposition shows that monotonicity and constant preservation automatically imply nonexpansiveness. There is no need to verify this separately. The intuition is that a monotone, constant-preserving operator acts like a weighted average that respects order structure and cannot amplify differences between functions.","type":"content","url":"/projdp#monotonicity-implies-nonexpansiveness","position":47},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl4":"Preservation of Contraction","lvl3":"Monotone Approximators and Stability","lvl2":"Monotone Projection and the Preservation of Contraction"},"type":"lvl4","url":"/projdp#preservation-of-contraction","position":48},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl4":"Preservation of Contraction","lvl3":"Monotone Approximators and Stability","lvl2":"Monotone Projection and the Preservation of Contraction"},"content":"Combining nonexpansiveness with the contraction property of the Bellman operator yields the main stability result.\n\nStability of projected value iteration (Santos-Vigo-Aguiar)\n\nLet \\mathrm{L}: B(\\mathcal{S}) \\to B(\\mathcal{S}) be a \\gamma-contraction on the space of bounded functions with respect to the sup norm. Let \\Psi: B(\\mathcal{S}) \\to B(\\mathcal{S}) be a linear approximation operator satisfying monotonicity and constant preservation.\n\nThen the composed operator \\Psi\\mathrm{L} is a \\gamma-contraction, and projected value iteration v_{k+1} = \\Psi\\mathrm{L} v_k converges globally to a unique fixed point v_\\Psi \\in \\text{Range}(\\Psi) with approximation error:\\|v_\\Psi - v^*\\|_\\infty \\leq \\frac{1}{1-\\gamma} \\|\\Psi v^* - v^*\\|_\\infty,\n\nwhere v^* is the true value function.\n\nSince \\mathrm{L} is a \\gamma-contraction, we have -\\gamma\\|f-g\\|_\\infty \\leq \\mathrm{L} f - \\mathrm{L} g \\leq \\gamma\\|f-g\\|_\\infty pointwise. By monotonicity of \\Psi, \\Psi(-\\gamma\\|f-g\\|_\\infty) \\leq \\Psi(\\mathrm{L} f - \\mathrm{L} g) \\leq \\Psi(\\gamma\\|f-g\\|_\\infty). By constant preservation, -\\gamma\\|f-g\\|_\\infty \\leq \\Psi(\\mathrm{L} f - \\mathrm{L} g) \\leq \\gamma\\|f-g\\|_\\infty, which implies \\|\\Psi\\mathrm{L} f - \\Psi\\mathrm{L} g\\|_\\infty \\leq \\gamma\\|f-g\\|_\\infty.\n\nThe error bound follows from fixed-point analysis: v^* - v_\\Psi = (I - \\Psi\\mathrm{L})^{-1}(v^* - \\Psi v^*), and since \\Psi\\mathrm{L} is a \\gamma-contraction, \\|(I - \\Psi\\mathrm{L})^{-1}\\| \\leq (1-\\gamma)^{-1}.\n\nThis error bound tells us that the fixed-point error is controlled by how well \\Psi can represent v^*. If v^* \\in \\text{Range}(\\Psi), then \\Psi v^* = v^* and the error vanishes. Otherwise, the error is proportional to the approximation error \\|\\Psi v^* - v^*\\|_\\infty, amplified by the factor (1-\\gamma)^{-1}.","type":"content","url":"/projdp#preservation-of-contraction","position":49},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl4":"Averagers in Discrete-State Problems","lvl3":"Monotone Approximators and Stability","lvl2":"Monotone Projection and the Preservation of Contraction"},"type":"lvl4","url":"/projdp#averagers-in-discrete-state-problems","position":50},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl4":"Averagers in Discrete-State Problems","lvl3":"Monotone Approximators and Stability","lvl2":"Monotone Projection and the Preservation of Contraction"},"content":"For discrete-state problems, the monotonicity conditions have a natural interpretation as averaging with nonnegative weights. This characterization was developed by Gordon in the context of reinforcement learning.\n\nAverager (Gordon)\n\nAn operator \\Psi: \\mathbb{R}^{|\\mathcal{S}|} \\to \\mathbb{R}^{|\\mathcal{S}|} is an averager if \\Psi v = Wv where W is a |\\mathcal{S}| \\times |\\mathcal{S}| stochastic matrix: w_{ij} \\geq 0 and \\sum_j w_{ij} = 1 for all i.\n\nAveragers automatically satisfy the monotonicity conditions: linearity follows from matrix multiplication, monotonicity follows from nonnegativity of entries, and constant preservation follows from row sums equaling one.\n\nStability with averagers (Gordon)\n\nIf \\Psi is an averager and \\mathrm{L} is the Bellman operator (a \\gamma-contraction), then \\Psi\\mathrm{L} is a \\gamma-contraction, and value iteration v_{k+1} = \\Psi\\mathrm{L} v_k converges to a unique fixed point.\n\nThis specializes the Santos-Vigo-Aguiar theorem to discrete states, expressed in the probabilistic language of stochastic matrices. The stochastic matrix characterization connects to Markov chain theory: \\Psi v represents expected values after one transition, and the monotonicity property reflects the fact that expectations preserve order.\n\nExamples of averagers include state aggregation (averaging values within groups), K-nearest neighbors (averaging over nearest states), kernel smoothing with positive kernels, and multilinear interpolation on grids (barycentric weights are nonnegative and sum to one). Counterexamples include linear least squares regression (projection matrix may have negative entries) and high-order polynomial interpolation (Runge phenomenon produces negative weights).","type":"content","url":"/projdp#averagers-in-discrete-state-problems","position":51},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl4":"Which Approximation Operators Are Monotone?","lvl3":"Monotone Approximators and Stability","lvl2":"Monotone Projection and the Preservation of Contraction"},"type":"lvl4","url":"/projdp#which-approximation-operators-are-monotone","position":52},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl4":"Which Approximation Operators Are Monotone?","lvl3":"Monotone Approximators and Stability","lvl2":"Monotone Projection and the Preservation of Contraction"},"content":"Method\n\nMonotone?\n\nNotes\n\nPiecewise linear interpolation\n\nYes\n\nAlways an averager; guaranteed stability\n\nMultilinear interpolation (grid)\n\nYes\n\nBarycentric weights are nonnegative and sum to one\n\nShape-preserving splines (Schumaker)\n\nYes\n\nDesigned to maintain monotonicity\n\nState aggregation\n\nYes\n\nExact averaging within groups\n\nKernel smoothing (positive kernels)\n\nYes\n\nIf kernel integrates to one\n\nHigh-order polynomial interpolation\n\nNo\n\nOscillations violate monotonicity (Runge phenomenon)\n\nLeast squares projection (arbitrary basis)\n\nNo\n\nProjection matrix may have negative entries\n\nFourier/spectral methods\n\nNo\n\nNot monotone-preserving in general\n\nNeural networks\n\nNo\n\nHighly flexible but no monotonicity guarantees\n\nThe distinction between “safe” (monotone) and “potentially unstable” (non-monotone) approximators provides rigorous foundation for the folk wisdom that linear interpolation is reliable while high-order polynomials can be dangerous for value iteration.","type":"content","url":"/projdp#which-approximation-operators-are-monotone","position":53},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl3":"Practical Implications","lvl2":"Monotone Projection and the Preservation of Contraction"},"type":"lvl3","url":"/projdp#practical-implications","position":54},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl3":"Practical Implications","lvl2":"Monotone Projection and the Preservation of Contraction"},"content":"When using successive approximation (fixed-point iteration):\n\nChoose monotone approximators to guarantee convergence\n\nPiecewise linear interpolation, state aggregation, and kernel methods with positive kernels are safe choices\n\nHigh-order polynomials and least squares regression may fail to converge even when the Bellman operator is a strong contraction\n\nWhen using rootfinding methods (Newton):\n\nMonotonicity is not required for convergence\n\nCan use smooth approximations (polynomials, splines, neural networks) for better approximation quality\n\nRequires good initial guesses and well-conditioned systems\n\nStability depends on numerical properties of the Jacobian, not contraction preservation\n\nHybrid strategies:\n\nUse smooth approximation for policy representation, but monotone averager for value iteration\n\nRegularize smooth approximations with monotonicity constraints (monotone neural networks)\n\nRun a few iterations with a monotone method to generate initial guess, then switch to Newton’s method with smooth approximation\n\nSolve projection equations directly (collocation with Newton) rather than iterating\n\nThis explains observed differences across research communities: reinforcement learning (traditionally using iterative TD methods) emphasized averagers, while computational economics (using collocation with Newton solvers) was more comfortable with polynomial bases.","type":"content","url":"/projdp#practical-implications","position":55},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl3":"Weighted Norms and Extensions","lvl2":"Monotone Projection and the Preservation of Contraction"},"type":"lvl3","url":"/projdp#weighted-norms-and-extensions","position":56},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl3":"Weighted Norms and Extensions","lvl2":"Monotone Projection and the Preservation of Contraction"},"content":"The monotone approximation theory provides complete characterization for contraction in the sup norm. Several important extensions remain active research areas:\n\nWeighted L^2 norms: For policy evaluation with Galerkin projection, the relevant norm is \\|\\cdot\\|_\\xi where \\xi is a state distribution. The contraction preservation condition becomes: \\xi must be stationary under the policy’s transition operator. On-policy TD methods converge while off-policy methods can diverge because the weighting distribution must match the policy dynamics.\n\nNonlinear approximation: Neural networks don’t fit the linear operator framework. Recent work on monotone and convex neural networks attempts to recover stability through architectural constraints, but a complete theory is still emerging.\n\nHigh-dimensional state spaces: Grid-based averagers become intractable due to curse of dimensionality. Understanding which non-averaging approximations provide acceptable stability-accuracy trade-offs is crucial for modern applications.\n\nOff-policy learning: The averager framework assumes on-policy evaluation. Off-policy methods require additional machinery (importance sampling, gradient corrections) to maintain stability, even with averaging operators.","type":"content","url":"/projdp#weighted-norms-and-extensions","position":57},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl2":"Galerkin Projection and Least Squares Temporal Difference"},"type":"lvl2","url":"/projdp#galerkin-projection-and-least-squares-temporal-difference","position":58},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl2":"Galerkin Projection and Least Squares Temporal Difference"},"content":"An important special case emerges when we apply Galerkin projection to the policy evaluation problem rather than the optimality problem. For a fixed policy \\pi, the policy evaluation Bellman equation is:v^\\pi(s) = \\mathrm{L}_\\pi v^\\pi(s) = r(s,\\pi(s)) + \\gamma \\sum_{s' \\in \\mathcal{S}} p(s'|s,\\pi(s)) v^\\pi(s').\n\nThis is a linear operator (no max), making the projection problem significantly simpler. Consider a linear function approximation \\hat{v}(s) = \\boldsymbol{\\varphi}(s)^\\top \\boldsymbol{\\theta} where \\boldsymbol{\\varphi}(s) = [\\varphi_1(s), \\ldots, \\varphi_n(s)]^\\top are basis functions and \\boldsymbol{\\theta} = [\\theta_1, \\ldots, \\theta_n]^\\top are coefficients to determine. The residual is:R(s; \\boldsymbol{\\theta}) = \\mathrm{L}_\\pi \\hat{v}(s) - \\hat{v}(s) = r(s,\\pi(s)) + \\gamma \\sum_{s'} p(s'|s,\\pi(s)) \\boldsymbol{\\varphi}(s')^\\top \\boldsymbol{\\theta} - \\boldsymbol{\\varphi}(s)^\\top \\boldsymbol{\\theta}.\n\nThe Galerkin projection requires the residual to be orthogonal to all basis functions with respect to some weighting:\\sum_{s \\in \\mathcal{S}} \\xi(s) R(s; \\boldsymbol{\\theta}) \\varphi_j(s) = 0, \\quad j = 1, \\ldots, n,\n\nwhere \\xi(s) is a distribution over states (often the stationary distribution under policy \\pi, or uniform over visited states). Substituting the residual:\\sum_s \\xi(s) \\left[ r(s,\\pi(s)) + \\gamma \\sum_{s'} p(s'|s,\\pi(s)) \\boldsymbol{\\varphi}(s')^\\top \\boldsymbol{\\theta} - \\boldsymbol{\\varphi}(s)^\\top \\boldsymbol{\\theta} \\right] \\varphi_j(s) = 0.\n\nRearranging and writing in matrix form, let \\boldsymbol{\\Xi} be a diagonal matrix with \\Xi_{ss} = \\xi(s), \\boldsymbol{\\Phi} be the |\\mathcal{S}| \\times n matrix with rows \\boldsymbol{\\varphi}(s)^\\top, and \\mathbf{P}_\\pi be the transition matrix under policy \\pi. The Galerkin conditions become:\\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} (\\mathbf{r}_\\pi + \\gamma \\mathbf{P}_\\pi \\boldsymbol{\\Phi} \\boldsymbol{\\theta} - \\boldsymbol{\\Phi} \\boldsymbol{\\theta}) = \\mathbf{0}.\n\nSolving for \\boldsymbol{\\theta}:\\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} (\\boldsymbol{\\Phi} - \\gamma \\mathbf{P}_\\pi \\boldsymbol{\\Phi}) \\boldsymbol{\\theta} = \\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} \\mathbf{r}_\\pi.\n\nWe have just derived the Least Squares Temporal Difference (LSTD) solution for policy evaluation. This shows that LSTD is Galerkin projection applied to the linear policy evaluation Bellman equation. The “least squares” name comes from the fact that this is the projection (in the weighted \\ell^2 sense) of the Bellman operator’s output onto the span of the basis functions.\n\nThe projection perspective makes clear an important aspect of approximate dynamic programming. The solution \\boldsymbol{\\theta} does not satisfy the true Bellman equation v = \\mathrm{L}_\\pi v (which is typically impossible within our finite-dimensional approximation space). Instead, it satisfies \\hat{v} = \\Pi \\mathrm{L}_\\pi \\hat{v}, where \\Pi is the projection operator onto \\text{span}\\{\\varphi_1, \\ldots, \\varphi_n\\}. We find the fixed point of the projected Bellman operator, not the Bellman operator itself. This is why approximation error persists even at convergence: the best we can do is find the value function whose Bellman operator output projects back onto itself.","type":"content","url":"/projdp#galerkin-projection-and-least-squares-temporal-difference","position":59},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl3":"The Projected Bellman Equations","lvl2":"Galerkin Projection and Least Squares Temporal Difference"},"type":"lvl3","url":"/projdp#the-projected-bellman-equations","position":60},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl3":"The Projected Bellman Equations","lvl2":"Galerkin Projection and Least Squares Temporal Difference"},"content":"The LSTD solution gives a closed-form expression and connects to iterative algorithms developed in the next chapter. Understanding convergence of these methods requires analyzing when the projected Bellman operator \\Pi \\mathrm{L}_\\pi is a contraction.\n\nNorms and projections. Fix a feature matrix \\boldsymbol{\\Phi} \\in \\mathbb{R}^{|\\mathcal{S}| \\times n} with full column rank and a probability distribution \\xi over states. Define the \\xi-weighted inner product and norm by\\langle u, v \\rangle_\\xi := \\sum_s \\xi(s) u(s) v(s) = u^\\top \\boldsymbol{\\Xi} v, \\qquad \\|v\\|_\\xi := \\sqrt{v^\\top \\boldsymbol{\\Xi} v},\n\nwhere \\boldsymbol{\\Xi} = \\text{diag}(\\xi). The orthogonal projection onto \\text{span}(\\boldsymbol{\\Phi}) with respect to \\langle \\cdot, \\cdot \\rangle_\\xi is\\Pi = \\boldsymbol{\\Phi}(\\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} \\boldsymbol{\\Phi})^{-1} \\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi}.\n\nAn operator \\mathrm{T} is a \\beta-contraction in norm \\|\\cdot\\| if \\|\\mathrm{T}v - \\mathrm{T}w\\| \\leq \\beta \\|v - w\\| for all v, w and some \\beta < 1. It is a non-expansion if the same holds with \\beta = 1.\n\nWhy \\Pi is a non-expansion. This follows from the Pythagorean identity in weighted inner product spaces. For any u \\in \\mathbb{R}^{|\\mathcal{S}|}, the projection \\Pi u and the residual (I - \\Pi)u are \\xi-orthogonal: \\langle \\Pi u, (I-\\Pi)u \\rangle_\\xi = 0. Therefore,\\|u\\|_\\xi^2 = \\|\\Pi u\\|_\\xi^2 + \\|(I-\\Pi)u\\|_\\xi^2.\n\nApplying this to u - v gives\\|\\Pi u - \\Pi v\\|_\\xi^2 = \\|\\Pi(u-v)\\|_\\xi^2 \\leq \\|\\Pi(u-v)\\|_\\xi^2 + \\|(I-\\Pi)(u-v)\\|_\\xi^2 = \\|u - v\\|_\\xi^2,\n\nproving \\|\\Pi u - \\Pi v\\|_\\xi \\leq \\|u - v\\|_\\xi.\n\nWhen is \\mathrm{L}_\\pi a contraction in \\|\\cdot\\|_\\xi? Write the policy evaluation operator as \\mathrm{L}_\\pi v = r_\\pi + \\gamma \\mathbf{P}_\\pi v, where \\mathbf{P}_\\pi is the transition matrix under policy \\pi. We know \\mathrm{L}_\\pi is a \\gamma-contraction in \\|\\cdot\\|_\\infty from earlier chapters. However, whether it contracts in \\|\\cdot\\|_\\xi depends on the relationship between \\xi and \\mathbf{P}_\\pi.\n\nWe need to establish when the stochastic matrix \\mathbf{P}_\\pi is non-expansive in \\|\\cdot\\|_\\xi. Following Bertsekas (Lemma 6.3.1), suppose \\xi is a steady-state probability vector for \\mathbf{P}_\\pi with positive components, meaning:\\xi^\\top \\mathbf{P}_\\pi = \\xi^\\top, \\qquad \\text{or equivalently,} \\qquad \\xi(s') = \\sum_s \\xi(s) p(s'|s,\\pi(s)) \\text{ for all } s'.\n\nThen for any z \\in \\mathbb{R}^{|\\mathcal{S}|}, using the convexity of the square function (Jensen’s inequality):\\begin{align*}\n\\|\\mathbf{P}_\\pi z\\|_\\xi^2 &= \\sum_s \\xi(s) \\left(\\sum_{s'} p(s'|s,\\pi(s)) z(s')\\right)^2 \\\\\n&\\leq \\sum_s \\xi(s) \\sum_{s'} p(s'|s,\\pi(s)) z(s')^2 \\\\\n&= \\sum_{s'} \\left(\\sum_s \\xi(s) p(s'|s,\\pi(s))\\right) z(s')^2\n\\end{align*}\n\nUsing the defining property of steady-state probabilities \\sum_s \\xi(s) p(s'|s,\\pi(s)) = \\xi(s'):= \\sum_{s'} \\xi(s') z(s')^2 = \\|z\\|_\\xi^2.\n\nTherefore \\|\\mathbf{P}_\\pi z\\|_\\xi \\leq \\|z\\|_\\xi, showing that \\mathbf{P}_\\pi is non-expansive in \\|\\cdot\\|_\\xi. Since \\|\\mathrm{L}_\\pi v - \\mathrm{L}_\\pi w\\|_\\xi = \\gamma \\|\\mathbf{P}_\\pi(v-w)\\|_\\xi:\\|\\mathrm{L}_\\pi v - \\mathrm{L}_\\pi w\\|_\\xi \\leq \\gamma \\|v - w\\|_\\xi.\n\nThus \\mathrm{L}_\\pi is a \\gamma-contraction in \\|\\cdot\\|_\\xi when \\xi is the steady-state distribution of \\pi.\n\nContraction of the composition. Combining our two results: \\Pi is a non-expansion and (under stationarity) \\mathrm{L}_\\pi is a \\gamma-contraction in \\|\\cdot\\|_\\xi. Therefore,\\|\\Pi \\mathrm{L}_\\pi v - \\Pi \\mathrm{L}_\\pi w\\|_\\xi \\leq \\|\\mathrm{L}_\\pi v - \\mathrm{L}_\\pi w\\|_\\xi \\leq \\gamma \\|v - w\\|_\\xi.\n\nBy the Banach fixed-point theorem, \\Pi \\mathrm{L}_\\pi has a unique fixed point in \\mathbb{R}^{|\\mathcal{S}|}, and iterates v_{k+1} = \\Pi \\mathrm{L}_\\pi v_k converge to it from any v_0. This fixed point satisfies the projected Bellman equationv = \\Pi(r_\\pi + \\gamma \\mathbf{P}_\\pi v), \\qquad v \\in \\text{span}(\\boldsymbol{\\Phi}).\n\nWriting v = \\boldsymbol{\\Phi} \\boldsymbol{\\theta} and left-multiplying by \\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} yields the normal equations\\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi}(\\boldsymbol{\\Phi} - \\gamma \\mathbf{P}_\\pi \\boldsymbol{\\Phi}) \\boldsymbol{\\theta} = \\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} r_\\pi,\n\nwhich are precisely the LSTD equations we derived earlier. This result provides the theoretical foundation for temporal difference learning with linear function approximation: when learning on-policy (so \\xi is stationary), convergence is guaranteed.\n\nOff-policy instability. When \\xi is not stationary for \\mathbf{P}_\\pi (as occurs when data come from a different behavior policy), the Jensen argument breaks down. The transition operator \\mathbf{P}_\\pi need not be non-expansive in \\|\\cdot\\|_\\xi, so \\Pi \\mathrm{L}_\\pi may fail to be a contraction. This is the root cause of off-policy divergence phenomena in linear TD learning (e.g., Baird’s counterexample). Importance weighting and other corrections are designed to restore stability in this regime.\n\nThe linearity of the policy evaluation operator \\mathrm{L}_\\pi is what gives us the closed-form solution. We could apply Galerkin projection to the Bellman optimality equation v^* = \\mathrm{L} v^*, setting up orthogonality conditions \\sum_s \\xi(s) R(s; \\boldsymbol{\\theta}) \\varphi_j(s) = 0. The max operator makes these conditions nonlinear in \\boldsymbol{\\theta}, eliminating the closed form and requiring iterative solution. This brings us back to the successive approximation methods discussed earlier for collocation.","type":"content","url":"/projdp#the-projected-bellman-equations","position":61},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl2":"Fitted-Value Iteration: Beyond Linear Projection"},"type":"lvl2","url":"/projdp#fitted-value-iteration-beyond-linear-projection","position":62},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl2":"Fitted-Value Iteration: Beyond Linear Projection"},"content":"Throughout this chapter, we have focused on polynomial approximations and linear projections, where the value function is represented as v(s) = \\sum_{j=1}^d \\theta_j \\varphi_j(s) and the projection operator \\Pi solves a linear system to find the best coefficients \\boldsymbol{\\theta}. This framework, while analytically tractable, is just one instance of a much more general pattern that encompasses modern function approximation methods including neural networks, decision trees, kernel methods, and ensemble models.\n\nThe projection operator \\Pi need not be a linear projection at all. Instead, it can be any computational procedure that fits an approximator to target data. This generalization leads us to fitted-value iteration (FVI), a universal template for approximate dynamic programming that subsumes classical projection methods as special cases while extending naturally to black-box function approximators.","type":"content","url":"/projdp#fitted-value-iteration-beyond-linear-projection","position":63},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl3":"The Fitting Operator","lvl2":"Fitted-Value Iteration: Beyond Linear Projection"},"type":"lvl3","url":"/projdp#the-fitting-operator","position":64},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl3":"The Fitting Operator","lvl2":"Fitted-Value Iteration: Beyond Linear Projection"},"content":"Rather than thinking of \\Pi as a mathematical projection onto a subspace, we can view it as a fitting operator \\mathtt{fit} that takes a dataset of state-value pairs \\{(s_i, y_i)\\}_{i=1}^n and produces a function \\hat{v} \\in \\mathcal{F} from some hypothesis class \\mathcal{F}:\\hat{v} = \\mathtt{fit}\\big(\\{(s_i, y_i)\\}_{i=1}^n; \\mathcal{F}\\big).\n\nThe specifics of \\mathtt{fit} depend on the function approximator:\n\nLinear basis functions: \\mathtt{fit} solves the weighted least-squares problem \\min_{\\boldsymbol{\\theta}} \\sum_{i=1}^n \\xi(s_i) \\big(y_i - \\sum_j \\theta_j \\varphi_j(s_i)\\big)^2, yielding the Galerkin projection we studied earlier.\n\nNeural networks: \\mathtt{fit} runs stochastic gradient descent to minimize the loss \\mathcal{L}(\\boldsymbol{\\theta}) = \\sum_{i=1}^n (y_i - v_{\\boldsymbol{\\theta}}(s_i))^2, where v_{\\boldsymbol{\\theta}} is a neural network with parameters \\boldsymbol{\\theta}.\n\nDecision trees: \\mathtt{fit} constructs a regression tree by recursively partitioning the state space to minimize the sum of squared residuals within each leaf.\n\nKernel methods: \\mathtt{fit} computes kernel weights \\alpha_i such that \\hat{v}(s) = \\sum_{i=1}^n \\alpha_i k(s, s_i) for some kernel function k.\n\nEnsemble methods: \\mathtt{fit} trains multiple base learners (e.g., via boosting or bagging) and combines their predictions.\n\nIn each case, \\mathtt{fit} maps a training dataset to a function, but the internal mechanism varies widely: closed-form solution, iterative optimization, tree construction, or ensemble aggregation.","type":"content","url":"/projdp#the-fitting-operator","position":65},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl3":"Fitted-Value Iteration as Successive Approximation","lvl2":"Fitted-Value Iteration: Beyond Linear Projection"},"type":"lvl3","url":"/projdp#fitted-value-iteration-as-successive-approximation","position":66},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl3":"Fitted-Value Iteration as Successive Approximation","lvl2":"Fitted-Value Iteration: Beyond Linear Projection"},"content":"Fitted-value iteration is the natural extension of the successive approximation methods we examined earlier. Starting from an initial approximation v_0 \\in \\mathcal{F}, we iterate:\n\nApply the Bellman operator at sample states \\{s_i\\}_{i=1}^n to compute target values:y_i = (\\mathrm{L} v_k)(s_i) = \\max_{a \\in \\mathcal{A}} \\bigg\\{ r(s_i, a) + \\gamma \\sum_{s'} p(s' \\mid s_i, a) v_k(s') \\bigg\\}.\n\nFit a new approximation to the targets using the fitting operator:v_{k+1} = \\mathtt{fit}\\big(\\{(s_i, y_i)\\}_{i=1}^n; \\mathcal{F}\\big).\n\nThis two-step rhythm (operator application followed by function fitting) mirrors exactly the structure of projected value iteration, where we computed \\Pi \\mathrm{L} v_k. The difference is that \\Pi has been replaced by the more general \\mathtt{fit}, which need not be a linear projection and need not have a closed form.\n\nFitted-Value Iteration\n\nInputs: Finite state set \\mathcal{S} (or sample \\{s_i\\}_{i=1}^n), discount factor \\gamma, function class \\mathcal{F}, fitting procedure \\mathtt{fit}, convergence tolerance \\epsilon\n\nOutput: Approximate value function \\hat{v} \\approx v^*\n\nInitialize v_0 \\in \\mathcal{F} arbitrarily\n\nSet k \\leftarrow 0\n\nrepeat\n\n\\quad for each state s_i \\in \\mathcal{S} do\n\n\\quad\\quad Compute target: y_i \\leftarrow \\displaystyle\\max_{a \\in \\mathcal{A}} \\Big\\{ r(s_i, a) + \\gamma \\sum_{s'} p(s' \\mid s_i, a) v_k(s') \\Big\\}\n\n\\quad end for\n\n\\quad Fit new approximation: v_{k+1} \\leftarrow \\mathtt{fit}\\big(\\{(s_i, y_i)\\}_{i=1}^n; \\mathcal{F}\\big)\n\n\\quad k \\leftarrow k+1\n\nuntil \\|v_k - v_{k-1}\\| < \\epsilon (or maximum iterations reached)\n\nreturn v_k\n\nThe abstraction \\mathtt{fit} encapsulates all the complexity of function approximation, whether that involves solving a linear system, running gradient descent for thousands of steps, growing a decision tree, or training an ensemble. From the algorithmic perspective, these are simply different implementations of the same conceptual operation: mapping a dataset to a function.","type":"content","url":"/projdp#fitted-value-iteration-as-successive-approximation","position":67},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl3":"Connection to the Projection Framework","lvl2":"Fitted-Value Iteration: Beyond Linear Projection"},"type":"lvl3","url":"/projdp#connection-to-the-projection-framework","position":68},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl3":"Connection to the Projection Framework","lvl2":"Fitted-Value Iteration: Beyond Linear Projection"},"content":"How does fitted-value iteration relate to the projection methods we studied? The connection becomes clear when we recognize that the projection operator \\Pi from earlier sections is simply one particular instantiation of \\mathtt{fit}:\n\nWhen \\mathcal{F} is a linear subspace spanned by basis functions \\{\\varphi_j\\}_{j=1}^d, and \\mathtt{fit} minimizes the weighted squared error \\sum_i \\xi(s_i)(y_i - \\sum_j \\theta_j \\varphi_j(s_i))^2, then \\mathtt{fit} coincides exactly with the Galerkin projection \\Pi.\n\nThe collocation method emerges when we choose evaluation points \\{s_i\\}_{i=1}^d equal in number to the basis functions and \\mathtt{fit} enforces exact interpolation: \\sum_j \\theta_j \\varphi_j(s_i) = y_i for all i.\n\nLeast-squares temporal difference (LSTD) methods, which we will encounter in the next chapter, can be viewed as performing \\mathtt{fit} on data collected from simulation rather than the full state space.\n\nThe residual function R(s; \\boldsymbol{\\theta}) = v(s; \\boldsymbol{\\theta}) - (\\mathrm{L} v(\\cdot; \\boldsymbol{\\theta}))(s) still governs approximation quality. In the linear case, we minimized \\sum_s \\xi(s) R(s; \\boldsymbol{\\theta})^2 by setting up orthogonality conditions. For nonlinear approximators, the residual remains the fundamental object of interest, but we typically cannot enforce orthogonality analytically. Instead, \\mathtt{fit} implicitly seeks to make the residual small in an empirical sense over the training data.\n\nLinear basis function approximation has many virtues: closed-form solutions, theoretical tractability, and well-understood convergence properties. But polynomial and radial basis functions require hand-crafted features, which can be difficult to design for high-dimensional state spaces or complex value function geometry. Neural networks, decision trees, kernel methods, and ensemble models can learn representations automatically or adapt their complexity to the data. The price is that we lose closed-form solutions and convergence guarantees, trading theoretical tractability for representational flexibility.","type":"content","url":"/projdp#connection-to-the-projection-framework","position":69},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl3":"Toward Simulation-Based Methods","lvl2":"Fitted-Value Iteration: Beyond Linear Projection"},"type":"lvl3","url":"/projdp#toward-simulation-based-methods","position":70},{"hierarchy":{"lvl1":"Projection Methods for Functional Equations","lvl3":"Toward Simulation-Based Methods","lvl2":"Fitted-Value Iteration: Beyond Linear Projection"},"content":"A limitation of the fitted-value iteration algorithm as presented is that it assumes we can evaluate the Bellman operator exactly at every state. That is, computing y_i = (\\mathrm{L} v_k)(s_i) requires knowing the transition probabilities p(j \\mid s_i, a) and being able to sum or integrate over all possible next states j.\n\nIn many real-world problems, we have neither. We might have access only to a simulator that generates sample transitions (s, a, r, j), or to a dataset of observed trajectories. We might not even know the full state space in advance. This brings us to the threshold of simulation-based approximate dynamic programming and reinforcement learning, where the Bellman operator must be approximated from samples rather than computed exactly.\n\nThe projection and collocation methods developed in this chapter provide the conceptual foundation for these simulation-based methods. The residual R(s; \\boldsymbol{\\theta}) = v(s; \\boldsymbol{\\theta}) - (\\mathrm{L} v(\\cdot; \\boldsymbol{\\theta}))(s) remains the central object. But instead of enforcing orthogonality conditions on the full state space, we will minimize empirical residuals on sampled data. The fitting operator \\mathtt{fit} will take in noisy, incomplete samples rather than exact values. And convergence will be probabilistic, characterized by sample complexity rather than deterministic fixed-point theorems.\n\nThe next chapter introduces Monte Carlo integration and temporal-difference learning, showing how to estimate the expectations in the Bellman operator from simulated experience. Together with the fitted-value iteration framework developed here, these tools form the backbone of modern approximate dynamic programming, connecting classical numerical methods to the data-driven paradigm of reinforcement learning.","type":"content","url":"/projdp#toward-simulation-based-methods","position":71},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations"},"type":"lvl1","url":"/regmdp","position":0},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations"},"content":"Dynamic programming methods suffer from the curse of dimensionality and can quickly become difficult to apply in practice. We may also be dealing with large or continuous state or action spaces. We have seen so far that we could address this problem using discretization, or interpolation. These were already examples of approximate dynamic programming. In this chapter, we will see other forms of approximations meant to facilitate the optimization problem, either by approximating the optimality equations, the value function, or the policy itself.\nApproximation theory is at the heart of learning methods, and fundamentally, this chapter will be about the application of learning ideas to solve complex decision-making problems.","type":"content","url":"/regmdp","position":1},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations"},"type":"lvl1","url":"/regmdp#smooth-bellman-optimality-equations","position":2},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations"},"content":"While the standard Bellman optimality equations use the max operator to determine the best action, an alternative formulation known as the smooth or soft Bellman optimality equations replaces this with a softmax operator. This approach originated from \n\nRust (1987) and was later rediscovered in the context of maximum entropy inverse reinforcement learning \n\nZiebart et al. (2008), which then led to soft Q-learning \n\nHaarnoja et al. (2017) and soft actor-critic \n\nHaarnoja et al. (2018), a state-of-the-art deep reinforcement learning algorithm.\n\nIn the infinite-horizon setting, the smooth Bellman optimality equations take the form:v_\\gamma^\\star(s) = \\frac{1}{\\beta} \\log \\sum_{a \\in A_s} \\exp\\left(\\beta\\left(r(s, a) + \\gamma \\sum_{j \\in S} p(j | s, a) v_\\gamma^\\star(j)\\right)\\right)\n\nAdopting an operator-theoretic perspective, we can define a nonlinear operator \\mathrm{L}_\\beta such that the smooth value function of an MDP is then the solution to the following fixed-point equation:(\\mathrm{L}_\\beta \\mathbf{v})(s) = \\frac{1}{\\beta} \\log \\sum_{a \\in \\mathcal{A}_s} \\exp\\left(\\beta\\left(r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j)\\right)\\right)\n\nAs \\beta \\to \\infty, \\mathrm{L}_\\beta converges to the standard Bellman operator \\mathrm{L}. Furthermore, it can be shown that the smooth Bellman operator is a contraction mapping in the supremum norm, and therefore has a unique fixed point. However, as opposed to the usual “hard” setting, the fixed point of \\mathrm{L}_\\beta is associated with the value function of an optimal stochastic policy defined by the softmax distribution:\\pi(a|s) = \\frac{\\exp\\left(\\beta\\left(r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v_\\gamma^\\star(j)\\right)\\right)}{\\sum_{a' \\in \\mathcal{A}_s} \\exp\\left(\\beta\\left(r(s,a') + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a') v_\\gamma^\\star(j)\\right)\\right)}\n\nDespite the confusing terminology, the above “softmax” policy is simply the smooth counterpart to the argmax operator in the original optimality equation: it acts as a soft-argmax.\n\nThis formulation is interesting for several reasons. First, smoothness is a desirable property from an optimization standpoint. Unlike \\gamma, we view \\beta as a hyperparameter of our algorithm, which we can control to achieve the desired level of accuracy.\n\nSecond, while presented from an intuitive standpoint where we replace the max by the log-sum-exp (a smooth maximum) and the argmax by the softmax (a smooth argmax), this formulation can also be obtained from various other perspectives, offering theoretical tools and solution methods. For example, \n\nRust (1987) derived this algorithm by considering a setting in which the rewards are stochastic and perturbed by a Gumbel noise variable. When considering the corresponding augmented state space and integrating the noise, we obtain smooth equations. This interpretation is leveraged by Rust for modeling purposes.","type":"content","url":"/regmdp#smooth-bellman-optimality-equations","position":3},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Smooth Value Iteration Algorithm"},"type":"lvl3","url":"/regmdp#smooth-value-iteration-algorithm","position":4},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Smooth Value Iteration Algorithm"},"content":"The smooth value iteration algorithm replaces the max operator in standard value iteration with the logsumexp operator. Here’s the algorithm structure:\n\nSmooth Value Iteration\n\nInput: MDP (S, A, r, p, \\gamma), inverse temperature \\beta > 0, tolerance \\epsilon > 0\n\nOutput: Approximate optimal value function v and stochastic policy \\pi\n\nInitialize v(s) \\leftarrow 0 for all s \\in S\n\nrepeat\n\n\\quad \\Delta \\leftarrow 0\n\n\\quad for each state s \\in S do\n\n\\quad\\quad for each action a \\in A_s do\n\n\\quad\\quad\\quad q(s,a) \\leftarrow r(s,a) + \\gamma \\sum_{j \\in S} p(j|s,a) v(j)\n\n\\quad\\quad end for\n\n\\quad\\quad v_{\\text{new}}(s) \\leftarrow \\frac{1}{\\beta} \\log \\sum_{a \\in A_s} \\exp(\\beta \\cdot q(s,a))\n\n\\quad\\quad \\Delta \\leftarrow \\max(\\Delta, |v_{\\text{new}}(s) - v(s)|)\n\n\\quad\\quad v(s) \\leftarrow v_{\\text{new}}(s)\n\n\\quad end for\n\nuntil \\Delta < \\epsilon\n\nExtract policy: for each state s \\in S do\n\n\\quad Compute q(s,a) for all a \\in A_s as in lines 5-7\n\n\\quad \\pi(a|s) \\leftarrow \\frac{\\exp(\\beta \\cdot q(s,a))}{\\sum_{a' \\in A_s} \\exp(\\beta \\cdot q(s,a'))} for all a \\in A_s\n\nend for\n\nreturn v, \\pi\n\nDifferences from standard value iteration:\n\nLine 8 uses \\frac{1}{\\beta} \\log \\sum_a \\exp(\\beta \\cdot q(s,a)) instead of \\max_a q(s,a)\n\nLine 15 extracts a stochastic policy using softmax instead of a deterministic argmax policy\n\nAs \\beta \\to \\infty, the algorithm converges to standard value iteration\n\nLower \\beta values produce more stochastic policies with higher entropy\n\nThere is also a way to obtain this equation by starting from the energy-based formulation often used in supervised learning, in which we convert an unnormalized probability distribution into a distribution using the softmax transformation. This is essentially what \n\nZiebart et al. (2008) did in their paper. Furthermore, this perspective bridges with the literature on probabilistic graphical models, in which we can now cast the problem of finding an optimal smooth policy into one of maximum likelihood estimation (an inference problem). This is the idea of control as inference, which also admits the converse - that of inference as control - used nowadays for deriving fast samples and amortized inference techniques using reinforcement learning \n\nLevine et al. (2018).\n\nFinally, it’s worth noting that we can also derive this form by considering an entropy-regularized formulation in which we penalize for the entropy of our policy in the reward function term. This formulation admits a solution that coincides with the smooth Bellman equations \n\nHaarnoja et al. (2017).","type":"content","url":"/regmdp#smooth-value-iteration-algorithm","position":5},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl2":"Gumbel Noise on the Rewards"},"type":"lvl2","url":"/regmdp#gumbel-noise-on-the-rewards","position":6},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl2":"Gumbel Noise on the Rewards"},"content":"We can obtain the smooth Bellman equation by considering a setting in which we have Gumbel noise added to the reward function. This derivation provides both theoretical insight and connects to practical modeling scenarios where rewards have random perturbations.","type":"content","url":"/regmdp#gumbel-noise-on-the-rewards","position":7},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Step 1: Define the Augmented MDP with Gumbel Noise","lvl2":"Gumbel Noise on the Rewards"},"type":"lvl3","url":"/regmdp#step-1-define-the-augmented-mdp-with-gumbel-noise","position":8},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Step 1: Define the Augmented MDP with Gumbel Noise","lvl2":"Gumbel Noise on the Rewards"},"content":"At each time period and state s, we draw an action-indexed shock vector:\\boldsymbol{\\epsilon}_t(s) = \\big(\\epsilon_t(s,a)\\big)_{a \\in \\mathcal{A}_s}, \\quad \\text{where } \\epsilon_t(s,a) \\text{ i.i.d.} \\sim \\mathrm{Gumbel}(\\mu_\\epsilon, 1/\\beta)\n\nThese shocks are independent across time periods, states, and actions, and are independent of the MDP transition dynamics p(\\cdot | s, a).\n\nThe Gumbel distribution with location parameter \\mu and scale parameter 1/\\beta has probability density function:f(x; \\mu, \\beta) = \\beta\\exp\\left(-\\beta(x-\\mu)-\\exp(-\\beta(x-\\mu))\\right)\n\nTo generate a Gumbel-distributed random variable, we can use inverse transform sampling: X = \\mu - \\frac{1}{\\beta} \\ln(-\\ln(U)) where U is uniform on (0,1).\n\nZero-Mean Shocks\n\nTo ensure the shocks have zero mean, we set \\mu_\\epsilon = -\\gamma_E/\\beta where \\gamma_E \\approx 0.5772 is the Euler-Mascheroni constant. This choice eliminates an additive constant that would otherwise appear in the smooth Bellman equation. For simplicity, we will adopt this convention throughout.\n\nWe now define an augmented MDP with:\n\nAugmented state: \\tilde{s} = (s, \\boldsymbol{\\epsilon}) where s \\in \\mathcal{S} and \\boldsymbol{\\epsilon} \\in \\mathbb{R}^{|\\mathcal{A}_s|}\n\nAugmented reward: \\tilde{r}(\\tilde{s}, a) = r(s,a) + \\epsilon(a)\n\nAugmented transition: \\tilde{p}(\\tilde{s}' | \\tilde{s}, a) = p(s' | s, a) \\cdot p(\\boldsymbol{\\epsilon}')\n\nThe transition factorizes because the next shock vector \\boldsymbol{\\epsilon}' is drawn independently of the current state and action (conditional independence).\n\nThe Augmented State Space is Infinite-Dimensional\n\nEven if the original state space \\mathcal{S} and action space \\mathcal{A} are finite, the augmented state space \\tilde{\\mathcal{S}} = \\mathcal{S} \\times \\mathbb{R}^{|\\mathcal{A}|} is uncountably infinite because each shock vector \\boldsymbol{\\epsilon} is a continuous random variable. Therefore:\n\nWe cannot enumerate the augmented states\n\nTabular dynamic programming methods do not apply directly\n\nThe augmented value function \\tilde{v}(s, \\boldsymbol{\\epsilon}) maps a continuous space to \\mathbb{R}\n\nThis motivates why we immediately marginalize over the shocks to obtain a finite-dimensional representation.","type":"content","url":"/regmdp#step-1-define-the-augmented-mdp-with-gumbel-noise","position":9},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Step 2: The Hard Bellman Equation on the Augmented State Space","lvl2":"Gumbel Noise on the Rewards"},"type":"lvl3","url":"/regmdp#step-2-the-hard-bellman-equation-on-the-augmented-state-space","position":10},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Step 2: The Hard Bellman Equation on the Augmented State Space","lvl2":"Gumbel Noise on the Rewards"},"content":"The Bellman optimality equation for the augmented MDP is:\\tilde{v}(s, \\boldsymbol{\\epsilon}) = \\max_{a \\in \\mathcal{A}_s} \\left\\{ r(s,a) + \\epsilon(a) + \\gamma \\mathbb{E}_{s', \\boldsymbol{\\epsilon}'}\\left[\\tilde{v}(s', \\boldsymbol{\\epsilon}') \\mid s, a\\right] \\right\\}\n\nHere the expectation is over the next augmented state (s', \\boldsymbol{\\epsilon}'), which includes both the next state s' \\sim p(\\cdot | s, a) and the next shock vector \\boldsymbol{\\epsilon}' \\sim p(\\cdot).\n\nThis is a perfectly well-defined Bellman equation, and an optimal stationary policy exists:\\pi(s, \\boldsymbol{\\epsilon}) \\in \\operatorname{argmax}_{a \\in \\mathcal{A}_s} \\left\\{ r(s,a) + \\epsilon(a) + \\gamma \\mathbb{E}_{s', \\boldsymbol{\\epsilon}'}\\left[\\tilde{v}(s', \\boldsymbol{\\epsilon}') \\mid s, a\\right] \\right\\}\n\nHowever, this equation is computationally intractable because:\n\nThe state space is continuous and infinite-dimensional\n\nThe shocks are fresh each period\n\nWe would need to solve for \\tilde{v} over an uncountable domain\n\nWe never solve this equation directly. Instead, we use it as a mathematical device to derive the smooth Bellman equation.","type":"content","url":"/regmdp#step-2-the-hard-bellman-equation-on-the-augmented-state-space","position":11},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Step 3: Define the Ex-Ante (Inclusive) Value Function","lvl2":"Gumbel Noise on the Rewards"},"type":"lvl3","url":"/regmdp#step-3-define-the-ex-ante-inclusive-value-function","position":12},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Step 3: Define the Ex-Ante (Inclusive) Value Function","lvl2":"Gumbel Noise on the Rewards"},"content":"The idea here is to consider the expected value before observing the current shocks. We define what some authors in econometrics call the inclusive value or ex-ante value:v(s) := \\mathbb{E}_{\\boldsymbol{\\epsilon}}\\big[\\tilde{v}(s, \\boldsymbol{\\epsilon})\\big]\n\nThis is the value of being in state s before we observe the current-period shock vector \\boldsymbol{\\epsilon}.\n\nTwo Different Value Functions\n\nIt is crucial to distinguish:\n\n\\tilde{v}(s, \\boldsymbol{\\epsilon}): the value after observing shocks (conditional on \\boldsymbol{\\epsilon}), defined on the augmented state space\n\nv(s): the value before observing shocks (marginalizing over \\boldsymbol{\\epsilon}), defined on the original state space\n\nThe function v(s) is what we actually compute and care about. The augmented value \\tilde{v} exists only as a proof device.","type":"content","url":"/regmdp#step-3-define-the-ex-ante-inclusive-value-function","position":13},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Step 4: Separate the Deterministic and Random Components","lvl2":"Gumbel Noise on the Rewards"},"type":"lvl3","url":"/regmdp#step-4-separate-the-deterministic-and-random-components","position":14},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Step 4: Separate the Deterministic and Random Components","lvl2":"Gumbel Noise on the Rewards"},"content":"Now we take the expectation of the augmented Bellman equation with respect to the current shocks only (everything that does not depend on the current \\boldsymbol{\\epsilon} can be pulled out).\n\nFirst, note that by the law of iterated expectations and independence of shocks across time:\\mathbb{E}_{\\boldsymbol{\\epsilon}'}\\big[\\tilde{v}(s', \\boldsymbol{\\epsilon}')\\big] = v(s')\n\nThis follows from our definition of v and the fact that the next shock is independent of everything else.\n\nNow define the deterministic part of the right-hand side:x_a(s) := r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j)\n\nThis is the expected return from taking action a in state s without the shock. Using this notation, the augmented Bellman equation becomes:\\tilde{v}(s, \\boldsymbol{\\epsilon}) = \\max_{a \\in \\mathcal{A}_s} \\left\\{ x_a(s) + \\epsilon(a) \\right\\}\n\nTaking the expectation over \\boldsymbol{\\epsilon} on both sides:v(s) = \\mathbb{E}_{\\boldsymbol{\\epsilon}}\\left[\\max_{a \\in \\mathcal{A}_s} \\left\\{ x_a(s) + \\epsilon(a) \\right\\}\\right]\n\nExpectation of a Max, Not Max of an Expectation\n\nNotice carefully: we have \\mathbb{E}[\\max(\\cdot)], not \\max \\mathbb{E}[\\cdot]. We are not swapping max and expectation.\n\nThe expression \\mathbb{E}_{\\boldsymbol{\\epsilon}}[\\max_a \\{x_a + \\epsilon(a)\\}] is the expected value of the maximum of Gumbel-perturbed utilities. The Gumbel random utility identity evaluates this quantity in closed form.","type":"content","url":"/regmdp#step-4-separate-the-deterministic-and-random-components","position":15},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Step 5: Apply the Gumbel Random Utility Identity","lvl2":"Gumbel Noise on the Rewards"},"type":"lvl3","url":"/regmdp#step-5-apply-the-gumbel-random-utility-identity","position":16},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Step 5: Apply the Gumbel Random Utility Identity","lvl2":"Gumbel Noise on the Rewards"},"content":"We now invoke a result from extreme value theory:\n\nGumbel Random Utility Identity\n\nLet \\epsilon_1, \\ldots, \\epsilon_m be i.i.d. \\mathrm{Gumbel}(\\mu_\\epsilon, 1/\\beta) random variables. For any deterministic values x_1, \\ldots, x_m \\in \\mathbb{R}:\\max_{i=1,\\ldots,m} \\{x_i + \\epsilon_i\\} \\overset{d}{=} \\frac{1}{\\beta} \\log \\sum_{i=1}^m \\exp(\\beta x_i) + \\zeta\n\nwhere \\zeta \\sim \\mathrm{Gumbel}(\\mu_\\epsilon, 1/\\beta) (same distribution as the original shocks).\n\nTaking expectations:\\mathbb{E}\\left[\\max_{i=1,\\ldots,m} \\{x_i + \\epsilon_i\\}\\right] = \\frac{1}{\\beta} \\log \\sum_{i=1}^m \\exp(\\beta x_i) + \\mu_\\epsilon + \\frac{\\gamma_E}{\\beta}\n\nwhere \\gamma_E \\approx 0.5772 is the Euler-Mascheroni constant.\n\nWith mean-zero shocks (\\mu_\\epsilon = -\\gamma_E/\\beta), the constant term vanishes:\\mathbb{E}\\left[\\max_{i=1,\\ldots,m} \\{x_i + \\epsilon_i\\}\\right] = \\frac{1}{\\beta} \\log \\sum_{i=1}^m \\exp(\\beta x_i)\n\nApplying this identity to our problem (with mean-zero shocks):v(s) = \\mathbb{E}_{\\boldsymbol{\\epsilon}}\\left[\\max_{a \\in \\mathcal{A}_s} \\{x_a(s) + \\epsilon(a)\\}\\right] = \\frac{1}{\\beta} \\log \\sum_{a \\in \\mathcal{A}_s} \\exp(\\beta x_a(s))\n\nSubstituting the definition of x_a(s):v(s) = \\frac{1}{\\beta} \\log \\sum_{a \\in \\mathcal{A}_s} \\exp\\left(\\beta\\left(r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j)\\right)\\right)\n\nWe have arrived at the smooth Bellman equation.","type":"content","url":"/regmdp#step-5-apply-the-gumbel-random-utility-identity","position":17},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Step 6: Summary of the Derivation","lvl2":"Gumbel Noise on the Rewards"},"type":"lvl3","url":"/regmdp#step-6-summary-of-the-derivation","position":18},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Step 6: Summary of the Derivation","lvl2":"Gumbel Noise on the Rewards"},"content":"To recap the logical flow:\n\nWe constructed an augmented MDP with state (s, \\boldsymbol{\\epsilon}) where shocks perturb rewards\n\nWe wrote the standard Bellman equation for this augmented MDP (hard max, but over an infinite-dimensional state space)\n\nWe defined the ex-ante value v(s) = \\mathbb{E}_{\\boldsymbol{\\epsilon}}[\\tilde{v}(s, \\boldsymbol{\\epsilon})] to eliminate the continuous shock component\n\nWe separated deterministic and random terms: \\tilde{v}(s, \\boldsymbol{\\epsilon}) = \\max_a \\{x_a(s) + \\epsilon(a)\\}\n\nWe applied the Gumbel identity to evaluate \\mathbb{E}_{\\boldsymbol{\\epsilon}}[\\max_a \\{\\cdots\\}] in closed form as a log-sum-exp\n\nThe augmented MDP with shocks exists only as a mathematical device. We never approximate \\tilde{v}, never discretize \\boldsymbol{\\epsilon}, and never enumerate the augmented state space. The only computational object we work with is v(s) on the original (finite) state space, which satisfies the smooth Bellman equation.","type":"content","url":"/regmdp#step-6-summary-of-the-derivation","position":19},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Deriving the Optimal Smooth Policy","lvl2":"Gumbel Noise on the Rewards"},"type":"lvl3","url":"/regmdp#deriving-the-optimal-smooth-policy","position":20},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Deriving the Optimal Smooth Policy","lvl2":"Gumbel Noise on the Rewards"},"content":"Now that we have derived the smooth value function, we can also obtain the corresponding optimal policy. The question is: what policy should we follow in the original MDP (without explicitly conditioning on shocks)?\n\nIn the augmented MDP, the optimal policy is deterministic but depends on the shock realization:\\pi(s, \\boldsymbol{\\epsilon}) \\in \\operatorname{argmax}_{a \\in \\mathcal{A}_s} \\left\\{ r(s,a) + \\epsilon(a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j) \\right\\}\n\nHowever, we want a policy for the original state space s (not the augmented state). We obtain this by marginalizing over the current shocks, essentially asking: “what is the probability that action a is optimal when we average over all possible shock realizations?”\n\nDefine an indicator function:I_a(\\boldsymbol{\\epsilon}) = \\begin{cases} \n   1 & \\text{if } a \\in \\operatorname{argmax}_{a' \\in \\mathcal{A}_s} \\left\\{ x_{a'}(s) + \\epsilon(a') \\right\\} \\\\\n   0 & \\text{otherwise}\n   \\end{cases}\n\nwhere x_a(s) = r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j) as before.\n\nThe ex-ante probability that action a is optimal at state s is:\\pi(a|s) = \\mathbb{E}_{\\boldsymbol{\\epsilon}}[I_a(\\boldsymbol{\\epsilon})] = \\mathbb{P}_{\\boldsymbol{\\epsilon}}\\left(a \\in \\operatorname{argmax}_{a'} \\left\\{ x_{a'}(s) + \\epsilon(a') \\right\\}\\right)\n\nThis is the probability that action a achieves the maximum when utilities are perturbed by Gumbel noise.\n\nGumbel-Max Probability (Softmax)\n\nLet \\epsilon_1, \\ldots, \\epsilon_m be i.i.d. \\mathrm{Gumbel}(\\mu_\\epsilon, 1/\\beta) random variables. For any deterministic values x_1, \\ldots, x_m \\in \\mathbb{R}, the probability that index i achieves the maximum is:\\mathbb{P}\\left(i \\in \\operatorname{argmax}_j \\{x_j + \\epsilon_j\\}\\right) = \\frac{\\exp(\\beta x_i)}{\\sum_{j=1}^m \\exp(\\beta x_j)}\n\nThis holds regardless of the location parameter \\mu_\\epsilon.\n\nApplying this result to our problem:\\pi(a|s) = \\frac{\\exp\\left(\\beta x_a(s)\\right)}{\\sum_{a' \\in \\mathcal{A}_s} \\exp\\left(\\beta x_{a'}(s)\\right)} = \\frac{\\exp\\left(\\beta\\left(r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a)v(j)\\right)\\right)}{\\sum_{a' \\in \\mathcal{A}_s} \\exp\\left(\\beta\\left(r(s,a') + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a')v(j)\\right)\\right)}\n\nThis is the softmax policy or Gibbs/Boltzmann policy with inverse temperature \\beta.\n\nProperties:\n\nAs \\beta \\to \\infty: the policy becomes deterministic, concentrating on the action(s) with highest x_a(s) (recovers standard greedy policy)\n\nAs \\beta \\to 0: the policy becomes uniform over all actions (maximum entropy)\n\nFor finite \\beta > 0: the policy is stochastic, with probability mass proportional to exponentiated Q-values\n\nThis completes the derivation: the smooth Bellman equation yields a value function v(s), and the corresponding optimal policy is the softmax over Q-values. ## Control as Inference Perspective\n\nThe smooth Bellman optimality equations can also be derived from probabilistic inference perspective. To see this, let's go back to the idea from the previous section in which we introduced an indicator function $I_a(\\epsilon)$ to represent whether an action $a$ is optimal given a particular realization of the noise $\\epsilon$:\n\n$$ I_a(\\epsilon) = \\begin{cases} \n   1 & \\text{if } a \\in \\operatorname{argmax}_{a' \\in \\mathcal{A}_s} \\left\\{ r(s,a') + \\epsilon(a') + \\gamma \\mathbb{E}_{s', \\epsilon'}\\left[v_\\gamma^\\star(s',\\epsilon')\\mid s, a'\\right] \\right\\} \\\\\n   0 & \\text{otherwise}\n   \\end{cases} $$\n\nWhen we took the expectation over the noise $\\epsilon$, we obtained a soft version of this indicator:\n\n$$ \\begin{align*}\n\\mathbb{E}_\\epsilon[I_a(\\epsilon)] &= \\mathbb{P}\\left(a \\in \\operatorname{argmax}_{a' \\in \\mathcal{A}_s} \\left\\{ r(s,a') + \\epsilon(a') + \\gamma \\mathbb{E}_{s', \\epsilon'}\\left[v_\\gamma^\\star(s',\\epsilon')\\mid s, \\epsilon, a'\\right] \\right\\}\\right) \\\\\n&= \\frac{\\exp\\left(\\beta\\left(r(s,a) + \\gamma \\mathbb{E}_{s'}\\left[v_\\gamma^\\star(s')\\mid s, a\\right]\\right)\\right)}{\\sum_{a' \\in \\mathcal{A}_s} \\exp\\left(\\beta\\left(r(s,a') + \\gamma \\mathbb{E}_{s'}\\left[v_\\gamma^\\star(s')\\mid s, a'\\right]\\right)\\right)}\n\\end{align*} $$\n\nGiven this indicator function, we can \"infer\" the optimal action in any state. This is the intuition and starting point behind the control as inference perspective in which we directly define a continuous-valued \"optimality\" variable $O_t$ at each time step $t$. We define the probability of optimality given a state-action pair as:\n\n$$ p(O_t = 1 | s_t, a_t) = \\exp(\\beta r(s_t, a_t)) $$\n\nBuilding on this notion of soft optimality, we can formulate the MDP as a probabilistic graphical model. We define the following probabilities:\n\n1. State transition probability: $p(s_{t+1} | s_t, a_t)$ (given by the MDP dynamics)\n2. Prior policy: $p(a_t | s_t)$ (which we'll assume to be uniform for simplicity)\n3. Optimality probability: $p(O_t = 1 | s_t, a_t) = \\exp(\\beta r(s_t, a_t))$\n\nThis formulation encodes the idea that more rewarding state-action pairs are more likely to be \"optimal,\" which directly parallels the soft assignment of optimality we obtained by taking the expectation over the Gumbel noise.\n\nThe control problem can now be framed as an inference problem: we want to find the posterior distribution over actions given that all time steps are optimal:\n\n$$ p(a_t | s_t, O_{1:T} = 1) $$\n\nwhere $O_{1:T} = 1$ means $O_t = 1$ for all $t$ from 1 to T. \n\n### Message Passing \n\nTo solve this inference problem, we can use a technique from probabilistic graphical models called message passing, specifically the belief propagation algorithm. Message passing is a way to efficiently compute marginal distributions in a graphical model by passing local messages between nodes. Messages are passed between nodes in both forward and backward directions. Each message represents a belief about the distribution of a variable, based on the information available to the sending node. After messages have been passed, each node updates its belief about its associated variable by combining all incoming messages.\n\nIn our specific case, we're particularly interested in the backward messages, which propagate information about future optimality backwards in time. Let's define the backward message $\\beta_t(s_t)$ as:\n\n$$ \\beta_t(s_t) = p(O_{t:T} = 1 | s_t) $$\n\nThis represents the probability of optimality for all future time steps given the current state. We can compute this recursively:\n\n$$ \\beta_t(s_t) = \\sum_{a_t} p(a_t | s_t) p(O_t = 1 | s_t, a_t) \\sum_{s_{t+1}} p(s_{t+1} | s_t, a_t) \\beta_{t+1}(s_{t+1}) $$\n\n\nTaking the log and assuming a uniform prior over actions, we get:\n\n$$ \\log \\beta_t(s_t) = \\log \\sum_{a_t} \\sum_{s_{t+1}} p(s_{t+1} | s_t, a_t) \\exp(\\beta (r(s_t, a_t) + \\gamma v(_{t+1}) + \\frac{1}{\\beta} \\log \\beta_{t+1}(s_{t+1}))) $$\n\nIf we define the soft value function as $V_t(s_t) = \\frac{1}{\\beta} \\log \\beta_t(s_t)$, we can rewrite the above equation as:\n\n$$ V_t(s_t) = \\frac{1}{\\beta} \\log \\sum_{a_t} \\sum_{s_{t+1}} p(s_{t+1} | s_t, a_t) \\exp(\\beta (r(s_t, a_t) + \\gamma V_{t+1}(s_{t+1}))) $$\n\nThis is exactly the smooth Bellman equation we derived earlier, but now interpreted as the result of probabilistic inference in a graphical model.\n\n### Deriving the Optimal Policy\n\nThe backward message recursion we derived earlier assumes a uniform prior policy $p(a_t | s_t)$. However, our goal is to find an optimal policy. We can extract this optimal policy efficiently by computing the posterior distribution over actions given our backward messages.\n\nStarting from the definition of conditional probability and applying Bayes' rule, we can write:\n\n$$ \\begin{align}\np(a_t | s_t, O_{1:T} = 1) &= \\frac{p(O_{1:T} = 1 | s_t, a_t) p(a_t | s_t)}{p(O_{1:T} = 1 | s_t)} \\\\\n&\\propto p(a_t | s_t) p(O_t = 1 | s_t, a_t) p(O_{t+1:T} = 1 | s_t, a_t) \\\\\n&= p(a_t | s_t) p(O_t = 1 | s_t, a_t) \\sum_{s_{t+1}} p(s_{t+1} | s_t, a_t) \\beta_{t+1}(s_{t+1})\n\\end{align} $$\n\nHere, $\\beta_{t+1}(s_{t+1}) = p(O_{t+1:T} = 1 | s_{t+1})$ is our backward message.\n\nNow, let's substitute our definitions for the optimality probability and the soft value function:\n\n$$ \\begin{align}\np(a_t | s_t, O_{1:T} = 1) &\\propto p(a_t | s_t) \\exp(\\beta r(s_t, a_t)) \\sum_{s_{t+1}} p(s_{t+1} | s_t, a_t) \\exp(\\beta \\gamma V_{t+1}(s_{t+1})) \\\\\n&= p(a_t | s_t) \\exp(\\beta (r(s_t, a_t) + \\gamma \\sum_{s_{t+1}} p(s_{t+1} | s_t, a_t) V_{t+1}(s_{t+1})))\n\\end{align} $$\n\nAfter normalization, and assuming a uniform prior $p(a_t | s_t)$, we obtain the randomized decision rule:\n\n$$ d(a_t | s_t) = \\frac{\\exp(\\beta (r(s_t, a_t) + \\gamma \\sum_{s_{t+1}} p(s_{t+1} | s_t, a_t) V_{t+1}(s_{t+1})))}{\\sum_{a'_t} \\exp(\\beta (r(s_t, a'_t) + \\gamma \\sum_{s_{t+1}} p(s_{t+1} | s_t, a'_t) V_{t+1}(s_{t+1})))} $$ ","type":"content","url":"/regmdp#deriving-the-optimal-smooth-policy","position":21},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl2":"Regularized Markov Decision Processes"},"type":"lvl2","url":"/regmdp#regularized-markov-decision-processes","position":22},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl2":"Regularized Markov Decision Processes"},"content":"Regularized MDPs \n\nGeist et al. (2019) provide another perspective on how the smooth Bellman equations come to be. This framework offers a more general approach in which we seek to find optimal policies under the infinite horizon criterion while also accounting for a regularizer that influences the kind of policies we try to obtain.\n\nLet’s set up some necessary notation. First, recall that the policy evaluation operator for a stationary policy with decision rule \\pi is defined as:\\mathrm{L}_\\pi \\mathbf{v} = \\mathbf{r}_\\pi + \\gamma \\mathbf{P}_\\pi \\mathbf{v}\n\nwhere \\mathbf{r}_\\pi is the expected reward vector under policy \\pi, \\gamma is the discount factor, and \\mathbf{P}_\\pi is the state transition probability matrix under \\pi. A complementary object to the value function is the q-function (or Q-factor) representation:\\begin{align*}\nq_\\gamma^{\\pi}(s, a) &= r(s, a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v_\\gamma^{\\pi}(j) \\\\\nv_\\gamma^{\\pi}(s) &= \\sum_{a \\in \\mathcal{A}_s} \\pi(a | s) q_\\gamma^{\\pi}(s, a) \n\\end{align*}\n\nThe policy evaluation operator can then be written in terms of the q-function as:[\\mathrm{L}_\\pi v](s) = \\langle \\pi(\\cdot | s), q(s, \\cdot) \\rangle","type":"content","url":"/regmdp#regularized-markov-decision-processes","position":23},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Legendre-Fenchel Transform","lvl2":"Regularized Markov Decision Processes"},"type":"lvl3","url":"/regmdp#legendre-fenchel-transform","position":24},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Legendre-Fenchel Transform","lvl2":"Regularized Markov Decision Processes"},"content":"The workhorse behind the theory of regularized MDPs is the Legendre-Fenchel transform, also known as the convex conjugate. For a strongly convex function \\Omega: \\Delta_{\\mathcal{A}} \\rightarrow \\mathbb{R}, its Legendre-Fenchel transform \\Omega^*: \\mathbb{R}^{\\mathcal{A}} \\rightarrow \\mathbb{R} is defined as:\\Omega^*(q(s, \\cdot)) = \\max_{\\pi(\\cdot|s) \\in \\Delta_{\\mathcal{A}}} \\langle \\pi(\\cdot | s), q(s, \\cdot) \\rangle - \\Omega(\\pi(\\cdot | s))\n\nAn important property of this transform is that it has a unique maximizing argument, given by the gradient of \\Omega^*. This gradient is Lipschitz and satisfies:\\nabla \\Omega^*(q(s, \\cdot)) = \\arg\\max_\\pi \\langle \\pi(\\cdot | s), q(s, \\cdot) \\rangle - \\Omega(\\pi(\\cdot | s))\n\nAn important example of a regularizer is the negative entropy, which gives rise to the smooth Bellman equations as we are about to see.","type":"content","url":"/regmdp#legendre-fenchel-transform","position":25},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl2":"Regularized Bellman Operators"},"type":"lvl2","url":"/regmdp#regularized-bellman-operators","position":26},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl2":"Regularized Bellman Operators"},"content":"With these concepts in place, we can now define the regularized Bellman operators:\n\nRegularized Policy Evaluation Operator (\\mathrm{L}_{\\pi,\\Omega}):[\\mathrm{L}_{\\pi,\\Omega} v](s) = \\langle q(s,\\cdot), \\pi(\\cdot | s) \\rangle - \\Omega(\\pi(\\cdot | s))\n\nRegularized Bellman Optimality Operator (\\mathrm{L}_\\Omega):[\\mathrm{L}_\\Omega v](s) = [\\max_\\pi \\mathrm{L}_{\\pi,\\Omega} v ](s) = \\Omega^*(q(s, \\cdot))\n\nIt can be shown that the addition of a regularizer in these regularized operators still preserves the contraction properties, and therefore the existence of a solution to the optimality equations and the convergence of successive approximation.\n\nThe regularized value function of a stationary policy with decision rule \\pi, denoted by v_{\\pi,\\Omega}, is the unique fixed point of the operator equation:\\text{find $v$ such that } \\enspace v = \\mathrm{L}_{\\pi,\\Omega} v\n\nUnder the usual assumptions on the discount factor and the boundedness of the reward, the value of a policy can also be found in closed form by solving for \\mathbf{v} in the linear system of equations:(\\mathbf{I} - \\gamma \\mathbf{P}_\\pi) \\mathbf{v} =  \\mathbf{r}_\\pi - \\boldsymbol{\\Omega}_\\pi\n\nwhere [\\boldsymbol{\\Omega}_\\pi](s) = \\Omega(\\pi(\\cdot|s)) is the vector of regularization terms at each state.\n\nThe associated state-action value function q_{\\pi,\\Omega} is given by:\\begin{align*}\nq_{\\pi,\\Omega}(s, a) &= r(s, a) + \\sum_{j \\in \\mathcal{S}} \\gamma p(j|s,a) v_{\\pi,\\Omega}(j) \\\\\nv_{\\pi,\\Omega}(s) &= \\sum_{a \\in \\mathcal{A}_s} \\pi(a | s) q_{\\pi,\\Omega}(s, a) - \\Omega(\\pi(\\cdot | s))\n\\end{align*}\n\nThe regularized optimal value function v^*_\\Omega is then the unique fixed point of \\mathrm{L}_\\Omega in the fixed point equation:\\text{find $v$ such that } v = \\mathrm{L}_\\Omega v\n\nThe associated state-action value function q^*_\\Omega is given by:\\begin{align*}\nq^*_\\Omega(s, a) &= r(s, a) + \\sum_{j \\in \\mathcal{S}} \\gamma p(j|s,a) v^*_\\Omega(j) \\\\\nv^*_\\Omega(s) &= \\Omega^*(q^*_\\Omega(s, \\cdot))\\end{align*}\n\nAn important result in the theory of regularized MDPs is that there exists a unique optimal regularized policy. Specifically, if \\pi^*_\\Omega is a conserving decision rule (i.e., \\pi^*_\\Omega = \\arg\\max_\\pi \\mathrm{L}_{\\pi,\\Omega} v^*_\\Omega), then the randomized stationary policy \\boldsymbol{\\pi} = \\mathrm{const}(\\pi^*_\\Omega) is the unique optimal regularized policy.\n\nIn practice, once we have found v^*_\\Omega, we can derive the optimal decision rule by taking the gradient of the convex conjugate evaluated at the optimal action-value function:\\pi^*(\\cdot | s) = \\nabla \\Omega^*(q^*_\\Omega(s, \\cdot))","type":"content","url":"/regmdp#regularized-bellman-operators","position":27},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Recovering the Smooth Bellman Equations","lvl2":"Regularized Bellman Operators"},"type":"lvl3","url":"/regmdp#recovering-the-smooth-bellman-equations","position":28},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Recovering the Smooth Bellman Equations","lvl2":"Regularized Bellman Operators"},"content":"Under this framework, we can recover the smooth Bellman equations by choosing \\Omega to be the negative entropy, and obtain the softmax policy as the gradient of the convex conjugate. Let’s show this explicitly:\n\nUsing the negative entropy regularizer:\\Omega(d(\\cdot|s)) = \\sum_{a \\in \\mathcal{A}_s} d(a|s) \\ln d(a|s)\n\nThe convex conjugate:\\Omega^*(q(s, \\cdot)) = \\ln \\sum_{a \\in \\mathcal{A}_s} \\exp q(s,a)\n\nNow, let’s write out the regularized Bellman optimality equation:v^*_\\Omega(s) = \\Omega^*(q^*_\\Omega(s, \\cdot))\n\nSubstituting the expressions for \\Omega^* and q^*_\\Omega:v^*_\\Omega(s) = \\ln \\sum_{a \\in \\mathcal{A}_s} \\exp \\left(r(s, a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v^*_\\Omega(j)\\right)\n\nThis matches the form of the smooth Bellman equation we derived earlier, with the log-sum-exp operation replacing the max operation of the standard Bellman equation.\n\nFurthermore, the optimal policy is given by the gradient of \\Omega^*:d^*(a|s) = \\nabla \\Omega^*(q^*_\\Omega(s, \\cdot)) = \\frac{\\exp(q^*_\\Omega(s,a))}{\\sum_{a' \\in \\mathcal{A}_s} \\exp(q^*_\\Omega(s,a'))}\n\nThis is the familiar softmax policy we encountered in the smooth MDP setting.","type":"content","url":"/regmdp#recovering-the-smooth-bellman-equations","position":29},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Smooth Policy Iteration Algorithm","lvl2":"Regularized Bellman Operators"},"type":"lvl3","url":"/regmdp#smooth-policy-iteration-algorithm","position":30},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Smooth Policy Iteration Algorithm","lvl2":"Regularized Bellman Operators"},"content":"Now that we’ve seen how the regularized MDP framework leads to smooth Bellman equations, we present smooth policy iteration. Unlike value iteration which directly iterates the Bellman operator, policy iteration alternates between policy evaluation and policy improvement steps.\n\nSmooth Policy Evaluation\n\nInput: MDP (S, A, r, p, \\gamma), policy \\pi, inverse temperature \\beta > 0, tolerance \\epsilon > 0\n\nOutput: Value function v^\\pi for policy \\pi\n\nInitialize v(s) \\leftarrow 0 for all s \\in S\n\nSet \\alpha \\leftarrow 1/\\beta\n\nrepeat\n\n\\quad \\Delta \\leftarrow 0\n\n\\quad for each state s \\in S do\n\n\\quad\\quad v_{\\text{old}} \\leftarrow v(s)\n\n\\quad\\quad for each action a \\in A_s do\n\n\\quad\\quad\\quad q(s,a) \\leftarrow r(s,a) + \\gamma \\sum_{j \\in S} p(j|s,a) v(j)\n\n\\quad\\quad end for\n\n\\quad\\quad Compute expected Q-value: \\bar{q} \\leftarrow \\sum_{a \\in A_s} \\pi(a|s) \\cdot q(s,a)\n\n\\quad\\quad Compute policy entropy: H \\leftarrow -\\sum_{a \\in A_s} \\pi(a|s) \\log \\pi(a|s)\n\n\\quad\\quad v(s) \\leftarrow \\bar{q} + \\alpha H\n\n\\quad\\quad \\Delta \\leftarrow \\max(\\Delta, |v(s) - v_{\\text{old}}|)\n\n\\quad end for\n\nuntil \\Delta < \\epsilon\n\nreturn v\n\nSmooth Policy Iteration\n\nInput: MDP (S, A, r, p, \\gamma), inverse temperature \\beta > 0, tolerance \\epsilon > 0\n\nOutput: Approximate optimal value function v and stochastic policy \\pi\n\nInitialize \\pi(a|s) \\leftarrow 1/|A_s| for all s \\in S, a \\in A_s (uniform policy)\n\nrepeat\n\n\\quad Policy Evaluation:\n\n\\quad\\quad v \\leftarrow SmoothPolicyEvaluation(S, A, r, p, \\gamma, \\pi, \\beta, \\epsilon)\n\n\\quad Policy Improvement:\n\n\\quad policy_stable \\leftarrow true\n\n\\quad for each state s \\in S do\n\n\\quad\\quad \\pi_{\\text{old}}(\\cdot|s) \\leftarrow \\pi(\\cdot|s)\n\n\\quad\\quad for each action a \\in A_s do\n\n\\quad\\quad\\quad q(s,a) \\leftarrow r(s,a) + \\gamma \\sum_{j \\in S} p(j|s,a) v(j)\n\n\\quad\\quad end for\n\n\\quad\\quad for each action a \\in A_s do\n\n\\quad\\quad\\quad \\pi(a|s) \\leftarrow \\frac{\\exp(\\beta \\cdot q(s,a))}{\\sum_{a' \\in A_s} \\exp(\\beta \\cdot q(s,a'))}\n\n\\quad\\quad end for\n\n\\quad\\quad if \\|\\pi(\\cdot|s) - \\pi_{\\text{old}}(\\cdot|s)\\| > \\epsilon then\n\n\\quad\\quad\\quad policy_stable \\leftarrow false\n\n\\quad\\quad end if\n\n\\quad end for\n\nuntil policy_stable\n\nreturn v, \\pi\n\nKey properties of smooth policy iteration:\n\nEntropy-regularized evaluation: The policy evaluation step (line 12 of Algorithm \n\nAlgorithm 2) accounts for the entropy bonus \\alpha H(\\pi(\\cdot|s)) where \\alpha = 1/\\beta\n\nStochastic policy improvement: The policy improvement step (lines 12-14 of Algorithm \n\nAlgorithm 3) uses softmax instead of deterministic argmax, producing a stochastic policy\n\nTemperature parameter:\n\nHigher \\beta → policies closer to deterministic (lower entropy)\n\nLower \\beta → more stochastic policies (higher entropy)\n\nAs \\beta \\to \\infty → recovers standard policy iteration\n\nConvergence: Like standard policy iteration, this algorithm converges to the unique optimal regularized value function and policy","type":"content","url":"/regmdp#smooth-policy-iteration-algorithm","position":31},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Equivalence Between Smooth Bellman Equations and Entropy-Regularized MDPs","lvl2":"Regularized Bellman Operators"},"type":"lvl3","url":"/regmdp#equivalence-between-smooth-bellman-equations-and-entropy-regularized-mdps","position":32},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Equivalence Between Smooth Bellman Equations and Entropy-Regularized MDPs","lvl2":"Regularized Bellman Operators"},"content":"We have now seen two distinct ways to arrive at smooth Bellman equations. Earlier in this chapter, we introduced the logsumexp operator as a smooth approximation to the max operator, motivated by analytical tractability and the desire for differentiability. Just now, we derived the same equations through the lens of regularized MDPs, where we explicitly penalize the entropy of policies. These two perspectives are mathematically equivalent: solving the smooth Bellman equation with inverse temperature parameter \\beta yields exactly the same optimal value function and optimal policy as solving the entropy-regularized MDP with regularization strength \\alpha = 1/\\beta. The two formulations are not merely similar. They describe identical optimization problems.\n\nTo see this equivalence clearly, consider the standard MDP problem with rewards r(s,a) and transition probabilities p(j|s,a). The regularized MDP framework tells us to solve:\\max_\\pi \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t r(s_t, a_t) \\right] + \\alpha \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t H(\\pi(\\cdot|s_t)) \\right],\n\nwhere H(\\pi(\\cdot|s)) = -\\sum_a \\pi(a|s) \\ln \\pi(a|s) is the entropy of the policy at state s, and \\alpha > 0 is the entropy regularization strength.\n\nWe can rewrite this objective by absorbing the entropy term into a modified reward function. Define the entropy-augmented reward:\\tilde{r}(s,a,\\pi) = r(s,a) + \\alpha H(\\pi(\\cdot|s)).\n\nHowever, this formulation makes the reward depend on the entire policy at each state, which is awkward. We can reformulate this more cleanly by expanding the entropy term. Recall that the entropy is:H(\\pi(\\cdot|s)) = -\\sum_a \\pi(a|s) \\ln \\pi(a|s).\n\nWhen we take the expectation over actions drawn from \\pi, we have:\\mathbb{E}_{a \\sim \\pi(\\cdot|s)} [H(\\pi(\\cdot|s))] = \\sum_a \\pi(a|s) \\left[-\\sum_{a'} \\pi(a'|s) \\ln \\pi(a'|s)\\right] = -\\sum_{a'} \\pi(a'|s) \\ln \\pi(a'|s),\n\nsince the entropy doesn’t depend on which action is actually sampled. But we can also write this as:H(\\pi(\\cdot|s)) = -\\sum_a \\pi(a|s) \\ln \\pi(a|s) = \\mathbb{E}_{a \\sim \\pi(\\cdot|s)}[-\\ln \\pi(a|s)].\n\nThis shows that adding \\alpha H(\\pi(\\cdot|s)) to the expected reward at state s is equivalent to adding -\\alpha \\ln \\pi(a|s) to the reward of taking action a at state s. More formally:\\begin{align*}\n&\\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t r(s_t, a_t) \\right] + \\alpha \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t H(\\pi(\\cdot|s_t)) \\right] \\\\\n&= \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t r(s_t, a_t) \\right] + \\alpha \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t \\mathbb{E}_{a_t \\sim \\pi(\\cdot|s_t)}[-\\ln \\pi(a_t|s_t)] \\right] \\\\\n&= \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t \\left( r(s_t, a_t) - \\alpha \\ln \\pi(a_t|s_t) \\right) \\right].\n\\end{align*}\n\nThe entropy bonus at each state, when averaged over the policy, becomes a per-action penalty proportional to the negative log probability of the action taken. This reformulation is more useful because the modified reward now depends only on the state, the action taken, and the probability assigned to that specific action by the policy, not on the entire distribution over actions.\n\nThis expression shows that entropy regularization is equivalent to adding a state-action dependent penalty term -\\alpha \\ln \\pi(a|s) to the reward. Intuititively, this terms amounts to paying a cost for low-entropy (deterministic) policies.\n\nNow, when we write down the Bellman equation for this entropy-regularized problem, at each state s we need to find the decision rule d(\\cdot|s) \\in \\Delta(\\mathcal{A}_s) (a probability distribution over actions) that maximizes:v(s) = \\max_{d(\\cdot|s) \\in \\Delta(\\mathcal{A}_s)} \\sum_a d(a|s) \\left[ r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j) - \\alpha \\ln d(a|s) \\right].\n\nHere \\Delta(\\mathcal{A}_s) = \\{d(\\cdot|s) : d(a|s) \\geq 0, \\sum_a d(a|s) = 1\\} denotes the probability simplex over actions available at state s. The optimization is over randomized decision rules at each state, constrained to be valid probability distributions.\n\nThis is a convex optimization problem with a linear constraint. We form the Lagrangian:\\mathcal{L}(d, \\lambda) = \\sum_a d(a|s) \\left[ r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j) - \\alpha \\ln d(a|s) \\right] - \\lambda \\left(\\sum_a d(a|s) - 1\\right),\n\nwhere \\lambda is the Lagrange multiplier enforcing the normalization constraint. Taking the derivative with respect to d(a|s) and setting it to zero:r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j) - \\alpha(1 + \\ln d^*(a|s)) - \\lambda = 0.\n\nSolving for d^*(a|s):d^*(a|s) = \\exp\\left(\\frac{1}{\\alpha}\\left(r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j) - \\lambda\\right)\\right).\n\nUsing the normalization constraint \\sum_a d^*(a|s) = 1 to solve for \\lambda:\\sum_a \\exp\\left(\\frac{1}{\\alpha}\\left(r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j)\\right)\\right) = \\exp\\left(\\frac{\\lambda}{\\alpha}\\right).\n\nTherefore:\\lambda = \\alpha \\ln \\sum_a \\exp\\left(\\frac{1}{\\alpha}\\left(r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j)\\right)\\right).\n\nSubstituting this back into the Bellman equation and simplifying:v(s) = \\alpha \\ln \\sum_a \\exp\\left(\\frac{1}{\\alpha}\\left(r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j)\\right)\\right).\n\nSetting \\beta = 1/\\alpha (the inverse temperature), this becomes:v(s) = \\frac{1}{\\beta} \\ln \\sum_a \\exp\\left(\\beta\\left(r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j)\\right)\\right).\n\nWe recover the smooth Bellman equation we derived earlier using the logsumexp operator. The inverse temperature parameter \\beta controls how closely the logsumexp approximates the max: as \\beta \\to \\infty, we recover the standard Bellman equation, while for finite \\beta, we have a smooth approximation that corresponds to optimizing with entropy regularization strength \\alpha = 1/\\beta.\n\nThe optimal policy is:\\pi^*(a|s) = \\frac{\\exp\\left(\\beta q^*(s,a)\\right)}{\\sum_{a'} \\exp\\left(\\beta q^*(s,a')\\right)} = \\text{softmax}_\\beta(q^*(s,\\cdot))(a),\n\nwhich is exactly the softmax policy parametrized by inverse temperature.\n\nThe derivation establishes the complete equivalence: the value function v^* that solves the smooth Bellman equation is identical to the optimal value function v^*_\\Omega of the entropy-regularized MDP (with \\Omega being negative entropy and \\alpha = 1/\\beta), and the softmax policy that is greedy with respect to this value function achieves the maximum of the entropy-regularized objective. Both approaches yield the same numerical solution: the same values at every state and the same policy prescriptions. The only difference is how we conceptualize the problem: as smoothing the Bellman operator for computational tractability, or as explicitly trading off reward maximization against policy entropy.\n\nThis equivalence has important implications. When we use smooth Bellman equations with a logsumexp operator, we are implicitly solving an entropy-regularized MDP. Conversely, when we explicitly add entropy regularization to an MDP objective, we arrive at smooth Bellman equations as the natural description of optimality. This dual perspective will prove valuable in understanding various algorithms and theoretical results. For instance, in soft actor-critic methods and other maximum entropy reinforcement learning algorithms, the connection between smooth operators and entropy regularization provides both computational benefits (differentiability) and conceptual clarity (why we want stochastic policies).","type":"content","url":"/regmdp#equivalence-between-smooth-bellman-equations-and-entropy-regularized-mdps","position":33},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Entropy-Regularized Dynamic Programming Algorithms","lvl2":"Regularized Bellman Operators"},"type":"lvl3","url":"/regmdp#entropy-regularized-dynamic-programming-algorithms","position":34},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Entropy-Regularized Dynamic Programming Algorithms","lvl2":"Regularized Bellman Operators"},"content":"While the smooth Bellman equations (using logsumexp) and entropy-regularized formulations are mathematically equivalent, it is instructive to present the algorithms explicitly in the entropy-regularized form, where the entropy bonus appears directly in the update equations.\n\nEntropy-Regularized Value Iteration\n\nInput: MDP (S, A, r, p, \\gamma), entropy weight \\alpha > 0, tolerance \\epsilon > 0\n\nOutput: Approximate optimal value function v and stochastic policy \\pi\n\nInitialize \\pi(a|s) \\leftarrow 1/|A_s| for all s \\in S, a \\in A_s (uniform policy)\n\nInitialize v(s) \\leftarrow 0 for all s \\in S\n\nrepeat\n\n\\quad \\Delta \\leftarrow 0\n\n\\quad for each state s \\in S do\n\n\\quad\\quad Policy Improvement: Update policy for current value estimate\n\n\\quad\\quad for each action a \\in A_s do\n\n\\quad\\quad\\quad q(s,a) \\leftarrow r(s,a) + \\gamma \\sum_{j \\in S} p(j|s,a) v(j)\n\n\\quad\\quad end for\n\n\\quad\\quad for each action a \\in A_s do\n\n\\quad\\quad\\quad \\pi_{\\text{new}}(a|s) \\leftarrow \\frac{\\exp(q(s,a)/\\alpha)}{\\sum_{a' \\in A_s} \\exp(q(s,a')/\\alpha)}\n\n\\quad\\quad end for\n\n\\quad\\quad Value Update: Compute regularized value\n\n\\quad\\quad v_{\\text{new}}(s) \\leftarrow \\sum_{a \\in A_s} \\pi_{\\text{new}}(a|s) \\cdot q(s,a) + \\alpha H(\\pi_{\\text{new}}(\\cdot|s))\n\n\\quad\\quad where H(\\pi_{\\text{new}}(\\cdot|s)) = -\\sum_{a \\in A_s} \\pi_{\\text{new}}(a|s) \\log \\pi_{\\text{new}}(a|s)\n\n\\quad\\quad \\Delta \\leftarrow \\max(\\Delta, |v_{\\text{new}}(s) - v(s)|)\n\n\\quad\\quad v(s) \\leftarrow v_{\\text{new}}(s)\n\n\\quad\\quad \\pi(\\cdot|s) \\leftarrow \\pi_{\\text{new}}(\\cdot|s)\n\n\\quad end for\n\nuntil \\Delta < \\epsilon\n\nreturn v, \\pi\n\nFeatures:\n\nLine 11 updates the policy using the softmax of Q-values, with temperature \\alpha\n\nLine 14 explicitly computes the entropy-regularized value: expected Q-value plus entropy bonus\n\nThe algorithm maintains and updates a stochastic policy throughout\n\nAs \\alpha \\to 0 (or equivalently \\beta \\to \\infty), this recovers standard value iteration\n\nEntropy-Regularized Policy Iteration\n\nInput: MDP (S, A, r, p, \\gamma), entropy weight \\alpha > 0, tolerance \\epsilon > 0\n\nOutput: Approximate optimal value function v and stochastic policy \\pi\n\nInitialize \\pi(a|s) \\leftarrow 1/|A_s| for all s \\in S, a \\in A_s (uniform policy)\n\nrepeat\n\n\\quad Policy Evaluation: Solve for v^\\pi such that for all s \\in S:\n\n\\quad\\quad Option 1 (Iterative):\n\n\\quad\\quad Initialize v(s) \\leftarrow 0 for all s \\in S\n\n\\quad\\quad repeat\n\n\\quad\\quad\\quad for each state s \\in S do\n\n\\quad\\quad\\quad\\quad Compute q^\\pi(s,a) \\leftarrow r(s,a) + \\gamma \\sum_{j \\in S} p(j|s,a) v(j) for all a \\in A_s\n\n\\quad\\quad\\quad\\quad v_{\\text{new}}(s) \\leftarrow \\sum_{a \\in A_s} \\pi(a|s) \\cdot q^\\pi(s,a) + \\alpha H(\\pi(\\cdot|s))\n\n\\quad\\quad\\quad end for\n\n\\quad\\quad\\quad if \\max_s |v_{\\text{new}}(s) - v(s)| < \\epsilon then break\n\n\\quad\\quad\\quad v \\leftarrow v_{\\text{new}}\n\n\\quad\\quad until convergence\n\n\\quad\\quad Option 2 (Direct): Solve linear system (\\mathbf{I} - \\gamma \\mathbf{P}_\\pi) \\mathbf{v} = \\mathbf{r}_\\pi + \\alpha \\mathbf{H}_\\pi\n\n\\quad\\quad where [\\mathbf{r}_\\pi](s) = \\sum_a \\pi(a|s) r(s,a) and [\\mathbf{H}_\\pi](s) = H(\\pi(\\cdot|s))\n\n\\quad Policy Improvement:\n\n\\quad policy_changed \\leftarrow false\n\n\\quad for each state s \\in S do\n\n\\quad\\quad \\pi_{\\text{old}}(\\cdot|s) \\leftarrow \\pi(\\cdot|s)\n\n\\quad\\quad for each action a \\in A_s do\n\n\\quad\\quad\\quad q(s,a) \\leftarrow r(s,a) + \\gamma \\sum_{j \\in S} p(j|s,a) v(j)\n\n\\quad\\quad end for\n\n\\quad\\quad for each action a \\in A_s do\n\n\\quad\\quad\\quad \\pi(a|s) \\leftarrow \\frac{\\exp(q(s,a)/\\alpha)}{\\sum_{a' \\in A_s} \\exp(q(s,a')/\\alpha)}\n\n\\quad\\quad end for\n\n\\quad\\quad if \\|\\pi(\\cdot|s) - \\pi_{\\text{old}}(\\cdot|s)\\| > \\epsilon then\n\n\\quad\\quad\\quad policy_changed \\leftarrow true\n\n\\quad\\quad end if\n\n\\quad end for\n\nuntil policy_changed = false\n\nreturn v, \\pi\n\nFeatures:\n\nPolicy Evaluation (lines 3-15): Computes the value of the current policy including entropy bonus\n\nOption 1: Iterative method (successive approximation)\n\nOption 2: Direct solution via linear system\n\nPolicy Improvement (lines 16-29): Updates policy to softmax over Q-values\n\nLine 14 shows the vector form: the linear system includes the entropy vector \\mathbf{H}_\\pi\n\nThe algorithm alternates between evaluating the current stochastic policy and improving it\n\nConverges to the unique optimal entropy-regularized policy","type":"content","url":"/regmdp#entropy-regularized-dynamic-programming-algorithms","position":35},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming"},"type":"lvl1","url":"/simadp","position":0},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming"},"content":"In the previous chapter, we developed the projection method framework for solving functional equations. We saw how to transform the infinite-dimensional problem of finding v such that \\mathrm{L}v = v into a finite-dimensional one by choosing basis functions and projection conditions (least squares, Galerkin, collocation). The main idea was to make the residual R(s) = \\mathrm{L}v(s) - v(s) “small” according to some criterion—whether by minimizing its squared norm, requiring orthogonality to basis functions, or forcing it to vanish at specific collocation points.\n\nHowever, we left one critical question unresolved: how do we actually evaluate the Bellman operator at a state s? Recall that applying \\mathrm{L} requires computing an expectation:(\\mathrm{L} v)(s) = \\max_{a \\in \\mathcal{A}_s} \\left\\{r(s,a) + \\gamma \\int v(s')p(ds'|s,a)\\right\\}\n\nIn our discussion of projection methods, we remained agnostic about numerical integration—we didn’t commit to a specific technique for evaluating this integral. When the state space is discrete and small, this expectation is simply a finite sum we can compute exactly. When the state space is continuous or very large, we face a computational challenge.\n\nThis chapter addresses that challenge by introducing Monte Carlo integration as our method for approximating expectations. Rather than requiring explicit knowledge of the transition probability function p(ds'|s,a) and using deterministic quadrature rules, we will work with samples—observed next states drawn from the transition dynamics. This is the defining feature of what the operations research community calls simulation-based approximate dynamic programming: the combination of projection methods (to handle infinite-dimensional function spaces) with Monte Carlo integration (to handle intractable expectations via sampling).\n\nThis same combination is precisely what the machine learning community recognizes as reinforcement learning. By relying on samples rather than exact probabilities, we move from planning with a known model to learning from data. The algorithms we develop will work in settings where we can simulate or interact with the system but may not have access to—or may not want to use—explicit transition probabilities. This is the bridge from the model-based dynamic programming of earlier chapters to the data-driven learning setting that defines modern RL.","type":"content","url":"/simadp","position":1},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl3":"Discretization and Numerical Quadrature"},"type":"lvl3","url":"/simadp#discretization-and-numerical-quadrature","position":2},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl3":"Discretization and Numerical Quadrature"},"content":"Recall that the Bellman operator for continuous state spaces takes the form:(\\mathrm{L} v)(s) \\equiv \\max_{a \\in \\mathcal{A}_s} \\left\\{r(s,a) + \\gamma \\int v(s')p(ds'|s,a)\\right\\}, \\, \\forall s \\in \\mathcal{S}\n\nTo make this computationally tractable for continuous state spaces, we can discretize the state space and approximate the integral over this discretized space. While this allows us to evaluate the Bellman operator componentwise, we must first decide how to represent value functions in this discretized setting.\n\nWhen working with discretized representations, we partition the state space into N_s cells with centers at grid points \\{s_i\\}_{i=1}^{N_s}. We then work with value functions that are piecewise constant on each cell: ie. for any s \\in S: v(s) = v(s_{k(s)}) where k(s) is the index of the cell containing s. We denote the discretized reward function by\n r_h(s, a) \\equiv r(s_{k(s)}, a) .\n\nFor transition probabilities, we need to be more careful. While we similarly map any state-action pair to its corresponding cell, we must ensure that integrating over the discretized transition function yields a valid probability distribution. We achieve this by normalizing:p_h(s'|s, a) \\equiv \\frac{p(s_{k(s')}|s_{k(s)}, a)}{\\int p(s_{k(s')}|s_{k(s)}, a) ds'}\n\nAfter defining our discretized reward and transition probability functions, we can write down our discretized Bellman operator.\nWe start with the Bellman operator using our discretized functions r_h and p_h. While these functions map to grid points, they’re still defined over continuous spaces - we haven’t yet dealt with the computational challenge of the integral. With this discretization approach, the value function is piecewise constant over cells. This lets us express the integral as a sum over cells, where each cell’s contribution is the probability of transitioning to that cell multiplied by the value at that cell’s grid point:\\begin{aligned}\n(\\widehat{\\mathrm{L}}_h v)(s) &= \\max_{k=1,\\ldots,N_a} \\left\\{r_h(s, a_k) + \\gamma \\int v(s')p_h(s'|s, a_k)ds'\\right\\} \\\\\n&= \\max_{k=1,\\ldots,N_a} \\left\\{r_h(s, a_k) + \\gamma \\int v(s_{k(s')})p_h(s'|s, a_k)ds'\\right\\} \\\\\n&= \\max_{k=1,\\ldots,N_a} \\left\\{r_h(s, a_k) + \\gamma \\sum_{i=1}^{N_s} v(s_i) \\int_{cell_i} p_h(s'|s, a_k)ds'\\right\\} \\\\\n&= \\max_{k=1,\\ldots,N_a} \\left\\{r_h(s, a_k) + \\gamma \\sum_{i=1}^{N_s} v(s_i)p_h(s_i|s_{k(s)}, a_k)\\right\\}\n\\end{aligned}\n\nThis form makes clear how discretization converts our continuous-space problem into a finite computation: we’ve replaced integration over continuous space with summation over grid points. The price we pay is that the number of terms in our sum grows exponentially with the dimension of our state space - the familiar curse of dimensionality.","type":"content","url":"/simadp#discretization-and-numerical-quadrature","position":3},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl3":"Monte Carlo Integration"},"type":"lvl3","url":"/simadp#monte-carlo-integration","position":4},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl3":"Monte Carlo Integration"},"content":"Numerical quadrature methods scale poorly with increasing dimension. Specifically, for a fixed error tolerance \\epsilon, the number of required quadrature points grows exponentially with dimension d as O\\left(\\left(\\frac{1}{\\epsilon}\\right)^d\\right). Furthermore, quadrature methods require explicit evaluation of the transition probability function p(s'|s,a) at specified points—a luxury we don’t have in the “model-free” setting where we only have access to samples from the MDP.\n\nLet \\mathcal{B} = \\{s_1, \\ldots, s_M\\} be our set of base points where we will evaluate the operator. At each base point s_k \\in \\mathcal{B}, Monte Carlo integration approximates the expectation using N samples:\\int v_n(s')p(ds'|s_k,a) \\approx \\frac{1}{N} \\sum_{i=1}^N v_n(s'_{k,i}), \\quad s'_{k,i} \\sim p(\\cdot|s_k,a)\n\nwhere s'_{k,i} denotes the i-th sample drawn from p(\\cdot|s_k,a) for base point s_k. This approach has two properties making it particularly attractive for high-dimensional problems and model-free settings:\n\nThe convergence rate is O\\left(\\frac{1}{\\sqrt{N}}\\right) regardless of the number of dimensions\n\nIt only requires samples from p(\\cdot|s_k,a), not explicit probability values\n\nUsing this approximation of the expected value over the next state, we can define a new “empirical” Bellman optimality operator:(\\widehat{\\mathrm{L}}_N v)(s_k) \\equiv \\max_{a \\in \\mathcal{A}_{s_k}} \\left\\{r(s_k,a) + \\frac{\\gamma}{N} \\sum_{i=1}^N v(s'_{k,i})\\right\\}, \\quad s'_{k,i} \\sim p(\\cdot|s_k,a)\n\nfor each s_k \\in \\mathcal{B}. A direct adaptation of the successive approximation method for this empirical operator leads to:\n\nMonte Carlo Value Iteration\n\nInput: MDP (S, A, P, R, \\gamma), , number of samples N, tolerance \\varepsilon > 0, maximum iterations K\nOutput: Value function v\n\nInitialize v_0(s) = 0 for all s \\in S\n\ni \\leftarrow 0\n\nrepeat\n\nFor each s \\in \\mathcal{B}:\n\nDraw s'_{j} \\sim p(\\cdot|s,a) for j = 1,\\ldots,N\n\nSet v_{i+1}(s) \\leftarrow \\max_{a \\in A} \\left\\{r(s,a) + \\frac{\\gamma}{N} \\sum_{j=1}^N v_i(s'_{j})\\right\\}\n\n\\delta \\leftarrow \\|v_{i+1} - v_i\\|_{\\infty}\n\ni \\leftarrow i + 1\n\nuntil \\delta < \\varepsilon or i \\geq K\n\nreturn final v_{i+1}\n\nNote that the original error bound derived as a termination criterion for value iteration need not hold in this approximate setting. Hence, we use a generic termination criterion based on computational budget and desired tolerance. While this aspect could be improved, we’ll focus on a more pressing matter: the algorithm’s tendency to produce upwardly biased values. In other words, this algorithm “thinks” the world is more rosy than it actually is - it overestimates values.","type":"content","url":"/simadp#monte-carlo-integration","position":5},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl4":"Overestimation Bias in Monte Carlo Value Iteration","lvl3":"Monte Carlo Integration"},"type":"lvl4","url":"/simadp#overestimation-bias-in-monte-carlo-value-iteration","position":6},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl4":"Overestimation Bias in Monte Carlo Value Iteration","lvl3":"Monte Carlo Integration"},"content":"In statistics, bias refers to a systematic error where an estimator consistently deviates from the true parameter value. For an estimator \\hat{\\theta} of a parameter \\theta, we define bias as: \\text{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta. While bias isn’t always problematic — sometimes we deliberately introduce bias to reduce variance, as in ridge regression — uncontrolled bias can lead to significantly distorted results. In the context of value iteration, this distortion gets amplified even more due to the recursive nature of the algorithm.\n\nConsider how the Bellman operator works in value iteration. At iteration n, we have a value function estimate v_i(s) and aim to improve it by applying the Bellman operator \\mathrm{L}. The ideal update would be:(\\mathrm{{L}}v_i)(s) = \\max_{a \\in \\mathcal{A}(s)} \\left\\{ r(s,a) + \\gamma \\int v_i(s') p(ds'|s,a) \\right\\}\n\nHowever, we can’t compute this integral exactly and use Monte Carlo integration instead, drawing N next-state samples for each state and action pair. The bias emerges when we take the maximum over actions:(\\widehat{\\mathrm{L}}v_i)(s) = \\max_{a \\in \\mathcal{A}(s)} \\hat{q}_i(s,a), \\enspace \\text{where} \\enspace \\hat{q}_i(s,a) \\equiv r(s,a) + \\frac{\\gamma}{N} \\sum_{j=1}^N v_i(s'_j), \\quad s'_j \\sim p(\\cdot|s,a)\n\nWhite the Monte Carlo estimate \\hat{q}_n(s,a) is unbiased for any individual action, the empirical Bellman operator is biased upward due to Jensen’s inequality, which states that for any convex function f, we have \\mathbb{E}[f(X)] \\geq f(\\mathbb{E}[X]). Since the maximum operator is convex, this implies:\\mathbb{E}[(\\widehat{\\mathrm{L}}v_i)(s)] = \\mathbb{E}\\left[\\max_{a \\in \\mathcal{A}(s)} \\hat{q}_i(s,a)\\right] \\geq \\max_{a \\in \\mathcal{A}(s)} \\mathbb{E}[\\hat{q}_i(s,a)] = (\\mathrm{L}v_i)(s)\n\nThis means that our Monte Carlo approximation of the Bellman operator is biased upward:b_i(s) = \\mathbb{E}[(\\widehat{\\mathrm{L}}v_i)(s)] - (\\mathrm{L}v_i)(s) \\geq 0\n\nEven worse, this bias compounds through iterations as each new value function estimate v_{n+1} is based on targets generated by the biased operator \\widehat{\\mathrm{L}}, creating a nested structure of bias accumulation.\nThis bias remains nonnegative at every step, and each application of the Bellman operator potentially adds more upward bias. As a result, instead of converging to the true value function v^*, the algorithm typically stabilizes at a biased approximation that systematically overestimates true values.","type":"content","url":"/simadp#overestimation-bias-in-monte-carlo-value-iteration","position":7},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl4":"The Keane-Wolpin Bias Correction Algorithm","lvl3":"Monte Carlo Integration"},"type":"lvl4","url":"/simadp#the-keane-wolpin-bias-correction-algorithm","position":8},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl4":"The Keane-Wolpin Bias Correction Algorithm","lvl3":"Monte Carlo Integration"},"content":"Keane and Wolpin proposed to de-bias such estimators by essentially “learning” the bias, then subtracting it when computing the empirical Bellman operator. If we knew this bias function, we could subtract it from our empirical estimate to get an unbiased estimate of the true Bellman operator:(\\widehat{\\mathrm{L}}v_n)(s) - \\text{bias}(s) = (\\widehat{\\mathrm{L}}v_n)(s) - (\\mathbb{E}[(\\widehat{\\mathrm{L}}v_n)(s)] - (\\mathrm{L}v_n)(s)) \\approx (\\mathrm{L}v_n)(s)\n\nThis equality holds in expectation, though any individual estimate would still have variance around the true value.\n\nSo how can we estimate the bias function? The Keane-Wolpin manages this using an important fact from extreme value theory: for normal random variables, the difference between the expected maximum and maximum of expectations scales with the standard deviation:\\mathbb{E}\\left[\\max_{a \\in \\mathcal{A}} \\hat{q}_i(s,a)\\right] - \\max_{a \\in \\mathcal{A}} \\mathbb{E}[\\hat{q}_i(s,a)] \\approx c \\cdot \\sqrt{\\max_{a \\in \\mathcal{A}} \\text{Var}_i(s,a)}\n\nThe variance term \\max_{a \\in \\mathcal{A}} \\text{Var}_i(s,a) will typically be dominated by the action with the largest value -- the greedy action a^*_i(s). Rather than deriving the constant c theoretically, Keane-Wolpin proposed learning the relationship between variance and bias empirically through these steps:\n\nSelect a small set of “benchmark” states (typically 20-50) that span the state space\n\nFor these states, compute more accurate value estimates using many more Monte Carlo samples (10-100x more than usual)\n\nCompute the empirical bias at each benchmark state s:\\hat{b}_i(s) = (\\hat{\\mathrm{L}}v_i)(s) - (\\hat{\\mathrm{L}}_{\\text{accurate}}v_i)(s)\n\nFit a linear relationship between this bias and the variance at the greedy action:\\hat{b}_i(s) = \\alpha_i \\cdot \\text{Var}_i(s,a^*_i(s)) + \\epsilon\n\nThis creates a dataset of pairs (\\text{Var}_i(s,a^*_i(s)), \\hat{b}_i(s)) that can be used to estimate \\alpha_i through ordinary least squares regression. Once we have learned this bias function \\hat{b}, we can define the bias-corrected Bellman operator:(\\widetilde{\\mathrm{L}}v_i)(s) \\triangleq (\\hat{\\mathrm{L}}v_i)(s) - \\hat{b}(s)\n\nWhile this bias correction approach has been influential in econometrics, it hasn’t gained much traction in the machine learning community. A major drawback is the need for accurate operator estimation at benchmark states, which requires allocating substantially more samples to these states. In the next section, we’ll explore an alternative strategy that, while requiring the maintenance of two sets of value estimates, achieves bias correction without demanding additional samples.","type":"content","url":"/simadp#the-keane-wolpin-bias-correction-algorithm","position":9},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl4":"Decoupling Selection and Evaluation","lvl3":"Monte Carlo Integration"},"type":"lvl4","url":"/simadp#decoupling-selection-and-evaluation","position":10},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl4":"Decoupling Selection and Evaluation","lvl3":"Monte Carlo Integration"},"content":"A simpler approach to addressing the upward bias is to maintain two separate q-function estimates - one for action selection and another for evaluation. Let’s first start by looking at the corresponding Monte Carlo value iteration algorithm and then convince ourselves that this is good idea using math. Assume a monte carlo integration setup over Q factors:\n\nDouble Monte Carlo Q-Value Iteration\n\nInput: MDP (S, A, P, R, \\gamma), number of samples N, tolerance \\varepsilon > 0, maximum iterations K\nOutput: Q-functions q^A, q^B\n\nInitialize q^A_0(s,a) = q^B_0(s,a) = 0 for all s \\in S, a \\in A\n\ni \\leftarrow 0\n\nrepeat\n\nFor each s \\in S, a \\in A:\n\nDraw s'_j \\sim p(\\cdot|s,a) for j = 1,\\ldots,N\n\nFor network A:\n\na^*_i \\leftarrow \\arg\\max_{a'} q^A_i(s'_j,a')\n\nq^A_{i+1}(s,a) \\leftarrow r(s,a) + \\frac{\\gamma}{N} \\sum_{j=1}^N q^B_i(s'_j,a^*_i)\n\nFor network B:\n\nb^*_i \\leftarrow \\arg\\max_{a'} q^B_i(s'_j,a')\n\nq^B_{i+1}(s,a) \\leftarrow r(s,a) + \\frac{\\gamma}{N} \\sum_{j=1}^N q^A_i(s'_j,b^*_i)\n\n\\delta \\leftarrow \\max(\\|q^A_{i+1} - q^A_i\\|_{\\infty}, \\|q^B_{i+1} - q^B_i\\|_{\\infty})\n\ni \\leftarrow i + 1\n\nuntil \\delta < \\varepsilon or i \\geq K\n\nreturn final q^A_{i+1}, q^B_{i+1}\n\nIn this algorithm, we maintain two separate Q-functions (q^A and q^B) and use them asymmetrically: when updating q^A, we use network A to select the best action (a^*_i = \\arg\\max_{a'} q^A_i(s'_j,a')) but then evaluate that action using network B’s estimates (q^B_i(s'_j,a^*_i)). We do the opposite for updating q^B. You can see this separation in steps 3.2.2 and 3.2.3 of the algorithm, where for each network update, we first use one network to pick the action and then plug that chosen action into the other network for evaluation. We will see that this decomposition helps mitigate the positive bias that occurs due to Jensen’s inequality.","type":"content","url":"/simadp#decoupling-selection-and-evaluation","position":11},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl5":"An HVAC analogy","lvl4":"Decoupling Selection and Evaluation","lvl3":"Monte Carlo Integration"},"type":"lvl5","url":"/simadp#an-hvac-analogy","position":12},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl5":"An HVAC analogy","lvl4":"Decoupling Selection and Evaluation","lvl3":"Monte Carlo Integration"},"content":"Consider a building where each HVAC unit i has some true maximum power draw \\mu_i under worst-case conditions. Let’s pretend that we don’t have access to manufacturer datasheets, so we need to estimate these maxima from actual measurements. Now the challenge is that power draw fluctuates with environmental conditions. If we use a single day’s measurements and look at the highest power draw, we systematically overestimate the true maximum draw across all units.\n\nTo see this, let X_A^i be unit i’s power draw on day A and X_B^i be unit i’s power draw on day. Wile both measurements are unbiased \\mathbb{E}[X_A^i] = \\mathbb{E}[X_B^i] = \\mu_i, their maximum is not due to Jensen’s inequality:\\mathbb{E}[\\max_i X_A^i] \\geq \\max_i \\mathbb{E}[X_A^i] = \\max_i \\mu_i\n\nIntuitively, this problem occurs because reading tends to come from units that experienced particularly demanding conditions (e.g., direct sunlight, full occupancy, peak humidity) rather than just those with high true maximum draw. To estimate the true maximum power draw more accurately, we use the following measurement protocol:\n\nUse day A measurements to select which unit hit the highest peak\n\nUse day B measurements to evaluate that unit’s power consumption\n\nThis yields the estimator:Y = X_B^{\\arg\\max_i X_A^i}\n\nWe can show that by decoupling selection and evaluation in this fashion, our estimator Y will no longer systematically overestimate the true maximum draw. First, observe that \\arg\\max_i X_A^i is a random variable (call it J) - it tells us which unit had highest power draw on day A. It has some probability distribution based on day A’s conditions:\nP(J = j) = P(\\arg\\max_i X_A^i = j).\nUsing the law of total expectation:\\begin{align*}\n\\mathbb{E}[Y] = \\mathbb{E}[X_B^J] &= \\mathbb{E}[\\mathbb{E}[X_B^J \\mid J]] \\text{ (by tower property)} \\\\\n&= \\sum_{j=1}^n \\mathbb{E}[X_B^j \\mid J = j] P(J = j) \\\\\n&= \\sum_{j=1}^n \\mathbb{E}[X_B^j \\mid \\arg\\max_i X_A^i = j] P(\\arg\\max_i X_A^i = j)\n\\end{align*}\n\nNow we need to make an imporant observation: Unit j’s power draw on day B (X_B^j) is independent of whether it had the highest reading on day A (\\{\\arg\\max_i X_A^i = j\\}). An extreme cold event on day A shouldn’t affect day B’s readings(especially in Quebec where the wheather tend to vary widely from day to day). Therefore:\\mathbb{E}[X_B^j \\mid \\arg\\max_i X_A^i = j] = \\mathbb{E}[X_B^j] = \\mu_j\n\nThis tells us that the two-day estimator is now an average of the true underlying power consumptions:\\mathbb{E}[Y] = \\sum_{j=1}^n \\mu_j P(\\arg\\max_i X_A^i = j)\n\nTo analyze  \\mathbb{E}[Y]  more closely, let’s use a general result: if we have a real-valued function  f  defined on a discrete set of units  \\{1, \\dots, n\\}  and a probability distribution  q(\\cdot)  over these units, then the maximum value of  f  across all units is at least as large as the weighted sum of  f  values with weights  q . Formally,\\max_{j \\in \\{1, \\dots, n\\}} f(j) \\geq \\sum_{j=1}^n q(j) f(j).\n\nApplying this to our setting, we set  f(j) = \\mu_j  (the true maximum power draw for unit  j ) and  q(j) = P(J = j)  (the probability that unit  j  achieves the maximum reading on day A). This gives us:\\max_{j \\in \\{1, \\dots, n\\}} \\mu_j \\geq \\sum_{j=1}^n P(J = j) \\mu_j = \\mathbb{E}[Y].\n\nTherefore, the expected value of  Y  (our estimator) will always be less than or equal to the true maximum value  \\max_j \\mu_j . In other words,  Y  provides a conservative estimate of the true maximum: it tends not to overestimate  \\max_j \\mu_j  but instead approximates it as closely as possible without systematic upward bias.","type":"content","url":"/simadp#an-hvac-analogy","position":13},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl5":"Consistency","lvl4":"Decoupling Selection and Evaluation","lvl3":"Monte Carlo Integration"},"type":"lvl5","url":"/simadp#consistency","position":14},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl5":"Consistency","lvl4":"Decoupling Selection and Evaluation","lvl3":"Monte Carlo Integration"},"content":"Even though  Y  is not an unbiased estimator of  \\max_j \\mu_j  (since  \\mathbb{E}[Y] \\leq \\max_j \\mu_j ), it is consistent. As more independent days (or measurements) are observed, the selection-evaluation procedure becomes more effective at isolating the intrinsic maximum, reducing the influence of day-specific environmental fluctuations. Over time, this approach yields a stable and increasingly accurate approximation of  \\max_j \\mu_j .\n\nTo show that  Y  is a consistent estimator of  \\max_i \\mu_i , we want to demonstrate that as the number of independent measurements (days, in this case) increases,  Y  converges in probability to  \\max_i \\mu_i . Let’s suppose we have  m  independent days of measurements for each unit. Denote:\n\n X_A^{(k),i}  as the power draw for unit  i  on day  A_k , where  k \\in \\{1, \\dots, m\\} .\n\n J_m = \\arg\\max_i \\left( \\frac{1}{m} \\sum_{k=1}^m X_A^{(k),i} \\right) , which identifies the unit with the highest average power draw over  m  days.\n\nThe estimator we construct is:\n\nY_m = X_B^{(J_m)},\nwhere  X_B^{(J_m)}  is the power draw of the selected unit  J_m  on an independent evaluation day  B .\nWe will now show that  Y_m  converges to  \\max_i \\mu_i  as  m \\to \\infty . This involves two main steps:\n\nConsistency of the Selection Step  J_m : As  m \\to \\infty , the unit selected by  J_m  will tend to be the one with the true maximum power draw  \\max_i \\mu_i .\n\nConvergence of  Y_m  to  \\mu_{J_m} : Since the evaluation day  B  measurement  X_B^{(J_m)}  is unbiased with expectation  \\mu_{J_m} , as  m \\to \\infty ,  Y_m  will converge to  \\mu_{J_m} , which in turn converges to  \\max_i \\mu_i .\n\nThe average power draw over  m  days for each unit  i  is:\\frac{1}{m} \\sum_{k=1}^m X_A^{(k),i}.\n\nBy the law of large numbers, as  m \\to \\infty , this sample average converges to the true expected power draw  \\mu_i  for each unit  i :\\frac{1}{m} \\sum_{k=1}^m X_A^{(k),i} \\xrightarrow{m \\to \\infty} \\mu_i.\n\nSince  J_m  selects the unit with the highest sample average, in the limit,  J_m  will almost surely select the unit with the highest true mean,  \\max_i \\mu_i . Thus, as  m \\to \\infty ,\\mu_{J_m} \\to \\max_i \\mu_i.\n\nGiven that  J_m  identifies the unit with the maximum true mean power draw in the limit, we now look at  Y_m = X_B^{(J_m)} , which is the power draw of unit  J_m  on the independent evaluation day  B .\n\nSince  X_B^{(J_m)}  is an unbiased estimator of  \\mu_{J_m} , we have:\\mathbb{E}[Y_m \\mid J_m] = \\mu_{J_m}.\n\nAs  m \\to \\infty ,  \\mu_{J_m}  converges to  \\max_i \\mu_i . Thus,  Y_m  will also converge in probability to  \\max_i \\mu_i  because  Y_m  is centered around  \\mu_{J_m}  and  J_m  converges to the index of the unit with  \\max_i \\mu_i .\n\nCombining these two steps, we conclude that:Y_m \\xrightarrow{m \\to \\infty} \\max_i \\mu_i \\text{ in probability}.\n\nThis establishes the consistency of  Y  as an estimator for  \\max_i \\mu_i : as the number of independent measurements grows,  Y_m  converges to the true maximum power draw  \\max_i \\mu_i .","type":"content","url":"/simadp#consistency","position":15},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl2":"Parametric Dynamic Programming"},"type":"lvl2","url":"/simadp#parametric-dynamic-programming","position":16},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl2":"Parametric Dynamic Programming"},"content":"We have so far considered a specific kind of approximation: that of the Bellman operator itself. We explored a modified version of the operator with the desirable property of smoothness, which we deemed beneficial for optimization purposes and due to its rich multifaceted interpretations. We now turn our attention to another form of approximation, complementary to the previous kind, which seeks to address the challenge of applying the operator across the entire state space.\n\nTo be precise, suppose we can compute the Bellman operator \\mathrm{L}v at some state s, producing a new function U whose value at state s is u(s) = (\\mathrm{L}v)(s). Then, putting aside the problem of pointwise evaluation, we want to carry out this update across the entire domain of v. When working with small state spaces, this is not an issue, and we can afford to carry out the update across the entirety of the state space. However, for larger or infinite state spaces, this becomes a major challenge.\n\nSo what can we do? Our approach will be to compute the operator at chosen “grid points,” then “fill in the blanks” for the states where we haven’t carried out the update by “fitting” the resulting output function on a dataset of input-output pairs. The intuition is that for sufficiently well-behaved functions and sufficiently expressive function approximators, we hope to generalize well enough. Our community calls this “learning,” while others would call it “function approximation” — a field of its own in mathematics. To truly have a “learning algorithm,” we’ll need to add one more piece of machinery: the use of samples — of simulation — to pick the grid points and perform numerical integration. But this is for the next section...","type":"content","url":"/simadp#parametric-dynamic-programming","position":17},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl3":"Partial Updates in the Tabular Case","lvl2":"Parametric Dynamic Programming"},"type":"lvl3","url":"/simadp#partial-updates-in-the-tabular-case","position":18},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl3":"Partial Updates in the Tabular Case","lvl2":"Parametric Dynamic Programming"},"content":"The ideas presented in this section apply more broadly to the successive approximation method applied to a fixed-point problem. Consider again the problem of finding the optimal value function v_\\gamma^\\star as the solution to the Bellman optimality operator \\mathrm{L}:\\mathrm{L} \\mathbf{v} \\equiv \\max_{\\pi \\in \\Pi^{MD}} \\left\\{\\mathbf{r}_\\pi + \\gamma \\mathbf{P}_\\pi \\mathbf{v}\\right\\}\n\nValue iteration -- the name for the method of successive approximation applied to \\mathrm{L} -- computes a sequence of iterates v_{n+1} = \\mathrm{L}v_n from some arbitrary v_0. Let’s pause to consider what the equality sign in this expression means: it represents an assignment (perhaps better denoted as :=) across the entire domain. This becomes clearer when writing the update in component form:v_{n+1}(s) := (\\mathrm{L} v_n)(s) \\equiv \\max_{a \\in \\mathcal{A}_s} \\left\\{r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v_n(j)\\right\\}, \\, \\forall s \\in \\mathcal{S}\n\nPay particular attention to the \\forall s \\in \\mathcal{S} notation: what happens when we can’t afford to update all components in each step of value iteration? A potential solution is to use Gauss-Seidel Value Iteration, which updates states sequentially, immediately using fresh values for subsequent updates.\n\nGauss-Seidel Value Iteration\n\nInput: MDP (S, A, P, R, \\gamma), convergence threshold \\varepsilon > 0Output: Value function v and policy d\n\nInitialization:\n\nInitialize v^0(s) for all s \\in S\n\nSet iteration counter n = 0\n\nMain Loop:\n\nSet state index j = 1\n\na) State Update: Compute v^{n+1}(s_j) as:v^{n+1}(s_j) = \\max_{a \\in A_j} \\left\\{r(s_j, a) + \\gamma \\left[\\sum_{i<j} p(s_i|s_j,a)v^{n+1}(s_i) + \\sum_{i \\geq j} p(s_i|s_j,a)v^n(s_i)\\right]\\right\\}\n\nb) If j = |S|, proceed to step 3\nOtherwise, increment j and return to step 2(a)\n\nConvergence Check:\n\nIf \\|v^{n+1} - v^n\\| < \\varepsilon(1-\\gamma)/(2\\gamma), proceed to step 4\n\nOtherwise, increment n and return to step 2\n\nPolicy Extraction:\nFor each s \\in S, compute optimal policy:d(s) \\in \\operatorname{argmax}_{a \\in A_s} \\left\\{r(s,a) + \\gamma\\sum_{j \\in S} p(j|s,a)v^{n+1}(j)\\right\\}\n\nNote: The algorithm differs from standard value iteration in that it immediately uses updated values within each iteration. This is reflected in the first sum of step 2(a), where v^{n+1} is used for already-updated states.\n\nThe Gauss-Seidel value iteration approach offers several advantages over standard value iteration: it can be more memory-efficient and often leads to faster convergence. This idea generalizes further (see for example \n\nBertsekas (1983)) to accommodate fully asynchronous updates in any order. However, these methods, while more flexible in their update patterns, still fundamentally rely on a tabular representation—that is, they require storing and eventually updating a separate value for each state in memory. Even if we update states one at a time or in blocks, we must maintain this complete table of values, and our convergence guarantee assumes that every entry in this table will eventually be revised.\n\nBut what if maintaining such a table is impossible? This challenge arises naturally when dealing with continuous state spaces, where we cannot feasibly store values for every possible state, let alone update them. This is where function approximation comes into play.","type":"content","url":"/simadp#partial-updates-in-the-tabular-case","position":19},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl3":"Parametric Value Iteration","lvl2":"Parametric Dynamic Programming"},"type":"lvl3","url":"/simadp#parametric-value-iteration","position":20},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl3":"Parametric Value Iteration","lvl2":"Parametric Dynamic Programming"},"content":"In the parametric approach to dynamic programming, instead of maintaining an explicit table of values, we represent the value function using a parametric function approximator v(s; \\boldsymbol{\\theta}), where \\boldsymbol{\\theta} are parameters that get adjusted across iterations rather than the entries of a tabular representation. This idea traces back to the inception of dynamic programming and was described as early as 1963 by Bellman himself, who considered polynomial approximations. For a value function v(s), we can write its polynomial approximation as:v(s) \\approx \\sum_{i=0}^{n} \\theta_i \\phi_i(s)\n\nwhere:\n\n\\{\\phi_i(s)\\} is the set of basis functions\n\n\\theta_i are the coefficients (our parameters)\n\nn is the degree of approximation\n\nAs we discussed earlier in the context of trajectory optimization, we can choose from different polynomial bases beyond the usual monomial basis \\phi_i(s) = s^i, such as Legendre or Chebyshev polynomials. While polynomials offer attractive mathematical properties, they become challenging to work with in higher dimensions due to the curse of dimensionality. This limitation motivates our later turn to neural network parameterizations, which scale better with dimensionality.\n\nGiven a parameterization, our value iteration procedure must now update the parameters \\boldsymbol{\\theta} rather than tabular values directly. At each iteration, we aim to find parameters that best approximate the Bellman operator’s output at chosen base points. More precisely, we collect a dataset:\\mathcal{D}_n = \\{(s_i, (\\mathrm{L}v)(s_i; \\boldsymbol{\\theta}_n)) \\mid s_i \\in B\\}\n\nand fit a regressor v(\\cdot; \\boldsymbol{\\theta}_{n+1}) to this data.\n\nThis process differs from standard supervised learning in a specific way: rather than working with a fixed dataset, we iteratively generate our training targets using the previous value function approximation. During this process, the parameters \\boldsymbol{\\theta}_n remain “frozen”, entering only through dataset creation. This naturally leads to maintaining two sets of parameters:\n\n\\boldsymbol{\\theta}_n: parameters of the target model used for generating training targets\n\n\\boldsymbol{\\theta}_{n+1}: parameters being optimized in the current iteration\n\nThis target model framework emerges naturally from the structure of parametric value iteration — an insight that provides theoretical grounding for modern deep reinforcement learning algorithms where we commonly hear about the importance of the “target network trick” .\n\nParametric successive approximation, known in reinforcement learning literature as Fitted Value Iteration, offers a flexible template for deriving new algorithms by varying the choice of function approximator. Various instantiations of this approach have emerged across different fields:\n\nUsing polynomial basis functions with linear regression yields Kortum’s method \n\nKortum (1992), known to econometricians. In reinforcement learning terms, this corresponds to value iteration with projected Bellman equations \n\nRust (1996).\n\nEmploying extremely randomized trees (via ExtraTreesRegressor) leads to the tree-based fitted value iteration of Ernst et al. \n\nErnst et al. (2005).\n\nNeural network approximation (via MLPRegressor) gives rise to Neural Fitted Q-Iteration as developed by Riedmiller \n\nRiedmiller (2005).\n\nThe \\texttt{fit} function in our algorithm represents this supervised learning step and can be implemented using any standard regression tool that follows the scikit-learn interface. This flexibility in choice of function approximator allows practitioners to leverage the extensive ecosystem of modern machine learning tools while maintaining the core dynamic programming structure.\n\nParametric Value Iteration\n\nInput Given an MDP (S, A, P, R, \\gamma), base points B \\subset S, function approximator class v(s; \\boldsymbol{\\theta}), maximum iterations N, tolerance \\varepsilon > 0\n\nOutput Parameters \\boldsymbol{\\theta} for value function approximation\n\nInitialize \\boldsymbol{\\theta}_0 (e.g., for zero initialization)\n\nn \\leftarrow 0\n\nrepeat\n\n\\mathcal{D} \\leftarrow \\emptyset\n\nFor each s \\in B:\n\ny_s \\leftarrow \\max_{a \\in A} \\left\\{r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a)v(j; \\boldsymbol{\\theta}_n)\\right\\}\n\n\\mathcal{D} \\leftarrow \\mathcal{D} \\cup \\{(s, y_s)\\}\n\n\\boldsymbol{\\theta}_{n+1} \\leftarrow \\texttt{fit}(\\mathcal{D})\n\n\\delta \\leftarrow \\frac{1}{|B|}\\sum_{s \\in B} (v(s; \\boldsymbol{\\theta}_{n+1}) - v(s; \\boldsymbol{\\theta}_n))^2\n\nn \\leftarrow n + 1\n\nuntil (\\delta < \\varepsilon or n \\geq N)\n\nreturn \\boldsymbol{\\theta}_n\n\nThe structure of the above algorithm mirrors value iteration in its core idea of iteratively applying the Bellman operator. However, several key modifications distinguish this fitted variant:\n\nFirst, rather than applying updates across the entire state space, we compute the operator only at selected base points B. The resulting values are then stored implicitly through the parameter vector \\boldsymbol{\\theta} via the fitting step, rather than explicitly as in the tabular case.\n\nThe fitting procedure itself may introduce an “inner optimization loop.” For instance, when using neural networks, this involves an iterative gradient descent procedure to optimize the parameters. This creates an interesting parallel with modified policy iteration: just as we might truncate policy evaluation steps there, we can consider variants where this inner loop runs for a fixed number of iterations rather than to convergence.\n\nFinally, the termination criterion from standard value iteration may no longer hold. The classical criterion relied on the sup-norm contractivity property of the Bellman operator — a property that isn’t generally preserved under function approximation. While certain function approximation schemes can maintain this sup-norm contraction property (as we’ll see later), this is the exception rather than the rule.","type":"content","url":"/simadp#parametric-value-iteration","position":21},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl4":"Parametric Policy Iteration","lvl3":"Parametric Value Iteration","lvl2":"Parametric Dynamic Programming"},"type":"lvl4","url":"/simadp#parametric-policy-iteration","position":22},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl4":"Parametric Policy Iteration","lvl3":"Parametric Value Iteration","lvl2":"Parametric Dynamic Programming"},"content":"We can extend this idea of fitting partial operator updates to the policy iteration setting. Remember, policy iteration involves iterating in the space of policies rather than in the space of value functions. Given an initial guess on a deterministic decision rule d_0, we iteratively:\n\nCompute the value function for the current policy (policy evaluation)\n\nDerive a new improved policy (policy improvement)\n\nWhen computationally feasible and under the model-based setting, we can solve the policy evaluation step directly as a linear system equation. Alternatively, we could carry out policy evaluation by applying successive approximation to the operator L_{d_n} until convergence, or as in modified policy iteration, for just a few steps.\n\nTo apply the idea of fitting partial updates, we start at the level of the policy evaluation operator L_{d_n}. For a given decision rule d_n, this operator in component form is:(L_{d_n}v)(s) = r(s,d_n(s)) + \\gamma \\int v(s')p(ds'|s,d_n(s))\n\nFor a set of base points B = \\{s_1, ..., s_M\\}, we form our dataset:\\mathcal{D}_n = \\{(s_k, y_k) : s_k \\in B\\}\n\nwhere:y_k = r(s_k,d_n(s_k)) + \\gamma \\int v_n(s')p(ds'|s_k,d_n(s_k))\n\nThis gives us a way to perform approximate policy evaluation through function fitting. However, we now face the question of how to perform policy improvement in this parametric setting. The solution comes from the fact that in the exact form of policy iteration, we don’t need to improve the policy everywhere to guarantee progress. In fact, improving the policy at even a single state is sufficient for convergence.\n\nThis suggests a natural approach: rather than trying to approximate an improved policy over the entire state space, we can simply:\n\nCompute improved actions at our base points:d_{n+1}(s_k) = \\arg\\max_{a \\in \\mathcal{A}} \\left\\{r(s_k,a) + \\gamma \\int v_n(s')p(ds'|s_k,a)\\right\\}, \\quad \\forall s_k \\in B\n\nLet the function approximation of the value function implicitly generalize these improvements to other states during the next policy evaluation phase.\n\nThis leads to the following algorithm:\n\nParametric Policy Iteration\n\nInput Given an MDP (S, A, P, R, \\gamma), base points B \\subset S, function approximator class v(s; \\boldsymbol{\\theta}), maximum iterations N, tolerance \\varepsilon > 0\n\nOutput Parameters \\boldsymbol{\\theta} for value function approximation\n\nInitialize \\boldsymbol{\\theta}_0, decision rules \\{d_0(s_k)\\}_{s_k \\in B}\n\nn \\leftarrow 0\n\nrepeat\n\n// Policy Evaluation\n\n\\mathcal{D} \\leftarrow \\emptyset\n\nFor each s_k \\in B:\n\ny_k \\leftarrow r(s_k,d_n(s_k)) + \\gamma \\int v_n(s')p(ds'|s_k,d_n(s_k))\n\n\\mathcal{D} \\leftarrow \\mathcal{D} \\cup \\{(s_k, y_k)\\}\n\n\\boldsymbol{\\theta}_{n+1} \\leftarrow \\texttt{fit}(\\mathcal{D})\n\n// Policy Improvement at Base Points\n\nFor each s_k \\in B:\n\nd_{n+1}(s_k) \\leftarrow \\arg\\max_{a \\in A} \\{r(s_k,a) + \\gamma \\int v_n(s')p(ds'|s_k,a)\\}\n\nn \\leftarrow n + 1\n\nuntil (n \\geq N or convergence criterion met)\n\nreturn \\boldsymbol{\\theta}_n\n\nAs opposed to exact policy iteration, the iterates of parametric policy iteration need not converge monotonically to the optimal value function. Intuitively, this is because we use function approximation to generalize  from base points to the entire state space which can lead to Value estimates improving at base points but degrading at other states or can cause interference between updates at different states due to the shared parametric representation","type":"content","url":"/simadp#parametric-policy-iteration","position":23},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl3":"Q-Factor Representation","lvl2":"Parametric Dynamic Programming"},"type":"lvl3","url":"/simadp#q-factor-representation","position":24},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl3":"Q-Factor Representation","lvl2":"Parametric Dynamic Programming"},"content":"As we discussed above, Monte Carlo integration is the method of choice when it comes to approximating the effect of the Bellman operator. This is due to both its computational advantages in higher dimensions and its compatibility with the model-free assumption. However, there is an additional important detail that we have neglected to properly cover: extracting actions from values in a model-free fashion. While we can obtain a value function using the Monte Carlo approach described above, we still face the challenge of extracting an optimal policy from this value function.\n\nMore precisely, recall that an optimal decision rule takes the form:d(s) = \\arg\\max_{a \\in \\mathcal{A}} \\left\\{r(s,a) + \\gamma \\int v(s')p(ds'|s,a)\\right\\}\n\nTherefore, even given an optimal value function v, deriving an optimal policy would still require Monte Carlo integration every time we query the decision rule/policy at a state.\n\nAn important idea in dynamic programming is that rather than approximating a state-value function, we can instead approximate a state-action value function. These two functions are related: the value function is the expectation of the Q-function (called Q-factors by some authors in the operations research literature) over the conditional distribution of actions given the current state:v(s) = \\mathbb{E}[q(s,a)|s]\n\nIf q^* is an optimal state-action value function, then v^*(s) = \\max_a q^*(s,a). Just as we had a Bellman operator for value functions, we can also define an optimality operator for Q-functions. In component form:(\\mathrm{L}q)(s,a) = r(s,a) + \\gamma \\int p(ds'|s,a)\\max_{a' \\in \\mathcal{A}(s')} q(s', a')\n\nFurthermore, this operator for Q-functions is also a contraction in the sup-norm and therefore has a unique fixed point q^*.\n\nThe advantage of iterating over Q-functions rather than value functions is that we can immediately extract optimal actions without having to represent the reward function or transition dynamics directly, nor perform numerical integration. Indeed, an optimal decision rule at state s is obtained as:d(s) = \\arg\\max_{a \\in \\mathcal{A}(s)} q(s,a)\n\nWith this insight, we can adapt our parametric value iteration algorithm to work with Q-functions:\n\nParametric Q-Value Iteration\n\nInput Given an MDP (S, A, P, R, \\gamma), base points \\mathcal{B} \\subset S, function approximator class q(s,a; \\boldsymbol{\\theta}), maximum iterations N, tolerance \\varepsilon > 0\n\nOutput Parameters \\boldsymbol{\\theta} for Q-function approximation\n\nInitialize \\boldsymbol{\\theta}_0 (e.g., for zero initialization)\n\nn \\leftarrow 0\n\nrepeat\n\n\\mathcal{D} \\leftarrow \\emptyset\n\nFor each (s,a) \\in \\mathcal{B} \\times A:\n\ny_{s,a} \\leftarrow r(s,a) + \\gamma \\int p(ds'|s,a)\\max_{a' \\in A} q(s',a'; \\boldsymbol{\\theta}_n)\n\n\\mathcal{D} \\leftarrow \\mathcal{D} \\cup \\{((s,a), y_{s,a})\\}\n\n\\boldsymbol{\\theta}_{n+1} \\leftarrow \\texttt{fit}(\\mathcal{D})\n\n\\delta \\leftarrow \\frac{1}{|\\mathcal{D}||A|}\\sum_{(s,a) \\in \\mathcal{D} \\times A} (q(s,a; \\boldsymbol{\\theta}_{n+1}) - q(s,a; \\boldsymbol{\\theta}_n))^2\n\nn \\leftarrow n + 1\n\nuntil (\\delta < \\varepsilon or n \\geq N)\n\nreturn \\boldsymbol{\\theta}_n","type":"content","url":"/simadp#q-factor-representation","position":25},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl3":"Initialization and Warmstarting","lvl2":"Parametric Dynamic Programming"},"type":"lvl3","url":"/simadp#initialization-and-warmstarting","position":26},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl3":"Initialization and Warmstarting","lvl2":"Parametric Dynamic Programming"},"content":"Parametric dynamic programming involves solving a sequence of related optimization problems, one for each fitting procedure at each iteration. While we’ve presented these as independent fitting problems, in practice we can leverage the relationship between successive iterations through careful initialization. This “warmstarting” strategy can significantly impact both computational efficiency and solution quality.\n\nThe basic idea is simple: rather than starting each fitting procedure from scratch, we initialize the function approximator with parameters from the previous iteration. This can speed up convergence since successive Q-functions tend to be similar. However, recent work suggests that persistent warmstarting might sometimes be detrimental, potentially leading to a form of overfitting. Alternative “reset” strategies that occasionally reinitialize parameters have shown promise in mitigating this issue.\n\nHere’s how warmstarting can be incorporated into parametric Q-learning with one-step Monte Carlo integration:\n\nWarmstarted Parametric Q-Learning with N=1 Monte Carlo Integration\n\nInput Given dataset \\mathcal{D} with transitions (s, a, r, s'), function approximator class q(s,a; \\boldsymbol{\\theta}), maximum iterations N, tolerance \\varepsilon > 0, warmstart frequency k\n\nOutput Parameters \\boldsymbol{\\theta} for Q-function approximation\n\nInitialize \\boldsymbol{\\theta}_0 randomly\n\nn \\leftarrow 0\n\nrepeat\n\n\\mathcal{D} \\leftarrow \\emptyset\n\nFor each (s,a,r,s') \\in \\mathcal{D}:\n\ny_{s,a} \\leftarrow r + \\gamma \\max_{a'} q(s',a'; \\boldsymbol{\\theta}_n)  // One-step Monte Carlo estimate\n\n\\mathcal{D} \\leftarrow \\mathcal{D} \\cup \\{((s,a), y_{s,a})\\}\n\nif n \\bmod k = 0:  // Reset parameters periodically\n\nInitialize \\boldsymbol{\\theta}_{temp} randomly\n\nelse:\n\n\\boldsymbol{\\theta}_{temp} \\leftarrow \\boldsymbol{\\theta}_n  // Warmstart from previous iteration\n\n\\boldsymbol{\\theta}_{n+1} \\leftarrow \\texttt{fit}(\\mathcal{D}, \\boldsymbol{\\theta}_{temp})  // Initialize optimizer with \\boldsymbol{\\theta}_{temp}\n\n\\delta \\leftarrow \\frac{1}{|\\mathcal{D}|}\\sum_{(s,a) \\in \\mathcal{D}} (q(s,a; \\boldsymbol{\\theta}_{n+1}) - q(s,a; \\boldsymbol{\\theta}_n))^2\n\nn \\leftarrow n + 1\n\nuntil (\\delta < \\varepsilon or n \\geq N)\n\nreturn \\boldsymbol{\\theta}_n\n\nThe main addition here is the periodic reset of parameters (controlled by frequency k) which helps balance the benefits of warmstarting with the need to avoid potential overfitting. When k=\\infty, we get traditional persistent warmstarting, while k=1 corresponds to training from scratch each iteration.","type":"content","url":"/simadp#initialization-and-warmstarting","position":27},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl3":"Inner Loop Convergence","lvl2":"Parametric Dynamic Programming"},"type":"lvl3","url":"/simadp#inner-loop-convergence","position":28},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl3":"Inner Loop Convergence","lvl2":"Parametric Dynamic Programming"},"content":"Beyond the choice of initialization and whether to chain optimization problems through warmstarting, we can also control how we terminate the inner optimization procedure. In the templates presented above, we implicitly assumed that \\texttt{fit} is run to convergence. However, this need not be the case, and different implementations handle this differently.\n\nFor example, scikit-learn’s MLPRegressor terminates based on several criteria: when the improvement in loss falls below a tolerance (default tol=1e-4), when it reaches the maximum number of iterations (default max_iter=200), or when the loss fails to improve for n_iter_no_change consecutive epochs. In contrast, ExtraTreesRegressor builds trees deterministically to completion based on its splitting criteria, with termination controlled by parameters like min_samples_split and max_depth.\n\nThe intuition for using early stopping in the inner optimization mirrors that of modified policy iteration in exact dynamic programming. Just as modified policy iteration truncates the Neumann series during policy evaluation rather than solving to convergence, we might only partially optimize our function approximator at each iteration. While this complicates the theoretical analysis, it often works well in practice and can be computationally more efficient.\n\nThis perspective helps us understand modern deep reinforcement learning algorithms. For instance, DQN can be viewed as an instance of fitted Q-iteration where the inner optimization is intentionally limited. Here’s how we can formalize this approach:\n\nEarly-Stopping Fitted Q-Iteration\n\nInput Given dataset \\mathcal{D} with transitions (s, a, r, s'), function approximator q(s,a; \\boldsymbol{\\theta}), maximum outer iterations N_{outer}, maximum inner iterations N_{inner}, outer tolerance \\varepsilon_{outer}, inner tolerance \\varepsilon_{inner}\n\nOutput Parameters \\boldsymbol{\\theta} for Q-function approximation\n\nInitialize \\boldsymbol{\\theta}_0 randomly\n\nn \\leftarrow 0\n\nrepeat\n\n\\mathcal{P} \\leftarrow \\emptyset\n\nFor each (s,a,r,s') \\in \\mathcal{D}:\n\ny_{s,a} \\leftarrow r + \\gamma \\max_{a'} q(s',a'; \\boldsymbol{\\theta}_n)\n\n\\mathcal{P} \\leftarrow \\mathcal{P} \\cup \\{((s,a), y_{s,a})\\}\n\n// Inner optimization loop with early stopping\n\n\\boldsymbol{\\theta}_{temp} \\leftarrow \\boldsymbol{\\theta}_n\n\nk \\leftarrow 0\n\nrepeat\n\nUpdate \\boldsymbol{\\theta}_{temp} using one step of optimizer on \\mathcal{P}\n\nCompute inner loop loss \\delta_{inner}\n\nk \\leftarrow k + 1\n\nuntil (\\delta_{inner} < \\varepsilon_{inner} or k \\geq N_{inner})\n\n\\boldsymbol{\\theta}_{n+1} \\leftarrow \\boldsymbol{\\theta}_{temp}\n\n\\delta_{outer} \\leftarrow \\frac{1}{|\\mathcal{D}|}\\sum_{(s,a) \\in \\mathcal{D}} (q(s,a; \\boldsymbol{\\theta}_{n+1}) - q(s,a; \\boldsymbol{\\theta}_n))^2\n\nn \\leftarrow n + 1\n\nuntil (\\delta_{outer} < \\varepsilon_{outer} or n \\geq N_{outer})\n\nreturn \\boldsymbol{\\theta}_n\n\nThis formulation makes explicit the two-level optimization structure and allows us to control the trade-off between inner loop optimization accuracy and overall computational efficiency. When N_{inner}=1, we recover something closer to DQN’s update rule, while larger values of N_{inner} bring us closer to the full fitted Q-iteration approach.","type":"content","url":"/simadp#inner-loop-convergence","position":29},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl2":"Example Methods"},"type":"lvl2","url":"/simadp#example-methods","position":30},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl2":"Example Methods"},"content":"There are several moving parts we can swap in and out when working with parametric dynamic programming - from the function approximator we choose, to how we warm start things, to the specific methods we use for numerical integration and inner optimization. In this section, we’ll look at some concrete examples and see how they fit into this general framework.","type":"content","url":"/simadp#example-methods","position":31},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl3":"Kernel-Based Reinforcement Learning (2002)","lvl2":"Example Methods"},"type":"lvl3","url":"/simadp#kernel-based-reinforcement-learning-2002","position":32},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl3":"Kernel-Based Reinforcement Learning (2002)","lvl2":"Example Methods"},"content":"Ormoneit and Sen’s Kernel-Based Reinforcement Learning (KBRL) \n\nOrmoneit & Sen (2002) helped establish the general paradigm of batch reinforcement learning later advocated by \n\nErnst et al. (2005). KBRL is a purely offline method that first collects a fixed set of transitions and then uses kernel regression to solve the optimal control problem through value iteration on this dataset. While the dominant approaches at the time were online methods like temporal difference, KBRL showed that another path to developping reinforcement learning algorithm was possible: one that capable of leveraging advances in supervised learning to provide both theoretical and practical benefits.\n\nAs the name suggests, KBRL uses kernel based regression within the general framework of outlined above.\n\nKernel-Based Q-Value Iteration\n\nInput Given an MDP (S, A, P, R, \\gamma), dataset \\mathcal{D} with observed transitions (s, a, r, s'), kernel bandwidth b, maximum iterations N, tolerance \\varepsilon > 0\n\nOutput Kernel-based Q-function approximation\n\nInitialize \\hat{Q}_0 to zero everywhere\n\nn \\leftarrow 0\n\nrepeat\n\n\\mathcal{D} \\leftarrow \\emptyset\n\nFor each (s, a, r, s') \\in \\mathcal{D}:\n\ny_{s,a} \\leftarrow r + \\gamma \\max_{a' \\in A} \\hat{Q}_n(s', a')\n\n\\mathcal{D} \\leftarrow \\mathcal{D} \\cup \\{((s,a), y_{s,a})\\}\n\n\\hat{Q}_{n+1}(s,a) \\leftarrow \\sum_{(s_i,a_i,r_i,s_i') \\in \\mathcal{D}} k_b(s_i, s)\\mathbb{1}[a_i=a] y_{s_i,a_i} / \\sum_{(s_i,a_i,r_i,s_i') \\in \\mathcal{D}} k_b(s_i, s)\\mathbb{1}[a_i=a]\n\n\\delta \\leftarrow \\frac{1}{|\\mathcal{D}|}\\sum_{(s,a,r,s') \\in \\mathcal{D}} (\\hat{Q}_{n+1}(s,a) - \\hat{Q}_n(s,a))^2\n\nn \\leftarrow n + 1\n\nuntil (\\delta < \\varepsilon or n \\geq N)\n\nreturn \\hat{Q}_n\n\nStep 3 is where KBRL uses kernel regression with a normalized weighting kernel:k_b(x^l_t, x) = \\frac{\\phi(\\|x^l_t - x\\|/b)}{\\sum_{l'} \\phi(\\|x^l_{t'} - x\\|/b)}\n\nwhere \\phi is a kernel function (often Gaussian) and b is the bandwidth parameter. Each iteration reuses the entire fixed dataset to re-estimate Q-values through this kernel regression.\n\nAn important theoretical contribution of KBRL is showing that this kernel-based approach ensures convergence of the Q-function sequence. The authors prove that, with appropriate choice of kernel bandwidth decreasing with sample size, the method is consistent - the estimated Q-function converges to the true Q-function as the number of samples grows.\n\nThe main practical limitation of KBRL is computational - being a batch method, it requires storing and using all transitions at each iteration, leading to quadratic complexity in the number of samples. The authors acknowledge this limitation for online settings, suggesting that modifications like discarding old samples or summarizing data clusters would be needed for online applications. Ernst’s later work with tree-based methods would help address this limitation while maintaining many of the theoretical advantages of the batch approach.","type":"content","url":"/simadp#kernel-based-reinforcement-learning-2002","position":33},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl3":"Ernst’s Fitted Q Iteration (2005)","lvl2":"Example Methods"},"type":"lvl3","url":"/simadp#ernsts-fitted-q-iteration-2005","position":34},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl3":"Ernst’s Fitted Q Iteration (2005)","lvl2":"Example Methods"},"content":"Ernst’s \n\nErnst et al. (2005) specific instantiation of parametric q-value iteration uses extremely randomized trees, an extension to random forests proposed by  \n\nGeurts et al. (2006). This algorithm became particularly well-known, partly because it was one of the first to demonstrate the advantages of offline reinforcement learning in practice on several challenging benchmarks at the time.\n\nRandom Forests and Extra-Trees differ primarily in how they construct individual trees. Random Forests creates diversity in two ways: it resamples the training data (bootstrap) for each tree, and at each node it randomly selects a subset of features but then searches exhaustively for the best cut-point within each selected feature. In contrast, Extra-Trees uses the full training set for each tree and injects randomization differently: at each node, it randomly selects both features and cut-points without searching for the optimal one. It then picks the best among these completely random splits according to a variance reduction criterion. This double randomization - in both feature and cut-point selection - combined with using the full dataset makes Extra-Trees faster than Random Forests while maintaining similar predictive accuracy.\n\nAn important implementation detail concerns how tree structures can be reused across iterations of fitted Q iteration. With parametric methods like neural networks, warmstarting is straightforward - you simply initialize the weights with values from the previous iteration. For decision trees, the situation is more subtle because the model structure is determined by how splits are chosen at each node. When the number of candidate splits per node is K=1 (totally randomized trees), the algorithm selects both the splitting variable and threshold purely at random, without looking at the target values (the Q-values we’re trying to predict) to evaluate the quality of the split. This means the tree structure only depends on the input variables and random choices, not on what we’re predicting. As a result, we can build the trees once in the first iteration and reuse their structure throughout all iterations, only updating the prediction values at the leaves.\n\nStandard Extra-Trees (K>1), however, uses target values to choose the best among K random splits by calculating which split best reduces the variance of the predictions. Since these target values change in each iteration of fitted Q iteration (as our estimate of Q evolves), we must rebuild the trees completely. While this is computationally more expensive, it allows the trees to better adapt their structure to capture the evolving Q-function.\n\nThe complete algorithm can be formalized as follows:\n\nExtra-Trees Fitted Q Iteration\n\nInput Given an MDP (S, A, P, R, \\gamma), dataset \\mathcal{D} with observed transitions (s, a, r, s'), Extra-Trees parameters (K, n_{min}, M), maximum iterations N, tolerance \\varepsilon > 0\n\nOutput Extra-Trees model for Q-function approximation\n\nInitialize \\hat{Q}_0 to zero everywhere\n\nn \\leftarrow 0\n\nrepeat\n\n\\mathcal{D} \\leftarrow \\emptyset\n\nFor each (s, a, r, s') \\in \\mathcal{D}:\n\ny_{s,a} \\leftarrow r + \\gamma \\max_{a' \\in A} \\hat{Q}_n(s', a')\n\n\\mathcal{D} \\leftarrow \\mathcal{D} \\cup \\{((s,a), y_{s,a})\\}\n\n\\hat{Q}_{n+1} \\leftarrow \\text{BuildExtraTrees}(\\mathcal{D}, K, n_{min}, M)\n\n\\delta \\leftarrow \\frac{1}{|\\mathcal{D}|}\\sum_{(s,a,r,s') \\in \\mathcal{D}} (\\hat{Q}_{n+1}(s,a) - \\hat{Q}_n(s,a))^2\n\nn \\leftarrow n + 1\n\nuntil (\\delta < \\varepsilon or n \\geq N)\n\nreturn \\hat{Q}_n","type":"content","url":"/simadp#ernsts-fitted-q-iteration-2005","position":35},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl3":"Neural Fitted Q Iteration (2005)","lvl2":"Example Methods"},"type":"lvl3","url":"/simadp#neural-fitted-q-iteration-2005","position":36},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl3":"Neural Fitted Q Iteration (2005)","lvl2":"Example Methods"},"content":"Riedmiller’s Neural Fitted Q Iteration (NFQI) \n\nRiedmiller (2005) is a natural instantiation of parametric Q-value iteration where:\n\nThe function approximator q(s,a; \\boldsymbol{\\theta}) is a multi-layer perceptron\n\nThe \\texttt{fit} function uses Rprop optimization trained to convergence on each iteration’s pattern set\n\nThe expected next-state values are estimated through Monte Carlo integration with N=1, using the observed next states from transitions\n\nSpecifically, rather than using numerical quadrature which would require known transition probabilities, NFQ approximates the expected future value using observed transitions:\\int q_n(s',a')p(ds'|s,a) \\approx q_n(s'_{observed},a')\n\nwhere s'_{observed} is the actual next state that was observed after taking action a in state s. This is equivalent to Monte Carlo integration with a single sample, making the algorithm fully model-free.\n\nThe algorithm follows from the parametric Q-value iteration template:\n\nNeural Fitted Q Iteration\n\nInput Given an MDP (S, A, P, R, \\gamma), dataset \\mathcal{D} with observed transitions (s, a, r, s'), MLP architecture q(s,a; \\boldsymbol{\\theta}), maximum iterations N, tolerance \\varepsilon > 0\n\nOutput Parameters \\boldsymbol{\\theta} for Q-function approximation\n\nInitialize \\boldsymbol{\\theta}_0 randomly\n\nn \\leftarrow 0\n\nrepeat\n\n\\mathcal{D} \\leftarrow \\emptyset\n\nFor each (s,a,r,s') \\in \\mathcal{D}:\n\ny_{s,a} \\leftarrow r + \\gamma \\max_{a'} q(s',a'; \\boldsymbol{\\theta}_n)  // Monte Carlo estimate with one sample\n\n\\mathcal{D} \\leftarrow \\mathcal{D} \\cup \\{((s,a), y_{s,a})\\}\n\n\\boldsymbol{\\theta}_{n+1} \\leftarrow \\text{Rprop}(\\mathcal{D}) // Train MLP to convergence\n\n\\delta \\leftarrow \\frac{1}{|\\mathcal{D}||A|}\\sum_{(s,a) \\in \\mathcal{D} \\times A} (q(s,a; \\boldsymbol{\\theta}_{n+1}) - q(s,a; \\boldsymbol{\\theta}_n))^2\n\nn \\leftarrow n + 1\n\nuntil (\\delta < \\varepsilon or n \\geq N)\n\nreturn \\boldsymbol{\\theta}_n\n\nWhile NFQI was originally introduced as an offline method with base points collected a priori, the authors also present a variant where base points are collected incrementally. In this online variant, new transitions are gathered using the current policy (greedy with respect to Q_k) and added to the experience set. This approach proves particularly useful when random exploration cannot efficiently collect representative experiences.","type":"content","url":"/simadp#neural-fitted-q-iteration-2005","position":37},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl3":"Deep Q Networks (2013)","lvl2":"Example Methods"},"type":"lvl3","url":"/simadp#deep-q-networks-2013","position":38},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl3":"Deep Q Networks (2013)","lvl2":"Example Methods"},"content":"DQN \n\nMnih et al. (2013) is a close relative of NFQI - in fact, Riedmiller, the author of NFQI, was also an author on the DQN paper. What at first glance might look like a different algorithm can actually be understood as a special case of parametric dynamic programming with practical adaptations. Let’s build this connection step by step.\n\nFirst, let’s start with basic parametric Q-value iteration using a neural network:\n\nBasic Offline Neural Fitted Q-Value Iteration\n\nInput Given an MDP (S, A, P, R, \\gamma), dataset of transitions \\mathcal{T}, neural network q(s,a; \\boldsymbol{\\theta}), maximum iterations N, tolerance \\varepsilon > 0, initialization \\boldsymbol{\\theta}_0\n\nOutput Parameters \\boldsymbol{\\theta} for Q-function approximation\n\nInitialize \\boldsymbol{\\theta}_0 randomly\n\nn \\leftarrow 0\n\nrepeat\n\n\\mathcal{D}_n \\leftarrow \\emptyset  // Regression dataset\n\nFor each (s,a,r,s') \\in \\mathcal{T}:\n\ny_{s,a} \\leftarrow r + \\gamma \\max_{a' \\in A} q(s',a'; \\boldsymbol{\\theta}_n)\n\n\\mathcal{D}_n \\leftarrow \\mathcal{D}_n \\cup \\{((s,a), y_{s,a})\\}\n\n\\boldsymbol{\\theta}_{n+1} \\leftarrow \\texttt{fit}(\\mathcal{D}_n, \\boldsymbol{\\theta}_0) // Fit neural network using built-in convergence criterion\n\nn \\leftarrow n + 1\n\nuntil training complete\n\nreturn \\boldsymbol{\\theta}_n\n\nNext, let’s open up the fit procedure to show the inner optimization loop using gradient descent:\n\nFitted Q-Value Iteration with Explicit Inner Loop\n\nInput Given MDP (S, A, P, R, \\gamma), dataset of transitions \\mathcal{T}, neural network q(s,a; \\boldsymbol{\\theta}), learning rate \\alpha, convergence test \\texttt{has\\_converged}(\\cdot), initialization \\boldsymbol{\\theta}_0, regression loss function \\mathcal{L}\n\nOutput Parameters \\boldsymbol{\\theta} for Q-function approximation\n\nInitialize \\boldsymbol{\\theta}_0 randomly\n\nn \\leftarrow 0  // Outer iteration index\n\nrepeat\n\n\\mathcal{D}_n \\leftarrow \\emptyset  // Regression dataset\n\nFor each (s,a,r,s') \\in \\mathcal{T}:\n\ny_{s,a} \\leftarrow r + \\gamma \\max_{a' \\in A} q(s',a'; \\boldsymbol{\\theta}_n)\n\n\\mathcal{D}_n \\leftarrow \\mathcal{D}_n \\cup \\{((s,a), y_{s,a})\\}\n\n// Inner optimization loop\n\n\\boldsymbol{\\theta}^{(0)} \\leftarrow \\boldsymbol{\\theta}_0  // Start from initial parameters\n\nk \\leftarrow 0  // Inner iteration index\n\nrepeat\n\n\\boldsymbol{\\theta}^{(k+1)} \\leftarrow \\boldsymbol{\\theta}^{(k)} - \\alpha \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}(\\boldsymbol{\\theta}^{(k)}; \\mathcal{D}_n)\n\nk \\leftarrow k + 1\n\nuntil \\texttt{has\\_converged}(\\boldsymbol{\\theta}^{(0)}, ..., \\boldsymbol{\\theta}^{(k)}, \\mathcal{D}_n)\n\n\\boldsymbol{\\theta}_{n+1} \\leftarrow \\boldsymbol{\\theta}^{(k)}\n\nn \\leftarrow n + 1\n\nuntil training complete\n\nreturn \\boldsymbol{\\theta}_n","type":"content","url":"/simadp#deep-q-networks-2013","position":39},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl4":"Warmstarting and Partial Fitting","lvl3":"Deep Q Networks (2013)","lvl2":"Example Methods"},"type":"lvl4","url":"/simadp#warmstarting-and-partial-fitting","position":40},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl4":"Warmstarting and Partial Fitting","lvl3":"Deep Q Networks (2013)","lvl2":"Example Methods"},"content":"A natural modification is to initialize the inner optimization loop with the previous iteration’s parameters - a strategy known as warmstarting - rather than starting from \\boldsymbol{\\theta}_0 each time. Additionally, similar to how modified policy iteration performs partial policy evaluation rather than solving to convergence, we can limit ourselves to a fixed number of optimization steps. These pragmatic changes, when combined, yield:\n\nNeural Fitted Q-Iteration with Warmstarting and Partial Optimization\n\nInput Given MDP (S, A, P, R, \\gamma), dataset of transitions \\mathcal{T}, neural network q(s,a; \\boldsymbol{\\theta}), learning rate \\alpha, number of steps K\n\nOutput Parameters \\boldsymbol{\\theta} for Q-function approximation\n\nInitialize \\boldsymbol{\\theta}_0 randomly\n\nn \\leftarrow 0  // Outer iteration index\n\nrepeat\n\n\\mathcal{D}_n \\leftarrow \\emptyset  // Regression dataset\n\nFor each (s,a,r,s') \\in \\mathcal{T}:\n\ny_{s,a} \\leftarrow r + \\gamma \\max_{a' \\in A} q(s',a'; \\boldsymbol{\\theta}_n)\n\n\\mathcal{D}_n \\leftarrow \\mathcal{D}_n \\cup \\{((s,a), y_{s,a})\\}\n\n// Inner optimization loop with warmstart and fixed steps\n\n\\boldsymbol{\\theta}^{(0)} \\leftarrow \\boldsymbol{\\theta}_n  // Warmstart from previous iteration\n\nFor k = 0 to K-1:\n\n\\boldsymbol{\\theta}^{(k+1)} \\leftarrow \\boldsymbol{\\theta}^{(k)} - \\alpha \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}(\\boldsymbol{\\theta}^{(k)}; \\mathcal{D}_n)\n\n\\boldsymbol{\\theta}_{n+1} \\leftarrow \\boldsymbol{\\theta}^{(K)}\n\nn \\leftarrow n + 1\n\nuntil training complete\n\nreturn \\boldsymbol{\\theta}_n","type":"content","url":"/simadp#warmstarting-and-partial-fitting","position":41},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl4":"Flattening the Updates with Target Swapping","lvl3":"Deep Q Networks (2013)","lvl2":"Example Methods"},"type":"lvl4","url":"/simadp#flattening-the-updates-with-target-swapping","position":42},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl4":"Flattening the Updates with Target Swapping","lvl3":"Deep Q Networks (2013)","lvl2":"Example Methods"},"content":"Now rather than maintaining two sets of indices for the outer and inner levels, we could also “flatten” this algorithm  under a single loop structure using modulo arithmetics. Here’s how we could rewrite it:\n\nFlattened Neural Fitted Q-Iteration\n\nInput Given MDP (S, A, P, R, \\gamma), dataset of transitions \\mathcal{T}, neural network q(s,a; \\boldsymbol{\\theta}), learning rate \\alpha, target update frequency K\n\nOutput Parameters \\boldsymbol{\\theta} for Q-function approximation\n\nInitialize \\boldsymbol{\\theta}_0 randomly\n\n\\boldsymbol{\\theta}_{target} \\leftarrow \\boldsymbol{\\theta}_0  // Initialize target parameters\n\nt \\leftarrow 0  // Single iteration counter\n\nwhile training:\n\n\\mathcal{D}_t \\leftarrow \\emptyset  // Regression dataset\n\nFor each (s,a,r,s') \\in \\mathcal{T}:\n\ny_{s,a} \\leftarrow r + \\gamma \\max_{a' \\in A} q(s',a'; \\boldsymbol{\\theta}_{target})  // Use target parameters\n\n\\mathcal{D}_t \\leftarrow \\mathcal{D}_t \\cup \\{((s,a), y_{s,a})\\}\n\n\\boldsymbol{\\theta}_{t+1} \\leftarrow \\boldsymbol{\\theta}_t - \\alpha \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}(\\boldsymbol{\\theta}_t; \\mathcal{D}_t)\n\nIf t \\bmod K = 0:  // Every K steps\n\n\\boldsymbol{\\theta}_{target} \\leftarrow \\boldsymbol{\\theta}_t  // Update target parameters\n\nt \\leftarrow t + 1\n\nreturn \\boldsymbol{\\theta}_t\n\nThe flattened version with target parameters achieves exactly the same effect as our previous nested-loop structure with warmstarting and K gradient steps. In the nested version, we would create a dataset using parameters \\boldsymbol{\\theta}_n, then perform K gradient steps to obtain \\boldsymbol{\\theta}_{n+1}. In our flattened version, we maintain a separate \\boldsymbol{\\theta}_{target} that gets updated every K steps, ensuring that the dataset \\mathcal{D}_n is created using the same parameters for K consecutive iterations - just as it would be in the nested version. The only difference is that we’ve restructured the algorithm to avoid explicitly nesting the loops, making it more suitable for continuous online training which we are about to introduce. The periodic synchronization of \\boldsymbol{\\theta}_{target} with the current parameters \\boldsymbol{\\theta}_n effectively marks the boundary of what would have been the outer loop in our previous version.","type":"content","url":"/simadp#flattening-the-updates-with-target-swapping","position":43},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl4":"Exponential Moving Average Targets","lvl3":"Deep Q Networks (2013)","lvl2":"Example Methods"},"type":"lvl4","url":"/simadp#exponential-moving-average-targets","position":44},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl4":"Exponential Moving Average Targets","lvl3":"Deep Q Networks (2013)","lvl2":"Example Methods"},"content":"An alternative to this periodic swap of parameters is to use an exponential moving average (EMA) of the parameters:\n\nFlattened Neural Fitted Q-Iteration with EMA\n\nInput Given MDP (S, A, P, R, \\gamma), dataset of transitions \\mathcal{T}, neural network q(s,a; \\boldsymbol{\\theta}), learning rate \\alpha, EMA rate \\tau\n\nOutput Parameters \\boldsymbol{\\theta} for Q-function approximation\n\nInitialize \\boldsymbol{\\theta}_0 randomly\n\n\\boldsymbol{\\theta}_{target} \\leftarrow \\boldsymbol{\\theta}_0  // Initialize target parameters\n\nn \\leftarrow 0  // Single iteration counter\n\nwhile training:\n\n\\mathcal{D}_n \\leftarrow \\emptyset  // Regression dataset\n\nFor each (s,a,r,s') \\in \\mathcal{T}:\n\ny_{s,a} \\leftarrow r + \\gamma \\max_{a' \\in A} q(s',a'; \\boldsymbol{\\theta}_{target})  // Use target parameters\n\n\\mathcal{D}_n \\leftarrow \\mathcal{D}_n \\cup \\{((s,a), y_{s,a})\\}\n\n\\boldsymbol{\\theta}_{n+1} \\leftarrow \\boldsymbol{\\theta}_n - \\alpha \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}(\\boldsymbol{\\theta}_n; \\mathcal{D}_n)\n\n\\boldsymbol{\\theta}_{target} \\leftarrow \\tau\\boldsymbol{\\theta}_{n+1} + (1-\\tau)\\boldsymbol{\\theta}_{target}  // Smooth update of target parameters\n\nn \\leftarrow n + 1\n\nreturn \\boldsymbol{\\theta}_n\n\nNote that the original DQN used the periodic swap of parameters rather than EMA targets.\nEMA targets (also called “Polyak averaging”) started becoming popular in deep RL with DDPG \n\nLillicrap et al. (2015) where they used a “soft” target update: \\boldsymbol{\\theta}_{target} \\leftarrow \\tau\\boldsymbol{\\theta} + (1-\\tau)\\boldsymbol{\\theta}_{target} with a small \\tau (like 0.001). This has since become a common choice in many algorithms like TD3 \n\nFujimoto et al. (2018) and SAC \n\nHaarnoja et al. (2018).","type":"content","url":"/simadp#exponential-moving-average-targets","position":45},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl4":"Online Data Collection and Experience Replay","lvl3":"Deep Q Networks (2013)","lvl2":"Example Methods"},"type":"lvl4","url":"/simadp#online-data-collection-and-experience-replay","position":46},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl4":"Online Data Collection and Experience Replay","lvl3":"Deep Q Networks (2013)","lvl2":"Example Methods"},"content":"Rather than using offline data, we now consider a modification where we incrementally gather samples under our current policy. A common exploration strategy is \\varepsilon-greedy: with probability \\varepsilon we select a random action, and with probability 1-\\varepsilon we select the greedy action \\arg\\max_a q(s,a;\\boldsymbol{\\theta}_n). This ensures we maintain some exploration even as our Q-function estimates improve. Typically \\varepsilon is annealed over time, starting with a high value (e.g., 1.0) to encourage early exploration and gradually decreasing to a small final value (e.g., 0.01) to maintain a minimal level of exploration while mostly exploiting our learned policy.\n\nFlattened Online Neural Fitted Q-Iteration\n\nInput Given MDP (S, A, P, R, \\gamma), neural network q(s,a; \\boldsymbol{\\theta}), learning rate \\alpha, target update frequency K\n\nOutput Parameters \\boldsymbol{\\theta} for Q-function approximation\n\nInitialize \\boldsymbol{\\theta}_0 randomly\n\n\\boldsymbol{\\theta}_{target} \\leftarrow \\boldsymbol{\\theta}_0  // Initialize target parameters\n\nInitialize \\mathcal{T} \\leftarrow \\emptyset  // Initialize transition dataset\n\nn \\leftarrow 0  // Single iteration counter\n\nwhile training:\n\nObserve current state s\n\nSelect action a using policy derived from q(s,\\cdot;\\boldsymbol{\\theta}_n) (e.g., ε-greedy)\n\nExecute a, observe reward r and next state s'\n\n\\mathcal{T}_n \\leftarrow \\mathcal{T}_n \\cup \\{(s,a,r,s')\\}  // Add transition to dataset\n\n\\mathcal{D}_n \\leftarrow \\emptyset  // Regression dataset\n\nFor each (s,a,r,s') \\in \\mathcal{T}_n:\n\ny_{s,a} \\leftarrow r + \\gamma \\max_{a' \\in A} q(s',a'; \\boldsymbol{\\theta}_{target})  // Use target parameters\n\n\\mathcal{D}_n \\leftarrow \\mathcal{D}_n \\cup \\{((s,a), y_{s,a})\\}\n\n\\boldsymbol{\\theta}_{n+1} \\leftarrow \\boldsymbol{\\theta}_n - \\alpha \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}(\\boldsymbol{\\theta}_n; \\mathcal{D}_n)\n\nIf n \\bmod K = 0:  // Every K steps\n\n\\boldsymbol{\\theta}_{target} \\leftarrow \\boldsymbol{\\theta}_n  // Update target parameters\n\nn \\leftarrow n + 1\n\nreturn \\boldsymbol{\\theta}_n\n\nThis version faces two practical challenges. First, the transition dataset \\mathcal{T}_n grows unbounded over time, creating memory issues. Second, computing gradients over the entire dataset becomes increasingly expensive. These are common challenges in online learning settings, and the standard solutions from supervised learning apply here:\n\nUse a fixed-size circular buffer (often called replay buffer, in reference to “experience replay” by \n\nLin (1992)) to limit memory usage\n\nCompute gradients on mini-batches rather than the full dataset\n\nHere’s how we can modify our algorithm to incorporate these ideas:\n\nDeep-Q Network\n\nInput Given MDP (S, A, P, R, \\gamma), neural network q(s,a; \\boldsymbol{\\theta}), learning rate \\alpha, target update frequency K, replay buffer size B, mini-batch size b\n\nOutput Parameters \\boldsymbol{\\theta} for Q-function approximation\n\nInitialize \\boldsymbol{\\theta}_0 randomly\n\n\\boldsymbol{\\theta}_{target} \\leftarrow \\boldsymbol{\\theta}_0  // Initialize target parameters\n\nInitialize replay buffer \\mathcal{R} with capacity B\n\nn \\leftarrow 0  // Single iteration counter\n\nwhile training:\n\nObserve current state s\n\nSelect action a using policy derived from q(s,\\cdot;\\boldsymbol{\\theta}_n) (e.g., ε-greedy)\n\nExecute a, observe reward r and next state s'\n\nStore (s,a,r,s') in \\mathcal{R}, replacing oldest if full  // Circular buffer update\n\n\\mathcal{D}_n \\leftarrow \\emptyset  // Regression dataset\n\nSample mini-batch of b transitions (s_i,a_i,r_i,s_i') from \\mathcal{R}\n\nFor each sampled (s_i,a_i,r_i,s_i'):\n\ny_i \\leftarrow r_i + \\gamma \\max_{a' \\in A} q(s_i',a'; \\boldsymbol{\\theta}_{target})  // Use target parameters\n\n\\mathcal{D}_n \\leftarrow \\mathcal{D}_n \\cup \\{((s_i,a_i), y_i)\\}\n\n\\boldsymbol{\\theta}_{n+1} \\leftarrow \\boldsymbol{\\theta}_n - \\alpha \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}(\\boldsymbol{\\theta}_n; \\mathcal{D}_n) // Replace by RMSProp to obtain DQN\n\nIf n \\bmod K = 0:  // Every K steps\n\n\\boldsymbol{\\theta}_{target} \\leftarrow \\boldsymbol{\\theta}_n  // Update target parameters\n\nn \\leftarrow n + 1\n\nreturn \\boldsymbol{\\theta}_n\n\nThis formulation naturally leads to an important concept in deep reinforcement learning: the replay ratio (or data reuse ratio). In our algorithm, for each new transition we collect, we sample a mini-batch of size b from our replay buffer and perform one update. This means we’re reusing past experiences at a ratio of b:1 - for every new piece of data, we’re learning from b experiences. This ratio can be tuned as a hyperparameter. Higher ratios mean more computation per environment step but better data efficiency, as we’re extracting more learning from each collected transition. This highlights one of the key benefits of experience replay: it allows us to decouple the rate of data collection from the rate of learning updates. Some modern algorithms like SAC or TD3 explicitly tune this ratio, sometimes using multiple gradient steps per environment step to achieve higher data efficiency.\n\nI’ll write a subsection that naturally follows from the previous material and introduces double Q-learning in the context of DQN.","type":"content","url":"/simadp#online-data-collection-and-experience-replay","position":47},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl4":"Double-Q Network Variant","lvl3":"Deep Q Networks (2013)","lvl2":"Example Methods"},"type":"lvl4","url":"/simadp#double-q-network-variant","position":48},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl4":"Double-Q Network Variant","lvl3":"Deep Q Networks (2013)","lvl2":"Example Methods"},"content":"As we saw earlier, the max operator in the target computation can lead to overestimation of Q-values. This happens because we use the same network to both select and evaluate actions in the target computation: y_i \\leftarrow r_i + \\gamma \\max_{a' \\in A} q(s_i',a'; \\boldsymbol{\\theta}_{target}). The max operator means we’re both choosing the action that looks best under our current estimates and then using that same set of estimates to evaluate how good that action is, potentially compounding any optimization bias.\n\nDouble DQN \n\nVan Hasselt et al. (2016) addresses this by using the current network parameters to select actions but the target network parameters to evaluate them. This leads to a simple modification of the DQN algorithm:\n\nDouble Deep-Q Network\n\nInput Given MDP (S, A, P, R, \\gamma), neural network q(s,a; \\boldsymbol{\\theta}), learning rate \\alpha, target update frequency K, replay buffer size B, mini-batch size b\n\nOutput Parameters \\boldsymbol{\\theta} for Q-function approximation\n\nInitialize \\boldsymbol{\\theta}_0 randomly\n\n\\boldsymbol{\\theta}_{target} \\leftarrow \\boldsymbol{\\theta}_0  // Initialize target parameters\n\nInitialize replay buffer \\mathcal{R} with capacity B\n\nn \\leftarrow 0  // Single iteration counter\n\nwhile training:\n\nObserve current state s\n\nSelect action a using policy derived from q(s,\\cdot;\\boldsymbol{\\theta}_n) (e.g., ε-greedy)\n\nExecute a, observe reward r and next state s'\n\nStore (s,a,r,s') in \\mathcal{R}, replacing oldest if full\n\n\\mathcal{D}_n \\leftarrow \\emptyset  // Regression dataset\n\nSample mini-batch of b transitions (s_i,a_i,r_i,s_i') from \\mathcal{R}\n\nFor each sampled (s_i,a_i,r_i,s_i'):\n\na^*_i \\leftarrow \\arg\\max_{a' \\in A} q(s_i',a'; \\boldsymbol{\\theta}_n)  // Select action using current network\n\ny_i \\leftarrow r_i + \\gamma q(s_i',a^*_i; \\boldsymbol{\\theta}_{target})  // Evaluate using target network\n\n\\mathcal{D}_n \\leftarrow \\mathcal{D}_n \\cup \\{((s_i,a_i), y_i)\\}\n\n\\boldsymbol{\\theta}_{n+1} \\leftarrow \\boldsymbol{\\theta}_n - \\alpha \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}(\\boldsymbol{\\theta}_n; \\mathcal{D}_n)\n\nIf n \\bmod K = 0:  // Every K steps\n\n\\boldsymbol{\\theta}_{target} \\leftarrow \\boldsymbol{\\theta}_n  // Update target parameters\n\nn \\leftarrow n + 1\n\nreturn \\boldsymbol{\\theta}_n\n\nThe main difference from the original DQN is in step 7, where we now separate action selection from action evaluation. Rather than directly taking the max over the target network’s Q-values, we first select the action using our current network (\\boldsymbol{\\theta}_n) and then evaluate that specific action using the target network (\\boldsymbol{\\theta}_{target}). This simple change has been shown to lead to more stable learning and better final performance across a range of tasks.","type":"content","url":"/simadp#double-q-network-variant","position":49},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl3":"Deep Q Networks with Resets (2022)","lvl2":"Example Methods"},"type":"lvl3","url":"/simadp#deep-q-networks-with-resets-2022","position":50},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl3":"Deep Q Networks with Resets (2022)","lvl2":"Example Methods"},"content":"In flattening neural fitted Q-iteration, our field had perhaps lost sight of an important structural element: the choice of inner-loop initializer inherent in the original FQI algorithm. The traditional structure explicitly separated outer iterations (computing targets) from inner optimization (fitting to those targets), with each inner optimization starting fresh from parameters \\boldsymbol{\\theta}_0.\n\nThe flattened version with persistent warmstarting seemed like a natural optimization - why throw away learned parameters? However, recent work \n\nD'Oro et al. (2023) has shown that persistent warmstarting can actually be detrimental to learning. Neural networks tend to lose their ability to learn and generalize over the course of training, suggesting that occasionally starting fresh from \\boldsymbol{\\theta}_0 might be beneficial. Here’s how this looks algorithmically in the context of DQN:\n\nDQN with Hard Resets\n\nInput Given MDP (S, A, P, R, \\gamma), neural network q(s,a; \\boldsymbol{\\theta}), learning rate \\alpha, reset interval K, replay buffer size B, mini-batch size b\n\nOutput Parameters \\boldsymbol{\\theta} for Q-function approximation\n\nInitialize \\boldsymbol{\\theta}_0 randomly\n\n\\boldsymbol{\\theta}_{target} \\leftarrow \\boldsymbol{\\theta}_0  // Initialize target parameters\n\nInitialize replay buffer \\mathcal{R} with capacity B\n\nn \\leftarrow 0  // Single iteration counter\n\nwhile training:\n\nObserve current state s\n\nSelect action a using policy derived from q(s,\\cdot;\\boldsymbol{\\theta}_n) (e.g., ε-greedy)\n\nExecute a, observe reward r and next state s'\n\nStore (s,a,r,s') in \\mathcal{R}, replacing oldest if full\n\n\\mathcal{D}_n \\leftarrow \\emptyset  // Regression dataset\n\nSample mini-batch of b transitions (s_i,a_i,r_i,s_i') from \\mathcal{R}\n\nFor each sampled (s_i,a_i,r_i,s_i'):\n\ny_i \\leftarrow r_i + \\gamma \\max_{a' \\in A} q(s_i',a'; \\boldsymbol{\\theta}_{target})\n\n\\mathcal{D}_n \\leftarrow \\mathcal{D}_n \\cup \\{((s_i,a_i), y_i)\\}\n\nIf n \\bmod K = 0:  // Periodic reset\n\n\\boldsymbol{\\theta}_{n+1} \\leftarrow \\boldsymbol{\\theta}_0  // Reset to initial parameters\n\nElse:\n\n\\boldsymbol{\\theta}_{n+1} \\leftarrow \\boldsymbol{\\theta}_n - \\alpha \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}(\\boldsymbol{\\theta}_n; \\mathcal{D}_n)\n\nIf n \\bmod K = 0:  // Every K steps\n\n\\boldsymbol{\\theta}_{target} \\leftarrow \\boldsymbol{\\theta}_n  // Update target parameters\n\nn \\leftarrow n + 1\n\nreturn \\boldsymbol{\\theta}_n\n\nThis algorithm change allows us to push the limits of our update ratio - the number of gradient steps we perform per environment interaction. Without resets, increasing this ratio leads to diminishing returns as the network’s ability to learn degrades. However, by periodically resetting the parameters while maintaining our dataset of transitions, we can perform many more updates per interaction, effectively making our algorithm more “offline” and thus more sample efficient.\n\nThe hard reset strategy, while effective, might be too aggressive in some settings as it completely discards learned parameters. An alternative approach is to use a softer form of reset, adapting the “Shrink and Perturb” technique originally introduced by \n\nAsh & Adams (2020) in the context of continual learning. In their work, they found that neural networks that had been trained on one task could better adapt to new tasks if their parameters were partially reset - interpolated with a fresh initialization - rather than either kept intact or completely reset.\n\nWe can adapt this idea to our setting. Instead of completely resetting to \\boldsymbol{\\theta}_0, we can perform a soft reset by interpolating between our current parameters and a fresh random initialization:\n\nDQN with Shrink and Perturb\n\nInput Given MDP (S, A, P, R, \\gamma), neural network q(s,a; \\boldsymbol{\\theta}), learning rate \\alpha, reset interval K, replay buffer size B, mini-batch size b, interpolation coefficient \\beta\n\nOutput Parameters \\boldsymbol{\\theta} for Q-function approximation\n\nInitialize \\boldsymbol{\\theta}_0 randomly\n\n\\boldsymbol{\\theta}_{target} \\leftarrow \\boldsymbol{\\theta}_0  // Initialize target parameters\n\nInitialize replay buffer \\mathcal{R} with capacity B\n\nn \\leftarrow 0  // Single iteration counter\n\nwhile training:\n\nObserve current state s\n\nSelect action a using policy derived from q(s,\\cdot;\\boldsymbol{\\theta}_n) (e.g., ε-greedy)\n\nExecute a, observe reward r and next state s'\n\nStore (s,a,r,s') in \\mathcal{R}, replacing oldest if full\n\n\\mathcal{D}_n \\leftarrow \\emptyset  // Regression dataset\n\nSample mini-batch of b transitions (s_i,a_i,r_i,s_i') from \\mathcal{R}\n\nFor each sampled (s_i,a_i,r_i,s_i'):\n\ny_i \\leftarrow r_i + \\gamma \\max_{a' \\in A} q(s_i',a'; \\boldsymbol{\\theta}_{target})\n\n\\mathcal{D}_n \\leftarrow \\mathcal{D}_n \\cup \\{((s_i,a_i), y_i)\\}\n\nIf n \\bmod K = 0:  // Periodic soft reset\n\nSample \\boldsymbol{\\phi} \\sim initializer  // Fresh random parameters\n\n\\boldsymbol{\\theta}_{n+1} \\leftarrow \\beta\\boldsymbol{\\theta}_n + (1-\\beta)\\boldsymbol{\\phi}\n\nElse:\n\n\\boldsymbol{\\theta}_{n+1} \\leftarrow \\boldsymbol{\\theta}_n - \\alpha \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}(\\boldsymbol{\\theta}_n; \\mathcal{D}_n)\n\nIf n \\bmod K = 0:  // Every K steps\n\n\\boldsymbol{\\theta}_{target} \\leftarrow \\boldsymbol{\\theta}_n  // Update target parameters\n\nn \\leftarrow n + 1\n\nreturn \\boldsymbol{\\theta}_n\n\nThe interpolation coefficient \\beta controls how much of the learned parameters we retain, with \\beta = 0 recovering the hard reset case and \\beta = 1 corresponding to no reset at all. This provides a more flexible approach to restoring learning capability while potentially preserving useful features that have been learned. Like hard resets, this softer variant still enables high update ratios by preventing the degradation of learning capability, but does so in a more gradual way.","type":"content","url":"/simadp#deep-q-networks-with-resets-2022","position":51},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl2":"Does Parametric Dynamic Programming Converge?"},"type":"lvl2","url":"/simadp#does-parametric-dynamic-programming-converge","position":52},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl2":"Does Parametric Dynamic Programming Converge?"},"content":"So far we have avoided the discussion of convergence and focused on intuitive algorithm development, showing how we can extend successive approximation by computing only a few operator evaluations which then get generalized over the entire domain at each step of the value iteration procedure. Now we turn our attention to understanding the conditions under which this general idea can be shown to converge.\n\nA crucial question to ask is whether our algorithm maintains the contraction property that made value iteration so appealing in the first place - the property that allowed us to show convergence to a unique fixed point. We must be careful here because the contraction mapping theorem is specific to a given norm. In the case of value iteration, we showed the Bellman optimality operator is a contraction in the sup-norm, which aligns naturally with how we compare policies based on their value functions.\n\nThe situation becomes more complicated with fitted methods because we are not dealing with just a single operator. At each iteration, we perform exact, unbiased pointwise evaluations of the Bellman operator, but instead of obtaining the next function exactly, we get the closest representable one under our chosen function approximation scheme. Gordon \n\nGordon (1995) showed that the fitting step can be conceptualized as an additional operator that gets applied on top of the exact Bellman operator to produce the next function parameters. This leads to viewing fitted value methods - which for simplicity we describe only for the value case, though the Q-value setting follows similarly - as the composition of two operators:v_{n+1} = \\Gamma(\\mathrm{L}(v_n))\n\nwhere \\mathrm{L} is the Bellman operator and \\Gamma represents the function approximation mapping.\n\nNow we arrive at the central question: if \\mathrm{L} was a sup-norm contraction, is \\Gamma composed with \\mathrm{L} still a sup-norm contraction? What conditions must hold for this to be true? This question is fundamental because if we can establish that the composition of these two operators maintains the contraction property in the sup-norm, we get directly that our resulting successive approximation method will converge.","type":"content","url":"/simadp#does-parametric-dynamic-programming-converge","position":53},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl3":"The Search for Nonexpansive Operators","lvl2":"Does Parametric Dynamic Programming Converge?"},"type":"lvl3","url":"/simadp#the-search-for-nonexpansive-operators","position":54},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl3":"The Search for Nonexpansive Operators","lvl2":"Does Parametric Dynamic Programming Converge?"},"content":"Consider what happens in the fitting step: we have two value functions v and w, and after applying the Bellman operator \\mathrm{L} to each, we get new target values that differ by at most \\gamma times their original difference in sup-norm (due to \\mathrm{L} being a \\gamma-contraction in the sup norm). But what happens when we fit to these target values? If the function approximator can exaggerate differences between its target values, even a small difference in the targets could lead to a larger difference in the fitted functions. This would be disastrous - even though the Bellman operator shrinks differences between value functions by a factor of \\gamma, the fitting step could amplify them back up, potentially breaking the contraction property of the composite operator.\n\nIn order to ensure that the composite operator is contractive, we need conditions on \\Gamma such that if \\mathrm{L} is a sup-norm contraction then the composition also is. A natural property to consider is when \\Gamma is a non-expansion. By definition, this means that for any functions v and w:\\|\\Gamma(v) - \\Gamma(w)\\|_\\infty \\leq \\|v - w\\|_\\infty\n\nThis turns out to be exactly what we need, since if \\Gamma is a non-expansion, then for any functions v and w:\\|\\Gamma(\\mathrm{L}(v)) - \\Gamma(\\mathrm{L}(w))\\|_\\infty \\leq \\|L(v) - L(w)\\|_\\infty \\leq \\gamma\\|v - w\\|_\\infty\n\nThe first inequality uses the non-expansion property of \\Gamma, while the second uses the fact that \\mathrm{L} is a \\gamma-contraction. Together they show that the composite operator \\Gamma \\circ L remains a \\gamma-contraction.","type":"content","url":"/simadp#the-search-for-nonexpansive-operators","position":55},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl3":"Gordon’s Averagers","lvl2":"Does Parametric Dynamic Programming Converge?"},"type":"lvl3","url":"/simadp#gordons-averagers","position":56},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl3":"Gordon’s Averagers","lvl2":"Does Parametric Dynamic Programming Converge?"},"content":"But which function approximators satisfy this non-expansion property? Gordon shows that “averagers”, approximators that compute their outputs as weighted averages of their training values, are always non-expansions in sup-norm. This includes many common approximation schemes like k-nearest neighbors, linear interpolation, and kernel smoothing with normalized weights. The intuition is that if you’re taking weighted averages with weights that sum to one, you can never extrapolate beyond the range of your training values. These methods “interpolate”. This theoretical framework explains why simple interpolation methods like k-nearest neighbors have proven very stable in practice, while more complex approximators can fail catastrophically. To guarantee convergence, we should either use averagers directly or modify other approximators to ensure they never extrapolate beyond their training targets.\n\nMore precisely, a function approximator \\Gamma is an averager if for any state s and any target function v, the fitted value can be written as:\\Gamma(v)(s) = \\sum_{i=1}^n w_i(s) v(s_i)\n\nwhere the weights w_i(s) satisfy:\n\nw_i(s) \\geq 0 for all i and s\n\n\\sum_{i=1}^n w_i(s) = 1 for all s\n\nThe weights w_i(s) depend only on s and the training points \\{s_i\\}, not on the values v(s_i)\n\nLet m = \\min_i v(s_i) and M = \\max_i v(s_i). Then:m = m\\sum_i w_i(s) \\leq \\sum_i w_i(s)v(s_i) \\leq M\\sum_i w_i(s) = M\n\nSo \\Gamma(v)(s) \\in [m,M] for all s, meaning the fitted function cannot take values outside the range of its training values. This property is what makes averagers “interpolate” rather than “extrapolate” and is directly related to why they preserve the contraction property when composed with the Bellman operator. To see why averagers are non-expansions, consider two functions v and w. At any state s:\\begin{align*}\n|\\Gamma(v)(s) - \\Gamma(w)(s)| &= \\left|\\sum_{i=1}^n w_i(s)v(s_i) - \\sum_{i=1}^n w_i(s)w(s_i)\\right| \\\\\n&= \\left|\\sum_{i=1}^n w_i(s)(v(s_i) - w(s_i))\\right| \\\\\n&\\leq \\sum_{i=1}^n w_i(s)|v(s_i) - w(s_i)| \\\\\n&\\leq \\|v - w\\|_\\infty \\sum_{i=1}^n w_i(s) \\\\\n&= \\|v - w\\|_\\infty\n\\end{align*}\n\nSince this holds for all s, we have \\|\\Gamma(v) - \\Gamma(w)\\|_\\infty \\leq \\|v - w\\|_\\infty, proving that \\Gamma is a non-expansion.","type":"content","url":"/simadp#gordons-averagers","position":57},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl3":"Which Function Approximators Interpolate vs Extrapolate?","lvl2":"Does Parametric Dynamic Programming Converge?"},"type":"lvl3","url":"/simadp#which-function-approximators-interpolate-vs-extrapolate","position":58},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl3":"Which Function Approximators Interpolate vs Extrapolate?","lvl2":"Does Parametric Dynamic Programming Converge?"},"content":"","type":"content","url":"/simadp#which-function-approximators-interpolate-vs-extrapolate","position":59},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl4":"K-nearest neighbors (KNN)","lvl3":"Which Function Approximators Interpolate vs Extrapolate?","lvl2":"Does Parametric Dynamic Programming Converge?"},"type":"lvl4","url":"/simadp#k-nearest-neighbors-knn","position":60},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl4":"K-nearest neighbors (KNN)","lvl3":"Which Function Approximators Interpolate vs Extrapolate?","lvl2":"Does Parametric Dynamic Programming Converge?"},"content":"Let’s look at specific examples, starting with k-nearest neighbors. For any state s, let s_{(1)}, ..., s_{(k)} denote the k nearest training points to s. Then:\\Gamma(v)(s) = \\frac{1}{k}\\sum_{i=1}^k v(s_{(i)})\n\nThis is an averager with weights w_i(s) = \\frac{1}{k} for the k nearest neighbors and 0 for all other points.\n\nFor kernel smoothing with a kernel function K, the fitted value is:\\Gamma(v)(s) = \\frac{\\sum_{i=1}^n K(s - s_i)v(s_i)}{\\sum_{i=1}^n K(s - s_i)}\n\nThe denominator normalizes the weights to sum to 1, making this an averager with weights w_i(s) = \\frac{K(s - s_i)}{\\sum_{j=1}^n K(s - s_j)}.","type":"content","url":"/simadp#k-nearest-neighbors-knn","position":61},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl4":"Linear Regression","lvl3":"Which Function Approximators Interpolate vs Extrapolate?","lvl2":"Does Parametric Dynamic Programming Converge?"},"type":"lvl4","url":"/simadp#linear-regression","position":62},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl4":"Linear Regression","lvl3":"Which Function Approximators Interpolate vs Extrapolate?","lvl2":"Does Parametric Dynamic Programming Converge?"},"content":"In contrast, methods like linear regression and neural networks can and often do extrapolate beyond their training targets. More precisely, given a dataset of state-value pairs \\{(s_i, v(s_i))\\}_{i=1}^n, these methods fit parameters to minimize some error criterion, and the resulting function \\Gamma(v)(s) may take values outside the interval [\\min_i v(s_i), \\max_i v(s_i)] even when evaluated at a new state s. For instance, linear regression finds parameters by minimizing squared error:\\min_{\\theta} \\sum_{i=1}^n (v(s_i) - \\theta^T\\phi(s_i))^2\n\nThe resulting fitted function is:\\Gamma(v)(s) = \\phi(s)^T(\\Phi^T\\Phi)^{-1}\\Phi^T v\n\nwhere \\Phi is the feature matrix with rows \\phi(s_i)^T. This cannot be written as a weighted average with weights independent of v. Indeed, we can construct examples where the fitted value at a point lies outside the range of training values. For example, consider two sets of target values defined on just three points s_1 = 0, s_2 = 1, and s_3 = 2:v = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}, \\quad w = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix}\n\nUsing a single feature \\phi(s) = s, our feature matrix is:\\Phi = \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\end{bmatrix}\n\nFor function v, the fitted parameters are:\\theta_v = (\\Phi^T\\Phi)^{-1}\\Phi^T v = \\frac{1}{14}(2)\n\nAnd for function w:\\theta_w = (\\Phi^T\\Phi)^{-1}\\Phi^T w = \\frac{1}{14}(8)\n\nNow if we evaluate these fitted functions at s = 3 (outside our training points):\\Gamma(v)(3) = 3\\theta_v = \\frac{6}{14} \\approx 0.43\\Gamma(w)(3) = 3\\theta_w = \\frac{24}{14} \\approx 1.71\n\nTherefore:|\\Gamma(v)(3) - \\Gamma(w)(3)| = \\frac{18}{14} > 1 = \\|v - w\\|_\\infty","type":"content","url":"/simadp#linear-regression","position":63},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl4":"Spline Interpolation","lvl3":"Which Function Approximators Interpolate vs Extrapolate?","lvl2":"Does Parametric Dynamic Programming Converge?"},"type":"lvl4","url":"/simadp#spline-interpolation","position":64},{"hierarchy":{"lvl1":"Simulation-Based Approximate Dynamic Programming","lvl4":"Spline Interpolation","lvl3":"Which Function Approximators Interpolate vs Extrapolate?","lvl2":"Does Parametric Dynamic Programming Converge?"},"content":"Linear interpolation between points -- the technique used earlier in this chapter -- is an averager since for any point s between knots s_i and s_{i+1}:\\Gamma(v)(s) = \\left(\\frac{s_{i+1}-s}{s_{i+1}-s_i}\\right)v(s_i) + \\left(\\frac{s-s_i}{s_{i+1}-s_i}\\right)v(s_{i+1})\n\nThe weights sum to 1 and are non-negative. However, cubic splines, despite their smoothness advantages, can violate the non-expansion property. To see this, consider fitting a natural cubic spline to three points:s_1 = 0,\\; s_2 = 1,\\; s_3 = 2\n\nwith two different sets of values:v = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad w = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\n\nThe natural cubic spline for v will overshoot at s \\approx 0.5 and undershoot at s \\approx 1.5 due to its attempt to minimize curvature, giving values outside the range [0,1]. Meanwhile, w fits a flat line at 0. Therefore:\\|v - w\\|_\\infty = 1\n\nbut\\|\\Gamma(v) - \\Gamma(w)\\|_\\infty > 1\n\nThis illustrates a general principle: methods that try to create smooth functions by minimizing some global criterion (like curvature in splines) often sacrifice the non-expansion property to achieve their smoothness goals.","type":"content","url":"/simadp#spline-interpolation","position":65},{"hierarchy":{"lvl1":"Programs as Models"},"type":"lvl1","url":"/simulation","position":0},{"hierarchy":{"lvl1":"Programs as Models"},"content":"","type":"content","url":"/simulation","position":1},{"hierarchy":{"lvl1":"Programs as Models"},"type":"lvl1","url":"/simulation#programs-as-models","position":2},{"hierarchy":{"lvl1":"Programs as Models"},"content":"Up to this point, we have described models using systems of equations—either differential or difference equations—that express how a system evolves over time. These analytical models define the transition structure explicitly. For instance, in discrete time, the evolution of the state is governed by a known function:\\mathbf{x}_{k+1} = f(\\mathbf{x}_k, \\mathbf{u}_k)\n\nGiven access to f, we can construct trajectories, analyze system behavior, and design control policies. The important feature here is not that the model evolves one step at a time, but that we are given the local dynamics function f itself.\n\nIn contrast, simulation-based models do not expose f directly. Instead, they define a procedure—implemented in code—that takes an initial state and input sequence and returns the resulting trajectory:\\{\\mathbf{x}_0, \\mathbf{x}_1, \\dots, \\mathbf{x}_T\\} = \\mathcal{S}(\\mathbf{x}_0, \\{\\mathbf{u}_t\\}_{t=0}^{T-1})\n\nHere, \\mathcal{S} represents the full simulator. Internally, it may apply numerical integration, scheduling logic, branching rules, or other computations. But these details are encapsulated. From the outside, we can only query the simulator by running it.\n\nThis distinction is subtle but important. Both types of models can generate trajectories. What matters is the interface: analytical models provide direct access to f; simulation models do not. They offer a trajectory-generation interface, but hide the internal structure that produces it.\n\nCase Study: Robotics — MuJoCo\n\nMuJoCo illustrates this distinction well. It simulates the dynamics of articulated rigid bodies under contact constraints. The equations it solves include:\n\n\nM(\\mathbf{q})\\ddot{\\mathbf{q}} + C(\\mathbf{q}, \\dot{\\mathbf{q}}) = \\boldsymbol{\\tau} + J^\\top \\boldsymbol{\\lambda}\n\n\n\\phi(\\mathbf{q}) = 0, \\quad \\boldsymbol{\\lambda} \\geq 0, \\quad \\boldsymbol{\\lambda}^\\top \\phi = 0\n\nHere \\mathbf{q} are joint positions, M is the mass matrix, and \\boldsymbol{\\lambda} are contact forces enforcing non-penetration. But these physical equations are part of a larger simulator that also includes:\n\ncollision detection,\n\ncontact force models,\n\nsensor and actuator emulation,\n\nand visual rendering.\n\nThe full behavior of a robot interacting with its environment emerges only when the simulator is executed. While the underlying physics are well-understood, the complexity of contact dynamics, collision detection, and sensor modeling makes it impractical to expose the local dynamics function f directly.","type":"content","url":"/simulation#programs-as-models","position":3},{"hierarchy":{"lvl1":"Programs as Models","lvl2":"Systems with Discrete Events"},"type":"lvl2","url":"/simulation#systems-with-discrete-events","position":4},{"hierarchy":{"lvl1":"Programs as Models","lvl2":"Systems with Discrete Events"},"content":"Many simulation models arise when a system’s dynamics are driven not by time-continuous evolution, but by the occurrence of events. These discrete-event systems (DES) change state only at specific, often asynchronous points in time. Between events, the state remains fixed.\n\nA discrete-event system can be described by:\n\na set of discrete states \\mathcal{X},\n\na set of events \\mathcal{E},\n\na transition function f: \\mathcal{X} \\times \\mathcal{E} \\rightarrow \\mathcal{X},\n\nand a time-advance function t_a: \\mathcal{X} \\rightarrow \\mathbb{R}_{\\geq 0}.\n\nAt each point, the system checks which events are enabled and advances to the next scheduled one.\n\nExample: Network Traffic Control System\n\nConsider a software-defined networking (SDN) controller managing traffic routing in a data center. The system must make real-time decisions about packet forwarding paths based on network conditions and service requirements.\n\nThe discrete states \\mathcal{X} represent the current network configuration: active routing tables, link utilization levels, and quality-of-service priority queues at each switch.\n\nThe events \\mathcal{E} include:\n\nNew flow requests arriving (video streaming, database queries, file transfers)\n\nLink failures or congestion threshold violations\n\nFlow completion notifications\n\nLoad balancing triggers when servers exceed capacity\n\nNetwork policy updates from administrators\n\nThe transition function f captures how routing decisions change the network state. When a high-priority video conference flow arrives while a link is congested, the controller might transition to a new state where low-priority background traffic is rerouted through alternative paths.\n\nThe time-advance function t_a determines when the next routing decision occurs. Flow arrivals follow traffic patterns (bursty during business hours), while link failures are rare but unpredictable events.\n\nBetween events, packets follow the established routing rules—the same forwarding tables remain active across all switches. The control problem here is to adapt routing decisions to discrete network events, balancing throughput, latency, and reliability constraints.","type":"content","url":"/simulation#systems-with-discrete-events","position":5},{"hierarchy":{"lvl1":"Programs as Models","lvl2":"Hybrid Systems"},"type":"lvl2","url":"/simulation#hybrid-systems","position":6},{"hierarchy":{"lvl1":"Programs as Models","lvl2":"Hybrid Systems"},"content":"Some systems evolve continuously most of the time but undergo discrete jumps in response to certain conditions. These hybrid systems are common in control applications.\n\nThe system consists of:\n\na set of discrete modes q \\in \\mathcal{Q},\n\ncontinuous dynamics in each mode: \\dot{\\mathbf{x}} = f_q(\\mathbf{x}),\n\nguards that specify when transitions between modes occur,\n\nand reset maps that update the state during such transitions.\n\nExample: Thermostat Control\n\nAn HVAC system can be in one of several modes: heating, cooling, or off. The temperature evolves continuously according to physical laws, but when it crosses certain thresholds, the system switches modes:if x < setpoint - delta:\n    mode = \"heating\"\nelif x > setpoint + delta:\n    mode = \"cooling\"\nelse:\n    mode = \"off\"\n\nWithin each mode, a different differential equation applies. This results in a piecewise-smooth trajectory with mode-dependent dynamics.\n\nCase Study: Building Energy — EnergyPlus\n\nEnergyPlus provides a sophisticated example of hybrid systems in building energy simulation. At its core are physical equations describing heat flows:C_i \\frac{dT_i}{dt} = \\sum_j h_{ij} A_{ij}(T_j - T_i) + Q_i\n\nIt also solves implicit equations representing HVAC component behavior:0 = f(T, \\dot{m}, P)\n\nBut the actual simulator includes hundreds of thousands of lines of code handling:\n\ninterpolated weather data,\n\noccupancy schedules,\n\nequipment performance curves,\n\nand control logic implemented as finite-state machines.\n\nThe result is a program that emulates how a building behaves over time, given environmental inputs and schedules. The hybrid nature emerges from the interaction between continuous thermal dynamics and discrete control decisions made by thermostats, occupancy sensors, and HVAC equipment.","type":"content","url":"/simulation#hybrid-systems","position":7},{"hierarchy":{"lvl1":"Programs as Models","lvl2":"Agent-Based Models"},"type":"lvl2","url":"/simulation#agent-based-models","position":8},{"hierarchy":{"lvl1":"Programs as Models","lvl2":"Agent-Based Models"},"content":"Some simulation models do not describe systems via global state transitions, but instead simulate the behavior of many individual components or agents, each following local rules. These agent-based models (ABMs) are widely used in epidemiology, ecology, and social modeling.\n\nEach agent maintains its own internal state and acts according to probabilistic or rule-based logic. The system’s behavior arises from the interactions among agents.\n\nExample: Residential Energy Consumption under Dynamic Pricing\n\nConsider a neighborhood where each household is an agent making energy consumption decisions based on real-time electricity pricing and thermal comfort preferences. Each household agent has:\n\nInternal state: current temperature, HVAC settings, comfort preferences, price sensitivity\n\nLocal decision rules: MPC algorithms that optimize the trade-off between energy cost and thermal comfort\n\nUnique characteristics: different utility functions, thermal mass, occupancy patterns\n\nThe simulator might execute something like:for household in neighborhood:\n    # Each household solves its own MPC optimization\n    current_price = utility.get_current_price()\n    comfort_weight = household.comfort_preference\n    \n    # Optimize over prediction horizon\n    optimal_setpoint = household.mpc_controller.optimize(\n        current_temp=household.temperature,\n        price_forecast=utility.price_forecast,\n        comfort_weight=comfort_weight\n    )\n    \n    household.set_hvac_setpoint(optimal_setpoint)\n    \n    # Update shared grid load\n    neighborhood.total_demand += household.power_consumption\n\nThe macro-level demand patterns—peak shifting, load leveling, rebound effects—emerge from individual household optimization decisions. No single equation describes the neighborhood’s energy consumption; it arises from the collective behavior of autonomous agents each solving their own control problems.\n\nCase Study: Traffic Simulation — SUMO\n\nSUMO demonstrates agent-based modeling in transportation systems. Each vehicle is an agent with its own route, driving behavior, and decision-making logic. The Krauss car-following rule shows how individual vehicle agents behave:def update_vehicle(v, v_leader, gap, dt):\n    v_safe = v_leader + (gap - min_gap) / tau\n    v_desired = min(v_max, v_safe)\n    ε = random.uniform(0, 1)\n    v_new = max(0, v_desired - ε * a_max * dt)\n    x_new = x + v_new * dt\n    return x_new, v_new\n\nBeyond car-following, each vehicle agent also:\n\nplans routes through the network based on travel time estimates,\n\nresponds to traffic signals and road conditions,\n\nmakes lane-changing decisions based on utility functions,\n\nand exhibits individual driving characteristics (aggressiveness, reaction time).\n\nThe emergent traffic patterns—congestion formation, traffic waves, bottlenecks—arise from the collective behavior of thousands of individual vehicle agents, each following local rules and making autonomous decisions.\n\nCase Study: Modeling Curbside Access at Montréal–Trudeau (YUL)\n\nAfternoon traffic at Montréal–Trudeau airport regularly backs up along the two-lane ramp leading to the departures curb. As passenger volumes rebound, the mix of private drop-offs, taxis, and shuttles converging in a confined space produces frequent delays. When curb dwell times rise—especially around wide-body departures—queues can spill back onto the access road and interfere with other flows on the airport campus.\n\nTo manage the situation, the airport operator relies on a dense sensor network. Cameras and license plate readers track vehicle trajectories across virtual gates, generating a real-time stream of entry points, curb interactions, and exit times. According to public statements, AI-based forecasting solutions have been deployed to anticipate congestion and suggest alternative routing options for passengers and drivers. While no technical details have been disclosed, this is a typical instance of a traffic prediction and control problem that lends itself to agent-based modeling.\n\nIn such a model, each vehicle is treated as an individual agent with internal state:s_t = \\bigl(\\text{lane},\\;x_t,\\;v_t,\\;\\text{intent}\\bigr),\n\nwhere x_t and v_t denote longitudinal position and speed, and lane and intent capture higher-level behavioural traits. The simulation proceeds in discrete time. At each step, agents update their acceleration based on local traffic density (e.g., via a car-following model like IDM), evaluate potential lane changes (e.g., using a utility or incentive rule), and advance position accordingly.\n\nThe layout of the ramp—its geometry, merges, and constraints—is fixed. What changes are the traffic patterns and driver behaviours. These can be estimated from historical trajectories and updated as new data arrives. In a real-time setting, a filtering step adjusts the simulation so that its predicted flows remain consistent with current observations.\n\nWhile the behaviour of each individual agent is governed by program logic and heuristics—such as car-following rules, desired speeds, or gap acceptance—some parameters are identified offline from historical data, while others are estimated online. This adaptation helps the model track observed conditions. But even with such adjustments, not all effects are easily captured.\n\nConstruction activity, weather disturbances, and irregular flight scheduling can introduce sudden shifts in flow that lie outside the scope of the structural model. To account for these, one can overlay a data-driven correction on top of the simulation. Suppose the simulator produces a queue length forecast q^{\\text{sim}}_{t+h} over horizon h. A statistical model can be trained to predict the residual between this forecast and the observed outcome:r_{t+h} = q^{\\text{obs}}_{t+h} - q^{\\text{sim}}_{t+h},\n\nas a function of exogenous features z_t, such as weather, incident flags, or scheduled arrivals. The final forecast then becomes:\\widehat{q}_{t+h} = q^{\\text{sim}}_{t+h} + \\phi(z_t),\n\nwhere \\phi is a learned mapping from external conditions to the expected correction. The result is a hybrid model: the simulation enforces physical structure and agent-level behaviour, while the residual model compensates for aspects of the system that are harder to express analytically.","type":"content","url":"/simulation#agent-based-models","position":9},{"hierarchy":{"lvl1":"Looking Ahead"},"type":"lvl1","url":"/simulation#looking-ahead","position":10},{"hierarchy":{"lvl1":"Looking Ahead"},"content":"There’s no universally correct way to model a system. Your choice depends on what you know, what you can observe, what you care about, and what tools you have.\n\nThis chapter laid out a spectrum—from explicit, mechanistic models to black-box simulators and learned dynamics. In every case, modeling choices define the structure of the problem and the space of possible solutions. In the next chapters, we’ll see how they anchor learning, optimization, and decision-making.","type":"content","url":"/simulation#looking-ahead","position":11},{"hierarchy":{"lvl1":"Dynamics Models for Decision Making"},"type":"lvl1","url":"/ssm","position":0},{"hierarchy":{"lvl1":"Dynamics Models for Decision Making"},"content":"","type":"content","url":"/ssm","position":1},{"hierarchy":{"lvl1":"Dynamics Models for Decision Making"},"type":"lvl1","url":"/ssm#dynamics-models-for-decision-making","position":2},{"hierarchy":{"lvl1":"Dynamics Models for Decision Making"},"content":"The kind of model we need here is a dynamics model. It does not just describe correlations. It tells us how a system evolves in time and, most importantly for control, how that evolution responds to inputs we choose.\n\nA dynamics model earns its keep by answering counterfactuals of the form: given an initial condition and an input schedule, what trajectory should I expect? That ability to roll a trajectory forward under different candidate inputs is the backbone of planning, policy evaluation, and learning from interaction.\n\nAt this level, we can think of the model as a trajectory generator:(\\mathbf{x}_0,\\ \\{\\mathbf{u}_t\\},\\ \\{\\mathbf{d}_t\\}) \\ \\longmapsto\\ \\{\\mathbf{x}_t,\\ \\mathbf{y}_t\\}_{t=0:T},\n\nwhere \\mathbf{u}_t are controls we set, \\mathbf{d}_t are exogenous drivers we do not control (weather, inflow, demand), \\mathbf{x}_t are internal system variables, and \\mathbf{y}_t are observations. The split between \\mathbf{u} and \\mathbf{d} is practical: it separates what we can act on from what we must accommodate.\n\nTwo design pressures shape such models:\n\nResponsiveness to inputs. The model must expose the levers that matter for the decision problem, even if everything underneath is approximate.\n\nMemory management. To simulate step by step, we need a compact summary of the past that is sufficient to predict the next step once an input arrives. That summary is what we will call the state.\n\nThis brings us to a standard but powerful representation. Rather than carry the full history, we look for a variable \\mathbf{x}_t that captures “what matters so far” for predicting what comes next under a given input. With that variable in hand, the model advances in small increments and can be composed with estimators and controllers.\n\nWith this motivation in place, we can now introduce the formalism.","type":"content","url":"/ssm#dynamics-models-for-decision-making","position":3},{"hierarchy":{"lvl1":"The State‑Space Perspective"},"type":"lvl1","url":"/ssm#the-state-space-perspective","position":4},{"hierarchy":{"lvl1":"The State‑Space Perspective"},"content":"Most dynamics models, whether derived from physics or learned from data, can be cast into state‑space form. The state \\mathbf{x} is the compact memory that summarizes the past for prediction and control. Inputs \\mathbf{u} perturb that state, exogenous drivers \\mathbf{d} push it around, and outputs \\mathbf{y} are what we can measure. The equations look the same whether time is treated in discrete steps or as a continuous variable.","type":"content","url":"/ssm#the-state-space-perspective","position":5},{"hierarchy":{"lvl1":"The State‑Space Perspective","lvl2":"Discrete versus continuous time"},"type":"lvl2","url":"/ssm#discrete-versus-continuous-time","position":6},{"hierarchy":{"lvl1":"The State‑Space Perspective","lvl2":"Discrete versus continuous time"},"content":"How we represent time is dictated by how we sense and actuate: digital controllers sample and apply inputs in steps; the underlying physics evolve continuously.\n\nTime can be represented in two complementary ways, depending on how the system is sensed, actuated, or modelled.\n\nIn discrete time, we treat time as an integer counter, t = 0, 1, 2, \\dots, advancing in fixed steps. This matches how digital systems operate: sensors are sampled periodically, decisions are made at regular intervals, and most logged data takes this form.\n\nContinuous time treats time as a real variable, t \\in \\mathbb{R}_{\\ge 0}. Many physical systems (mechanical, thermal, chemical) are most naturally expressed this way, using differential equations to describe how state changes.\n\nThe two views are interchangeable to some extent. A continuous-time model can be discretized through numerical integration, although this involves approximation. The degree of approximation depends on both the step size \\Delta t and the integration algorithm used. Conversely, a discrete-time policy can be extended to continuous time by holding inputs constant over time intervals (a zeroth-order hold), or by interpolating between values.\n\nIn physical systems, this hybrid setup is almost always present. Control software sends discrete commands to hardware (say, the output of a PID controller) which are then processed by a DAC (digital-to-analog converter) and applied to the plant through analog signals. The hardware might hold a voltage constant, ramp it, or apply some analog shaping. On the sensing side, continuous signals are sampled via ADCs before reaching a digital controller. So in practice, even systems governed by continuous dynamics end up interfacing with the digital world through discrete-time approximations.\n\nThis raises a natural question: if everything eventually gets discretized anyway, why not just model everything in discrete time from the start?\n\nIn many cases, we do. But continuous-time models can still be useful, sometimes even necessary. They often make physical assumptions more explicit, connect more naturally to domain knowledge (e.g. differential equations in mechanics or thermodynamics), and expose invariances or conserved quantities that get obscured by time discretization. They also make it easier to model systems at different time scales, or to reason about how behaviors change as resolution increases. So while implementation happens in discrete time, thinking in continuous time can clarify the structure of the model.\n\nStill, it’s helpful to see how both representations look in mathematical form. The state-space equations are nearly identical with different notations depending on how time is represented.\n\nDiscrete time\n\nHaving defined state as the summary we carry forward, a step of prediction applies the chosen input and advances the state.\n\n\\mathbf{x}_{t+1} = f_t(\\mathbf{x}_t, \\mathbf{u}_t), \\qquad \\mathbf{y}_t = h_t(\\mathbf{x}_t, \\mathbf{u}_t).\n\nContinuous time\n\n\\dot{\\mathbf{x}}(t) = f(\\mathbf{x}(t), \\mathbf{u}(t)), \\qquad \\mathbf{y}(t) = h(\\mathbf{x}(t), \\mathbf{u}(t)).\n\nThe dot denotes a derivative with respect to real time; everything else (state, control, observation) remains the same.\n\nWhen the functions f and h are linear we obtain\n\nLinearity is not a belief about the world, it is a modeling choice that trades fidelity for transparency and speed.\n\n\\dot{\\mathbf{x}} = A\\mathbf{x} + B\\mathbf{u}, \\qquad \\mathbf{y} = C\\mathbf{x} + D\\mathbf{u}.\n\nThe matrices A, B, C, D may vary with t.  Readers with an ML background will recognise the parallel with recurrent neural networks: the state is the hidden vector, the control the input, and the output the read‑out layer.\n\nClassical control often moves to the frequency domain, using Laplace and Z‑transforms to turn differential and difference equations into algebraic ones. That is invaluable for stability analysis of linear time‑invariant systems, but the time‑domain state‑space view is more flexible for learning and simulation, so we will keep our primary focus there.","type":"content","url":"/ssm#discrete-versus-continuous-time","position":7},{"hierarchy":{"lvl1":"Examples of Deterministic Dynamics: HVAC Control"},"type":"lvl1","url":"/ssm#examples-of-deterministic-dynamics-hvac-control","position":8},{"hierarchy":{"lvl1":"Examples of Deterministic Dynamics: HVAC Control"},"content":"Imagine you’re in Montréal, in the middle of February. Outside it’s -20°C, but inside your home, a thermostat tries to keep things comfortable. When the indoor temperature drops below your setpoint, the heating system kicks in. That system (a small building, a heater, the surrounding weather) can be modeled mathematically.\n\nWe start with a very simple approximation: treat the entire room as a single “thermal mass,” like a big air-filled box that heats up or cools down depending on how much heat flows in or out.\n\nLet \\mathbf{x}(t) be the indoor air temperature at time t, and \\mathbf{u}(t) be the heating power supplied by the HVAC system. The outside air temperature, denoted \\mathbf{d}(t), affects the system too, acting as a known disturbance. Then the rate of change of indoor temperature is:\\dot{\\mathbf{x}}(t) = -\\frac{1}{RC}\\mathbf{x}(t) + \\frac{1}{RC}\\mathbf{d}(t) + \\frac{1}{C}\\mathbf{u}(t).\n\nHere:\n\nR is a thermal resistance: how well the walls insulate.\n\nC is a thermal capacitance: how much energy it takes to heat the air.\n\nThis is a continuous-time linear system, and we can write it in standard state-space form:\\dot{\\mathbf{x}}(t) = \\mathbf{A}\\mathbf{x}(t) + \\mathbf{B}\\mathbf{u}(t) + \\mathbf{E}\\mathbf{d}(t), \\quad \\mathbf{y}(t) = \\mathbf{C}\\mathbf{x}(t),\n\nwith:\n\n\\mathbf{x}(t): indoor air temperature (the state)\n\n\\mathbf{u}(t): heater input (the control)\n\n\\mathbf{d}(t): outdoor temperature (disturbance)\n\n\\mathbf{y}(t): observed indoor temperature (output)\n\n\\mathbf{A} = -\\frac{1}{RC}\n\n\\mathbf{B} = \\frac{1}{C}\n\n\\mathbf{E} = \\frac{1}{RC}\n\n\\mathbf{C} = 1\n\nThis model is simple, but too simplistic. It ignores the fact that the walls themselves store heat and release it slowly. This kind of delay is called thermal inertia: even if you turn the heater off, the walls might continue to warm the room for a while.\n\nTo capture this effect, we need to expand our state to include the wall temperature. We now model two coupled thermal masses: one for the air, and one for the wall. Heat can flow from the heater into the air, from the air into the wall, and from the wall out to the environment. This gives a more realistic description of how heat moves through a building envelope.\n\nWe write down an energy balance for each mass:\n\nFor the air:C_{\\text{air}} \\frac{dT_{\\text{in}}}{dt} = \\frac{T_{\\text{wall}} - T_{\\text{in}}}{R_{\\text{ia}}} + u(t),\n\nFor the wall:C_{\\text{wall}} \\frac{dT_{\\text{wall}}}{dt} = \\frac{T_{\\text{out}} - T_{\\text{wall}}}{R_{\\text{wo}}} - \\frac{T_{\\text{wall}} - T_{\\text{in}}}{R_{\\text{ia}}}.\n\nEach term on the right-hand side corresponds to a flow of heat: the air gains heat from the wall and the heater, and the wall exchanges heat with both the air and the outside.\n\nNow define the state vector:\\mathbf{x}(t) = \\begin{bmatrix} T_{\\text{in}}(t) \\\\ T_{\\text{wall}}(t) \\end{bmatrix},\n\\quad \\mathbf{u}(t) = u(t),\n\\quad \\mathbf{d}(t) = T_{\\text{out}}(t).\n\nDividing both equations by their respective capacitances and rearranging terms, we arrive at the coupled system:\\dot{\\mathbf{x}}(t) = \\mathbf{A}\\mathbf{x}(t) + \\mathbf{B}\\mathbf{u}(t) + \\mathbf{E}\\mathbf{d}(t), \\quad \\mathbf{y}(t) = \\mathbf{C}\\mathbf{x}(t),\n\nwith:\\mathbf{A} = \\begin{bmatrix}\n-\\frac{1}{R_{\\text{ia}}C_{\\text{air}}} & \\frac{1}{R_{\\text{ia}}C_{\\text{air}}} \\\\\n\\frac{1}{R_{\\text{ia}}C_{\\text{wall}}} & -\\left(\\frac{1}{R_{\\text{ia}}} + \\frac{1}{R_{\\text{wo}}}\\right) \\frac{1}{C_{\\text{wall}}}\n\\end{bmatrix},\n\\quad\n\\mathbf{B} = \\begin{bmatrix} \\frac{1}{C_{\\text{air}}} \\\\ 0 \\end{bmatrix},\n\\quad\n\\mathbf{E} = \\begin{bmatrix} 0 \\\\ \\frac{1}{R_{\\text{wo}}C_{\\text{wall}}} \\end{bmatrix},\n\\quad\n\\mathbf{C} = \\begin{bmatrix} 1 & 0 \\end{bmatrix}.\n\nEach entry in \\mathbf{A} has a physical interpretation:\n\nA_{11}: heat loss from the air to the wall\n\nA_{12}: heat gain by the air from the wall\n\nA_{21}: heat gain by the wall from the air\n\nA_{22}: net loss from the wall to both the air and the outside\n\nThe temperatures are now dynamically coupled: any change in one affects the other. The wall acts as a buffer that absorbs and releases heat over time.\n\nThis is still a linear system, just with a 2D state. But already it behaves differently. The walls absorb and release heat, smoothing out fluctuations and slowing down the system’s response.\n\nAs we add more rooms, walls, or building elements, the system grows. Each new temperature adds a new state. The equations still have the same structure, and their sparsity follows the building layout. Nodes represent temperatures; edges encode how heat flows between them.","type":"content","url":"/ssm#examples-of-deterministic-dynamics-hvac-control","position":9},{"hierarchy":{"lvl1":"Examples of Deterministic Dynamics: HVAC Control","lvl2":"What Do We Control?"},"type":"lvl2","url":"/ssm#what-do-we-control","position":10},{"hierarchy":{"lvl1":"Examples of Deterministic Dynamics: HVAC Control","lvl2":"What Do We Control?"},"content":"This network of states is what we control. What we mean by “control input” \\mathbf{u}(t) depends on both what we want to achieve and what we can implement in practice.\n\nThe most direct interpretation is to let \\mathbf{u}(t) represent the actual heating power delivered to the system, measured in watts. This makes sense when modeling from physical principles or simulating a system with fine-grained actuation.\n\nIn many real buildings, however, thermostats don’t issue power commands. They activate a relay, turning the heater on or off based on whether the measured temperature crosses a setpoint. Some systems allow for modulated control—such as varying fan speed or partially opening a valve—but those details are often hidden behind firmware or closed controllers.\n\nA common implementation involves a PID control loop that compares the measured temperature to a setpoint and adjusts the control signal accordingly. While the actual logic might be simple, the resulting behavior appears smoothed or delayed from the perspective of the building.\n\nDepending on the abstraction level, we might:\n\nTreat \\mathbf{u}(t) as continuous power input, if designing the full control logic.\n\nUse it as a setpoint input, assuming a lower-level controller handles the rest.\n\nOr reduce it to a binary signal—heater on or off—when working with logged behavior from a smart thermostat.\n\nEach perspective shapes the kind of model we build and the kind of control problem we pose. If we’re aiming to design a controller from scratch, it may be worth modeling the full closed-loop dynamics. If the goal is to tune setpoints or learn policies from data, a coarser abstraction might be not only sufficient, but more robust.","type":"content","url":"/ssm#what-do-we-control","position":11},{"hierarchy":{"lvl1":"Examples of Deterministic Dynamics: HVAC Control","lvl2":"Why This Model?"},"type":"lvl2","url":"/ssm#why-this-model","position":12},{"hierarchy":{"lvl1":"Examples of Deterministic Dynamics: HVAC Control","lvl2":"Why This Model?"},"content":"At this point, you might wonder: why go through the trouble of building this kind of physics-based model at all? After all, if we can log indoor temperatures, thermostat actions, and weather data, isn’t it easier to just learn a model from data? A neural ODE, for example, would let us define a parameterized function:\\dot{\\mathbf{z}}(t) = f_{\\boldsymbol{\\theta}}(\\mathbf{z}(t), \\mathbf{u}(t), \\mathbf{d}(t)), \\quad \\mathbf{y}(t) = g_{\\boldsymbol{\\theta}}(\\mathbf{z}(t)),\n\nwith both f_{\\boldsymbol{\\theta}} and g_{\\boldsymbol{\\theta}} learned from data. The internal state \\mathbf{z}(t) is not tied to any physical quantity. It just needs to be expressive enough to explain the observations.\n\nThat flexibility can be useful, particularly when a large dataset is already available. But in building control and energy modeling, the constraints are usually different.\n\nOften, the engineer or consultant on site is working under tight time and information budgets. A floor plan might be available, along with some basic specs on insulation or window types, and a few days of logged sensor data. The task might be to simulate load under different weather scenarios, tune a controller, or just help understand why a room is slow to heat up. The model has to be built quickly, adapted easily, and remain understandable to others working on the same system.\n\nIn that context, RC models are often the default choice: not because they are inherently better, but because they fit the workflow.\n\nInterpretability.\nThe parameters correspond to things you can reason about: thermal resistance, capacitance, heat transfer between zones. You can cross-check values against architectural plans, or adjust them manually when something doesn’t line up. You can tell which wall or zone is contributing to slow recovery times.\n\nIdentifiability with limited data.\nRC models can often be calibrated from short data traces, even when not all state variables are directly observable. The structure already imposes constraints: heat flows from hot to cold, dynamics are passive, responses are smooth. Those properties help narrow the space of valid parameter settings. A neural ODE, in contrast, typically needs more data to settle into stable and plausible dynamics—especially if no additional constraints are enforced during training.\n\nSimplicity and reuse.\nOnce the model is built, it’s straightforward to modify. If a window is replaced, or a wall gets insulated, you only need to update a few numbers.  It’s easy to pass along to another engineer or embed in a larger simulation. A model like\\dot{\\mathbf{x}} = \\mathbf{A}\\mathbf{x} + \\mathbf{B}\\mathbf{u} + \\mathbf{E}\\mathbf{d}\n\nis linear and low-dimensional. Simulating it is cheap, even if you do it many times. That may not matter now, but it will matter later, when we want to optimize over trajectories or learn from them.\n\nThis doesn’t mean RC models are always sufficient. They simplify or ignore many effects: solar gains, occupancy, nonlinearities, humidity, equipment switching behavior. If those effects are significant, and you have enough data, a black-box model (neural ODE or otherwise) might achieve lower prediction error. In practice, though, it’s common to combine the two: use the RC structure as a backbone, and learn a residual model to correct for unmodeled dynamics.","type":"content","url":"/ssm#why-this-model","position":13},{"hierarchy":{"lvl1":"From Deterministic to Stochastic"},"type":"lvl1","url":"/ssm#from-deterministic-to-stochastic","position":14},{"hierarchy":{"lvl1":"From Deterministic to Stochastic"},"content":"The models we’ve seen so far were deterministic: given an initial state and input sequence, the system evolves in a fixed, predictable way. But real systems rarely behave so neatly. Sensors are noisy. Parameters drift. The world changes in ways we can’t fully model.\n\nTo account for this uncertainty, we move from deterministic dynamics to stochastic models. There are two equivalent but conceptually distinct ways to do this.","type":"content","url":"/ssm#from-deterministic-to-stochastic","position":15},{"hierarchy":{"lvl1":"From Deterministic to Stochastic","lvl2":"Function plus Noise"},"type":"lvl2","url":"/ssm#function-plus-noise","position":16},{"hierarchy":{"lvl1":"From Deterministic to Stochastic","lvl2":"Function plus Noise"},"content":"The most direct extension adds a noise term to the dynamics:\\mathbf{x}_{t+1} = f_t(\\mathbf{x}_t, \\mathbf{u}_t, \\mathbf{w}_t), \\quad \\mathbf{w}_t \\sim p_{\\mathbf{w}}.\n\nIf the noise is additive and Gaussian, we recover the standard linear-Gaussian setup used in Kalman filtering:\\mathbf{x}_{t+1} = A\\mathbf{x}_t + B\\mathbf{u}_t + \\mathbf{w}_t, \\quad \\mathbf{w}_t \\sim \\mathcal{N}(0, Q).\n\nBut we’re not restricted to Gaussian or additive noise. For instance, if the noise distribution is non-Gaussian:\\mathbf{x}_{t+1} = f(\\mathbf{x}_t, \\mathbf{u}_t) + \\mathbf{w}_t, \\quad \\mathbf{w}_t \\sim \\text{Laplace}, \\ \\text{or}\\ \\text{Student-t},\n\nthen \\mathbf{x}_{t+1} inherits those properties. This is known as a convolution model: the next-state distribution is a shifted version of the noise distribution, centered around the deterministic prediction. More formally, it’s a special case of a pushforward measure: the randomness from \\mathbf{w}_t is “pushed forward” through the function f to yield a distribution over outcomes.\n\nOr the noise might enter multiplicatively:\\mathbf{x}_{t+1} = f(\\mathbf{x}_t, \\mathbf{u}_t) + \\Gamma(\\mathbf{x}_t, \\mathbf{u}_t) \\mathbf{w}_t,\n\nwhere \\Gamma is a matrix that modulates the effect of the noise, potentially depending on state and control. If \\Gamma is invertible, we can even write down an explicit density via a change-of-variables:p(\\mathbf{x}_{t+1} \\mid \\mathbf{x}_t, \\mathbf{u}_t) = p_{\\mathbf{w}}\\left(\\Gamma^{-1}(\\mathbf{x}_t, \\mathbf{u}_t)\\left[\\mathbf{x}_{t+1} - f(\\mathbf{x}_t, \\mathbf{u}_t)\\right] \\right)\\cdot \\left| \\det \\Gamma^{-1} \\right|.\n\nThis kind of structured noise is common in practice, for example, when disturbances are amplified at certain operating points.\n\nThe function-plus-noise view is natural when we have a physical or simulator-based model and want to account for uncertainty around it. It is constructive: we know how the system evolves and how the randomness enters. This means we can track the source of variability along a trajectory, which is particularly useful for techniques like reparameterization or infinitesimal perturbation analysis (IPA). These methods rely on being able to differentiate through the noise injection mechanism, something that is much easier when the noise is explicit and structured.","type":"content","url":"/ssm#function-plus-noise","position":17},{"hierarchy":{"lvl1":"From Deterministic to Stochastic","lvl2":"Transition Kernel"},"type":"lvl2","url":"/ssm#transition-kernel","position":18},{"hierarchy":{"lvl1":"From Deterministic to Stochastic","lvl2":"Transition Kernel"},"content":"The second perspective skips over the internal noise and defines the system directly in terms of the probability distribution over next states:p(\\mathbf{x}_{t+1} \\mid \\mathbf{x}_t, \\mathbf{u}_t).\n\nThis transition kernel encodes all the uncertainty in the system’s evolution, without reference to any underlying noise source or functional form.\n\nThis view is strictly more general: it includes the function-plus-noise case as a special instance. If we do know the function f and the noise distribution p_{\\mathbf{w}} from the generative model, then the transition kernel is obtained by “pushing” the randomness through the function:p(\\mathbf{x}_{t+1} \\mid \\mathbf{x}_t, \\mathbf{u}_t) = \\int \\delta(\\mathbf{x}_{t+1} - f(\\mathbf{x}_t, \\mathbf{u}_t, \\mathbf{w})) \\, p_{\\mathbf{w}}(\\mathbf{w}) \\, d\\mathbf{w}.\n\nThis might look abstract, but it’s just marginalization: for each possible noise value \\mathbf{w}, we compute the resulting next state, and then average over all possible \\mathbf{w}, weighted by how likely each one is.\n\nIf the noise were discrete, this becomes a sum:p(\\mathbf{x}_{t+1} \\mid \\mathbf{x}_t, \\mathbf{u}_t) = \\sum_{i=1}^k \\mathbb{1}\\{f(\\mathbf{x}_t, \\mathbf{u}_t, w_i) = \\mathbf{x}_{t+1}\\} \\cdot p_i\n\nThis abstraction is especially useful when we don’t know (or don’t care about) the underlying function or noise distribution. All we need is the ability to sample transitions or estimate their likelihoods. This is the default formulation in reinforcement learning, econometrics, and other settings focused on behavior rather than mechanism.","type":"content","url":"/ssm#transition-kernel","position":19},{"hierarchy":{"lvl1":"From Deterministic to Stochastic","lvl2":"Continuous-Time Analogue"},"type":"lvl2","url":"/ssm#continuous-time-analogue","position":20},{"hierarchy":{"lvl1":"From Deterministic to Stochastic","lvl2":"Continuous-Time Analogue"},"content":"In continuous time, the stochastic dynamics of a system are often described using a stochastic differential equation (SDE):d\\mathbf{X}_t = f(\\mathbf{X}_t, \\mathbf{U}_t)\\,dt + \\sigma(\\mathbf{X}_t, \\mathbf{U}_t)\\,d\\mathbf{W}_t,\n\nwhere \\mathbf{W}_t is Brownian motion. The first term, called the drift, describes the average motion of the system. The second, scaled by \\sigma, models how random fluctuations (diffusion) enter over time. Just like in discrete time, this is a function + noise model: the state evolves through a deterministic path perturbed by stochastic input.\n\nThis generative view again induces a probability distribution over future states. At any future time t + \\Delta t, the system doesn’t land at a single state but is described by a distribution that depends on the initial condition and the noise along the way.\n\nMathematically, this distribution evolves according to what’s called the Fokker–Planck equation—a partial differential equation that governs how probability density “flows” through time. It plays the same role here as the transition kernel did in discrete time: describing how likely the system is to be in any given state, without referring to the noise directly.\n\nWhile the mathematical generalization is clean, working with continuous-time stochastic models can be more challenging. Simulating sample paths is often straightforward (eg. nowadays diffusion models in generative AI), but writing down or computing the exact transition distribution usually isn’t. That’s why many practical methods still rely on discrete-time approximations, even when the underlying system is continuous.","type":"content","url":"/ssm#continuous-time-analogue","position":21},{"hierarchy":{"lvl1":"From Deterministic to Stochastic","lvl3":"Example: Managing a Québec Hydroelectric Reservoir","lvl2":"Continuous-Time Analogue"},"type":"lvl3","url":"/ssm#example-managing-a-qu-bec-hydroelectric-reservoir","position":22},{"hierarchy":{"lvl1":"From Deterministic to Stochastic","lvl3":"Example: Managing a Québec Hydroelectric Reservoir","lvl2":"Continuous-Time Analogue"},"content":"On the James Bay plateau, 1 400 km north of Montréal, the Robert-Bourassa reservoir stores roughly 62 km³ of water, more than the volume of Lake Ontario above its minimum operating level. Sixteen giant turbines sit 140 m below the surface, converting that stored head into 5.6 GW of electricity, about a fifth of Hydro-Québec’s total capacity. A steady share of that output feeds Québec’s aluminium smelters, which depend on stable, uninterrupted power.\n\nWater managers face competing objectives:\n\nFlood safety. Sudden snowmelt or storms can overfill the basin, forcing emergency spillways to open. These events are spectacular, but carry real downstream risk and economic cost.\n\nEnergy reliability. If the level falls too low, turbines sit idle and contracts go unmet. Voltage dips at the smelters are measured in lost millions.\n\nA basic deterministic model for the reservoir’s mass balance is just bookkeeping:\\mathbf{x}_{t+1} = \\mathbf{x}_t + \\mathbf{r}_t - \\mathbf{u}_t,\n\nwhere \\mathbf{x}_t is the current reservoir level, \\mathbf{u}_t is the controlled outflow through turbines, and \\mathbf{r}_t is the natural inflow from rainfall and upstream runoff.\n\nBut inflow is variable, and its statistical structure matters. Two hydrological regimes dominate:\n\nIn spring, melting snow over days can produce a long-tailed inflow distribution, often modeled as log-normal or Gamma.\n\nIn summer, convective storms yield a skewed mixture: a point mass at zero (no rain), and a thin but heavy tail capturing sudden bursts.\n\nThis motivates a simple stochastic extension:\\mathbf{x}_{t+1} = \\mathbf{x}_t - \\mathbf{u}_t + \\mathbf{w}_t, \\quad\n\\mathbf{w}_t \\sim\n\\begin{cases}\n0 & \\text{with prob. } p_0, \\\\\\\\\n\\text{LogNormal}(\\mu, \\sigma^2) & \\text{with prob. } 1 - p_0.\n\\end{cases}\n\nHere the physics is fixed, and all uncertainty sits in the inflow term \\mathbf{w}_t. Rather than fitting a full transition model from (\\mathbf{x}_t, \\mathbf{u}_t) to \\mathbf{x}_{t+1}, we can isolate the inflow by rearranging the mass balance:\\hat{\\mathbf{w}}_t = \\mathbf{x}_{t+1} - \\mathbf{x}_t + \\mathbf{u}_t.\n\nThis gives a direct estimate of the realized inflow at each timestep. From there, the problem becomes one of density estimation: fit a probabilistic model to the residuals \\hat{\\mathbf{w}}_t. In spring, this might be a log-normal distribution. In summer, a two-part mixture: a point mass at zero, and an exponential tail. These distributions can be estimated by maximum likelihood, or adjusted using additional features (covariates) such as upstream snowpack or forecasted temperature.\n\nThis setup has practical benefits. Fixing the physical part of the model (how levels respond to inflow and outflow) helps focus the statistical modeling effort. Rather than fitting a full system model, we only need to estimate the variability in inflows. This reduces the number of degrees of freedom and makes the estimation problem easier to interpret. It also avoids conflating uncertainty in inflow with uncertainty in the system’s response.\n\nCompare this to a more generic approach, such as linear regression:\\mathbf{x}_{t+1} = a \\mathbf{x}_t + b \\mathbf{u}_t + \\varepsilon_t.\n\nThis is straightforward to fit, but offers no guarantee that the result behaves sensibly. The model might violate conservation of mass, or compensate for inflow variation by adjusting coefficients a and b. This can lead to misleading conclusions, especially when extrapolating beyond the training data. Hydro‑Québec engineers rely on structured models in practice. Over 150 gauging stations across the La Grande basin report real-time flows, levels, and precipitation to Environment Canada's HYDAT database, which is accessible through a public API. These data feed into Hydro‑Québec's SCADA systems, along with snow-course readings and rainfall estimates. From there, engineers build seasonal inflow models and update them daily.\n\nSynthetic years are then generated by sampling from these models. Each sampled inflow sequence is pushed through the deterministic mass balance, producing a possible reservoir trajectory. These Monte Carlo rollouts are used directly for planning. They help evaluate turbine schedules, size safety margins, and identify periods of elevated risk.\n\nStructured models are not just a matter of physical fidelity. They shape how data is used, how uncertainty is handled, and how downstream decisions are informed. The separation between known dynamics and unknown inputs gives a cleaner interface between estimation and control. ","type":"content","url":"/ssm#example-managing-a-qu-bec-hydroelectric-reservoir","position":23},{"hierarchy":{"lvl1":"Partial Observability"},"type":"lvl1","url":"/ssm#partial-observability","position":24},{"hierarchy":{"lvl1":"Partial Observability"},"content":"So far, we’ve assumed that the full system state \\mathbf{x}_t is available. But in most real-world settings, only a partial or noisy observation is accessible. Sensors have limited coverage, measurements come with noise, and some variables aren’t observable at all.\n\nTo model this, we introduce an observation equation alongside the system dynamics:\\begin{aligned}\n\\mathbf{x}_{t+1} &= f_t(\\mathbf{x}_t, \\mathbf{u}_t, \\mathbf{w}_t), \\quad \\mathbf{w}_t \\sim p_{\\mathbf{w}}, \\\\\n\\mathbf{y}_t &= h_t(\\mathbf{x}_t, \\mathbf{v}_t), \\quad \\mathbf{v}_t \\sim p_{\\mathbf{v}}.\n\\end{aligned}\n\nThe state \\mathbf{x}_t evolves under control inputs \\mathbf{u}_t and process noise \\mathbf{w}_t, but we don’t get to see \\mathbf{x}_t directly. Instead, we observe \\mathbf{y}_t, which depends on \\mathbf{x}_t through some possibly nonlinear, noisy function h_t. The noise \\mathbf{v}_t captures measurement uncertainty.\n\nThis setup defines a partially observed system. Even if the underlying dynamics are known, we still face uncertainty due to limited visibility into the true state. The controller or estimator must rely on the observations \\mathbf{y}_{0\\:t} to make sense of the hidden trajectory.\n\nIn the deterministic case, if the output map h_t is full-rank and invertible, we may be able to reconstruct the state directly from the output: no filtering required. But once noise is introduced, that invertibility becomes more subtle: even if h_t is bijective, the presence of \\mathbf{v}_t prevents us from recovering \\mathbf{x}_t exactly. In this case, we must shift from inversion to estimation, often via probabilistic inference.\n\nIn the linear-Gaussian case, the model simplifies to:\\begin{aligned}\n\\mathbf{x}_{t+1} &= A\\mathbf{x}_t + B\\mathbf{u}_t + \\mathbf{w}_t, \\quad \\mathbf{w}_t \\sim \\mathcal{N}(0, Q), \\\\\n\\mathbf{y}_t &= C\\mathbf{x}_t + D\\mathbf{u}_t + \\mathbf{v}_t, \\quad \\mathbf{v}_t \\sim \\mathcal{N}(0, R).\n\\end{aligned}\n\nThis is the classical state-space model used in signal processing and control. It’s fully specified by the system matrices and the covariances Q and R. The state is no longer known, but under these assumptions it can be estimated recursively using the Kalman filter, which maintains a Gaussian belief over \\mathbf{x}_t.\n\nEven when the model is nonlinear or non-Gaussian, the structure remains the same: a dynamic state evolves, and a separate observation process links it to the data we see. Many modern estimation techniques, including extended and unscented Kalman filters, particle filters, and learned neural estimators, build on this core structure.","type":"content","url":"/ssm#partial-observability","position":25},{"hierarchy":{"lvl1":"Partial Observability","lvl2":"Observation Kernel View"},"type":"lvl2","url":"/ssm#observation-kernel-view","position":26},{"hierarchy":{"lvl1":"Partial Observability","lvl2":"Observation Kernel View"},"content":"Just as we moved from function-based dynamics to transition kernels, we can abstract away the noise source and define the observation distribution directly:p(\\mathbf{y}_t \\mid \\mathbf{x}_t).\n\nThis kernel summarizes what the sensors tell us about the hidden state. If we know the generative model—say, that \\mathbf{y}_t = h_t(\\mathbf{x}_t) + \\mathbf{v}_t with known p_{\\mathbf{v}}—then this kernel is induced by marginalizing out \\mathbf{v}_t:p(\\mathbf{y}_t \\mid \\mathbf{x}_t) = \\int \\delta\\bigl(\\mathbf{y}_t - h_t(\\mathbf{x}_t, \\mathbf{v})\\bigr)\\, p_{\\mathbf{v}}(\\mathbf{v})\\, d\\mathbf{v}.\n\nBut we don’t have to start from the generative form. In practice, we might define or learn p(\\mathbf{y}_t \\mid \\mathbf{x}_t) directly, especially when dealing with black-box sensors, perception models, or abstract measurement processes.","type":"content","url":"/ssm#observation-kernel-view","position":27},{"hierarchy":{"lvl1":"Partial Observability","lvl2":"Example – Stabilizing a Telescope’s Vision with Adaptive Optics"},"type":"lvl2","url":"/ssm#example-stabilizing-a-telescopes-vision-with-adaptive-optics","position":28},{"hierarchy":{"lvl1":"Partial Observability","lvl2":"Example – Stabilizing a Telescope’s Vision with Adaptive Optics"},"content":"On Earth, even the largest telescopes can’t see perfectly. As starlight travels through the atmosphere, tiny air pockets with different temperatures bend the light in slightly different directions. The result is a distorted image: instead of a sharp point, a star looks like a flickering blob. The distortion happens fast, on the order of milliseconds, and changes continuously as wind moves the turbulent layers overhead.\n\nThis is where adaptive optics (AO) comes in. AO systems aim to cancel out these distortions in real time. They do this by measuring how the incoming wavefront of light is distorted and using a flexible mirror to apply a counter-distortion that straightens it back out. But there’s a catch: you can’t observe the wavefront directly. You only get noisy measurements of its slopes (the angles of tilt at various points), and you have to act fast, before the atmosphere changes again.\n\nTo design a controller here, we need a model of how the distortions evolve. And that means building a decision-making model: one that includes uncertainty, partial observability, and fast feedback.\n\nState. The main object we’re trying to track is the distortion of the incoming wavefront. We can’t observe this phase field \\phi(\\mathbf{r}, t) directly, but we can represent it approximately using a finite basis (e.g., Fourier or Zernike). The coefficients of this expansion form our internal state:\\mathbf{x}_t \\in \\mathbb{R}^n \\quad \\text{(wavefront distortion at time } t).\n\nDynamics. The atmosphere evolves in time. A simple but surprisingly effective model assumes the turbulence is “frozen” and just blown across the telescope by the wind. That gives us a discrete-time linear model:\\mathbf{x}_{t+1} = \\mathbf{A} \\mathbf{x}_t + \\mathbf{w}_t,\n\nwhere \\mathbf{A} shifts the distortion pattern in space, and \\mathbf{w}_t is a small random change from evolving turbulence. This noise is not arbitrary: its statistics follow a power law derived from Kolmogorov’s turbulence model. In particular, higher spatial frequencies (small-scale wiggles) have less energy than low ones. That lets us build a prior on how likely different distortions are.\n\nObservations. We can’t see the full wavefront. Instead, we use a wavefront sensor: a camera that captures how the light bends. What it actually measures are local slopes: the gradients of the wavefront, not the wavefront itself. So our observation model is:\\mathbf{y}_t = \\mathbf{C} \\mathbf{x}_t + \\boldsymbol{\\varepsilon}_t,\n\nwhere \\mathbf{C} is a known matrix that maps wavefront distortion to measurable slope angles, and \\boldsymbol{\\varepsilon}_t is measurement noise (e.g., due to photon limits).\n\nControl. Our job is to flatten the wavefront using a deformable mirror. The mirror can apply a small counter-distortion \\mathbf{u}_t that subtracts from the atmospheric one:\\text{Residual state:} \\quad \\mathbf{x}_t^{\\text{res}} = \\mathbf{x}_t - \\mathbf{B} \\mathbf{u}_t.\n\nThe goal is to choose \\mathbf{u}_t to minimize the residual distortion by making the light flat again.\n\nWhy a model matters. Without a model, we’d just react to the current noisy measurements. But with a model, we can predict how the wavefront will evolve, filter out noise, and act preemptively. This is essential in AO, where decisions must be made every millisecond. Kalman filters are often used to track the hidden state \\mathbf{x}_t, combining model predictions with noisy measurements, and linear-quadratic regulators (LQR) or other optimal controllers use those estimates to choose the best correction.\n\nTime structure. This is a rare case where continuous-time modeling also plays a role. The true evolution of the turbulence is continuous, and we can model it using a stochastic differential equation (SDE):d\\mathbf{x}(t) = \\mathbf{F} \\mathbf{x}(t)\\,dt + \\mathbf{G}\\,d\\mathbf{W}(t),\n\nwhere \\mathbf{W}(t) is Brownian motion and the matrix \\mathbf{G} encodes the Kolmogorov spectrum. Discretizing this equation gives us the \\mathbf{A} and \\mathbf{Q} matrices for the discrete-time model above. ## Data-Driven Identification\n\nNot all models come from physics. Sometimes, we fit them directly from data.\n\nEven a basic linear regression of the form:\n\n$$\nx_{t+1} = a x_t + b u_t + c + \\varepsilon_t\n$$\n\nis a dynamical model. But things can get more sophisticated. Subspace identification methods, sparse regressions like SINDy, Koopman embeddings, neural ODEs—all of these let us learn models from observed trajectories. The key question is how much structure we assume. Do we enforce linearity? Time-invariance? Do we try to model the noise?  \n# Comparing Physics-Based RC Models with Black-Box Fits\n\nThe data used in this experiment comes from the *Building Energy Geeks* repository, an open-source collection created to demonstrate statistical learning techniques in building energy performance. The file `statespace.csv` provides a time series of indoor and outdoor temperatures together with heating power and solar irradiance. While not tied to a specific building description in the repository, it is designed to mimic realistic conditions either from actual sensor measurements or detailed simulation outputs. This dataset serves as a concrete foundation to explore the contrast between physics-based modeling and purely data-driven approaches.\n\nAt the core of our study lies the so-called **2R2C model**, a reduced-order representation of building thermal dynamics. The name refers to two resistances and two capacitances arranged in a thermal network that captures how heat flows between the indoor environment, the building envelope, and the outdoors. The indoor air temperature $T_i$ is influenced by the envelope temperature $T_e$, which itself exchanges heat with the external environment at temperature $T_o$. The resistances $R_i$ and $R_o$ describe the ease of heat conduction across these boundaries, while the capacitances $C_i$ and $C_e$ represent the heat storage capacity of the indoor air and of the building mass. By including the effect of heating input $\\Phi_H$ and solar gains $\\Phi_S$, the model balances both controllable and environmental influences.  \n\nMathematically, the dynamics are written as a pair of coupled ordinary differential equations. The first governs the indoor air temperature and is given by\n\n$$\n\\frac{dT_i}{dt} = \\frac{T_e - T_i}{R_i C_i} + \\frac{\\eta_H \\Phi_H}{C_i} + \\frac{A_i \\Phi_S}{C_i},\n$$\n\nwhile the second governs the envelope temperature,\n\n$$\n\\frac{dT_e}{dt} = \\frac{T_i - T_e}{R_i C_e} + \\frac{T_o - T_e}{R_o C_e} + \\frac{A_e \\Phi_S}{C_e}.\n$$\n\nHere $\\eta_H$ represents the efficiency of the heating system and $A_i, A_e$ are effective areas for solar gains. Since the original dataset is indexed in hours rather than seconds, the right-hand side of both equations must be scaled by a factor of 3600 to ensure correct integration over the chosen time unit.\n\nThe task is to identify the parameters of this model from data. To do so, we fit the parameters by minimizing the discrepancy between the simulated indoor temperature $T_i$ and the measured trajectory within a training window of 10 to 40 hours. A robust least-squares method with Huber loss is used so that large deviations, possibly due to noise or outliers, do not dominate the fit. Early time points in the training window are given slightly higher weight to ensure that the transient behavior is captured accurately, which is important when the system is initialized away from equilibrium. Once fitted, the model is simulated forward over the entire 0–100 hour horizon, allowing us to test its predictive power on an unseen window spanning 50 to 90 hours.\n\nTo provide meaningful context, we benchmark this physics-based model against two black-box alternatives. The first is a linear regression model that directly maps the contemporaneous values of outdoor temperature, heating power, and solar irradiance to the indoor temperature. This approach ignores temporal dynamics and treats the problem as a purely static regression. The second is a multilayer perceptron (MLP) that is trained with autoregressive lags. Specifically, the MLP is provided with the recent history of indoor temperatures together with the external inputs to predict the next indoor temperature. During training, a technique known as teacher forcing is employed, meaning the true past values of $T_i$ are always supplied, which allows the network to achieve a very tight fit on the training window. However, when rolled out on the test window without access to the ground-truth future values, small prediction errors accumulate, and the model struggles to generalize.\n\nThe results of this comparison illustrate a fundamental point. Although the MLP is highly flexible and achieves excellent accuracy on the training window, its predictions deteriorate rapidly on unseen data, demonstrating the pitfalls of overfitting and the instability of purely data-driven models in autoregressive settings. The linear regression baseline performs moderately but fails to capture the underlying physics, leading to systematic errors. In contrast, the 2R2C model, despite being governed by only a handful of parameters, extrapolates much more consistently. It responds in the correct direction to changes in heating and solar inputs, maintains stable long-term predictions, and provides parameters that map directly to interpretable physical properties such as insulation and thermal mass.  \n\nThis example therefore highlights the dual advantages of physics-based modeling: the ability to generalize beyond the training window and the guarantee of action-consistency rooted in thermodynamic reasoning. At the same time, it underscores the limitations of purely data-driven black-box models when asked to predict system behavior under conditions not seen during training.\n\n```{code-cell} ipython3\n:tags: [hide-input]\n\n%run _static/rcnetwork.py\n``` ","type":"content","url":"/ssm#example-stabilizing-a-telescopes-vision-with-adaptive-optics","position":29}]}