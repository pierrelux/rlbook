{"version":"1","records":[{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces"},"type":"lvl1","url":"/amortization","position":0},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces"},"content":"The previous chapter showed how fitted Q-iteration handles large state spaces through function approximation. FQI maintains a unified structure across batch and online settings: a replay buffer \\mathcal{B}_t inducing empirical distribution \\hat{P}_{\\mathcal{B}_t}, a target map T_q derived from the Bellman operator, a loss function \\ell, and an optimization budget. Algorithms differ in how they instantiate these components (buffer evolution, hard vs soft Bellman, update frequency), but all follow the same template.\n\nHowever, this framework breaks down when the action space becomes large or continuous. Computing Bellman targets requires evaluating \\max_{a' \\in \\mathcal{A}} q(s',a';\\boldsymbol{\\theta}) for each next state s'. When actions are continuous (\\mathcal{A} \\subset \\mathbb{R}^m), this maximization requires solving a nonlinear program at every target computation. For a replay buffer with millions of transitions, this becomes computationally prohibitive.\n\nThis chapter addresses the continuous action problem while maintaining the FQI framework. We develop several approaches, unified by a common theme: amortization. Rather than solving the optimization problem \\max_a q(s,a;\\boldsymbol{\\theta}) repeatedly at inference time, we invest computational effort during training to learn a mapping that directly produces good actions. This trades training-time cost for inference-time speed.\n\nThe strategies we examine are:\n\nExplicit optimization (Section 2): Solve the maximization numerically for a subset of states, accepting the computational cost for exact solutions.\n\nPolicy network amortization (Sections 3-5): Learn a deterministic or stochastic policy network \\pi_{\\boldsymbol{w}} that approximates \\arg\\max_a q(s,a;\\boldsymbol{\\theta}) or the optimal stochastic policy, enabling fast action selection via a single forward pass. This includes both hard-max methods (DDPG, TD3) and soft-max methods (SAC, PCL).\n\nEach approach represents a different point in the computation-accuracy trade-off, and all fit within the FQI template by modifying how targets are computed.","type":"content","url":"/amortization","position":1},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl2":"Explicit Optimization"},"type":"lvl2","url":"/amortization#explicit-optimization","position":2},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl2":"Explicit Optimization"},"content":"Recall that in fitted Q methods, the main idea is to compute the Bellman operator only at a subset of all states, relying on function approximation to generalize to the remaining states. At each step of the successive approximation loop, we build a dataset of input state-action pairs mapped to their corresponding optimality operator evaluations:\\mathcal{D}_n = \\{((s, a), (\\Bellman q)(s, a; \\boldsymbol{\\theta}_n)) \\mid (s,a) \\in \\mathcal{B}\\}\n\nThis dataset is then fed to our function approximator (neural network, random forest, linear model) to obtain the next set of parameters:\\boldsymbol{\\theta}_{n+1} \\leftarrow \\texttt{fit}(\\mathcal{D}_n)\n\nWhile this strategy allows us to handle very large or even infinite (continuous) state spaces, it still requires maximizing over actions (\\max_{a \\in \\mathcal{A}}) during the dataset creation when computing the operator \\Bellman for each basepoint. This maximization becomes computationally expensive for large action spaces. We can address this by adding another level of optimization: for each sample added to our regression dataset, we employ numerical optimization methods to find actions that maximize the Bellman operator for the given state.\n\nFitted Q-Iteration with Explicit Optimization\n\nInput: MDP (S, \\mathcal{A}, P, R, \\gamma), base points \\mathcal{B}, function approximator q(s,a; \\boldsymbol{\\theta}), maximum iterations N, tolerance \\varepsilon > 0\n\nOutput: Parameters \\boldsymbol{\\theta} for Q-function approximation\n\nInitialize \\boldsymbol{\\theta}_0\n\nn \\leftarrow 0\n\nrepeat\n\n\\mathcal{D} \\leftarrow \\emptyset \\quad // Regression dataset\n\nfor each (s,a,r,s') \\in \\mathcal{B} do \\quad // Monte Carlo with one sample\n\ny_{s,a} \\leftarrow r + \\gamma \\texttt{maximize}(q(s', \\cdot; \\boldsymbol{\\theta}_n))\n\n\\mathcal{D} \\leftarrow \\mathcal{D} \\cup \\{((s,a), y_{s,a})\\}\n\n\\boldsymbol{\\theta}_{n+1} \\leftarrow \\texttt{fit}(\\mathcal{D})\n\n\\delta \\leftarrow \\frac{1}{|\\mathcal{D}||\\mathcal{A}|}\\sum_{(s,a) \\in \\mathcal{D} \\times \\mathcal{A}} (q(s,a; \\boldsymbol{\\theta}_{n+1}) - q(s,a; \\boldsymbol{\\theta}_n))^2\n\nn \\leftarrow n + 1\n\nuntil \\delta < \\varepsilon or n \\geq N\n\nreturn \\boldsymbol{\\theta}_n\n\nThe above pseudocode introduces a generic \\texttt{maximize} routine which represents any numerical optimization method that searches for an action maximizing the given function. This approach is versatile and can be adapted to different types of action spaces. For continuous action spaces, we can employ standard nonlinear optimization methods like gradient descent or L-BFGS (e.g., using scipy.optimize.minimize). For large discrete action spaces, we can use integer programming solvers - linear integer programming if the Q-function approximator is linear in actions, or mixed-integer nonlinear programming (MINLP) solvers for nonlinear Q-functions. The choice of solver depends on the structure of our Q-function approximator and the constraints on our action space.\n\nWhile explicit optimization provides exact solutions, it becomes computationally expensive when we need to compute targets for millions of transitions in a replay buffer. Can we avoid solving an optimization problem at every decision? The answer is amortization.","type":"content","url":"/amortization#explicit-optimization","position":3},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl3":"Amortized Optimization Approach","lvl2":"Explicit Optimization"},"type":"lvl3","url":"/amortization#amortized-optimization-approach","position":4},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl3":"Amortized Optimization Approach","lvl2":"Explicit Optimization"},"content":"This process is computationally intensive. We can “amortize” some of this computation by replacing the explicit optimization for each sample with a direct mapping that gives us an approximate maximizer directly.\nFor Q-functions, recall that the operator is given by:(\\Bellman q)(s,a) = r(s,a) + \\gamma \\int p(ds'|s,a)\\max_{a' \\in \\mathcal{A}(s')} q(s', a')\n\nIf q^* is the optimal state-action value function, then v^*(s) = \\max_a q^*(s,a), and we can derive the optimal policy directly by computing the decision rule:\\pi^\\star(s) = \\arg\\max_{a \\in \\mathcal{A}(s)} q^\\star(s,a)\n\nSince q^* is a fixed point of \\Bellman, we can write:\\begin{align*}\nq^\\star(s,a) &= (\\Bellman q^*)(s,a) \\\\\n&= r(s,a) + \\gamma \\int p(ds'|s,a) \\max_{a' \\in \\mathcal{A}(s')} q^\\star(s', a') \\\\\n&= r(s,a) + \\gamma \\int p(ds'|s,a) q^\\star(s', \\pi^\\star(s'))\n\\end{align*}\n\nNote that \\pi^\\star is implemented by our \\texttt{maximize} numerical solver in the procedure above. A practical strategy would be to collect these maximizer values at each step and use them to train a function approximator that directly predicts these solutions. Due to computational constraints, we might want to compute these exact maximizer values only for a subset of states, based on some computational budget, and use the fitted decision rule to generalize to the remaining states. This leads to the following amortized version:\n\nFitted Q-Iteration with Amortized Optimization\n\nInput: MDP (S, \\mathcal{A}, P, R, \\gamma), base points \\mathcal{B}, subset for exact optimization \\mathcal{B}_{\\text{opt}} \\subset \\mathcal{B}, Q-function approximator q(s,a; \\boldsymbol{\\theta}), policy approximator \\pi_{\\boldsymbol{w}}, maximum iterations N, tolerance \\varepsilon > 0\n\nOutput: Parameters \\boldsymbol{\\theta} for Q-function, \\boldsymbol{w} for policy\n\nInitialize \\boldsymbol{\\theta}_0, \\boldsymbol{w}_0\n\nn \\leftarrow 0\n\nrepeat\n\n\\mathcal{D}_q \\leftarrow \\emptyset \\quad // Q-function regression dataset\n\n\\mathcal{D}_\\pi \\leftarrow \\emptyset \\quad // Policy regression dataset\n\nfor each (s,a,r,s') \\in \\mathcal{B} do\n\n// Determine next state’s action via exact optimization or approximation\n\nif s' \\in \\mathcal{B}_{\\text{opt}} then\n\na^*_{s'} \\leftarrow \\texttt{maximize}(q(s', \\cdot; \\boldsymbol{\\theta}_n))\n\n\\mathcal{D}_\\pi \\leftarrow \\mathcal{D}_\\pi \\cup \\{(s', a^*_{s'})\\}\n\nelse\n\na^*_{s'} \\leftarrow \\pi_{\\boldsymbol{w}_n}(s')\n\n// Compute Q-function target using chosen action\n\ny_{s,a} \\leftarrow r + \\gamma q(s', a^*_{s'}; \\boldsymbol{\\theta}_n)\n\n\\mathcal{D}_q \\leftarrow \\mathcal{D}_q \\cup \\{((s,a), y_{s,a})\\}\n\n// Update both function approximators\n\n\\boldsymbol{\\theta}_{n+1} \\leftarrow \\texttt{fit}(\\mathcal{D}_q)\n\n\\boldsymbol{w}_{n+1} \\leftarrow \\texttt{fit}(\\mathcal{D}_\\pi)\n\n// Compute convergence criteria\n\n\\delta_q \\leftarrow \\frac{1}{|\\mathcal{D}_q|}\\sum_{(s,a) \\in \\mathcal{D}_q} (q(s,a; \\boldsymbol{\\theta}_{n+1}) - q(s,a; \\boldsymbol{\\theta}_n))^2\n\n\\delta_\\pi \\leftarrow \\frac{1}{|\\mathcal{D}_\\pi|}\\sum_{(s,a^*) \\in \\mathcal{D}_\\pi} \\|a^* - \\pi_{\\boldsymbol{w}_{n+1}}(s)\\|^2\n\nn \\leftarrow n + 1\n\nuntil \\max(\\delta_q, \\delta_\\pi) < \\varepsilon or n \\geq N\n\nreturn \\boldsymbol{\\theta}_n, \\boldsymbol{w}_n\n\nNote that the policy \\pi_{\\boldsymbol{w}} is being trained on a dataset \\mathcal{D}_\\pi containing optimal actions computed with respect to an evolving Q-function. Specifically, at iteration n, we collect pairs (s', a^*_{s'}) where a^*_{s'} = \\arg\\max_a q(s', a; \\boldsymbol{\\theta}_n). However, after updating to \\boldsymbol{\\theta}_{n+1}, these actions may no longer be optimal with respect to the new Q-function.\n\nA natural approach to handle this staleness would be to maintain only the most recent optimization data. We could modify our procedure to keep a sliding window of K iterations, where at iteration n, we only use data from iterations \\max(0, n-K) to n. This would be implemented by augmenting each entry in \\mathcal{D}_\\pi with a timestamp:\\mathcal{D}_\\pi^{(n)} = \\{(s', a^*_{s'}, t) \\mid t \\in \\{n-K,\\ldots,n\\}\\}\n\nwhere t indicates the iteration at which the optimal action was computed. When fitting the policy network, we would then only use data points that are at most K iterations old:\\boldsymbol{w}_{n+1} \\leftarrow \\texttt{fit}(\\{(s', a^*_{s'}) \\mid (s', a^*_{s'}, t) \\in \\mathcal{D}_\\pi^{(n)}, n-K \\leq t \\leq n\\})\n\nThis introduces a trade-off between using more data (larger K) versus using more recent, accurate data (smaller K). The choice of K would depend on how quickly the Q-function evolves and the computational budget available for computing exact optimal actions.\n\nThe main limitation of this approach, beyond the out-of-distribution drift, is that it requires computing exact optimal actions via the solver for states in \\mathcal{B}_{\\text{opt}}. Can we reduce or eliminate this computational expense? As the policy improves at selecting actions, we can bootstrap from these increasingly better choices. Continuously amortizing these improving actions over time creates a virtuous cycle of self-improvement toward the optimal policy. However, this bootstrapping process requires careful management: moving too quickly can destabilize training.","type":"content","url":"/amortization#amortized-optimization-approach","position":5},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl2":"Deterministic Parametrized Policies"},"type":"lvl2","url":"/amortization#deterministic-parametrized-policies","position":6},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl2":"Deterministic Parametrized Policies"},"content":"In this section, we consider deterministic parametrized policies of the form \\pi_{\\boldsymbol{w}}(s) which directly output an action given a state. This approach differs from stochastic policies that output probability distributions over actions, making it particularly suitable for continuous control problems where the optimal policy is often deterministic. Fitted Q-value methods can be naturally extended to simultaneously learn both the Q-function and such a deterministic policy.","type":"content","url":"/amortization#deterministic-parametrized-policies","position":7},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl3":"The Amortization Problem for Continuous Actions","lvl2":"Deterministic Parametrized Policies"},"type":"lvl3","url":"/amortization#the-amortization-problem-for-continuous-actions","position":8},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl3":"The Amortization Problem for Continuous Actions","lvl2":"Deterministic Parametrized Policies"},"content":"When actions are continuous, a \\in \\mathbb{R}^d, extracting a greedy policy from a Q-function becomes computationally expensive. Consider a robot arm control task where the action is a d-dimensional torque vector. To act greedily given Q-function q(s,a; \\boldsymbol{\\theta}), we must solve:\\pi(s) = \\arg\\max_{a \\in \\mathcal{A}} q(s, a; \\boldsymbol{\\theta}),\n\nwhere \\mathcal{A} \\subset \\mathbb{R}^d is a continuous set (often a box or polytope). This requires running an optimization algorithm at every time step. For neural network Q-functions, this means solving a nonlinear program whose objective involves forward passes through the network.\n\nAfter training converges, the agent must select actions in real-time during deployment. Running interior-point methods or gradient-based optimizers at every decision creates unacceptable latency, especially in high-frequency control where decisions occur at 100Hz or faster.\n\nThe solution is to amortize the optimization cost by learning a separate policy network \\pi_{\\boldsymbol{w}}(s) that directly outputs actions. During training, we optimize \\boldsymbol{w} so that \\pi_{\\boldsymbol{w}}(s) \\approx \\arg\\max_a q(s,a; \\boldsymbol{\\theta}) for states we encounter. At deployment, action selection reduces to a single forward pass through the policy network: a = \\pi_{\\boldsymbol{w}}(s). The computational cost of optimization is paid during training (where time is less constrained) rather than at inference.\n\nThis introduces a second approximation beyond the Q-function. We now have two function approximators: a critic q(s,a; \\boldsymbol{\\theta}) that estimates values, and an actor \\pi_{\\boldsymbol{w}}(s) that selects actions. The critic is trained using Bellman targets as in standard fitted Q-iteration. The actor is trained to maximize the critic:\\boldsymbol{w} \\leftarrow \\boldsymbol{w} + \\alpha \\mathbb{E}_s \\left[\\nabla_{\\boldsymbol{w}} q(s, \\pi_{\\boldsymbol{w}}(s); \\boldsymbol{\\theta})\\right],\n\nwhere the expectation is over states in the dataset or replay buffer. This gradient ascent pushes the actor toward actions that the critic considers valuable. By the chain rule, this equals (\\nabla_a q(s,a; \\boldsymbol{\\theta})|_{a=\\pi_{\\boldsymbol{w}}(s)}) \\cdot (\\nabla_{\\boldsymbol{w}} \\pi_{\\boldsymbol{w}}(s)), which can be efficiently computed via backpropagation through the composition of the two networks.","type":"content","url":"/amortization#the-amortization-problem-for-continuous-actions","position":9},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl3":"Neural Fitted Q-Iteration for Continuous Actions (NFQCA)","lvl2":"Deterministic Parametrized Policies"},"type":"lvl3","url":"/amortization#neural-fitted-q-iteration-for-continuous-actions-nfqca","position":10},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl3":"Neural Fitted Q-Iteration for Continuous Actions (NFQCA)","lvl2":"Deterministic Parametrized Policies"},"content":"NFQCA \n\nHafner & Riedmiller, 2011 extends the NFQI template from the \n\nprevious chapter to handle continuous action spaces by replacing the \\max_{a'} q(s',a'; \\boldsymbol{\\theta}) operator in the Bellman target with a parameterized policy \\pi_{\\boldsymbol{w}}(s'). This transforms fitted Q-iteration into an actor-critic method: the critic q(s,a; \\boldsymbol{\\theta}) evaluates state-action pairs via the standard regression step, while the actor \\pi_{\\boldsymbol{w}}(s) provides actions by directly maximizing the learned Q-function.\n\nThe algorithm retains the two-level structure of NFQI: an outer loop performs approximate value iteration by computing Bellman targets, and an inner loop fits the Q-function to those targets. NFQCA adds a third component (policy improvement) that updates \\boldsymbol{w} to maximize the Q-function over states sampled from the dataset.","type":"content","url":"/amortization#neural-fitted-q-iteration-for-continuous-actions-nfqca","position":11},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl4":"From Discrete to Continuous Actions","lvl3":"Neural Fitted Q-Iteration for Continuous Actions (NFQCA)","lvl2":"Deterministic Parametrized Policies"},"type":"lvl4","url":"/amortization#from-discrete-to-continuous-actions","position":12},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl4":"From Discrete to Continuous Actions","lvl3":"Neural Fitted Q-Iteration for Continuous Actions (NFQCA)","lvl2":"Deterministic Parametrized Policies"},"content":"Recall from the \n\nFQI chapter that NFQI computes Bellman targets using the hard max:y_{s,a} = r + \\gamma \\max_{a' \\in \\mathcal{A}} q(s',a'; \\boldsymbol{\\theta}_n)\n\nWhen \\mathcal{A} is finite and small, this max is computed by enumeration. When \\mathcal{A} is continuous or high-dimensional, enumeration is intractable. NFQCA replaces the max with a parameterized policy that approximately solves the maximization:y_{s,a} = r + \\gamma q(s', \\pi_{\\boldsymbol{w}_n}(s'); \\boldsymbol{\\theta}_n)\n\nThe policy \\pi_{\\boldsymbol{w}}(s) acts as an amortized optimizer: instead of solving \\arg\\max_{a'} q(s',a') from scratch at each state s' during target computation, we train a neural network to output near-optimal actions directly. The term “amortized” refers to spreading the cost of optimization across training: we pay once to learn \\pi_{\\boldsymbol{w}}, then reuse it for all future target computations.\n\nTo train the policy, we maximize the expected Q-value under the distribution of states in the dataset. If we had access to the optimal Q-function q^*, we would solve:\\max_{\\boldsymbol{w}} \\mathbb{E}_{s \\sim \\hat{P}_{\\mathcal{D}}}[q^*(s, \\pi_{\\boldsymbol{w}}(s))]\n\nwhere \\hat{P}_{\\mathcal{D}} is the empirical distribution over states induced by the offline dataset \\mathcal{D}. In practice, we use the current Q-function approximation q(s,a; \\boldsymbol{\\theta}_{n+1}) after it has been fitted to the latest targets. The expectation is approximated by the sample average over states appearing in \\mathcal{D}:\\max_{\\boldsymbol{w}} \\frac{1}{|\\mathcal{D}|} \\sum_{(s,a,r,s') \\in \\mathcal{D}} q(s, \\pi_{\\boldsymbol{w}}(s); \\boldsymbol{\\theta}_{n+1})\n\nThis policy improvement step runs after the Q-function has been updated, using the newly-fitted critic to guide the actor toward higher-value actions. Both the Q-function fitting and policy improvement use gradient-based optimization on the respective objectives.\n\nNeural Fitted Q-Iteration with Continuous Actions (NFQCA)\n\nInput: Dataset \\mathcal{D} = \\{(s_i, a_i, r_i, s'_i)\\}_{i=1}^N, Q-function q(s,a; \\boldsymbol{\\theta}), policy \\pi_{\\boldsymbol{w}}(s), discount factor \\gamma, learning rates \\alpha_q, \\alpha_\\pi, inner optimization steps K_q, K_\\pi\n\nOutput: Q-function parameters \\boldsymbol{\\theta}, policy parameters \\boldsymbol{w}\n\nInitialize \\boldsymbol{\\theta}_0, \\boldsymbol{w}_0\n\nn \\leftarrow 0\n\nrepeat \\quad // Outer loop: Policy Evaluation and Improvement\n\n\\quad // Construct regression dataset with policy-based Bellman targets\n\n\\quad \\mathcal{D}_n^{\\text{fit}} \\leftarrow \\emptyset\n\n\\quad for each (s,a,r,s') \\in \\mathcal{D} do\n\n\\quad\\quad a' \\leftarrow \\pi_{\\boldsymbol{w}_n}(s') \\quad // Actor selects action (replaces max)\n\n\\quad\\quad y_{s,a} \\leftarrow r + \\gamma q(s', a'; \\boldsymbol{\\theta}_n) \\quad // Critic evaluates\n\n\\quad\\quad \\mathcal{D}_n^{\\text{fit}} \\leftarrow \\mathcal{D}_n^{\\text{fit}} \\cup \\{((s,a), y_{s,a})\\}\n\n\\quad // Policy evaluation: fit Q-function to targets\n\n\\quad \\boldsymbol{\\theta}_{n+1} \\leftarrow \\texttt{fit}_q(\\mathcal{D}_n^{\\text{fit}}, \\boldsymbol{\\theta}_n, K_q, \\alpha_q)\n\n\\quad // Policy improvement: maximize Q-function over dataset states\n\n\\quad \\boldsymbol{w}_{n+1} \\leftarrow \\texttt{fit}_\\pi(\\mathcal{D}, \\boldsymbol{w}_n, \\boldsymbol{\\theta}_{n+1}, K_\\pi, \\alpha_\\pi)\n\n\\quad n \\leftarrow n + 1\n\nuntil convergence or n \\geq n_{\\max}\n\nreturn \\boldsymbol{\\theta}_n, \\boldsymbol{w}_n\n\nwhere:\n\n\\texttt{fit}_q(\\mathcal{D}_n^{\\text{fit}}, \\boldsymbol{\\theta}_n, K_q, \\alpha_q) runs K_q gradient steps minimizing \\frac{1}{|\\mathcal{D}_n^{\\text{fit}}|} \\sum_{((s,a), y) \\in \\mathcal{D}_n^{\\text{fit}}} (q(s,a; \\boldsymbol{\\theta}) - y)^2, warm starting from \\boldsymbol{\\theta}_n\n\n\\texttt{fit}_\\pi(\\mathcal{D}, \\boldsymbol{w}_n, \\boldsymbol{\\theta}_{n+1}, K_\\pi, \\alpha_\\pi) runs K_\\pi gradient steps maximizing \\frac{1}{|\\mathcal{D}|} \\sum_{(s,a,r,s') \\in \\mathcal{D}} q(s, \\pi_{\\boldsymbol{w}}(s); \\boldsymbol{\\theta}_{n+1}), warm starting from \\boldsymbol{w}_n\n\nThe algorithm structure mirrors NFQI (Algorithm \n\nAlgorithm 1 in the \n\nFQI chapter) with two extensions. First, target computation (line 7-8) replaces the discrete max with a policy network call \\pi_{\\boldsymbol{w}_n}(s'), making the Bellman operator tractable for continuous actions. Second, after fitting the Q-function (line 11), we add a policy improvement step (line 13) that updates \\boldsymbol{w} to maximize the Q-function evaluated at policy-generated actions over states in the dataset.\n\nBoth fit operations use gradient descent with warm starting, consistent with the NFQI template. The Q-function minimizes squared Bellman error using targets computed with the current policy. The policy maximizes the Q-function via gradient ascent on the composition q(s, \\pi_{\\boldsymbol{w}}(s); \\boldsymbol{\\theta}_{n+1}), which is differentiable end-to-end when both networks are differentiable. The gradient with respect to \\boldsymbol{w} is:\\nabla_{\\boldsymbol{w}} q(s, \\pi_{\\boldsymbol{w}}(s); \\boldsymbol{\\theta}) = \\nabla_a q(s, a; \\boldsymbol{\\theta})\\Big|_{a=\\pi_{\\boldsymbol{w}}(s)} \\cdot \\nabla_{\\boldsymbol{w}} \\pi_{\\boldsymbol{w}}(s)\n\ncomputed via the chain rule (backpropagation through the actor into the critic). Modern automatic differentiation libraries handle this composition automatically.","type":"content","url":"/amortization#from-discrete-to-continuous-actions","position":13},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl3":"Deep Deterministic Policy Gradient (DDPG)","lvl2":"Deterministic Parametrized Policies"},"type":"lvl3","url":"/amortization#deep-deterministic-policy-gradient-ddpg","position":14},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl3":"Deep Deterministic Policy Gradient (DDPG)","lvl2":"Deterministic Parametrized Policies"},"content":"We now extend NFQCA to the online setting with evolving replay buffers, mirroring how DQN extended NFQI in the \n\nFQI chapter. Just as DQN allowed \\mathcal{B}_t and \\hat{P}_{\\mathcal{B}_t} to evolve during learning instead of using a fixed offline dataset, DDPG \n\nLillicrap et al., 2015 collects new transitions during training and stores them in a circular replay buffer.\n\nLike DQN, DDPG uses the flattened FQI structure with target networks. But where DQN maintains a single target network \\boldsymbol{\\theta}_{\\text{target}} for the Q-function, DDPG maintains two target networks: one for the critic \\boldsymbol{\\theta}_{\\text{target}} and one for the actor \\boldsymbol{w}_{\\text{target}}. Both are updated periodically (every K steps) to mark outer-iteration boundaries, following the same nested-to-flattened transformation shown for DQN.\n\nThe online network now plays a triple role in DDPG: (1) the parameters being actively trained (\\boldsymbol{\\theta}_t for critic, \\boldsymbol{w}_t for actor), (2) the policy used to collect new data, and (3) the gradient source for policy improvement. The target networks serve only one purpose: computing stable Bellman targets.","type":"content","url":"/amortization#deep-deterministic-policy-gradient-ddpg","position":15},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl4":"Exploration via Action Noise","lvl3":"Deep Deterministic Policy Gradient (DDPG)","lvl2":"Deterministic Parametrized Policies"},"type":"lvl4","url":"/amortization#exploration-via-action-noise","position":16},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl4":"Exploration via Action Noise","lvl3":"Deep Deterministic Policy Gradient (DDPG)","lvl2":"Deterministic Parametrized Policies"},"content":"Since the policy \\pi_{\\boldsymbol{w}}(s) is deterministic, exploration requires adding noise to actions during data collection:a = \\pi_{\\boldsymbol{w}_t}(s) + \\eta_t\n\nwhere \\eta_t is exploration noise. The original DDPG paper used an Ornstein-Uhlenbeck (OU) process, which generates temporally correlated noise through the discretized stochastic differential equation:\\eta_{t+1} = \\eta_t + \\theta(\\mu - \\eta_t)\\Delta t + \\sigma\\sqrt{\\Delta t}\\epsilon_t, \\quad \\epsilon_t \\sim \\mathcal{N}(0,1)\n\nwhere \\mu is the long-term mean (typically 0), \\theta controls the strength of mean reversion, \\sigma scales the random fluctuations, and \\Delta t is the time step. The term \\theta(\\mu - \\eta_t)\\Delta t acts like damped motion through a viscous fluid: when \\eta_t deviates from \\mu, this force pulls it back smoothly without oscillation. The random term \\sigma\\sqrt{\\Delta t}\\epsilon_t adds perturbations, creating noise that wanders but is gently pulled back toward \\mu. This temporal correlation produces smoother exploration trajectories than independent Gaussian noise.\n\nHowever, later work (including TD3, discussed below) found that simple uncorrelated Gaussian noise \\eta_t \\sim \\mathcal{N}(0, \\sigma^2) works equally well and is easier to tune. The exploration mechanism is orthogonal to the core algorithmic structure.\n\nDeep Deterministic Policy Gradient (DDPG)\n\nInput: MDP (S, \\mathcal{A}, P, R, \\gamma), Q-network q(s,a; \\boldsymbol{\\theta}), policy network \\pi_{\\boldsymbol{w}}(s), learning rates \\alpha_q, \\alpha_\\pi, target update frequency K, replay buffer capacity B, mini-batch size b, exploration noise \\eta\n\nOutput: Q-function parameters \\boldsymbol{\\theta}, policy parameters \\boldsymbol{w}\n\nInitialize \\boldsymbol{\\theta}_0, \\boldsymbol{w}_0 randomly\n\n\\boldsymbol{\\theta}_{\\text{target}} \\leftarrow \\boldsymbol{\\theta}_0, \\boldsymbol{w}_{\\text{target}} \\leftarrow \\boldsymbol{w}_0\n\nInitialize replay buffer \\mathcal{B} with capacity B\n\nInitialize exploration noise process \\eta (e.g., OU process or Gaussian)\n\nt \\leftarrow 0\n\nwhile training do\n\nObserve current state s\n\n// Use online network to collect data with exploration noise\n\nSelect action: a \\leftarrow \\pi_{\\boldsymbol{w}_t}(s) + \\eta_t\n\nExecute a, observe reward r and next state s'\n\nStore (s,a,r,s') in \\mathcal{B}, replacing oldest if full\n\nSample mini-batch of b transitions \\{(s_i,a_i,r_i,s_i')\\}_{i=1}^b from \\mathcal{B}\n\nfor each sampled transition (s_i,a_i,r_i,s_i') do\n\ny_i \\leftarrow r_i + \\gamma q(s'_i, \\pi_{\\boldsymbol{w}_{\\text{target}}}(s'_i); \\boldsymbol{\\theta}_{\\text{target}}) \\quad // Actor target selects, critic target evaluates\n\n\\boldsymbol{\\theta}_{t+1} \\leftarrow \\boldsymbol{\\theta}_t - \\alpha_q \\nabla_{\\boldsymbol{\\theta}} \\frac{1}{b}\\sum_{i=1}^b (q(s_i,a_i;\\boldsymbol{\\theta}_t) - y_i)^2\n\n\\boldsymbol{w}_{t+1} \\leftarrow \\boldsymbol{w}_t + \\alpha_\\pi \\frac{1}{b}\\sum_{i=1}^b \\nabla_a q(s_i,a;\\boldsymbol{\\theta}_{t+1})\\Big|_{a=\\pi_{\\boldsymbol{w}_t}(s_i)} \\cdot \\nabla_{\\boldsymbol{w}} \\pi_{\\boldsymbol{w}_t}(s_i)\n\nif t \\bmod K = 0 then\n\n\\boldsymbol{\\theta}_{\\text{target}} \\leftarrow \\boldsymbol{\\theta}_t\n\n\\boldsymbol{w}_{\\text{target}} \\leftarrow \\boldsymbol{w}_t\n\nt \\leftarrow t + 1\n\nreturn \\boldsymbol{\\theta}_t, \\boldsymbol{w}_t\n\nThe algorithm structure parallels DQN (Algorithm \n\nAlgorithm 5 in the \n\nFQI chapter) with the continuous-action extensions from NFQCA. Lines 1-5 initialize both networks and their targets, following the same pattern as DQN but with an additional actor network. Line 3 uses the online actor with exploration noise for data collection, replacing DQN’s \\varepsilon-greedy selection. Line 7 computes targets using both target networks: the actor target \\pi_{\\boldsymbol{w}_{\\text{target}}}(s'_i) selects the next action, the critic target q(\\cdot; \\boldsymbol{\\theta}_{\\text{target}}) evaluates it. This replaces the \\max_{a'} operator in DQN. Lines 8-9 update both networks: critic via TD error minimization, actor via policy gradient through the updated critic. Line 10 performs periodic hard updates every K steps, marking outer-iteration boundaries.\n\nThe policy gradient in line 9 uses the chain rule to backpropagate through the actor-critic composition:\\nabla_{\\boldsymbol{w}} q(s, \\pi_{\\boldsymbol{w}}(s); \\boldsymbol{\\theta}) = \\nabla_a q(s,a; \\boldsymbol{\\theta})\\Big|_{a=\\pi_{\\boldsymbol{w}}(s)} \\cdot \\nabla_{\\boldsymbol{w}} \\pi_{\\boldsymbol{w}}(s)\n\nThis is identical to the NFQCA gradient, but now computed on mini-batches sampled from an evolving replay buffer rather than a fixed offline dataset. The critic gradient \\nabla_a q(s,a; \\boldsymbol{\\theta}) at the policy-generated action provides the direction of steepest ascent in Q-value space, weighted by how sensitive the policy output is to its parameters via \\nabla_{\\boldsymbol{w}} \\pi_{\\boldsymbol{w}}(s).","type":"content","url":"/amortization#exploration-via-action-noise","position":17},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl3":"Twin Delayed Deep Deterministic Policy Gradient (TD3)","lvl2":"Deterministic Parametrized Policies"},"type":"lvl3","url":"/amortization#twin-delayed-deep-deterministic-policy-gradient-td3","position":18},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl3":"Twin Delayed Deep Deterministic Policy Gradient (TD3)","lvl2":"Deterministic Parametrized Policies"},"content":"DDPG inherits the overestimation bias from DQN’s use of the max operator in Bellman targets. TD3 \n\nFujimoto et al., 2018 addresses this through three modifications to the DDPG template, following similar principles to Double DQN but adapted for continuous actions and taking a more conservative approach.","type":"content","url":"/amortization#twin-delayed-deep-deterministic-policy-gradient-td3","position":19},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl4":"Twin Q-Networks and the Minimum Operator","lvl3":"Twin Delayed Deep Deterministic Policy Gradient (TD3)","lvl2":"Deterministic Parametrized Policies"},"type":"lvl4","url":"/amortization#twin-q-networks-and-the-minimum-operator","position":20},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl4":"Twin Q-Networks and the Minimum Operator","lvl3":"Twin Delayed Deep Deterministic Policy Gradient (TD3)","lvl2":"Deterministic Parametrized Policies"},"content":"Recall from the \n\nMonte Carlo chapter that overestimation arises when we use the same noisy estimate both to select which action looks best and to evaluate that action. Double Q-learning breaks this coupling by maintaining two independent estimators with noise terms \\varepsilon^{(1)}_a and \\varepsilon^{(2)}_a:a^\\star = \\arg\\max_{a} \\left\\{r(s,a) + \\gamma \\hat{\\mu}^{(1)}_N(s,a)\\right\\}, \\quad Y = r(s,a^\\star) + \\gamma \\hat{\\mu}^{(2)}_N(s,a^\\star).\n\nWhen \\varepsilon^{(1)} and \\varepsilon^{(2)} are independent, the tower property of conditional expectation gives \\mathbb{E}[\\varepsilon^{(2)}_{a^\\star} \\mid a^\\star] = \\mathbb{E}[\\varepsilon^{(2)}_{a^\\star}] = 0 because a^\\star (determined by \\varepsilon^{(1)}) is independent of \\varepsilon^{(2)}. This eliminates evaluation bias: we no longer use the same positive noise that selected an action to also inflate its value. By conditioning on the selected action and then taking expectations over the independent evaluation noise, the bias in the evaluation term vanishes.\n\nDouble DQN (Algorithm \n\nAlgorithm 6) implements this principle in the discrete action setting by using the online network \\boldsymbol{\\theta}_t for selection (a^*_i \\leftarrow \\arg\\max_{a'} q(s_i',a'; \\boldsymbol{\\theta}_t)) and the target network \\boldsymbol{\\theta}_{\\text{target}} for evaluation (y_i \\leftarrow r_i + \\gamma q(s_i',a^*_i; \\boldsymbol{\\theta}_{\\text{target}})). Since these networks experience different training noise, their errors are approximately independent, achieving the independence condition needed to eliminate evaluation bias. However, selection bias remains: the argmax still picks actions that received positive noise in the selection network, so \\mathbb{E}_{\\varepsilon^{(1)}}[\\mu(s,a^\\star)] \\ge \\max_a \\mu(s,a).\n\nTD3 takes a more conservative approach. Instead of decoupling selection from evaluation, TD3 maintains twin Q-networks q^A(s,a; \\boldsymbol{\\theta}^A) and q^B(s,a; \\boldsymbol{\\theta}^B) trained on the same data with different random initializations. When computing targets, TD3 uses the target policy \\pi_{\\boldsymbol{w}_{\\text{target}}}(s') to select actions (no maximization over a discrete set), then takes the minimum of the two Q-networks’ evaluations:y_i = r_i + \\gamma \\min\\left(q^A(s'_i, \\tilde{a}_i; \\boldsymbol{\\theta}^A_{\\text{target}}), q^B(s'_i, \\tilde{a}_i; \\boldsymbol{\\theta}^B_{\\text{target}})\\right)\n\nwhere \\tilde{a}_i = \\pi_{\\boldsymbol{w}_{\\text{target}}}(s'_i). This minimum operation provides a pessimistic estimate: if the two Q-networks have independent errors q^A(s',a) = q^*(s',a) + \\varepsilon^A and q^B(s',a) = q^*(s',a) + \\varepsilon^B, then \\mathbb{E}[\\min(q^A, q^B)] \\le q^*(s',a), producing systematic underestimation rather than overestimation.\n\nThe connection to the conditional independence framework is subtle but important. While Double DQN uses independence to eliminate bias in expectation (one network selects, another evaluates), TD3 uses independence to construct a deliberate lower bound. Both approaches rely on maintaining two Q-functions with partially decorrelated errors, achieved through different initializations and stochastic gradients during training, but they aggregate these functions differently. Double DQN’s decoupling targets unbiased estimation by breaking the correlation between selection and evaluation noise. TD3’s minimum operation targets robust estimation by taking the most pessimistic view when the two networks disagree.\n\nThis trade-off between bias and robustness is deliberate. In actor-critic methods, the policy gradient pushes toward actions with high Q-values. Overestimation is particularly harmful because it can lead the policy to exploit erroneous high-value regions. Underestimation is generally safer: the policy may ignore some good actions, but it will not be misled into pursuing actions that only appear valuable due to approximation error. The minimum operation implements a “trust the pessimist” principle that complements the policy optimization objective.\n\nTD3 also introduces two additional modifications beyond the clipped double Q-learning. First, target policy smoothing adds clipped noise to the target policy’s actions when computing targets: \\tilde{a} = \\pi_{\\boldsymbol{w}_{\\text{target}}}(s') + \\text{clip}(\\varepsilon, -c, c). This regularization prevents the policy from exploiting narrow peaks in the Q-function approximation error by averaging over nearby actions. Second, delayed policy updates change the actor update frequency: the actor updates every d steps instead of every step. This reduces per-update error by letting the critics converge before the actor adapts to them.\n\nTD3 also replaces DDPG’s hard target updates with exponential moving average (EMA) updates, following the smooth update scheme from Algorithm \n\nAlgorithm 4 in the \n\nFQI chapter. Instead of copying \\boldsymbol{\\theta}_{\\text{target}} \\leftarrow \\boldsymbol{\\theta}_t every K steps, EMA smoothly tracks the online network: \\boldsymbol{\\theta}_{\\text{target}} \\leftarrow \\tau \\boldsymbol{\\theta}_t + (1-\\tau)\\boldsymbol{\\theta}_{\\text{target}} at every update. For small \\tau \\in [0.001, 0.01], the target lags behind the online network by roughly 1/\\tau steps, providing smoother learning dynamics.\n\nTwin Delayed Deep Deterministic Policy Gradient (TD3)\n\nInput: MDP (S, \\mathcal{A}, P, R, \\gamma), twin Q-networks q^A(s,a; \\boldsymbol{\\theta}^A), q^B(s,a; \\boldsymbol{\\theta}^B), policy network \\pi_{\\boldsymbol{w}}(s), learning rates \\alpha_q, \\alpha_\\pi, replay buffer capacity B, mini-batch size b, policy delay d, EMA rate \\tau, target noise \\sigma, noise clip c, exploration noise \\sigma_{\\text{explore}}\n\nOutput: Twin Q-function parameters \\boldsymbol{\\theta}^A, \\boldsymbol{\\theta}^B, policy parameters \\boldsymbol{w}\n\nInitialize \\boldsymbol{\\theta}^A_0, \\boldsymbol{\\theta}^B_0, \\boldsymbol{w}_0 randomly\n\n\\boldsymbol{\\theta}^A_{\\text{target}} \\leftarrow \\boldsymbol{\\theta}^A_0, \\boldsymbol{\\theta}^B_{\\text{target}} \\leftarrow \\boldsymbol{\\theta}^B_0, \\boldsymbol{w}_{\\text{target}} \\leftarrow \\boldsymbol{w}_0\n\nInitialize replay buffer \\mathcal{B} with capacity B\n\nt \\leftarrow 0\n\nwhile training do\n\nObserve current state s\n\n// Data collection with exploration noise\n\nSelect action: a \\leftarrow \\pi_{\\boldsymbol{w}_t}(s) + \\varepsilon, where \\varepsilon \\sim \\mathcal{N}(0, \\sigma_{\\text{explore}}^2)\n\nExecute a, observe reward r and next state s'\n\nStore (s,a,r,s') in \\mathcal{B}, replacing oldest if full\n\nSample mini-batch of b transitions \\{(s_i,a_i,r_i,s_i')\\}_{i=1}^b from \\mathcal{B}\n\n// Compute targets with clipped double Q-learning\n\nfor each sampled transition (s_i,a_i,r_i,s_i') do\n\n\\tilde{a}_i \\leftarrow \\pi_{\\boldsymbol{w}_{\\text{target}}}(s'_i) + \\text{clip}(\\varepsilon_i, -c, c), \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2) \\quad // Target policy smoothing\n\ny_i \\leftarrow r_i + \\gamma \\color{blue}{\\min\\big(q^A(s'_i, \\tilde{a}_i; \\boldsymbol{\\theta}^A_{\\text{target}}), q^B(s'_i, \\tilde{a}_i; \\boldsymbol{\\theta}^B_{\\text{target}})\\big)} \\quad // Minimum of twin targets\n\n// Update both Q-networks toward same targets\n\n\\boldsymbol{\\theta}^A_{t+1} \\leftarrow \\boldsymbol{\\theta}^A_t - \\alpha_q \\nabla_{\\boldsymbol{\\theta}} \\frac{1}{b}\\sum_{i=1}^b(q^A(s_i,a_i;\\boldsymbol{\\theta}^A_t) - y_i)^2\n\n\\boldsymbol{\\theta}^B_{t+1} \\leftarrow \\boldsymbol{\\theta}^B_t - \\alpha_q \\nabla_{\\boldsymbol{\\theta}} \\frac{1}{b}\\sum_{i=1}^b(q^B(s_i,a_i;\\boldsymbol{\\theta}^B_t) - y_i)^2\n\n// Delayed policy update and target network updates\n\nif t \\bmod d = 0 then\n\n\\boldsymbol{w}_{t+1} \\leftarrow \\boldsymbol{w}_t + \\alpha_\\pi \\frac{1}{b}\\sum_{i=1}^b \\nabla_a q^A(s_i,a;\\boldsymbol{\\theta}^A_{t+1})\\Big|_{a=\\pi_{\\boldsymbol{w}_t}(s_i)} \\cdot \\nabla_{\\boldsymbol{w}} \\pi_{\\boldsymbol{w}_t}(s_i)\n\n\\boldsymbol{\\theta}^A_{\\text{target}} \\leftarrow \\tau\\boldsymbol{\\theta}^A_{t+1} + (1-\\tau)\\boldsymbol{\\theta}^A_{\\text{target}} (EMA update)\n\n\\boldsymbol{\\theta}^B_{\\text{target}} \\leftarrow \\tau\\boldsymbol{\\theta}^B_{t+1} + (1-\\tau)\\boldsymbol{\\theta}^B_{\\text{target}}\n\n\\boldsymbol{w}_{\\text{target}} \\leftarrow \\tau\\boldsymbol{w}_{t+1} + (1-\\tau)\\boldsymbol{w}_{\\text{target}}\n\nt \\leftarrow t + 1\n\nreturn \\boldsymbol{\\theta}^A_t, \\boldsymbol{\\theta}^B_t, \\boldsymbol{w}_t\n\nThe algorithm structure parallels Double DQN but with continuous actions. Lines 8.1-8.2 implement clipped double Q-learning: smoothing adds noise to target actions (preventing exploitation of Q-function artifacts), and the min operation (highlighted in blue) provides pessimistic value estimates. Both critics update toward the same shared target (lines 10-11), but their different initializations and stochastic gradient noise keep their errors partially decorrelated, following the same principle underlying Double DQN’s independence assumption. Line 13 gates policy updates to every d steps (typically d=2), and lines 13.2-13.4 use EMA updates following Algorithm \n\nAlgorithm 4.\n\nTD3 simplifies exploration by replacing DDPG’s Ornstein-Uhlenbeck process with uncorrelated Gaussian noise \\varepsilon \\sim \\mathcal{N}(0, \\sigma_{\\text{explore}}^2) (line 5.3). This eliminates the need to tune multiple OU parameters while providing equally effective exploration.","type":"content","url":"/amortization#twin-q-networks-and-the-minimum-operator","position":21},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl2":"Soft Actor-Critic"},"type":"lvl2","url":"/amortization#soft-actor-critic","position":22},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl2":"Soft Actor-Critic"},"content":"DDPG and TD3 address continuous actions by learning a deterministic policy that amortizes the \\arg\\max_a q(s,a) operation. But deterministic policies have a fundamental limitation: they require external exploration noise (Gaussian perturbations in TD3) and can converge to suboptimal deterministic behaviors without adequate coverage of the state-action space.\n\nThe \n\nsmoothing chapter presents an alternative: entropy-regularized MDPs, where the agent maximizes expected return plus a bonus for policy randomness. This yields stochastic policies with exploration built into the objective itself. The smooth Bellman operator replaces the hard max with a soft-max:v^*(s) = \\frac{1}{\\beta} \\log \\sum_{a \\in \\mathcal{A}} \\exp\\left(\\beta \\cdot q^*(s,a)\\right)\n\nwhere \\beta = 1/\\alpha is the inverse temperature and \\alpha is the entropy regularization weight. For finite action spaces, this log-sum-exp is easy to compute. But for continuous actions \\mathcal{A} \\subset \\mathbb{R}^m, the sum becomes an integral:v^*(s) = \\frac{1}{\\beta} \\log \\int_{\\mathcal{A}} \\exp\\left(\\beta \\cdot q^*(s,a)\\right) da\n\nThis integral is intractable. We face an infinite-dimensional sum over the continuous action space. The very smoothness that gives us stochastic policies creates a new computational barrier, distinct from but analogous to the \\arg\\max problem in standard FQI.","type":"content","url":"/amortization#soft-actor-critic","position":23},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl3":"From Intractable Integral to Tractable Expectation","lvl2":"Soft Actor-Critic"},"type":"lvl3","url":"/amortization#from-intractable-integral-to-tractable-expectation","position":24},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl3":"From Intractable Integral to Tractable Expectation","lvl2":"Soft Actor-Critic"},"content":"Soft actor-critic (SAC) \n\nHaarnoja et al., 2018\n\nHaarnoja et al., 2018 exploits an equivalence between the intractable integral and an expectation. The optimal policy under entropy regularization is the Boltzmann distribution \\pi^*(a|s) \\propto \\exp(\\beta \\cdot q^*(s,a)). Under this policy, the soft value function becomes:v^*(s) = \\mathbb{E}_{a \\sim \\pi^*(\\cdot|s)}\\left[q^*(s,a) - \\alpha \\log \\pi^*(a|s)\\right]\n\nThis converts the intractable integral into an expectation we can estimate by sampling. SAC learns a parametric policy \\pi_{\\boldsymbol{\\phi}} that approximates the Boltzmann distribution, enabling fast action selection via a single forward pass. For bootstrap targets, SAC samples \\tilde{a}' \\sim \\pi_{\\boldsymbol{\\phi}}(\\cdot|s') and computes:y = r + \\gamma \\left[\\min_{j=1,2} q^j_{\\boldsymbol{\\theta}_{\\text{target}}}(s', \\tilde{a}') - \\alpha \\log \\pi_{\\boldsymbol{\\phi}}(\\tilde{a}'|s')\\right]\n\nThe minimum over twin Q-networks applies the clipped double-Q trick from TD3. Exploration comes from the policy’s stochasticity rather than external noise.","type":"content","url":"/amortization#from-intractable-integral-to-tractable-expectation","position":25},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl3":"Learning the Policy: Matching the Boltzmann Distribution","lvl2":"Soft Actor-Critic"},"type":"lvl3","url":"/amortization#learning-the-policy-matching-the-boltzmann-distribution","position":26},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl3":"Learning the Policy: Matching the Boltzmann Distribution","lvl2":"Soft Actor-Critic"},"content":"The Q-network update assumes a policy \\pi_{\\boldsymbol{\\phi}} that approximates the Boltzmann distribution \\pi^*(a|s) \\propto \\exp(\\beta \\cdot q^*(s,a)). Training such a policy presents a problem: the Boltzmann distribution requires the partition function Z(s) = \\int_{\\mathcal{A}} \\exp(\\beta \\cdot q(s,a))da, the very integral we are trying to avoid. SAC sidesteps this by minimizing the KL divergence from the policy to the (unnormalized) Boltzmann distribution:\\min_{\\boldsymbol{\\phi}} \\mathbb{E}_{s \\sim \\mathcal{D}}\\left[D_{KL}\\left(\\pi_{\\boldsymbol{\\phi}}(\\cdot|s) \\| \\frac{\\exp(\\beta \\cdot q_{\\boldsymbol{\\theta}}(s,\\cdot))}{Z_{\\boldsymbol{\\theta}}(s)}\\right)\\right]\n\nSince \\log Z_{\\boldsymbol{\\theta}}(s) does not depend on \\boldsymbol{\\phi}, this reduces to:\\min_{\\boldsymbol{\\phi}} \\mathbb{E}_{s \\sim \\mathcal{D}}\\mathbb{E}_{a \\sim \\pi_{\\boldsymbol{\\phi}}(\\cdot|s)}\\left[\\alpha \\log \\pi_{\\boldsymbol{\\phi}}(a|s) - q_{\\boldsymbol{\\theta}}(s,a)\\right]\n\nThis pushes probability toward high Q-value actions while the \\log \\pi_{\\boldsymbol{\\phi}} term penalizes concentrating probability mass, maintaining entropy. The entropy bonus comes from the KL divergence structure rather than from an explicit regularization term.\n\nTo estimate gradients of this objective, we face a technical problem: the policy parameters \\boldsymbol{\\phi} appear in the sampling distribution \\pi_{\\boldsymbol{\\phi}}, making \\nabla_{\\boldsymbol{\\phi}} \\mathbb{E}_{a \\sim \\pi_{\\boldsymbol{\\phi}}}[\\cdot] difficult to compute. SAC uses a Gaussian policy \\pi_{\\boldsymbol{\\phi}}(a|s) = \\mathcal{N}(\\mu_{\\boldsymbol{\\phi}}(s), \\sigma_{\\boldsymbol{\\phi}}(s)^2) with the reparameterization trick. Express samples as a deterministic function of parameters and independent noise:a = f_{\\boldsymbol{\\phi}}(s, \\epsilon) = \\mu_{\\boldsymbol{\\phi}}(s) + \\sigma_{\\boldsymbol{\\phi}}(s) \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)\n\nThis moves \\boldsymbol{\\phi} out of the sampling distribution and into the integrand:\\min_{\\boldsymbol{\\phi}} \\mathbb{E}_{s \\sim \\mathcal{D}}\\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,I)}\\left[\\alpha \\log \\pi_{\\boldsymbol{\\phi}}(f_{\\boldsymbol{\\phi}}(s,\\epsilon)|s) - q_{\\boldsymbol{\\theta}}(s,f_{\\boldsymbol{\\phi}}(s,\\epsilon))\\right]\n\nWe can now differentiate through f_{\\boldsymbol{\\phi}} and the Q-network, as DDPG differentiates through a deterministic policy. SAC extends this by sampling noise \\epsilon at each gradient step rather than outputting a single deterministic action.\n\nSoft Actor-Critic (SAC)\n\nInput: MDP (S, \\mathcal{A}, P, R, \\gamma), twin Q-networks q^1(s,a; \\boldsymbol{\\theta}^1), q^2(s,a; \\boldsymbol{\\theta}^2), policy \\pi_{\\boldsymbol{\\phi}}, learning rates \\alpha_q, \\alpha_\\pi, replay buffer capacity B, mini-batch size b, EMA rate \\tau, entropy weight \\alpha\n\nOutput: Q-function parameters \\boldsymbol{\\theta}^1, \\boldsymbol{\\theta}^2, policy parameters \\boldsymbol{\\phi}\n\nInitialize \\boldsymbol{\\theta}^1_0, \\boldsymbol{\\theta}^2_0, \\boldsymbol{\\phi}_0 randomly\n\n\\boldsymbol{\\theta}^1_{\\text{target}} \\leftarrow \\boldsymbol{\\theta}^1_0, \\boldsymbol{\\theta}^2_{\\text{target}} \\leftarrow \\boldsymbol{\\theta}^2_0\n\nInitialize replay buffer \\mathcal{B} with capacity B\n\nt \\leftarrow 0\n\nwhile training do\n\nObserve state s\n\nSample action: a \\sim \\pi_{\\boldsymbol{\\phi}_t}(\\cdot|s) \\quad // Stochastic policy provides exploration\n\nExecute a, observe reward r and next state s'\n\nStore (s,a,r,s') in \\mathcal{B}, replacing oldest if full\n\nSample mini-batch \\{(s_i,a_i,r_i,s_i')\\}_{i=1}^b from \\mathcal{B}\n\n// Update Q-networks: bootstrap using single-sample soft value estimate\n\nfor each s'_i do sample \\tilde{a}'_i \\sim \\pi_{\\boldsymbol{\\phi}_t}(\\cdot|s'_i)\n\ny_i \\leftarrow r_i + \\gamma \\left[\\min(q^1(s'_i, \\tilde{a}'_i; \\boldsymbol{\\theta}^1_{\\text{target}}), q^2(s'_i, \\tilde{a}'_i; \\boldsymbol{\\theta}^2_{\\text{target}})) - \\alpha \\log \\pi_{\\boldsymbol{\\phi}_t}(\\tilde{a}'_i|s'_i)\\right]\n\n\\boldsymbol{\\theta}^1_{t+1} \\leftarrow \\boldsymbol{\\theta}^1_t - \\alpha_q \\nabla_{\\boldsymbol{\\theta}} \\frac{1}{b}\\sum_{i=1}^b(q^1(s_i,a_i;\\boldsymbol{\\theta}^1_t) - y_i)^2\n\n\\boldsymbol{\\theta}^2_{t+1} \\leftarrow \\boldsymbol{\\theta}^2_t - \\alpha_q \\nabla_{\\boldsymbol{\\theta}} \\frac{1}{b}\\sum_{i=1}^b(q^2(s_i,a_i;\\boldsymbol{\\theta}^2_t) - y_i)^2\n\n// Update policy: minimize KL to Boltzmann distribution\n\nfor each s_i do sample \\epsilon_i \\sim \\mathcal{N}(0,I) and compute \\hat{a}_i = f_{\\boldsymbol{\\phi}_t}(s_i, \\epsilon_i) \\quad // Reparameterization\n\n\\boldsymbol{\\phi}_{t+1} \\leftarrow \\boldsymbol{\\phi}_t - \\alpha_\\pi \\nabla_{\\boldsymbol{\\phi}} \\frac{1}{b}\\sum_{i=1}^b \\left[\\alpha \\log \\pi_{\\boldsymbol{\\phi}_t}(\\hat{a}_i|s_i) - \\min_{j=1,2} q^j(s_i, \\hat{a}_i; \\boldsymbol{\\theta}^j_{t+1})\\right]\n\n// EMA update for Q-network targets\n\n\\boldsymbol{\\theta}^1_{\\text{target}} \\leftarrow \\tau \\boldsymbol{\\theta}^1_{t+1} + (1-\\tau)\\boldsymbol{\\theta}^1_{\\text{target}}\n\n\\boldsymbol{\\theta}^2_{\\text{target}} \\leftarrow \\tau \\boldsymbol{\\theta}^2_{t+1} + (1-\\tau)\\boldsymbol{\\theta}^2_{\\text{target}}\n\nt \\leftarrow t + 1\n\nreturn \\boldsymbol{\\theta}^1_t, \\boldsymbol{\\theta}^2_t, \\boldsymbol{\\phi}_t\n\nThe algorithm interleaves three updates. The Q-networks (lines 7-10) follow fitted Q-iteration with the soft Bellman target: sample a next action \\tilde{a}'_i from the current policy, compute the entropy-adjusted target y_i = r_i + \\gamma[\\min_j q^j_{\\text{target}}(s'_i, \\tilde{a}'_i) - \\alpha \\log \\pi_{\\boldsymbol{\\phi}}(\\tilde{a}'_i|s'_i)], and minimize squared error. The minimum over twin Q-networks mitigates overestimation as in TD3. The policy (lines 12-13) updates to match the Boltzmann distribution induced by the current Q-function, using the reparameterization trick for gradient estimation. Target networks update via EMA (lines 15-16) to stabilize training.\n\nThe stochastic policy serves the same amortization purpose as in DDPG and TD3: it replaces the intractable \\arg\\max operation with a fast network forward pass. SAC’s entropy regularization produces exploration through the policy’s inherent stochasticity rather than external noise. This makes SAC more robust to hyperparameters and eliminates the need to tune exploration schedules.","type":"content","url":"/amortization#learning-the-policy-matching-the-boltzmann-distribution","position":27},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl3":"Path Consistency Learning (PCL)","lvl2":"Soft Actor-Critic"},"type":"lvl3","url":"/amortization#path-consistency-learning-pcl","position":28},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl3":"Path Consistency Learning (PCL)","lvl2":"Soft Actor-Critic"},"content":"DDPG, TD3, and SAC all follow the same solution template from fitted Q-iteration: compute Bellman targets using the current Q-function, fit the Q-function to those targets, repeat. This is successive approximation, the function iteration approach v_{k+1} = \\Proj \\Bellman v_k from the \n\nprojection methods chapter.\n\nPath Consistency Learning (PCL) \n\nNachum et al., 2017 solves the Bellman equation differently. Instead of iterating the operator, it directly minimizes a residual. This is the least-squares approach from projection methods: solve \\Residual(v) = 0 by minimizing \\|\\Residual(v)\\|^2. The method exploits special structure (smooth Bellman operators under deterministic dynamics) that conventional methods cannot leverage.","type":"content","url":"/amortization#path-consistency-learning-pcl","position":29},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl4":"The Path Consistency Property","lvl3":"Path Consistency Learning (PCL)","lvl2":"Soft Actor-Critic"},"type":"lvl4","url":"/amortization#the-path-consistency-property","position":30},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl4":"The Path Consistency Property","lvl3":"Path Consistency Learning (PCL)","lvl2":"Soft Actor-Critic"},"content":"Consider the entropy-regularized Q-function Bellman equation from the \n\nsmoothing chapter. Under general stochastic dynamics, it involves an expectation over next states:q^*(s,a) = r(s,a) + \\gamma \\mathbb{E}_{s'}[v^*(s')]\n\nSuppose the dynamics are deterministic: s' = f(s,a). The next state is uniquely determined, so the expectation disappears:q^*(s,a) = r(s,a) + \\gamma v^*(f(s,a))\n\nThe value function relates to Q-functions through the soft-max:v^*(s) = \\alpha \\log \\int_{\\mathcal{A}} \\exp(q^*(s,a)/\\alpha) da\n\nContrast two cases: general policies versus the optimal Boltzmann policy.\n\nFor general policies, the value equals an expectation:v^\\pi(s) = \\mathbb{E}_{a \\sim \\pi(\\cdot|s)}[q^\\pi(s,a) - \\alpha \\log \\pi(a|s)]\n\nThis is an average. For a single observed action a, we have:q^\\pi(s,a) - \\alpha\\log\\pi(a|s) = v^\\pi(s) + \\varepsilon(s,a)\n\nwhere \\varepsilon(s,a) is sampling error with \\mathbb{E}_{a \\sim \\pi}[\\varepsilon(s,a)] = 0. Individual actions give noisy estimates that fluctuate around the mean.\n\nFor the optimal policy under entropy regularization, the Boltzmann structure produces an exact pointwise identity. The optimal policy is:\\pi^*(a|s) = \\frac{\\exp(q^*(s,a)/\\alpha)}{\\exp(v^*(s)/\\alpha)}\n\nTaking logarithms and rearranging:v^*(s) = q^*(s,a) - \\alpha \\log \\pi^*(a|s) \\quad \\text{for all } a\n\nThis holds exactly for every action a, not just in expectation. There is no sampling error. The advantage q^*(s,a) - v^*(s) is encoded in the log-probability: suboptimal actions have low q^*(s,a) but also large -\\alpha\\log\\pi^*(a|s) (low probability means large negative log-probability), and these terms balance exactly to give v^*(s).\n\nNow take a trajectory segment (s_0, a_0, s_1, a_1, \\ldots, s_d) where each transition follows the deterministic dynamics s_{t+1} = f(s_t, a_t). Start with q^*(s_0, a_0) = r_0 + \\gamma v^*(s_1) and use equation \n\n(34) to substitute v^*(s_1) = q^*(s_1,a_1) - \\alpha\\log\\pi^*(a_1|s_1) exactly:q^*(s_0, a_0) = r_0 + \\gamma[q^*(s_1,a_1) - \\alpha\\log\\pi^*(a_1|s_1)]\n\nSubstitute q^*(s_1,a_1) = r_1 + \\gamma v^*(s_2):q^*(s_0, a_0) = r_0 + \\gamma r_1 - \\gamma\\alpha\\log\\pi^*(a_1|s_1) + \\gamma^2 v^*(s_2)\n\nContinue this telescoping for d steps. Each substitution is exact:q^*(s_0, a_0) = \\sum_{t=0}^{d-1} \\gamma^t r_t - \\alpha \\sum_{t=1}^{d-1} \\gamma^t \\log\\pi^*(a_t|s_t) + \\gamma^d v^*(s_d)\n\nApply equation \n\n(34) once more to get v^*(s_0) = q^*(s_0,a_0) - \\alpha\\log\\pi^*(a_0|s_0):v^*(s_0) = \\sum_{t=0}^{d-1} \\gamma^t [r_t - \\alpha\\log\\pi^*(a_t|s_t)] + \\gamma^d v^*(s_d)\n\nRearranging gives the path consistency residual:R(s_0:s_d; \\pi^*, v^*) = v^*(s_0) - \\gamma^d v^*(s_d) - \\sum_{t=0}^{d-1} \\gamma^t[r_t - \\alpha\\log\\pi^*(a_t|s_t)] = 0\n\nThe telescoping produces an exact identity: R = 0 for every action sequence, not just in expectation. The behavior policy never appears because the constraint holds as a deterministic identity for any observed (s_0, a_0, \\ldots, s_d). This enables off-policy learning without importance sampling.\n\nContrasting General Policies and Optimal Boltzmann Policies\n\nThe distinction between equations \n\n(31) and \n\n(34) is subtle but crucial.\n\nFor general policies (equation \n\n(31)), the value is an average over actions sampled from the policy. Individual actions give noisy estimates: if we draw a \\sim \\pi(\\cdot|s), then q^\\pi(s,a) - \\alpha\\log\\pi(a|s) = v^\\pi(s) + \\varepsilon where \\varepsilon is a zero-mean random variable. We need to average many samples to estimate v^\\pi(s) accurately. Multi-step telescoping would accumulate these sampling errors \\varepsilon_0, \\varepsilon_1, \\ldots, \\varepsilon_{d-1}, producing noisy residuals even at the true solution. Off-policy learning would require importance weights to correct for using actions from a different behavior policy.\n\nFor the optimal entropy-regularized policy (equation \n\n(34)), the Boltzmann structure collapses the expectation to a pointwise identity. The relationship v^*(s) = q^*(s,a) - \\alpha\\log\\pi^*(a|s) holds exactly for every action a, optimal or not. A suboptimal action has low q^*(s,a) (low expected return) and low \\pi^*(a|s) (low probability), making -\\alpha\\log\\pi^*(a|s) large. These terms balance precisely to give v^*(s). No sampling error exists. The telescoping is exact, producing a residual that equals zero for every action sequence, not just in expectation. Off-policy learning works because the constraint holds as a deterministic identity for any observed path.\n\nThis property is unique to soft-max operators. For hard-max, v^*(s) = \\max_a q^*(s,a) holds only when a is optimal. Suboptimal actions satisfy v^*(s) > q^*(s,a), an inequality that cannot be used to construct a residual.","type":"content","url":"/amortization#the-path-consistency-property","position":31},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl4":"Structural Requirements: Deterministic Dynamics and Entropy Regularization","lvl3":"Path Consistency Learning (PCL)","lvl2":"Soft Actor-Critic"},"type":"lvl4","url":"/amortization#structural-requirements-deterministic-dynamics-and-entropy-regularization","position":32},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl4":"Structural Requirements: Deterministic Dynamics and Entropy Regularization","lvl3":"Path Consistency Learning (PCL)","lvl2":"Soft Actor-Critic"},"content":"PCL’s two structural requirements (deterministic dynamics and entropy regularization) are not arbitrary design choices. Each addresses a fundamental theoretical issue.","type":"content","url":"/amortization#structural-requirements-deterministic-dynamics-and-entropy-regularization","position":33},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl5":"Deterministic Dynamics: Avoiding the Double Sampling Problem","lvl4":"Structural Requirements: Deterministic Dynamics and Entropy Regularization","lvl3":"Path Consistency Learning (PCL)","lvl2":"Soft Actor-Critic"},"type":"lvl5","url":"/amortization#deterministic-dynamics-avoiding-the-double-sampling-problem","position":34},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl5":"Deterministic Dynamics: Avoiding the Double Sampling Problem","lvl4":"Structural Requirements: Deterministic Dynamics and Entropy Regularization","lvl3":"Path Consistency Learning (PCL)","lvl2":"Soft Actor-Critic"},"content":"Under stochastic dynamics, the Q-function Bellman equation has an expectation over next states:q^*(s,a) = r(s,a) + \\gamma \\mathbb{E}_{s' \\sim p(\\cdot|s,a)}[v^*(s')]\n\nThe exact relationship \n\n(34) still holds, so we can write the path consistency constraint. But now consider what PCL minimizes: the squared residual \\mathbb{E}[R^2] whereR = v_{\\boldsymbol{\\phi}}(s_0) - \\gamma^d v_{\\boldsymbol{\\phi}}(s_d) - \\sum_{t=0}^{d-1} \\gamma^t[r_t - \\alpha\\log\\pi_{\\boldsymbol{\\theta}}(a_t|s_t)]\n\nAt the true optimum (v^*, \\pi^*), the constraint is \\mathbb{E}[R] = 0, which implies (\\mathbb{E}[R])^2 = 0. But PCL minimizes \\mathbb{E}[R^2], and by Jensen’s inequality:\\mathbb{E}[R^2] \\geq (\\mathbb{E}[R])^2\n\nwith equality only when R has zero variance. Under stochastic dynamics, even at optimality, individual trajectory residuals are random variables with mean zero but positive variance (due to transition noise). Minimizing \\mathbb{E}[R^2] to zero would require driving \\text{Var}(R) \\to 0, which is impossible and pushes the solution away from the true optimum.\n\nThis is Baird’s double sampling problem \n\nBaird, 1995. To get an unbiased gradient of (\\mathbb{E}[R])^2, we need:\\nabla (\\mathbb{E}[R])^2 = 2\\mathbb{E}[R] \\cdot \\nabla \\mathbb{E}[R] = 2\\mathbb{E}[R] \\cdot \\mathbb{E}[\\nabla R]\n\nThis requires two independent samples of the next state from the same (s,a) pair: one for estimating \\mathbb{E}[R] and one for \\mathbb{E}[\\nabla R]. With a simulator, this is possible. With real trajectories, it is not.\n\nUnder deterministic dynamics, R is deterministic (no transition noise), so \\mathbb{E}[R^2] = (\\mathbb{E}[R])^2 and Jensen’s inequality holds with equality. Minimizing the squared residual is equivalent to solving \\mathbb{E}[R] = 0.","type":"content","url":"/amortization#deterministic-dynamics-avoiding-the-double-sampling-problem","position":35},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl5":"Entropy Regularization: Enabling All-Action Consistency","lvl4":"Structural Requirements: Deterministic Dynamics and Entropy Regularization","lvl3":"Path Consistency Learning (PCL)","lvl2":"Soft Actor-Critic"},"type":"lvl5","url":"/amortization#entropy-regularization-enabling-all-action-consistency","position":36},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl5":"Entropy Regularization: Enabling All-Action Consistency","lvl4":"Structural Requirements: Deterministic Dynamics and Entropy Regularization","lvl3":"Path Consistency Learning (PCL)","lvl2":"Soft Actor-Critic"},"content":"Attempt the same path consistency derivation with the hard-max Bellman operator. Under deterministic dynamics, the Q-function satisfies:q^*(s,a) = r(s,a) + \\gamma v^*(f(s,a))\n\nwhere v^*(s) = \\max_{a'} q^*(s,a') and the optimal policy is \\pi^*(s) = \\arg\\max_a q^*(s,a) (deterministic).\n\nNow try to relate v^*(s) to an arbitrary observed action a. For the optimal action a^* \\in \\arg\\max_{a'} q^*(s,a'), we have:v^*(s) = q^*(s,a^*)\n\nBut for a suboptimal action a \\ne a^*:v^*(s) = \\max_{a'} q^*(s,a') > q^*(s,a)\n\nThis is an inequality, not an equation. There is no formula expressing v^*(s) in terms of q^*(s,a) for suboptimal actions.\n\nAttempt the multi-step telescoping. Start with q^*(s_0, a_0) = r_0 + \\gamma v^*(s_1). To continue, we need to express v^*(s_1) using the observed action a_1. But we only have:v^*(s_1) \\geq q^*(s_1, a_1)\n\nwith equality only if a_1 happens to be optimal at s_1. We cannot substitute this into the Q-function equation to get an exact telescoping. The derivation breaks at the first step.\n\nCompare this to the soft-max case. The Boltzmann structure gives equation \n\n(34): v^*(s) = q^*(s,a) - \\alpha\\log\\pi^*(a|s) for all actions a. The log-probability term compensates exactly for suboptimality: low-probability actions have large -\\alpha\\log\\pi^*(a|s), which adds to the low q^*(s,a) to recover v^*(s). This enables exact substitution at every step:v^*(s_1) = q^*(s_1, a_1) - \\alpha\\log\\pi^*(a_1|s_1) \\quad \\text{(exact for any } a_1\\text{)}\n\nThe telescoping proceeds without inequalities or restrictions on which actions were chosen. Multi-step hard-max Q-learning lacks theoretical justification for off-policy data because when we observe a trajectory with suboptimal actions, we cannot write an exact path consistency constraint.\n\nBoth requirements are structural:\n\nRequirement\n\nAddresses\n\nDeterministic dynamics\n\nDouble sampling bias: ensures \\mathbb{E}[R^2] = (\\mathbb{E}[R])^2\n\nEntropy regularization\n\nAll-action consistency (equation \n\n(34))\n\nWithout deterministic dynamics, residual minimization is biased. Without entropy regularization, the constraint holds only for optimal actions.","type":"content","url":"/amortization#entropy-regularization-enabling-all-action-consistency","position":37},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl4":"The Learning Objective","lvl3":"Path Consistency Learning (PCL)","lvl2":"Soft Actor-Critic"},"type":"lvl4","url":"/amortization#the-learning-objective","position":38},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl4":"The Learning Objective","lvl3":"Path Consistency Learning (PCL)","lvl2":"Soft Actor-Critic"},"content":"Equation \n\n(39) provides a constraint that the optimal (v^*, \\pi^*) must satisfy: the residual equals zero for every observed path. For parametric approximations (v_{\\boldsymbol{\\phi}}, \\pi_{\\boldsymbol{\\theta}}) that are not yet optimal, the residual is nonzero:R(s_0:s_d; \\boldsymbol{\\theta}, \\boldsymbol{\\phi}) = v_{\\boldsymbol{\\phi}}(s_0) - \\gamma^d v_{\\boldsymbol{\\phi}}(s_d) - \\sum_{t=0}^{d-1} \\gamma^t[r_t - \\alpha \\log \\pi_{\\boldsymbol{\\theta}}(a_t|s_t)]\n\nPCL minimizes the squared residual over observed path segments:\\min_{\\boldsymbol{\\theta}, \\boldsymbol{\\phi}} \\sum_{\\text{segments}} \\frac{1}{2} R(s_i:s_{i+d}; \\boldsymbol{\\theta}, \\boldsymbol{\\phi})^2\n\nThis is the least-squares residual approach from the \n\nprojection methods chapter. SAC computes targets y_i and fits to them (successive approximation). PCL directly minimizes the residual without computing targets or performing a separate fitting step.\n\nGradient descent gives:\\begin{align}\n\\boldsymbol{\\theta}_{k+1} &= \\boldsymbol{\\theta}_k + \\eta_\\pi \\sum_i R_i \\cdot \\alpha \\sum_{t=0}^{d-1} \\gamma^t \\nabla_{\\boldsymbol{\\theta}} \\log \\pi_{\\boldsymbol{\\theta}_k}(a_{i+t}|s_{i+t}) \\\\\n\\boldsymbol{\\phi}_{k+1} &= \\boldsymbol{\\phi}_k - \\eta_v \\sum_i R_i \\left[\\nabla_{\\boldsymbol{\\phi}} v_{\\boldsymbol{\\phi}_k}(s_i) - \\gamma^d \\nabla_{\\boldsymbol{\\phi}} v_{\\boldsymbol{\\phi}_k}(s_{i+d})\\right]\n\\end{align}\n\nwhere R_i = R(s_i:s_{i+d}; \\boldsymbol{\\theta}_k, \\boldsymbol{\\phi}_k). Large residuals drive larger updates.\n\nPath Consistency Learning (PCL)\n\nInput: MDP with deterministic dynamics s_{t+1} = f(s_t, a_t), policy \\pi_{\\boldsymbol{\\theta}}, value function v_{\\boldsymbol{\\phi}}, entropy weight \\alpha, path length d, learning rates \\eta_\\pi, \\eta_v, replay buffer capacity B\n\nOutput: Policy parameters \\boldsymbol{\\theta}, value parameters \\boldsymbol{\\phi}\n\nInitialize \\boldsymbol{\\theta}_0, \\boldsymbol{\\phi}_0\n\nInitialize replay buffer \\mathcal{R} with capacity B\n\nk \\leftarrow 0\n\nwhile training do\n\nSample trajectory \\tau = (s_0, a_0, r_0, \\ldots, s_T) from \\pi_{\\boldsymbol{\\theta}_k} and store in \\mathcal{R}\n\nSample trajectory \\tau' from \\mathcal{R}\n\nfor each d-step segment in \\tau' do\n\nCompute residual: R_i \\leftarrow v_{\\boldsymbol{\\phi}_k}(s_i) - \\gamma^d v_{\\boldsymbol{\\phi}_k}(s_{i+d}) - \\sum_{t=0}^{d-1} \\gamma^t[r_{i+t} - \\alpha \\log \\pi_{\\boldsymbol{\\theta}_k}(a_{i+t}|s_{i+t})]\n\nUpdate policy: \\boldsymbol{\\theta}_{k+1} \\leftarrow \\boldsymbol{\\theta}_k + \\eta_\\pi \\alpha R_i \\sum_{t=0}^{d-1} \\gamma^t \\nabla_{\\boldsymbol{\\theta}} \\log \\pi_{\\boldsymbol{\\theta}_k}(a_{i+t}|s_{i+t})\n\nUpdate value: \\boldsymbol{\\phi}_{k+1} \\leftarrow \\boldsymbol{\\phi}_k - \\eta_v R_i\\left[\\nabla_{\\boldsymbol{\\phi}} v_{\\boldsymbol{\\phi}_k}(s_i) - \\gamma^d \\nabla_{\\boldsymbol{\\phi}} v_{\\boldsymbol{\\phi}_k}(s_{i+d})\\right]\n\nRemove oldest trajectories if |\\mathcal{R}| > B\n\nk \\leftarrow k + 1\n\nreturn \\boldsymbol{\\theta}_k, \\boldsymbol{\\phi}_k\n\nThe algorithm collects trajectories from the current policy and stores them in a replay buffer. At each iteration, it samples a trajectory (possibly old) and performs gradient descent on the path residual for all d-step segments. The replay buffer enables off-policy learning: trajectories from old policies, expert demonstrations, or exploratory behavior all provide valid training signals.","type":"content","url":"/amortization#the-learning-objective","position":39},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl4":"Unified Parameterization: Single Q-Network","lvl3":"Path Consistency Learning (PCL)","lvl2":"Soft Actor-Critic"},"type":"lvl4","url":"/amortization#unified-parameterization-single-q-network","position":40},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl4":"Unified Parameterization: Single Q-Network","lvl3":"Path Consistency Learning (PCL)","lvl2":"Soft Actor-Critic"},"content":"Algorithm \n\nAlgorithm 7 uses separate networks for policy and value. But we can use a single Q-network q_{\\boldsymbol{\\theta}}(s,a) and derive both:v_{\\boldsymbol{\\theta}}(s) = \\alpha \\log \\sum_{a} \\exp(q_{\\boldsymbol{\\theta}}(s,a)/\\alpha), \\qquad \\pi_{\\boldsymbol{\\theta}}(a|s) = \\frac{\\exp(q_{\\boldsymbol{\\theta}}(s,a)/\\alpha)}{\\sum_{a'} \\exp(q_{\\boldsymbol{\\theta}}(s,a')/\\alpha)}\n\nThe path residual becomes:R(s_i:s_{i+d}; \\boldsymbol{\\theta}) = v_{\\boldsymbol{\\theta}}(s_i) - \\gamma^d v_{\\boldsymbol{\\theta}}(s_{i+d}) - \\sum_{t=0}^{d-1} \\gamma^t[r_{i+t} - \\alpha \\log \\pi_{\\boldsymbol{\\theta}}(a_{i+t}|s_{i+t})]\n\nand the gradient combines both value and policy contributions through the same parameters. This unified architecture eliminates the actor-critic separation: one Q-network serves both roles.","type":"content","url":"/amortization#unified-parameterization-single-q-network","position":41},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl4":"Connection to Existing Methods","lvl3":"Path Consistency Learning (PCL)","lvl2":"Soft Actor-Critic"},"type":"lvl4","url":"/amortization#connection-to-existing-methods","position":42},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl4":"Connection to Existing Methods","lvl3":"Path Consistency Learning (PCL)","lvl2":"Soft Actor-Critic"},"content":"Single-step case (d=1): The path residual becomes R(s:s'; \\boldsymbol{\\theta}, \\boldsymbol{\\phi}) = v_{\\boldsymbol{\\phi}}(s) - \\gamma v_{\\boldsymbol{\\phi}}(s') - r + \\alpha\\log\\pi_{\\boldsymbol{\\theta}}(a|s). For unified parameterization where v_{\\boldsymbol{\\theta}}(s) = q_{\\boldsymbol{\\theta}}(s,a) - \\alpha\\log\\pi_{\\boldsymbol{\\theta}}(a|s) exactly, this becomes R = q_{\\boldsymbol{\\theta}}(s,a) - r - \\gamma v_{\\boldsymbol{\\theta}}(s'), the soft Bellman residual. Minimizing \\sum_i R_i^2 is equivalent to soft Q-learning, though SAC solves this via successive approximation (compute targets, fit) rather than direct residual minimization.\n\nNo entropy (\\alpha \\to 0): The residual becomes R = v(s_i) - \\gamma^d v(s_{i+d}) - \\sum_t \\gamma^t r_t, the negative d-step advantage. But unlike A2C/A3C where v tracks the current policy’s value, PCL’s value converges to v^* because the residual couples policy and value through the optimality condition.\n\nMulti-step with hard-max: No analog exists. The hard-max Bellman operator \\max_a q(s,a) does not have an exact pointwise relationship like equation \n\n(34). Multi-step telescoping would accumulate errors from the max operator, making the constraint valid only in expectation under the optimal policy. The soft-max structure enables exact off-policy path consistency.","type":"content","url":"/amortization#connection-to-existing-methods","position":43},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl4":"PCL vs SAC: Residual Minimization vs Successive Approximation","lvl3":"Path Consistency Learning (PCL)","lvl2":"Soft Actor-Critic"},"type":"lvl4","url":"/amortization#pcl-vs-sac-residual-minimization-vs-successive-approximation","position":44},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl4":"PCL vs SAC: Residual Minimization vs Successive Approximation","lvl3":"Path Consistency Learning (PCL)","lvl2":"Soft Actor-Critic"},"content":"Both methods solve entropy-regularized MDPs but use fundamentally different solution strategies:\n\nAspect\n\nSAC\n\nPCL\n\nSolution method\n\nSuccessive approximation: compute targets y_i, fit q to targets\n\nResidual minimization: minimize \\sum_i R_i^2 directly\n\nUpdate structure\n\nTarget computation + regression step\n\nSingle gradient step on squared residual\n\nTarget networks\n\nRequired (mark outer-iteration boundaries)\n\nNone (residual constraint, not target fitting)\n\nTemporal horizon\n\nSingle-step TD: y = r + \\gamma V(s')\n\nMulti-step paths: accumulate over d steps\n\nOff-policy handling\n\nReplay buffer with single-sample bias\n\nNo importance sampling (works for any trajectory)\n\nDynamics requirement\n\nGeneral stochastic transitions\n\nDeterministic transitions s' = f(s,a)\n\nArchitecture\n\nTwin Q-networks + policy network\n\nSingle Q-network (unified parameterization)\n\nPCL requires deterministic dynamics. It gains multi-step telescoping and off-policy learning without importance weights, but only for deterministic systems (robotic manipulation, many control tasks). SAC works for general stochastic MDPs.","type":"content","url":"/amortization#pcl-vs-sac-residual-minimization-vs-successive-approximation","position":45},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl4":"PCL as Amortization","lvl3":"Path Consistency Learning (PCL)","lvl2":"Soft Actor-Critic"},"type":"lvl4","url":"/amortization#pcl-as-amortization","position":46},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl4":"PCL as Amortization","lvl3":"Path Consistency Learning (PCL)","lvl2":"Soft Actor-Critic"},"content":"PCL amortizes at a different level than DDPG/TD3/SAC. Those methods amortize the action maximization: learn a policy network that outputs \\arg\\max_a q(s,a) directly. PCL amortizes the solution of the Bellman equation itself. Instead of repeatedly applying the Bellman operator (which requires \\int_{\\mathcal{A}} \\exp(q/\\alpha) da at every iteration), PCL samples path segments and minimizes their residual. The computational cost of verifying optimality across all states and path lengths is distributed across training through sampled gradient updates.","type":"content","url":"/amortization#pcl-as-amortization","position":47},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl3":"Model Predictive Path Integral Control","lvl2":"Soft Actor-Critic"},"type":"lvl3","url":"/amortization#model-predictive-path-integral-control","position":48},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl3":"Model Predictive Path Integral Control","lvl2":"Soft Actor-Critic"},"content":"SAC and PCL both learn policies that approximate the Boltzmann distribution \\pi^*(a|s) \\propto \\exp(\\beta \\cdot q^*(s,a)) induced by entropy regularization. This amortization allows fast action selection at deployment: a single forward pass through the policy network. An alternative approach forgoes learning entirely and instead performs optimization at every decision.\n\nModel Predictive Path Integral control (MPPI) \n\nWilliams et al., 2017 uses the Boltzmann weighting directly for action sequence selection. Given a dynamics model s_{t+1} = f(s_t, a_t) and current state s_0, MPPI samples K action sequences \\{\\boldsymbol{a}^{(i)}\\}_{i=1}^K, rolls them out to get costs C^{(i)} = \\sum_{t=0}^{H-1} c(s_t^{(i)}, a_t^{(i)}), and computes the optimal action as a weighted average:a_0^* = \\sum_{i=1}^K w^{(i)} a_0^{(i)}, \\quad w^{(i)} = \\frac{\\exp(-C^{(i)}/\\lambda)}{\\sum_j \\exp(-C^{(j)}/\\lambda)}\n\nwhere \\lambda > 0 is a temperature parameter. The weighting w^{(i)} \\propto \\exp(-C^{(i)}/\\lambda) is exactly the Boltzmann distribution. MPPI solves the entropy-regularized objective:\\min_{\\boldsymbol{a}} \\mathbb{E}_{\\boldsymbol{\\xi}}\\left[\\sum_{t=0}^{H-1} c(s_t, a_t) + \\lambda H(\\pi)\\right]\n\nwhere \\pi is the distribution over action sequences and H(\\pi) = -\\mathbb{E}[\\log \\pi(\\boldsymbol{a})] is entropy. The importance sampling estimate approximates the optimal action under this objective. The temperature \\lambda controls the trade-off between exploitation (focus on low-cost sequences) and exploration (maintain entropy).\n\nModel Predictive Path Integral Control (MPPI)\n\nInput: Dynamics model s_{t+1} = f(s_t, a_t), cost function c(s,a), horizon H, number of samples K, temperature \\lambda, noise distribution \\epsilon \\sim \\mathcal{N}(0, \\Sigma)\n\nOutput: Action a_0^*\n\nObserve current state s_0\n\nfor i = 1, \\ldots, K do\n\nSample action sequence: a_t^{(i)} \\leftarrow \\bar{a}_t + \\epsilon_t^{(i)} for t = 0, \\ldots, H-1 \\quad // Perturb nominal\n\nRoll out: s_{t+1}^{(i)} \\leftarrow f(s_t^{(i)}, a_t^{(i)}) for t = 0, \\ldots, H-1\n\nCompute cost: C^{(i)} \\leftarrow \\sum_{t=0}^{H-1} c(s_t^{(i)}, a_t^{(i)})\n\nCompute Boltzmann weights: w^{(i)} \\leftarrow \\exp(-C^{(i)}/\\lambda) / \\sum_j \\exp(-C^{(j)}/\\lambda)\n\nreturn a_0^* = \\sum_{i=1}^K w^{(i)} a_0^{(i)}\n\nThe algorithm samples perturbed action sequences around a nominal trajectory \\{\\bar{a}_t\\} (often the previous optimal sequence, shifted forward). The Boltzmann weights assign high probability to low-cost sequences. After executing a_0^*, the agent observes the next state and replans.","type":"content","url":"/amortization#model-predictive-path-integral-control","position":49},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl4":"MPPI as Non-Amortized Optimization","lvl3":"Model Predictive Path Integral Control","lvl2":"Soft Actor-Critic"},"type":"lvl4","url":"/amortization#mppi-as-non-amortized-optimization","position":50},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl4":"MPPI as Non-Amortized Optimization","lvl3":"Model Predictive Path Integral Control","lvl2":"Soft Actor-Critic"},"content":"The contrast between MPPI and the methods in this chapter illuminates what amortization provides. SAC learns a policy \\pi_{\\boldsymbol{\\phi}} that approximates the Boltzmann distribution over actions at each state. PCL learns a Q-function from which the Boltzmann policy can be derived. Both invest computational effort during training to enable fast action selection at deployment: a single forward pass.\n\nMPPI performs full optimization at every decision. At each state, it samples action sequences, weights them by exponentiated costs, and returns the weighted average. No learning occurs. The policy is implicitly defined by the optimization procedure itself.\n\nThis trade-off has practical consequences:\n\nAspect\n\nAmortized (SAC, PCL)\n\nNon-Amortized (MPPI)\n\nAction selection\n\nSingle forward pass\n\nO(KH) model evaluations\n\nGeneralization\n\nPolicy generalizes across states\n\nOptimization from scratch at each state\n\nModel requirement\n\nNone (SAC) or deterministic (PCL)\n\nAccurate dynamics model\n\nApproximation error\n\nPolicy network approximation\n\nNone (exact optimization)\n\nAdaptability\n\nRequires retraining for new tasks\n\nAdapts immediately to new cost functions\n\nMPPI excels at real-time control for systems with fast, accurate models (robotics, autonomous vehicles). The replanning handles model errors and disturbances without retraining. However, the per-step computation (K \\approx 100-1000 rollouts) makes it expensive for complex dynamics or long horizons.\n\nThe entropy regularization that connects SAC, PCL, and MPPI is not coincidental. All three methods solve variants of the soft Bellman equation. SAC and PCL amortize the solution by learning value functions and policies. MPPI solves it directly through sampling. The Boltzmann weighting emerges in all cases as the optimal policy structure under entropy regularization.","type":"content","url":"/amortization#mppi-as-non-amortized-optimization","position":51},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl2":"An Alternative: Euler Equation Methods"},"type":"lvl2","url":"/amortization#an-alternative-euler-equation-methods","position":52},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl2":"An Alternative: Euler Equation Methods"},"content":"The methods developed in this chapter all parameterize policies, but they remain rooted in the Bellman equation. NFQCA, DDPG, TD3, and SAC learn Q-functions through successive approximation, then derive policies by maximizing these Q-functions. PCL minimizes a path residual derived from the soft Bellman equation. The policy serves as an amortized optimizer for a value-based objective.\n\nThere is a different approach, developed in computational economics \n\nJudd, 1992\n\nRust, 1996\n\nJudd, 1998, that also parameterizes policies but solves an entirely different functional equation. Consider a control problem with continuous states and actions, deterministic dynamics s' = f(s,a), and differentiable reward r(s,a). The optimal action \\pi^*(s) satisfies the first-order condition:\\frac{\\partial r(s,a)}{\\partial a}\\Big|_{a=\\pi^*(s)}\n+\n\\gamma\\, \\frac{\\partial v^*(s')}{\\partial s'}\\Big|_{s'=f(s,\\pi^*(s))}\\,\n\\frac{\\partial f(s,a)}{\\partial a}\\Big|_{a=\\pi^*(s)}\n=\n0.\n\nThis Euler equation expresses optimality through derivatives rather than through the max operator. For problems with special structure (the Euler class, where dynamics are affine in the controlled state), envelope theorems eliminate v^* entirely, yielding a closed functional equation \\mathcal{E}(\\pi)(s) = 0 in the policy alone.\n\nWith a parameterized policy \\pi_{\\boldsymbol{\\theta}}(s), we can discretize via collocation or Galerkin projection:G(\\boldsymbol{\\theta}) := \\begin{bmatrix}\n\\mathcal{E}(\\pi_{\\boldsymbol{\\theta}})(s_1) \\\\\n\\vdots \\\\\n\\mathcal{E}(\\pi_{\\boldsymbol{\\theta}})(s_N)\n\\end{bmatrix} = 0.\n\nThis is root-finding, not fixed-point iteration. Newton-type methods replace the successive approximation of fitted Q-iteration. The Euler operator is not a contraction, so convergence guarantees are problem-dependent.\n\nWhat does this mean for reinforcement learning? The Euler approach shares the amortization idea: learn a policy network that directly outputs actions. But the training objective comes from first-order optimality conditions rather than from Bellman residuals or Q-function maximization. This raises questions worth considering. Could Euler-style objectives provide useful training signals for actor-critic methods? When dynamics are known or learned, could first-order conditions offer advantages over value-based objectives? The connection between these traditions remains underexplored.","type":"content","url":"/amortization#an-alternative-euler-equation-methods","position":53},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl2":"Summary"},"type":"lvl2","url":"/amortization#summary","position":54},{"hierarchy":{"lvl1":"Fitted Q-Iteration for Continuous Action Spaces","lvl2":"Summary"},"content":"When actions are continuous, computing \\max_{a} q(s,a;\\boldsymbol{\\theta}) at each Bellman target requires solving a nonlinear program. Amortization replaces this runtime optimization with a learned policy network: a single forward pass instead of an optimization loop. The FQI structure from the \n\nprevious chapter remains intact, with the policy network providing actions for target computation.\n\nNFQCA, DDPG, TD3, and SAC use successive approximation: compute Bellman targets, fit Q-function to targets, update policy to maximize Q-function. Deterministic policies (DDPG, TD3) require external exploration noise; stochastic policies (SAC) explore through their inherent randomness. TD3 and SAC use twin Q-networks with \\min(q^1, q^2) to mitigate overestimation. PCL takes a different approach, minimizing the path consistency residual directly rather than iterating the Bellman operator. This requires deterministic dynamics but enables multi-step constraints without importance sampling.\n\nMPPI forgoes learning entirely, performing Boltzmann-weighted optimization at every decision. This avoids policy approximation error but requires O(KH) model rollouts per action. SAC, PCL, and MPPI all solve entropy-regularized objectives; SAC and PCL amortize the solution while MPPI computes it directly.\n\nThe \n\nnext chapter takes a different approach: parameterize the policy directly and optimize via gradient ascent on expected return. The starting point is derivative estimation for stochastic optimization rather than Bellman equations, though value functions return as variance reduction tools in actor-critic methods.","type":"content","url":"/amortization#summary","position":55},{"hierarchy":{"lvl1":"Example COCPs"},"type":"lvl1","url":"/appendix-examples","position":0},{"hierarchy":{"lvl1":"Example COCPs"},"content":"","type":"content","url":"/appendix-examples","position":1},{"hierarchy":{"lvl1":"Example COCPs","lvl2":"Inverted Pendulum"},"type":"lvl2","url":"/appendix-examples#inverted-pendulum","position":2},{"hierarchy":{"lvl1":"Example COCPs","lvl2":"Inverted Pendulum"},"content":"The inverted pendulum is a classic problem in control theory and robotics that demonstrates the challenge of stabilizing a dynamic system that is inherently unstable. The objective is to keep a pendulum balanced in the upright position by applying a control force, typically at its base. This setup is analogous to balancing a broomstick on your finger: any deviation from the vertical position will cause the system to tip over unless you actively counteract it with appropriate control actions.\n\nWe typically assume that the pendulum is mounted on a cart or movable base, which can move horizontally. The system’s state is then characterized by four variables:\n\nCart position:  x(t)  — the horizontal position of the base.\n\nCart velocity:  \\dot{x}(t)  — the speed of the cart.\n\nPendulum angle:  \\theta(t)  — the angle between the pendulum and the vertical upright position.\n\nAngular velocity:  \\dot{\\theta}(t)  — the rate at which the pendulum’s angle is changing.\n\nThis setup is more complex because the controller must deal with interactions between two different types of motion: linear (the cart) and rotational (the pendulum). This system is said to be “underactuated” because the number of control inputs (one) is less than the number of state variables (four). This makes the problem more challenging and interesting from a control perspective.\n\nWe can simplify the problem by assuming that the base of the pendulum is fixed.  This is akin to having the bottom of the stick attached to a fixed pivot on a table. You can’t move the base anymore; you can only apply small nudges at the pivot point to keep the stick balanced upright. In this case, you’re only focusing on adjusting the stick’s tilt without worrying about moving the base. This reduces the problem to stabilizing the pendulum’s upright orientation using only the rotational dynamics. The system’s state can now be described by just two variables:\n\nPendulum angle:  \\theta(t)  — the angle of the pendulum from the upright vertical position.\n\nAngular velocity:  \\dot{\\theta}(t)  — the rate at which the pendulum’s angle is changing.\n\nThe evolution of these two varibles is governed by the following ordinary differential equation:\\begin{bmatrix} \\dot{\\theta}(t) \\\\ \\ddot{\\theta}(t) \\end{bmatrix} = \\begin{bmatrix} \\dot{\\theta}(t) \\\\ \\frac{mgl}{J_t} \\sin{\\theta(t)} - \\frac{\\gamma}{J_t} \\dot{\\theta}(t) + \\frac{l}{J_t} u(t) \\cos{\\theta(t)} \\end{bmatrix}, \\quad y(t) = \\theta(t)\n\nwhere:\n\nm is the mass of the pendulum\n\ng is the acceleration due to gravity\n\nl is the length of the pendulum\n\n\\gamma is the coefficient of rotational friction\n\nJ_t = J + ml^2 is the total moment of inertia, with J being the pendulum’s moment of inertia about its center of mass\n\nu(t) is the control force applied at the base\n\ny(t) = \\theta(t) is the measured output (the pendulum’s angle)\n\nWe expect that when no control is applied to the system, the rod should be falling down when started from the upright position.\n\n#  label: appendix_examples-cell-01\n#  caption: Rendered output from the preceding code cell.\n\n%config InlineBackend.figure_format = 'retina'\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Apply book style\ntry:\n    import scienceplots\n    plt.style.use(['science', 'notebook'])\nexcept (ImportError, OSError):\n    pass  # Use matplotlib defaults\nfrom scipy.integrate import odeint\nfrom IPython.display import HTML, display\nfrom matplotlib.animation import FuncAnimation\n\n# System parameters\nm = 1.0  # mass of the pendulum (kg)\ng = 9.81  # acceleration due to gravity (m/s^2)\nl = 1.0  # length of the pendulum (m)\ngamma = 0.1  # coefficient of rotational friction\nJ = 1/3 * m * l**2  # moment of inertia of a rod about its center of mass\nJ_t = J + m * l**2  # total moment of inertia\n\n# Define the ODE for the inverted pendulum\ndef pendulum_ode(state, t):\n    theta, omega = state\n    dtheta = omega\n    domega = (m*g*l/J_t) * np.sin(theta) - (gamma/J_t) * omega\n    return [dtheta, domega]\n\n# Initial conditions: slightly off vertical position\ntheta0 = 0.1  # initial angle (radians)\nomega0 = 0  # initial angular velocity (rad/s)\ny0 = [theta0, omega0]\n\n# Time array for integration\nt = np.linspace(0, 10, 500)  # Reduced number of points\n\n# Solve ODE\nsolution = odeint(pendulum_ode, y0, t)\n\n# Extract theta and omega from the solution\ntheta = solution[:, 0]\nomega = solution[:, 1]\n\n# Create two separate plots\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))\n\n# Plot for angle\nax1.plot(t, theta)\nax1.set_xlabel('Time (s)')\nax1.set_ylabel('Angle (rad)')\nax1.set_title('Pendulum Angle over Time')\nax1.grid(True)\n\n# Plot for angular velocity\nax2.plot(t, omega)\nax2.set_xlabel('Time (s)')\nax2.set_ylabel('Angular velocity (rad/s)')\nax2.set_title('Pendulum Angular Velocity over Time')\nax2.grid(True)\n\nplt.tight_layout()\n\n# Function to create animation frames\ndef get_pendulum_position(theta):\n    x = l * np.sin(theta)\n    y = l * np.cos(theta)\n    return x, y\n\n# Create animation\nfig, ax = plt.subplots(figsize=(8, 8))\nline, = ax.plot([], [], 'o-', lw=2)\ntime_text = ax.text(0.02, 0.95, '', transform=ax.transAxes)\n\ndef init():\n    ax.set_xlim(-1.2*l, 1.2*l)\n    ax.set_ylim(-1.2*l, 1.2*l)  # Adjusted to show full range of motion\n    ax.set_aspect('equal', adjustable='box')\n    return line, time_text\n\ndef animate(i):\n    x, y = get_pendulum_position(theta[i])\n    line.set_data([0, x], [0, y])\n    time_text.set_text(f'Time: {t[i]:.2f} s')\n    return line, time_text\n\nanim = FuncAnimation(fig, animate, init_func=init, frames=len(t), interval=40, blit=True)\nplt.title('Inverted Pendulum Animation')\nax.grid(True)\n\n# Convert animation to JavaScript\njs_anim = anim.to_jshtml()\n\n# Close the figure to prevent it from being displayed\nplt.close(fig)\n\n# Display only the JavaScript animation\ndisplay(HTML(js_anim))\n\n","type":"content","url":"/appendix-examples#inverted-pendulum","position":3},{"hierarchy":{"lvl1":"Example COCPs","lvl2":"Pendulum in the Gym Environment"},"type":"lvl2","url":"/appendix-examples#pendulum-in-the-gym-environment","position":4},{"hierarchy":{"lvl1":"Example COCPs","lvl2":"Pendulum in the Gym Environment"},"content":" Gym is a widely used abstraction layer for defining discrete-time reinforcement learning problems. In reinforcement learning research, there's often a desire to develop general-purpose algorithms that are problem-agnostic. This research mindset leads us to voluntarily avoid considering the implementation details of a given environment. While this approach is understandable from a research perspective, it may not be optimal from a pragmatic, solution-driven standpoint where we care about solving specific problems efficiently. If we genuinely wanted to solve this problem without prior knowledge, why not look under the hood and embrace its nature as a trajectory optimization problem? \n\nLet’s examine the code and reverse-engineer the original continuous-time problem hidden behind the abstraction layer. Although the pendulum problem may have limited practical relevance as a real-world application, it serves as an excellent example for our analysis. In the current version of \n\nPendulum, we find that the Gym implementation uses a simplified model. Like our implementation, it assumes a fixed base and doesn’t model cart movement. The state is also represented by the pendulum angle and angular velocity.\nHowever, the equations of motion implemented in the Gym environment are different and correspond to the following ODE:\\begin{align*}\n\\dot{\\theta} &= \\theta_{dot} \\\\\n\\dot{\\theta}_{dot} &= \\frac{3g}{2l} \\sin(\\theta) + \\frac{3}{ml^2} u\n\\end{align*}\n\nCompared to our simplified model, the Gym implementation makes the following additional assumptions:\n\nIt omits the term \\frac{\\gamma}{J_t} \\dot{\\theta}(t), which represents damping or air resistance. This means that it assumes an idealized pendulum that doesn’t naturally slow down over time.\n\nIt uses ml^2 instead of J_t = J + ml^2, which assumes that all mass is concentrated at the pendulum’s end (like a point mass on a massless rod), rather than accounting for mass distribution along the pendulum.\n\nThe control input u is applied directly, without a \\cos \\theta(t) term, which means that the applied torque has the same effect regardless of the pendulum’s position, rather than varying with angle. For example, imagine trying to push a door open. When the door is almost closed (pendulum near vertical), a small push perpendicular to the door (analogous to our control input) can easily start it moving. However, when the door is already wide open (pendulum horizontal), the same push has little effect on the door’s angle. In a more detailed model, this would be captured by the \\cos \\theta(t) term, which is maximum when the pendulum is vertical (\\cos 0° = 1) and zero when horizontal (\\cos 90° = 0).\n\nThe goal remains to stabilize the rod upright, but the way in which this encoded is through the following instantenous cost function:\\begin{align*}\nc(\\theta, \\dot{\\theta}, u) &= (\\text{normalize}(\\theta))^2 + 0.1\\dot{\\theta}^2 + 0.001u^2\\\\\n\\text{normalize}(\\theta) &= ((\\theta + \\pi) \\bmod 2\\pi) - \\pi\n\\end{align*}\n\nThis cost function penalizes deviations from the upright position (first term), discouraging rapid motion (second term), and limiting control effort (third term). The relative weights has been manually chosen to balance the primary goal of upright stabilization with the secondary aims of smooth motion and energy efficiency. The normalization ensures that the angle is always in the range [-\\pi, \\pi] so that the pendulum positions (e.g., 0 and 2\\pi) are treated identically, which could otherwise confuse learning algorithms.\n\nStudying the code further, we find that it imposes bound constraints on both the control input and the angular velocity through clipping operations:\\begin{align*}\nu &= \\max(\\min(u, u_{max}), -u_{max}) \\\\\n\\dot{\\theta} &= \\max(\\min(\\dot{\\theta}, \\dot{\\theta}_{max}), -\\dot{\\theta}_{max})\n\\end{align*}\n\nWhere u_{max} = 2.0 and \\dot{\\theta}_{max} = 8.0.  Finally, when inspecting the \n\nstep function, we find that the dynamics are discretized using forward Euler under a fixed step size of h=0.0.5. Overall, the discrete-time trajectory optimization problem implemented in Gym is the following:\\begin{align*}\n\\min_{u_k} \\quad & J = \\sum_{k=0}^{N-1} c(\\theta_k, \\dot{\\theta}_k, u_k) \\\\\n\\text{subject to:} \\quad & \\theta_{k+1} = \\theta_k + \\dot{\\theta}_k \\cdot h \\\\\n& \\dot{\\theta}_{k+1} = \\dot{\\theta}_k + \\left(\\frac{3g}{2l}\\sin(\\theta_k) + \\frac{3}{ml^2}u_k\\right) \\cdot h \\\\\n& -u_{\\max} \\leq u_k \\leq u_{\\max} \\\\\n& -\\dot{\\theta}_{\\max} \\leq \\dot{\\theta}_k \\leq \\dot{\\theta}_{\\max}, \\quad k = 0, 1, ..., N-1 \\\\\n\\text{given:} \\quad      & \\theta_0 = \\theta_{\\text{initial}}, \\quad \\dot{\\theta}_0 = \\dot{\\theta}_{\\text{initial}}, \\quad N = 200\n\\end{align*}\n\nwith g = 10.0, l = 1.0, m = 1.0, u_{max} = 2.0, and \\dot{\\theta}_{max} = 8.0. This discrete-time problem corresponds to the following continuous-time optimal control problem:\\begin{align*}\n\\min_{u(t)} \\quad & J = \\int_{0}^{T} c(\\theta(t), \\dot{\\theta}(t), u(t)) dt \\\\\n\\text{subject to:} \\quad & \\dot{\\theta}(t) = \\dot{\\theta}(t) \\\\\n& \\ddot{\\theta}(t) = \\frac{3g}{2l}\\sin(\\theta(t)) + \\frac{3}{ml^2}u(t) \\\\\n& -u_{\\max} \\leq u(t) \\leq u_{\\max} \\\\\n& -\\dot{\\theta}_{\\max} \\leq \\dot{\\theta}(t) \\leq \\dot{\\theta}_{\\max} \\\\\n\\text{given:} \\quad      & \\theta(0) = \\theta_0, \\quad \\dot{\\theta}(0) = \\dot{\\theta}_0, \\quad T = 10 \\text{ seconds}\n\\end{align*}","type":"content","url":"/appendix-examples#pendulum-in-the-gym-environment","position":5},{"hierarchy":{"lvl1":"Example COCPs","lvl2":"Heat Exchanger"},"type":"lvl2","url":"/appendix-examples#heat-exchanger","position":6},{"hierarchy":{"lvl1":"Example COCPs","lvl2":"Heat Exchanger"},"content":"\n\nWe are considering a system where fluid flows through a tube, and the goal is to control the temperature of the fluid by adjusting the temperature of the tube’s wall over time. The wall temperature, denoted as  T_w(t) , can be changed as a function of time, but it remains the same along the length of the tube. On the other hand, the temperature of the fluid inside the tube,  T(z, t) , depends both on its position along the tube  z  and on time  t . It evolves according to the following partial differential equation:\\frac{\\partial T}{\\partial t} = -v \\frac{\\partial T}{\\partial z} + \\frac{h}{\\rho C_p} (T_w(t) - T)\n\nwhere we have:\n\n v : the average speed of the fluid moving through the tube,\n\n h : how easily heat transfers from the wall to the fluid,\n\n \\rho  and  C_p : the fluid’s density and heat capacity.\n\nThis equation describes how the fluid’s temperature changes as it moves along the tube and interacts with the tube’s wall temperature. The fluid enters the tube with an initial temperature  T_0  at the inlet (where  z = 0 ). Our objective is to adjust the wall temperature  T_w(t)  so that by a specific final time  t_f , the fluid’s temperature reaches a desired distribution  T_s(z)  along the length of the tube. The relationship for  T_s(z)  under steady-state conditions (ie. when changes over time are no longer considered), is given by:\\frac{d T_s}{d z} = \\frac{h}{v \\rho C_p}[\\theta - T_s]\n\nwhere  \\theta  is a constant temperature we want to maintain at the wall. The objective is to control the wall temperature  T_w(t)  so that by the end of the time interval  t_f , the fluid temperature  T(z, t_f)  is as close as possible to the desired distribution  T_s(z) . This can be formalized by minimizing the following quantity:I = \\int_0^L \\left[T(z, t_f) - T_s(z)\\right]^2 dz\n\nwhere  L  is the length of the tube. Additionally, we require that the wall temperature cannot exceed a maximum allowable value  T_{\\max} :T_w(t) \\leq T_{\\max}","type":"content","url":"/appendix-examples#heat-exchanger","position":7},{"hierarchy":{"lvl1":"Example COCPs","lvl2":"Nuclear Reactor"},"type":"lvl2","url":"/appendix-examples#nuclear-reactor","position":8},{"hierarchy":{"lvl1":"Example COCPs","lvl2":"Nuclear Reactor"},"content":"\n\nIn a nuclear reactor, neutrons interact with fissile nuclei, causing nuclear fission. This process produces more neutrons and smaller fissile nuclei called precursors. The precursors subsequently absorb more neutrons, generating “delayed” neutrons. The kinetic energy of these products is converted into thermal energy through collisions with neighboring atoms. The reactor’s power output is determined by the concentration of neutrons available for nuclear fission.\n\nThe reaction kinetics can be modeled using a system of ordinary differential equations:\\begin{align*}\n\\dot{x}(t) &= \\frac{r(t)x(t) - \\alpha x^2(t) - \\beta x(t)}{\\tau} + \\mu y(t), & x(0) &= x_0 \\\\\n\\dot{y}(t) &= \\frac{\\beta x(t)}{\\tau} - \\mu y(t), & y(0) &= y_0\n\\end{align*}\n\nwhere:\n\nx(t): concentration of neutrons at time t\n\ny(t): concentration of precursors at time t\n\nt: time\n\nr(t) = r[u(t)]: degree of change in neutron multiplication at time t as a function of control rod displacement u(t)\n\n\\alpha: reactivity coefficient\n\n\\beta: fraction of delayed neutrons\n\n\\mu: decay constant for precursors\n\n\\tau: average time taken by a neutron to produce a neutron or precursor\n\nThe power output can be adjusted based on demand by inserting or retracting a neutron-absorbing control rod. Inserting the control rod absorbs neutrons, reducing the heat flux and power output, while retracting the rod has the opposite effect.\n\nThe objective is to change the neutron concentration x(t) from an initial value x_0 to a stable value x_\\mathrm{f} at time t_\\mathrm{f} while minimizing the displacement of the control rod. This can be formulated as an optimal control problem, where the goal is to find the control function u(t) that minimizes the objective functional:I = \\int_0^{t_\\mathrm{f}} u^2(t) \\, \\mathrm{d}t\n\nsubject to the final conditions:\\begin{align*}\nx(t_\\mathrm{f}) &= x_\\mathrm{f} \\\\\n\\dot{x}(t_\\mathrm{f}) &= 0\n\\end{align*}\n\nand the constraint |u(t)| \\leq u_\\mathrm{max}","type":"content","url":"/appendix-examples#nuclear-reactor","position":9},{"hierarchy":{"lvl1":"Example COCPs","lvl2":"Chemotherapy"},"type":"lvl2","url":"/appendix-examples#chemotherapy","position":10},{"hierarchy":{"lvl1":"Example COCPs","lvl2":"Chemotherapy"},"content":"Chemotherapy uses drugs to kill cancer cells. However, these drugs can also have toxic effects on healthy cells in the body. To optimize the effectiveness of chemotherapy while minimizing its side effects, we can formulate an optimal control problem.\n\nThe drug concentration y_1(t) and the number of immune cells y_2(t), healthy cells y_3(t), and cancer cells y_4(t) in an organ at any time t during chemotherapy can be modeled using a system of ordinary differential equations:\\begin{align*}\n\\dot{y}_1(t) &= u(t) - \\gamma_6 y_1(t) \\\\\n\\dot{y}_2(t) &= \\dot{y}_{2,\\text{in}} + r_2 \\frac{y_2(t) y_4(t)}{\\beta_2 + y_4(t)} - \\gamma_3 y_2(t) y_4(t) - \\gamma_4 y_2(t) - \\alpha_2 y_2(t) \\left(1 - e^{-y_1(t) \\lambda_2}\\right) \\\\\n\\dot{y}_3(t) &= r_3 y_3(t) \\left(1 - \\beta_3 y_3(t)\\right) - \\gamma_5 y_3(t) y_4(t) - \\alpha_3 y_3(t) \\left(1 - e^{-y_1(t) \\lambda_3}\\right) \\\\\n\\dot{y}_4(t) &= r_1 y_4(t) \\left(1 - \\beta_1 y_4(t)\\right) - \\gamma_1 y_3(t) y_4(t) - \\gamma_2 y_2(t) y_4(t) - \\alpha_1 y_4(t) \\left(1 - e^{-y_1(t) \\lambda_1}\\right)\n\\end{align*}\n\nwhere:\n\ny_1(t): drug concentration in the organ at time t\n\ny_2(t): number of immune cells in the organ at time t\n\ny_3(t): number of healthy cells in the organ at time t\n\ny_4(t): number of cancer cells in the organ at time t\n\n\\dot{y}_{2,\\text{in}}: constant rate of immune cells entering the organ to fight cancer cells\n\nu(t): rate of drug injection into the organ at time t\n\nr_i, \\beta_i: constants in the growth terms\n\n\\alpha_i, \\lambda_i: constants in the decay terms due to the action of the drug\n\n\\gamma_i: constants in the remaining decay terms\n\nThe objective is to minimize the number of cancer cells y_4(t) in a specified time t_\\mathrm{f} while using the minimum amount of drug to reduce its toxic effects. This can be formulated as an optimal control problem, where the goal is to find the control function u(t) that minimizes the objective functional:I = y_4(t_\\mathrm{f}) + \\int_0^{t_\\mathrm{f}} u(t) \\, \\mathrm{d}t\n\nsubject to the system dynamics, initial conditions, and the constraint u(t) \\geq 0.\n\nAdditional constraints may include:\n\nMaintaining a minimum number of healthy cells during treatment:y_3(t) \\geq y_{3,\\min}\n\nImposing an upper limit on the drug dosage:u(t) \\leq u_{\\max}","type":"content","url":"/appendix-examples#chemotherapy","position":11},{"hierarchy":{"lvl1":"Example COCPs","lvl2":"Government Corruption"},"type":"lvl2","url":"/appendix-examples#government-corruption","position":12},{"hierarchy":{"lvl1":"Example COCPs","lvl2":"Government Corruption"},"content":"In this model from Feichtinger and Wirl (1994), we aim to understand the incentives for politicians to engage in corrupt activities or to combat corruption. The model considers a politician’s popularity as a dynamic process that is influenced by the public’s memory of recent and past corruption. The objective is to find conditions under which self-interested politicians would choose to be honest or dishonest.\n\nThe model introduces the following notation:\n\nC(t): accumulated awareness (knowledge) of past corruption at time t\n\nu(t): extent of corruption (politician’s control variable) at time t\n\n\\delta: rate of forgetting past corruption\n\nP(t): politician’s popularity at time t\n\ng(P): growth function of popularity; g''(P) < 0\n\nf(C): function measuring the loss of popularity caused by C; f'(C) > 0, f''(C) \\geq 0\n\nU_1(P): benefits associated with being popular; U_1'(P) > 0, U_1''(P) \\leq 0\n\nU_2(u): benefits resulting from bribery and fraud; U_2'(u) > 0, U_2''(u) < 0\n\nr: discount rate\n\nThe dynamics of the public’s memory of recent and past corruption C(t) are modeled as:\\begin{align*}\n\\dot{C}(t) &= u(t) - \\delta C(t), \\quad C(0) = C_0\n\\end{align*}\n\nThe evolution of the politician’s popularity P(t) is governed by:\\begin{align*}\n\\dot{P}(t) &= g(P(t)) - f(C(t)), \\quad P(0) = P_0\n\\end{align*}\n\nThe politician’s objective is to maximize the following objective:\\int_0^{\\infty} e^{-rt} [U_1(P(t)) + U_2(u(t))] \\, \\mathrm{d}t\n\nsubject to the dynamics of corruption awareness and popularity.\n\nThe optimal control problem can be formulated as follows:\\begin{align*}\n\\max_{u(\\cdot)} \\quad & \\int_0^{\\infty} e^{-rt} [U_1(P(t)) + U_2(u(t))] \\, \\mathrm{d}t \\\\\n\\text{s.t.} \\quad & \\dot{C}(t) = u(t) - \\delta C(t), \\quad C(0) = C_0 \\\\\n& \\dot{P}(t) = g(P(t)) - f(C(t)), \\quad P(0) = P_0\n\\end{align*}\n\nThe state variables are the accumulated awareness of past corruption C(t) and the politician’s popularity P(t). The control variable is the extent of corruption u(t). The objective functional represents the discounted stream of benefits coming from being honest (popularity) and from being dishonest (corruption).","type":"content","url":"/appendix-examples#government-corruption","position":13},{"hierarchy":{"lvl1":"Solving Initial Value Problems"},"type":"lvl1","url":"/appendix-ivps","position":0},{"hierarchy":{"lvl1":"Solving Initial Value Problems"},"content":"An ODE is an implicit representation of a state-space trajectory: it tells us how the state changes in time but not precisely what the state is at any given time. To find out this information, we need to either solve the ODE analytically (for some special structure) or, as we’re going to do, solve them numerically. This numerical procedure is meant to solve what is called an IVP (initial value problem) of the form:\\text{Find } x(t) \\text{ given } \\dot{x}(t) = f(x(t), t) \\text{ and } x(t_0) = x_0","type":"content","url":"/appendix-ivps","position":1},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Euler’s Method"},"type":"lvl2","url":"/appendix-ivps#eulers-method","position":2},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Euler’s Method"},"content":"The algorithm to solve this problem is, in its simplest form, a for loop which closely resembles the updates encountered in gradient descent (in fact, gradient descent can be derived from the gradient flow ODE, but that’s another discussion). The so-called explicit Euler’s method can be implemented as follow:\n\nEuler’s method\n\nInput: f(x, t), x_0, t_0, t_{end}, h\n\nOutput: Approximate solution x(t) at discrete time points\n\nInitialize t = t_0, x = x_0\n\nWhile t < t_{end}:\n\nCompute x_{new} = x + h f(x, t)\n\nUpdate t = t + h\n\nUpdate x = x_{new}\n\nStore or output the pair (t, x)\n\nEnd While\n\nConsider the following simple dynamical system of a ballistic motion model, neglecting air resistance. The state of the system is described by two variables: y(t): vertical position at time t and v(t), the vertical velocity at time t. The corresponding ODE is:\\begin{aligned}\n\\frac{dy}{dt} &= v \\\\\n\\frac{dv}{dt} &= -g\n\\end{aligned}\n\nwhere g \\approx 9.81 \\text{ m/s}^2 is the acceleration due to gravity. In our code, we use the initial conditions\ny(0) = 0 \\text{ m} and v(0) = v_0 \\text{ m/s} where v_0 is the initial velocity (in this case, v_0 = 20 \\text{ m/s}).\nThe analytical solution to this system is:\\begin{aligned}\ny(t) &= v_0t - \\frac{1}{2}gt^2 \\\\\nv(t) &= v_0 - gt\n\\end{aligned}\n\nThis system models the vertical motion of an object launched upward, reaching a maximum height before falling back down due to gravity.\n\nEuler’s method can be obtained by taking the first-order Taylor expansion of x(t) at t:x(t + h) \\approx x(t) + h \\frac{dx}{dt}(t) = x(t) + h f(x(t), t)\n\nEach step of the algorithm therefore involves approximating the function with a linear function of slope f over the given interval h.\n\n#  label: appendix_ivps-cell-01\n#  caption: Rendered output from the preceding code cell.\n\n%config InlineBackend.figure_format = 'retina'\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Apply book style\ntry:\n    import scienceplots\n    plt.style.use(['science', 'notebook'])\nexcept (ImportError, OSError):\n    pass  # Use matplotlib defaults\n\ndef f(y, t):\n    \"\"\"\n    Derivative function for vertical motion under gravity.\n    y[0] is position, y[1] is velocity.\n    \"\"\"\n    g = 9.81  # acceleration due to gravity (m/s^2)\n    return np.array([y[1], -g])\n\ndef euler_method(f, y0, t0, t_end, h):\n    \"\"\"\n    Implement Euler's method for the entire time range.\n    \"\"\"\n    t = np.arange(t0, t_end + h, h)\n    y = np.zeros((len(t), 2))\n    y[0] = y0\n    for i in range(1, len(t)):\n        y[i] = y[i-1] + h * f(y[i-1], t[i-1])\n    return t, y\n\ndef true_solution(t):\n    \"\"\"\n    Analytical solution for the ballistic trajectory.\n    \"\"\"\n    y0, v0 = 0, 20  # initial height and velocity\n    g = 9.81\n    return y0 + v0*t - 0.5*g*t**2, v0 - g*t\n\n# Set up the problem\nt0, t_end = 0, 4\ny0 = np.array([0, 20])  # initial height = 0, initial velocity = 20 m/s\n\n# Different step sizes\nstep_sizes = [1.0, 0.5, 0.1]\ncolors = ['r', 'g', 'b']\nmarkers = ['o', 's', '^']\n\n# True solution\nt_fine = np.linspace(t0, t_end, 1000)\ny_true, v_true = true_solution(t_fine)\n\n# Plotting\nplt.figure(figsize=(12, 8))\n\n# Plot Euler approximations\nfor h, color, marker in zip(step_sizes, colors, markers):\n    t, y = euler_method(f, y0, t0, t_end, h)\n    plt.plot(t, y[:, 0], color=color, marker=marker, linestyle='--', \n             label=f'Euler h = {h}', markersize=6, markerfacecolor='none')\n\n# Plot true solution last so it's on top\nplt.plot(t_fine, y_true, 'k-', label='True trajectory', linewidth=2, zorder=10)\n\nplt.xlabel('Time (s)', fontsize=12)\nplt.ylabel('Height (m)', fontsize=12)\nplt.title(\"Euler's Method: Effect of Step Size on Ballistic Trajectory Approximation\", fontsize=14)\nplt.legend(fontsize=10)\nplt.grid(True, linestyle=':', alpha=0.7)\n\n# Add text to explain the effect of step size\nplt.text(2.5, 15, \"Smaller step sizes\\nyield better approximations\", \n         bbox=dict(facecolor='white', edgecolor='black', alpha=0.7),\n         fontsize=10, ha='center', va='center')\n\nplt.tight_layout()\n\nAnother way to understand Euler’s method is through the fundamental theorem of calculus:x(t + h) = x(t) + \\int_t^{t+h} f(x(\\tau), \\tau) d\\tau\n\nWe then approximate the integral term with a box of width h and height f, and therefore of area h f.\n\n#  label: appendix_ivps-cell-02\n#  caption: Rendered output from the preceding code cell.\n\n%config InlineBackend.figure_format = 'retina'\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Apply book style\ntry:\n    import scienceplots\n    plt.style.use(['science', 'notebook'])\nexcept (ImportError, OSError):\n    pass  # Use matplotlib defaults\nfrom matplotlib.patches import Rectangle\n\ndef v(t):\n    \"\"\"\n    Velocity function for the ballistic trajectory.\n    \"\"\"\n    v0 = 20   # initial velocity (m/s)\n    g = 9.81  # acceleration due to gravity (m/s^2)\n    return v0 - g * t\n\ndef position(t):\n    \"\"\"\n    Position function (integral of velocity).\n    \"\"\"\n    v0 = 20\n    g = 9.81\n    return v0*t - 0.5*g*t**2\n\n# Set up the problem\nt0, t_end = 0, 2\nnum_points = 1000\nt = np.linspace(t0, t_end, num_points)\n\n# Calculate true velocity and position\nv_true = v(t)\nx_true = position(t)\n\n# Euler's method with a large step size for visualization\nh = 0.5\nt_euler = np.arange(t0, t_end + h, h)\nx_euler = np.zeros_like(t_euler)\n\nfor i in range(1, len(t_euler)):\n    x_euler[i] = x_euler[i-1] + h * v(t_euler[i-1])\n\n# Plotting\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12), sharex=True)\n\n# Plot velocity function and its approximation\nax1.plot(t, v_true, 'b-', label='True velocity')\nax1.fill_between(t, 0, v_true, alpha=0.3, label='True area (displacement)')\n\n# Add rectangles with hashed pattern, ruler-like annotations, and area values\nfor i in range(len(t_euler) - 1):\n    t_i = t_euler[i]\n    v_i = v(t_i)\n    rect = Rectangle((t_i, 0), h, v_i, \n                     fill=True, facecolor='red', edgecolor='r', \n                     alpha=0.15, hatch='///')\n    ax1.add_patch(rect)\n    \n    # Add ruler-like annotations\n    # Vertical ruler (height)\n    ax1.annotate('', xy=(t_i, 0), xytext=(t_i, v_i),\n                 arrowprops=dict(arrowstyle='<->', color='red'))\n    ax1.text(t_i - 0.05, v_i/2, f'v(t{i}) = {v_i:.2f}', rotation=90, \n             va='center', ha='right', color='red', fontweight='bold')\n    \n    # Horizontal ruler (width)\n    ax1.annotate('', xy=(t_i, -1), xytext=(t_i + h, -1),\n                 arrowprops=dict(arrowstyle='<->', color='red'))\n    ax1.text(t_i + h/2, -2, f'h = {h}', ha='center', va='top', \n             color='red', fontweight='bold')\n    \n    # Add area value in the middle of each rectangle\n    area = h * v_i\n    ax1.text(t_i + h/2, v_i/2, f'Area = {area:.2f}', ha='center', va='center', \n             color='white', fontweight='bold', bbox=dict(facecolor='red', edgecolor='none', alpha=0.7))\n\n# Plot only the points for Euler's method\nax1.plot(t_euler, v(t_euler), 'ro', markersize=6, label=\"Euler's points\")\nax1.set_ylabel('Velocity (m/s)', fontsize=12)\nax1.set_title(\"Velocity Function and Euler's Approximation\", fontsize=14)\nax1.legend(fontsize=10)\nax1.grid(True, linestyle=':', alpha=0.7)\nax1.set_ylim(bottom=-3)  # Extend y-axis to show horizontal rulers\n\n# Plot position function and its approximation\nax2.plot(t, x_true, 'b-', label='True position')\nax2.plot(t_euler, x_euler, 'ro--', label=\"Euler's approximation\", markersize=6, linewidth=2)\n\n# Add vertical arrows and horizontal lines to show displacement and time step\nfor i in range(1, len(t_euler)):\n    t_i = t_euler[i]\n    x_prev = x_euler[i-1]\n    x_curr = x_euler[i]\n    \n    # Vertical line for displacement\n    ax2.plot([t_i, t_i], [x_prev, x_curr], 'g:', linewidth=2)\n    \n    # Horizontal line for time step\n    ax2.plot([t_i - h, t_i], [x_prev, x_prev], 'g:', linewidth=2)\n    \n    # Add text to show the displacement value\n    displacement = x_curr - x_prev\n    ax2.text(t_i + 0.05, (x_prev + x_curr)/2, f'+{displacement:.2f}', \n             color='green', fontweight='bold', va='center')\n    \n    # Add text to show the time step\n    ax2.text(t_i - h/2, x_prev - 0.5, f'h = {h}', \n             color='green', fontweight='bold', ha='center', va='top')\n\nax2.set_xlabel('Time (s)', fontsize=12)\nax2.set_ylabel('Position (m)', fontsize=12)\nax2.set_title(\"Position: True vs Euler's Approximation\", fontsize=14)\nax2.legend(fontsize=10)\nax2.grid(True, linestyle=':', alpha=0.7)\n\n# Add explanatory text\nax1.text(1.845, 15, \"Red hashed areas show\\nEuler's approximation\\nof the area under the curve\", \n         bbox=dict(facecolor='white', edgecolor='black', alpha=0.7),\n         fontsize=10, ha='center', va='center')\n\nplt.tight_layout()\n\n","type":"content","url":"/appendix-ivps#eulers-method","position":3},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Implicit Euler’s Method"},"type":"lvl2","url":"/appendix-ivps#implicit-eulers-method","position":4},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Implicit Euler’s Method"},"content":"An alternative approach is the Implicit Euler method, also known as the Backward Euler method. Instead of using the derivative at the current point to step forward, it uses the derivative at the end of the interval. This leads to the following update rule:x_{new} = x + h f(x_{new}, t_{new})\n\nNote that x_{new} appears on both sides of the equation, making this an implicit method. The algorithm for the Implicit Euler method can be described as follows:\n\nImplicit Euler’s Method\n\nInput: f(x, t), x_0, t_0, t_{end}, h\n\nOutput: Approximate solution x(t) at discrete time points\n\nInitialize t = t_0, x = x_0\n\nWhile t < t_{end}:\n\nSet t_{new} = t + h\n\nSolve for x_{new} in the equation: x_{new} = x + h f(x_{new}, t_{new})\n\nUpdate t = t_{new}\n\nUpdate x = x_{new}\n\nStore or output the pair (t, x)\n\nEnd While\n\nThe main difference in the Implicit Euler method is step 4, where we need to solve a (potentially nonlinear) equation to find x_{new}. This is typically done using iterative methods such as fixed-point iteration or Newton’s method.","type":"content","url":"/appendix-ivps#implicit-eulers-method","position":5},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl3":"Stiff ODEs","lvl2":"Implicit Euler’s Method"},"type":"lvl3","url":"/appendix-ivps#stiff-odes","position":6},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl3":"Stiff ODEs","lvl2":"Implicit Euler’s Method"},"content":"While the Implicit Euler method requires more computation per step, it often allows for larger step sizes and can provide better stability for certain types of problems, especially stiff ODEs.\n\nStiff ODEs are differential equations for which certain numerical methods for solving the equation are numerically unstable, unless the step size is taken to be extremely small. These ODEs typically involve multiple processes occurring at widely different rates. In a stiff problem, the fastest-changing component of the solution can make the numerical method unstable unless the step size is extremely small. However, such a small step size may lead to an impractical amount of computation to traverse the entire interval of interest.\n\nFor example, consider a chemical reaction where some reactions occur very quickly while others occur much more slowly. The fast reactions quickly approach their equilibrium, but small perturbations in the slower reactions can cause rapid changes in the fast reactions.\n\nA classic example of a stiff ODE is the Van der Pol oscillator with a large parameter. The Van der Pol equation is:\\frac{d^2x}{dt^2} - \\mu(1-x^2)\\frac{dx}{dt} + x = 0\n\nwhere \\mu is a scalar parameter. This second-order ODE can be transformed into a system of first-order ODEs by introducing a new variable y = \\frac{dx}{dt}:\\begin{aligned}\n\\frac{dx}{dt} &= y \\\\\n\\frac{dy}{dt} &= \\mu(1-x^2)y - x\n\\end{aligned}\n\nWhen \\mu is large (e.g., \\mu = 1000), this system becomes stiff. The large \\mu causes rapid changes in y when x is near ±1, but slower changes elsewhere. This leads to a solution with sharp transitions followed by periods of gradual change.","type":"content","url":"/appendix-ivps#stiff-odes","position":7},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Trapezoid Method"},"type":"lvl2","url":"/appendix-ivps#trapezoid-method","position":8},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Trapezoid Method"},"content":"The trapezoid method, also known as the trapezoidal rule, offers improved accuracy and stability compared to the simple Euler method. The name “trapezoid method” comes from the idea of using a trapezoid to approximate the integral term in the fundamental theorem of calculus. This leads to the following update rule:x_{new} = x + \\frac{h}{2}[f(x, t) + f(x_{new}, t_{new})]\n\nwhere  t_{new} = t + h . Note that this formula involves  x_{new}  on both sides of the equation, making it an implicit method, similar to the implicit Euler method discussed earlier.\n\n#  label: appendix_ivps-cell-03\n#  caption: Rendered output from the preceding code cell.\n\n%config InlineBackend.figure_format = 'retina'\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Apply book style\ntry:\n    import scienceplots\n    plt.style.use(['science', 'notebook'])\nexcept (ImportError, OSError):\n    pass  # Use matplotlib defaults\nfrom matplotlib.patches import Rectangle, Polygon\n\ndef v(t):\n    \"\"\"Velocity function for the ballistic trajectory.\"\"\"\n    v0 = 20   # initial velocity (m/s)\n    g = 9.81  # acceleration due to gravity (m/s^2)\n    return v0 - g * t\n\ndef position(t):\n    \"\"\"Position function (integral of velocity).\"\"\"\n    v0 = 20\n    g = 9.81\n    return v0*t - 0.5*g*t**2\n\n# Set up the problem\nt0, t_end = 0, 2\nnum_points = 1000\nt = np.linspace(t0, t_end, num_points)\n\n# Calculate true velocity and position\nv_true = v(t)\nx_true = position(t)\n\n# Euler's method and Trapezoid method with a large step size for visualization\nh = 0.5\nt_numeric = np.arange(t0, t_end + h, h)\nx_euler = np.zeros_like(t_numeric)\nx_trapezoid = np.zeros_like(t_numeric)\n\nfor i in range(1, len(t_numeric)):\n    # Euler's method\n    x_euler[i] = x_euler[i-1] + h * v(t_numeric[i-1])\n    \n    # Trapezoid method (implicit, so we use a simple fixed-point iteration)\n    x_trapezoid[i] = x_trapezoid[i-1]\n    for _ in range(5):  # 5 iterations should be enough for this simple problem\n        x_trapezoid[i] = x_trapezoid[i-1] + h/2 * (v(t_numeric[i-1]) + v(t_numeric[i]))\n\n# Plotting\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 16), sharex=True)\n\n# Plot velocity function and its approximations\nax1.plot(t, v_true, 'b-', label='True velocity')\nax1.fill_between(t, 0, v_true, alpha=0.3, label='True area (displacement)')\n\n# Add trapezoids and rectangles\nfor i in range(len(t_numeric) - 1):\n    t_i, t_next = t_numeric[i], t_numeric[i+1]\n    v_i, v_next = v(t_i), v(t_next)\n    \n    # Euler's rectangle (hashed pattern)\n    rect = Rectangle((t_i, 0), h, v_i, fill=True, facecolor='red', edgecolor='r', alpha=0.15, hatch='///')\n    ax1.add_patch(rect)\n    \n    # Trapezoid (dot pattern)\n    trapezoid = Polygon([(t_i, 0), (t_i, v_i), (t_next, v_next), (t_next, 0)], \n                        fill=True, facecolor='green', edgecolor='g', alpha=0.15, hatch='....')\n    ax1.add_patch(trapezoid)\n    \n    # Add area values\n    euler_area = h * v_i\n    trapezoid_area = h * (v_i + v_next) / 2\n    ax1.text(t_i + h/2, v_i/2, f'Euler: {euler_area:.2f}', ha='center', va='bottom', color='red', fontweight='bold')\n    ax1.text(t_i + h/2, (v_i + v_next)/4, f'Trapezoid: {trapezoid_area:.2f}', ha='center', va='top', color='green', fontweight='bold')\n\n# Plot points for Euler's and Trapezoid methods\nax1.plot(t_numeric, v(t_numeric), 'ro', markersize=6, label=\"Euler's points\")\nax1.plot(t_numeric, v(t_numeric), 'go', markersize=6, label=\"Trapezoid points\")\n\nax1.set_ylabel('Velocity (m/s)', fontsize=12)\nax1.set_title(\"Velocity Function: True vs Numerical Approximations\", fontsize=14)\nax1.legend(fontsize=10)\nax1.grid(True, linestyle=':', alpha=0.7)\n\n# Plot position function and its approximations\nax2.plot(t, x_true, 'b-', label='True position')\nax2.plot(t_numeric, x_euler, 'ro--', label=\"Euler's approximation\", markersize=6, linewidth=2)\nax2.plot(t_numeric, x_trapezoid, 'go--', label=\"Trapezoid approximation\", markersize=6, linewidth=2)\n\nax2.set_xlabel('Time (s)', fontsize=12)\nax2.set_ylabel('Position (m)', fontsize=12)\nax2.set_title(\"Position: True vs Numerical Approximations\", fontsize=14)\nax2.legend(fontsize=10)\nax2.grid(True, linestyle=':', alpha=0.7)\n\n# Add explanatory text\nax1.text(1.76, 17, \"Red hashed areas: Euler's approximation\\nGreen dotted areas: Trapezoid approximation\", \n         bbox=dict(facecolor='white', edgecolor='black', alpha=0.7),\n         fontsize=10, ha='center', va='center')\n\nplt.tight_layout()\n\nAlgorithmically, the trapezoid method can be described as follows:\n\nTrapezoid Method\n\nInput:  f(x, t) ,  x_0 ,  t_0 ,  t_{end} ,  h \n\nOutput: Approximate solution  x(t)  at discrete time points\n\nInitialize  t = t_0 ,  x = x_0 \n\nWhile  t < t_{end} :\n\nSet  t_{new} = t + h \n\nSolve for  x_{new} in the equation:  x_{new} = x + \\frac{h}{2}[f(x, t) + f(x_{new}, t_{new})] \n\nUpdate  t = t_{new} \n\nUpdate  x = x_{new} \n\nStore or output the pair  (t, x) \n\nThe trapezoid method can also be derived by averaging the forward Euler and backward Euler methods. Recall that:\n\nForward Euler method:x_{n+1} = x_n + h f(x_n, t_n)\n\nBackward Euler method:x_{n+1} = x_n + h f(x_{n+1}, t_{n+1})\n\nTaking the average of these two methods yields:\\begin{aligned}\nx_{n+1} &= \\frac{1}{2} \\left( x_n + h f(x_n, t_n) \\right) + \\frac{1}{2} \\left( x_n + h f(x_{n+1}, t_{n+1}) \\right) \\\\\n&= x_n + \\frac{h}{2} \\left( f(x_n, t_n) + f(x_{n+1}, t_{n+1}) \\right)\n\\end{aligned}\n\nThis gives us the update rule for the trapezoid method. Recall that the forward Euler method approximates the solution by extrapolating linearly using the slope at the beginning of the interval [t_n, t_{n+1}] . In contrast, the backward Euler method extrapolates linearly using the slope at the end of the interval. The trapezoid method, on the other hand, averages these two slopes. This averaging provides better approximation properties than either of the methods alone, offering both stability and accuracy. Note finally that unlike the forward or backward Euler methods, the trapezoid method is also symmetric in time. This means that if you were to reverse time and apply the method backward, you would get the same results (up to numerical precision).","type":"content","url":"/appendix-ivps#trapezoid-method","position":9},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Trapezoidal Predictor-Corrector"},"type":"lvl2","url":"/appendix-ivps#trapezoidal-predictor-corrector","position":10},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Trapezoidal Predictor-Corrector"},"content":"The trapezoid method can also be implemented under the so-called predictor-corrector framework. This interpretation reformulates the implicit trapezoid rule into an explicit two-step process:\n\nPredictor Step:We make an initial guess for  x_{n+1}  using the forward Euler method:x_{n+1}^* = x_n + h f(x_n, t_n)\n\nThis is our “predictor” step, where  x_{n+1}^*  is the predicted value of  x_{n+1} .\n\nCorrector Step:We then use this predicted value to estimate  f(x_{n+1}^*, t_{n+1})  and apply the trapezoid formula:x_{n+1} = x_n + \\frac{h}{2} \\left[ f(x_n, t_n) + f(x_{n+1}^*, t_{n+1}) \\right]\n\nThis is our “corrector” step, where the initial guess  x_{n+1}^*  is corrected by taking into account the slope at  (x_{n+1}^*, t_{n+1}) .\n\nThis two-step process is similar to performing one iteration of Newton’s method to solve the implicit trapezoid equation, starting from the Euler prediction. However, to fully solve the implicit equation, multiple iterations would be necessary until convergence is achieved.\n\n#  label: appendix_ivps-cell-04\n#  caption: Rendered output from the preceding code cell.\n\n%config InlineBackend.figure_format = 'retina'\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Apply book style\ntry:\n    import scienceplots\n    plt.style.use(['science', 'notebook'])\nexcept (ImportError, OSError):\n    pass  # Use matplotlib defaults\n\ndef f(y, t):\n    \"\"\"\n    Derivative function for vertical motion under gravity.\n    y[0] is position, y[1] is velocity.\n    \"\"\"\n    g = 9.81  # acceleration due to gravity (m/s^2)\n    return np.array([y[1], -g])\n\ndef true_solution(t):\n    \"\"\"\n    Analytical solution for the ballistic trajectory.\n    \"\"\"\n    y0, v0 = 0, 20  # initial height and velocity\n    g = 9.81\n    return y0 + v0*t - 0.5*g*t**2, v0 - g*t\n\ndef trapezoid_method_visual(f, y0, t0, t_end, h):\n    \"\"\"\n    Implement the trapezoid method for the entire time range.\n    Returns predictor and corrector steps for visualization.\n    \"\"\"\n    t = np.arange(t0, t_end + h, h)\n    y = np.zeros((len(t), 2))\n    y_predictor = np.zeros((len(t), 2))\n    y[0] = y_predictor[0] = y0\n    for i in range(1, len(t)):\n        # Predictor step (Euler forward)\n        slope_start = f(y[i-1], t[i-1])\n        y_predictor[i] = y[i-1] + h * slope_start\n        \n        # Corrector step\n        slope_end = f(y_predictor[i], t[i])\n        y[i] = y[i-1] + h * 0.5 * (slope_start + slope_end)\n    \n    return t, y, y_predictor\n\n# Set up the problem\nt0, t_end = 0, 2\ny0 = np.array([0, 20])  # initial height = 0, initial velocity = 20 m/s\nh = 0.5  # Step size\n\n# Compute trapezoid method steps\nt, y_corrector, y_predictor = trapezoid_method_visual(f, y0, t0, t_end, h)\n\n# Plotting\nplt.figure(figsize=(12, 8))\n\n# Plot the true solution for comparison\nt_fine = np.linspace(t0, t_end, 1000)\ny_true, v_true = true_solution(t_fine)\nplt.plot(t_fine, y_true, 'k-', label='True trajectory', linewidth=1.5)\n\n# Plot the predictor and corrector steps\nfor i in range(len(t)-1):\n    # Points for the predictor step\n    p0 = [t[i], y_corrector[i, 0]]\n    p1_predictor = [t[i+1], y_predictor[i+1, 0]]\n    \n    # Points for the corrector step\n    p1_corrector = [t[i+1], y_corrector[i+1, 0]]\n    \n    # Plot predictor step\n    plt.plot([p0[0], p1_predictor[0]], [p0[1], p1_predictor[1]], 'r--', linewidth=2)\n    plt.plot(p1_predictor[0], p1_predictor[1], 'ro', markersize=8)\n    \n    # Plot corrector step\n    plt.plot([p0[0], p1_corrector[0]], [p0[1], p1_corrector[1]], 'g--', linewidth=2)\n    plt.plot(p1_corrector[0], p1_corrector[1], 'go', markersize=8)\n    \n    # Add arrows to show the predictor and corrector adjustments\n    plt.arrow(p0[0], p0[1], h, y_predictor[i+1, 0] - p0[1], color='r', width=0.005, \n              head_width=0.02, head_length=0.02, length_includes_head=True, zorder=5)\n    plt.arrow(p1_predictor[0], p1_predictor[1], 0, y_corrector[i+1, 0] - y_predictor[i+1, 0], \n              color='g', width=0.005, head_width=0.02, head_length=0.02, length_includes_head=True, zorder=5)\n\n# Add legend entries for predictor and corrector steps\nplt.plot([], [], 'r--', label='Predictor step (Forward Euler)')\nplt.plot([], [], 'g-', label='Corrector step (Trapezoid)')\n\n# Labels and title\nplt.xlabel('Time (s)', fontsize=12)\nplt.ylabel('Height (m)', fontsize=12)\nplt.title(\"Trapezoid Method: Predictor-Corrector Structure\", fontsize=14)\nplt.legend(fontsize=10)\nplt.grid(True, linestyle=':', alpha=0.7)\n\nplt.tight_layout()\n\n","type":"content","url":"/appendix-ivps#trapezoidal-predictor-corrector","position":11},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Collocation Methods"},"type":"lvl2","url":"/appendix-ivps#collocation-methods","position":12},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Collocation Methods"},"content":"The numerical integration methods we discussed earlier are inherently sequential: given an initial state, we step forward in time and approximate what happens over a short interval. The accuracy of this procedure depends on the chosen rule (Euler, trapezoid, Runge–Kutta) and on the information available locally. Each new state is obtained by evaluating a formula that approximates the derivative or integral over that small step.\n\nCollocation methods provide an alternative viewpoint. Instead of advancing one step at a time, they approximate the entire trajectory with a finite set of basis functions and require the dynamics to hold at selected points. This replaces the original differential equation with a system of algebraic equations: relations among the coefficients of the basis functions that must all be satisfied simultaneously. Solving these equations fixes the whole trajectory in one computation.\n\nSeen from this angle, integration rules, spline interpolation, quadrature, and collocation are all instances of the same principle: an infinite-dimensional problem is reduced to finitely many parameters linked by numerical rules. The difference is mainly in scope. Sequential integration advances the state forward one interval at a time, which makes it simple but prone to error accumulation. Collocation belongs to the class of simultaneous methods already introduced for DOCPs: the entire trajectory is represented at once, the dynamics are imposed everywhere in the discretization, and approximation error is spread across the horizon rather than accumulating step by step.\n\nThis global enforcement comes at a computational cost since the resulting algebraic system is larger and denser. However, the benefit is precisely the structural one we saw in simultaneous methods earlier: by exposing the coupling between states, controls, and dynamics explicitly, collocation allows solvers to exploit sparsity and to enforce path constraints directly at the collocation points. This is why collocation is especially effective for challenging continuous-time optimal control problems where robustness and constraint satisfaction are central.","type":"content","url":"/appendix-ivps#collocation-methods","position":13},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Quick Primer on Polynomials"},"type":"lvl2","url":"/appendix-ivps#quick-primer-on-polynomials","position":14},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Quick Primer on Polynomials"},"content":"Collocation methods are based on polynomial approximation theory. Therefore, the first step in developing collocation-based optimal control techniques is to review the fundamentals of polynomial functions.\n\nPolynomials are typically introduced through their standard form:p(t) = a_n t^n + a_{n-1} t^{n-1} + \\cdots + a_1 t + a_0\n\nIn this expression, the a_i are coefficients which linearly combine the powers of t to represent a function. The set of functions \\{ 1, t, t^2, t^3, \\ldots, t^n \\} used in the standard polynomial representation is called the monomial basis.\n\nIn linear algebra, a basis is a set of vectors in a vector space such that any vector in the space can be uniquely represented as a linear combination of these basis vectors. In the same way, a polynomial basis is such that any function  f(x)  (within the function space) to be expressed as:f(x) = \\sum_{k=0}^{\\infty} c_k p_k(x),\n\nwhere the coefficients  c_k  are generally determined by solving a system of equation.\n\nJust as vectors can be represented in different coordinate systems (bases), functions can also be expressed using various polynomial bases. However, the ability to apply a change of basis does not imply that all types of polynomials are equivalent from a practical standpoint. In practice, our choice of polynomial basis is dictated by considerations of efficiency, accuracy, and stability when approximating a function.\n\nFor instance, despite the monomial basis being easy to understand and implement, it often performs poorly in practice due to numerical instability. This instability arises as its coefficients take on large values: an ill-conditioning problem. The following kinds of polynomial often remedy this issues.","type":"content","url":"/appendix-ivps#quick-primer-on-polynomials","position":15},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl3":"Orthogonal Polynomials","lvl2":"Quick Primer on Polynomials"},"type":"lvl3","url":"/appendix-ivps#orthogonal-polynomials","position":16},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl3":"Orthogonal Polynomials","lvl2":"Quick Primer on Polynomials"},"content":"An orthogonal polynomial basis is a set of polynomials that are orthogonal to each other and form a complete basis for a certain space of functions. This means that any function within that space can be represented as a linear combination of these polynomials.\n\nMore precisely, let  \\{ p_0(x), p_1(x), p_2(x), \\dots \\}  be a sequence of polynomials where each  p_n(x)  is a polynomial of degree  n . We say that this set forms an orthogonal polynomial basis if any polynomial  q(x)  of degree  n  or less can be uniquely expressed as a linear combination of  \\{ p_0(x), p_1(x), \\dots, p_n(x) \\} . Furthermore, the orthogonality property means that for any  i \\neq j :\\langle p_i, p_j \\rangle = \\int_a^b p_i(x) p_j(x) w(x) \\, dx = 0.\n\nfor some weight function  w(x)  over a given interval of orthogonality  [a, b] .\n\nThe orthogonality property allows to simplify the computation of the coefficients involved in the polynomial representation of a function. At a high level, what happens is that when taking the inner product of  f(x)  with each basis polynomial,  p_k(x)  isolates the corresponding coefficient  c_k , which can be found to be:c_k = \\frac{\\langle f, p_k \\rangle}{\\langle p_k, p_k \\rangle} = \\frac{\\int_a^b f(x) p_k(x) w(x) \\, dx}{\\int_a^b p_k(x)^2 w(x) \\, dx}.\n\nHere are some examples of the most common orthogonal polynomials used in practice.","type":"content","url":"/appendix-ivps#orthogonal-polynomials","position":17},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl4":"Legendre Polynomials","lvl3":"Orthogonal Polynomials","lvl2":"Quick Primer on Polynomials"},"type":"lvl4","url":"/appendix-ivps#legendre-polynomials","position":18},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl4":"Legendre Polynomials","lvl3":"Orthogonal Polynomials","lvl2":"Quick Primer on Polynomials"},"content":"Legendre polynomials  \\{ P_n(x) \\}  are defined on the interval [-1, 1] and satisfy the orthogonality condition:\\int_{-1}^{1} P_n(x) P_m(x) \\, dx = \n\\begin{cases}\n0 & \\text{if } n \\neq m, \\\\\n\\frac{2}{2n + 1} & \\text{if } n = m.\n\\end{cases}\n\nThey can be generated using the recurrence relation:(n+1) P_{n+1}(x) = (2n + 1) x P_n(x) - n P_{n-1}(x),\n\nwith initial conditions:P_0(x) = 1, \\quad P_1(x) = x.\n\nThe first four Legendre polynomials resulting from this recurrence are the following:\n\n#  label: appendix_ivps-cell-05\n#  caption: Rendered output from the preceding code cell.\n\nimport numpy as np\nfrom IPython.display import display, Math\n\ndef legendre_polynomial(n, x):\n    if n == 0:\n        return np.poly1d([1])\n    elif n == 1:\n        return x\n    else:\n        p0 = np.poly1d([1])\n        p1 = x\n        for k in range(2, n + 1):\n            p2 = ((2 * k - 1) * x * p1 - (k - 1) * p0) / k\n            p0, p1 = p1, p2\n        return p1\n\ndef legendre_coefficients(n):\n    x = np.poly1d([1, 0])  # Define a poly1d object to represent x\n    poly = legendre_polynomial(n, x)\n    return poly\n\ndef poly_to_latex(poly):\n    coeffs = poly.coefficients\n    variable = poly.variable\n    \n    terms = []\n    for i, coeff in enumerate(coeffs):\n        power = len(coeffs) - i - 1\n        if coeff == 0:\n            continue\n        coeff_str = f\"{coeff:.2g}\" if coeff not in {1, -1} or power == 0 else (\"-\" if coeff == -1 else \"\")\n        if power == 0:\n            term = f\"{coeff_str}\"\n        elif power == 1:\n            term = f\"{coeff_str}{variable}\"\n        else:\n            term = f\"{coeff_str}{variable}^{power}\"\n        terms.append(term)\n    \n    latex_poly = \" + \".join(terms).replace(\" + -\", \" - \")\n    return latex_poly\n\nfor n in range(4):\n    poly = legendre_coefficients(n)\n    display(Math(f\"P_{n}(x) = {poly_to_latex(poly)}\"))\n\n","type":"content","url":"/appendix-ivps#legendre-polynomials","position":19},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl4":"Chebyshev Polynomials","lvl3":"Orthogonal Polynomials","lvl2":"Quick Primer on Polynomials"},"type":"lvl4","url":"/appendix-ivps#chebyshev-polynomials","position":20},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl4":"Chebyshev Polynomials","lvl3":"Orthogonal Polynomials","lvl2":"Quick Primer on Polynomials"},"content":"There are two types of Chebyshev polynomials: Chebyshev polynomials of the first kind,  \\{ T_n(x) \\} , and Chebyshev polynomials of the second kind,  \\{ U_n(x) \\} . We typically focus on the first kind. They are defined on the interval [-1, 1] and satisfy the orthogonality condition:\\int_{-1}^{1} \\frac{T_n(x) T_m(x)}{\\sqrt{1 - x^2}} \\, dx = \n\\begin{cases}\n0 & \\text{if } n \\neq m, \\\\\n\\frac{\\pi}{2} & \\text{if } n = m \\neq 0, \\\\\n\\pi & \\text{if } n = m = 0.\n\\end{cases}\n\nThe Chebyshev polynomials of the first kind can be generated using the recurrence relation:T_{n+1}(x) = 2x T_n(x) - T_{n-1}(x),\n\nwith initial conditions:T_0(x) = 1, \\quad T_1(x) = x.\n\nThis recurrence relation also admits an explicit formula:T_n(x) = \\cos(n \\cos^{-1}(x)).\n\nLet’s now implement it in Python:\n\n#  label: appendix_ivps-cell-06\n#  caption: Rendered output from the preceding code cell.\n\ndef chebyshev_polynomial(n, x):\n    if n == 0:\n        return np.poly1d([1])\n    elif n == 1:\n        return x\n    else:\n        t0 = np.poly1d([1])\n        t1 = x\n        for _ in range(2, n + 1):\n            t2 = 2 * x * t1 - t0\n            t0, t1 = t1, t2\n        return t1\n\ndef chebyshev_coefficients(n):\n    x = np.poly1d([1, 0])  # Define a poly1d object to represent x\n    poly = chebyshev_polynomial(n, x)\n    return poly\n\nfor n in range(4):\n    poly = chebyshev_coefficients(n)\n    display(Math(f\"T_{n}(x) = {poly_to_latex(poly)}\"))\n\n","type":"content","url":"/appendix-ivps#chebyshev-polynomials","position":21},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl4":"Hermite Polynomials","lvl3":"Orthogonal Polynomials","lvl2":"Quick Primer on Polynomials"},"type":"lvl4","url":"/appendix-ivps#hermite-polynomials","position":22},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl4":"Hermite Polynomials","lvl3":"Orthogonal Polynomials","lvl2":"Quick Primer on Polynomials"},"content":"Hermite polynomials  \\{ H_n(x) \\}  are defined on the entire real line and are orthogonal with respect to the weight function  w(x) = e^{-x^2} . They satisfy the orthogonality condition:\\int_{-\\infty}^{\\infty} H_n(x) H_m(x) e^{-x^2} \\, dx = \n\\begin{cases}\n0 & \\text{if } n \\neq m, \\\\\n2^n n! \\sqrt{\\pi} & \\text{if } n = m.\n\\end{cases}\n\nHermite polynomials can be generated using the recurrence relation:H_{n+1}(x) = 2x H_n(x) - 2n H_{n-1}(x),\n\nwith initial conditions:H_0(x) = 1, \\quad H_1(x) = 2x.\n\nThe following code computes the coefficients of the first four Hermite polynomials:\n\n#  label: appendix_ivps-cell-07\n#  caption: Rendered output from the preceding code cell.\n\ndef hermite_polynomial(n, x):\n    if n == 0:\n        return np.poly1d([1])\n    elif n == 1:\n        return 2 * x\n    else:\n        h0 = np.poly1d([1])\n        h1 = 2 * x\n        for k in range(2, n + 1):\n            h2 = 2 * x * h1 - 2 * (k - 1) * h0\n            h0, h1 = h1, h2\n        return h1\n\ndef hermite_coefficients(n):\n    x = np.poly1d([1, 0])  # Define a poly1d object to represent x\n    poly = hermite_polynomial(n, x)\n    return poly\n\nfor n in range(4):\n    poly = hermite_coefficients(n)\n    display(Math(f\"H_{n}(x) = {poly_to_latex(poly)}\"))\n\n","type":"content","url":"/appendix-ivps#hermite-polynomials","position":23},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Collocation Conditions"},"type":"lvl2","url":"/appendix-ivps#collocation-conditions","position":24},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Collocation Conditions"},"content":"Consider a general ODE of the form:\\dot{y}(t) = f(y(t), t), \\quad y(t_0) = y_0,\n\nwhere  y(t) \\in \\mathbb{R}^n  is the state vector, and  f: \\mathbb{R}^n \\times \\mathbb{R} \\rightarrow \\mathbb{R}^n  is a known function. The goal is to approximate the solution  y(t)  over a given interval [t_0, t_f]. Collocation methods achieve this by:\n\nChoosing a basis to approximate  y(t)  using a finite sum of basis functions  \\phi_i(t) :y(t) \\approx \\sum_{i=0}^{N} c_i \\phi_i(t),\n\nwhere  \\{c_i\\}  are the coefficients to be determined.\n\nSelecting collocation points  t_1, t_2, \\ldots, t_N  within the interval [t_0, t_f]. These are typically chosen to be the roots of certain orthogonal polynomials, like Legendre or Chebyshev polynomials, or can be spread equally across the interval.\n\nEnforcing the ODE at the collocation points for each  t_j :\\dot{y}(t_j) = f(y(t_j), t_j).\n\nTo implement this, we differentiate the approximate solution  y(t)  with respect to time:\\dot{y}(t) \\approx \\sum_{i=0}^{N} c_i \\dot{\\phi}_i(t).\n\nSubstituting this into the ODE at the collocation points gives:\\sum_{i=0}^{N} c_i \\dot{\\phi}_i(t_j) = f\\left(\\sum_{i=0}^{N} c_i \\phi_i(t_j), t_j\\right), \\quad j = 1, \\ldots, N.\n\nThe collocation equations are formed by enforcing the ODE at all collocation points, leading to a system of nonlinear equations:\\sum_{i=0}^{N} c_i \\dot{\\phi}_i(t_j) - f\\left(\\sum_{i=0}^{N} c_i \\phi_i(t_j), t_j\\right) = 0, \\quad j = 1, \\ldots, N.\n\nFurthermore when solving an initial value problem (IVP),  we also need to incorporate the initial condition  y(t_0) = y_0  as an additional constraint:\\sum_{i=0}^{N} c_i \\phi_i(t_0) = y_0.\n\nThe collocation conditions and IVP condition are combined together to form a root-finding problem, which we can generically solve numerically using Newton’s method.","type":"content","url":"/appendix-ivps#collocation-conditions","position":25},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Common Numerical Integration Techniques as Collocation Methods"},"type":"lvl2","url":"/appendix-ivps#common-numerical-integration-techniques-as-collocation-methods","position":26},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Common Numerical Integration Techniques as Collocation Methods"},"content":"Many common numerical integration techniques can be viewed as special cases of collocation methods.\nWhile the general collocation method we discussed earlier applies to the entire interval [t_0, t_f], many numerical integration techniques can be viewed as collocation methods applied locally, step by step.\n\nIn practical numerical integration, we often divide the full interval [t_0, t_f] into smaller subintervals or steps. In general, this allows us to user simpler basis functions thereby reducing computational complexity, and gives us more flexibility in dynamically ajusting the step size using local error estimates. When we apply collocation locally, we’re essentially using the collocation method to “step” from t_n to t_{n+1}. As we did, earlier we still apply the following three steps:\n\nWe choose a basis function to approximate y(t) over [t_n, t_{n+1}].\n\nWe select collocation points within this interval.\n\nWe enforce the ODE at these points to determine the coefficients of our basis function.\n\nWe can make this idea clearer by re-deriving some of the numerical integration methods seen before using this perspective.","type":"content","url":"/appendix-ivps#common-numerical-integration-techniques-as-collocation-methods","position":27},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl3":"Explicit Euler Method","lvl2":"Common Numerical Integration Techniques as Collocation Methods"},"type":"lvl3","url":"/appendix-ivps#explicit-euler-method","position":28},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl3":"Explicit Euler Method","lvl2":"Common Numerical Integration Techniques as Collocation Methods"},"content":"For the Explicit Euler method, we use a linear basis function for each step:\\phi(t) = 1 + c(t - t_n)\n\nNote that we use (t - t_n) rather than just t because we’re approximating the solution locally, relative to the start of each step. We then choose one collocation point at t_{n+1} where we have:y'(t_{n+1}) = c = f(y_n, t_n)\n\nOur local approximation is:y(t) \\approx y_n + c(t - t_n)\n\nAt t = t_{n+1}, this gives:y_{n+1} = y_n + c(t_{n+1} - t_n) = y_n + hf(y_n, t_n)\n\nwhere h = t_{n+1} - t_n. This is the classic Euler update formula.","type":"content","url":"/appendix-ivps#explicit-euler-method","position":29},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl3":"Implicit Euler Method","lvl2":"Common Numerical Integration Techniques as Collocation Methods"},"type":"lvl3","url":"/appendix-ivps#implicit-euler-method","position":30},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl3":"Implicit Euler Method","lvl2":"Common Numerical Integration Techniques as Collocation Methods"},"content":"The Implicit Euler method uses the same linear basis function:\\phi(t) = 1 + c(t - t_n)\n\nAgain, we choose one collocation point at t_{n+1}. The main difference is that we enforce the ODE using y_{n+1}:y'(t_{n+1}) = c = f(y_{n+1}, t_{n+1})\n\nOur approximation remains:y(t) \\approx y_n + c(t - t_n)\n\nAt t = t_{n+1}, this leads to the implicit equation:y_{n+1} = y_n + hf(y_{n+1}, t_{n+1})","type":"content","url":"/appendix-ivps#implicit-euler-method","position":31},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl3":"Trapezoidal Method","lvl2":"Common Numerical Integration Techniques as Collocation Methods"},"type":"lvl3","url":"/appendix-ivps#trapezoidal-method","position":32},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl3":"Trapezoidal Method","lvl2":"Common Numerical Integration Techniques as Collocation Methods"},"content":"The Trapezoidal method uses a quadratic basis function:\\phi(t) = 1 + c(t - t_n) + a(t - t_n)^2\n\nWe use two collocation points: t_n and t_{n+1}. Enforcing the ODE at these points gives:\n\nAt t_n:y'(t_n) = c = f(y_n, t_n)\n\nAt t_{n+1}:y'(t_{n+1}) = c + 2ah = f(y_n + ch + ah^2, t_{n+1})\n\nOur approximation is:y(t) \\approx y_n + c(t - t_n) + a(t - t_n)^2\n\nAt t = t_{n+1}, this gives:y_{n+1} = y_n + ch + ah^2\n\nSolving the system of equations leads to the trapezoidal update:y_{n+1} = y_n + \\frac{h}{2}[f(y_n, t_n) + f(y_{n+1}, t_{n+1})]","type":"content","url":"/appendix-ivps#trapezoidal-method","position":33},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl3":"Runge-Kutta Methods","lvl2":"Common Numerical Integration Techniques as Collocation Methods"},"type":"lvl3","url":"/appendix-ivps#runge-kutta-methods","position":34},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl3":"Runge-Kutta Methods","lvl2":"Common Numerical Integration Techniques as Collocation Methods"},"content":"Higher-order Runge-Kutta methods can also be interpreted as collocation methods. The RK4 method corresponds to a collocation method using a cubic polynomial basis:\\phi(t) = 1 + c_1(t - t_n) + c_2(t - t_n)^2 + c_3(t - t_n)^3\n\nHere, we’re using a cubic polynomial to approximate the solution over each step, rather than the linear or quadratic approximations of the other methods above. For RK4, we use four collocation points:\n\nt_n (the start of the step)\n\nt_n + h/2\n\nt_n + h/2\n\nt_n + h (the end of the step)\n\nThese points are called the “Gauss-Lobatto” points, scaled to our interval [t_n, t_n + h].\nThe RK4 method enforces the ODE at these collocation points, leading to four stages:\\begin{aligned}\nk_1 &= hf(y_n, t_n) \\\\\nk_2 &= hf(y_n + \\frac{1}{2}k_1, t_n + \\frac{h}{2}) \\\\\nk_3 &= hf(y_n + \\frac{1}{2}k_2, t_n + \\frac{h}{2}) \\\\\nk_4 &= hf(y_n + k_3, t_n + h)\n\\end{aligned}\n\nThe final update formula for RK4 can be derived by solving the system of equations resulting from enforcing the ODE at our collocation points:y_{n+1} = y_n + \\frac{1}{6}(k_1 + 2k_2 + 2k_3 + k_4)","type":"content","url":"/appendix-ivps#runge-kutta-methods","position":35},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Example: Solving a Simple ODE by Collocation"},"type":"lvl2","url":"/appendix-ivps#example-solving-a-simple-ode-by-collocation","position":36},{"hierarchy":{"lvl1":"Solving Initial Value Problems","lvl2":"Example: Solving a Simple ODE by Collocation"},"content":"Consider a simple ODE:\\frac{dy}{dt} = -y, \\quad y(0) = 1, \\quad t \\in [0, 2]\n\nThe analytical solution is y(t) = e^{-t}. We apply the collocation method with a monomial basis of order N:\\phi_i(t) = t^i, \\quad i = 0, 1, \\ldots, N\n\nWe select N equally spaced points \\{t_1, \\ldots, t_N\\} in [0, 2] as collocation points.\n\n#  label: appendix_ivps-cell-08\n#  caption: Rendered output from the preceding code cell.\n\n\nimport numpy as np\nfrom scipy.optimize import root\nimport matplotlib.pyplot as plt\n\n# Apply book style\ntry:\n    import scienceplots\n    plt.style.use(['science', 'notebook'])\nexcept (ImportError, OSError):\n    pass  # Use matplotlib defaults\n\ndef ode_function(y, t):\n    \"\"\"Define the ODE: dy/dt = -y\"\"\"\n    return -y\n\ndef solve_ode_collocation(ode_func, t_span, y0, order):\n    t0, tf = t_span\n    n_points = order + 1  # number of collocation points\n    t_points = np.linspace(t0, tf, n_points)\n    \n    def collocation_residuals(coeffs):\n        residuals = []\n        # Initial condition residual\n        y_init = sum(c * t_points[0]**i for i, c in enumerate(coeffs))\n        residuals.append(y_init - y0)\n        # Collocation point residuals\n        for t in t_points[1:]:  # Skip the first point as it's used for initial condition\n            y = sum(c * t**i for i, c in enumerate(coeffs))\n            dy_dt = sum(c * i * t**(i-1) for i, c in enumerate(coeffs) if i > 0)\n            residuals.append(dy_dt - ode_func(y, t))\n        return residuals\n\n    # Initial guess for coefficients\n    initial_coeffs = [y0] + [0] * order\n\n    # Solve the system of equations with more robust settings\n    solution = root(collocation_residuals, initial_coeffs, \n                   method='hybr', options={'maxfev': 10000, 'xtol': 1e-8})\n    \n    if not solution.success:\n        # Try with a different method\n        solution = root(collocation_residuals, initial_coeffs, \n                       method='lm', options={'maxiter': 5000})\n        \n    if not solution.success:\n        print(f\"Warning: Collocation solver did not fully converge for order {order}\")\n        # Continue anyway with the best solution found\n\n    coeffs = solution.x\n\n    # Generate solution\n    t_fine = np.linspace(t0, tf, 100)\n    y_solution = sum(c * t_fine**i for i, c in enumerate(coeffs))\n\n    return t_fine, y_solution, t_points, coeffs\n\n# Example usage\nt_span = (0, 2)\ny0 = 1\norders = [1, 2, 3, 4, 5]  # Different polynomial orders to try\n\nplt.figure(figsize=(12, 8))\n\nfor order in orders:\n    t, y, t_collocation, coeffs = solve_ode_collocation(ode_function, t_span, y0, order)\n    \n    # Calculate y values at collocation points\n    y_collocation = sum(c * t_collocation**i for i, c in enumerate(coeffs))\n    \n    # Plot the results\n    plt.plot(t, y, label=f'Order {order}')\n    plt.scatter(t_collocation, y_collocation, s=50, zorder=5)\n\n# Plot the analytical solution\nt_analytical = np.linspace(t_span[0], t_span[1], 100)\ny_analytical = y0 * np.exp(-t_analytical)\nplt.plot(t_analytical, y_analytical, 'k--', label='Analytical')\n\nplt.xlabel('t')\nplt.ylabel('y')\nplt.title('ODE Solutions: dy/dt = -y, y(0) = 1')\nplt.legend()\nplt.grid(True)\n\n# Print error for each order\nprint(\"Maximum absolute errors:\")\nfor order in orders:\n    t, y, _, _ = solve_ode_collocation(ode_function, t_span, y0, order)\n    y_true = y0 * np.exp(-t)\n    max_error = np.max(np.abs(y - y_true))\n    print(f\"Order {order}: {max_error:.6f}\")","type":"content","url":"/appendix-ivps#example-solving-a-simple-ode-by-collocation","position":37},{"hierarchy":{"lvl1":"Nonlinear Programming"},"type":"lvl1","url":"/appendix-nlp","position":0},{"hierarchy":{"lvl1":"Nonlinear Programming"},"content":"Unless specific assumptions are made on the dynamics and cost structure, a DOCP is, in its most general form, a nonlinear mathematical program (commonly referred to as an NLP, not to be confused with Natural Language Processing). An NLP can be formulated as follows:\\begin{aligned}\n\\text{minimize } & f(\\mathbf{x}) \\\\\n\\text{subject to } & \\mathbf{g}(\\mathbf{x}) \\leq \\mathbf{0} \\\\\n& \\mathbf{h}(\\mathbf{x}) = \\mathbf{0}\n\\end{aligned}\n\nWhere:\n\nf: \\mathbb{R}^n \\to \\mathbb{R} is the objective function\n\n\\mathbf{g}: \\mathbb{R}^n \\to \\mathbb{R}^m represents inequality constraints\n\n\\mathbf{h}: \\mathbb{R}^n \\to \\mathbb{R}^\\ell represents equality constraints\n\nUnlike unconstrained optimization commonly used in deep learning, the optimality of a solution in constrained optimization must consider both the objective value and constraint feasibility. To illustrate this, consider the following problem, which includes both equality and inequality constraints:\\begin{align*}\n\\text{Minimize} \\quad & f(x_1, x_2) = (x_1 - 1)^2 + (x_2 - 2.5)^2 \\\\\n\\text{subject to} \\quad & g(x_1, x_2) = (x_1 - 1)^2 + (x_2 - 1)^2 \\leq 1.5, \\\\\n& h(x_1, x_2) = x_2 - \\left(0.5 \\sin(2 \\pi x_1) + 1.5\\right) = 0.\n\\end{align*}\n\nIn this example, the objective function f(x_1, x_2) is quadratic, the inequality constraint g(x_1, x_2) defines a circular feasible region centered at (1, 1) with a radius of \\sqrt{1.5} and the equality constraint h(x_1, x_2) requires x_2 to lie on a sine wave function. The following code demonstrates the difference between the unconstrained, and constrained solutions to this problem.\n\n#  label: appendix_nlp-cell-01\n#  caption: Rendered output from the preceding code cell.\n\n%config InlineBackend.figure_format = 'retina'\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Apply book style\ntry:\n    import scienceplots\n    plt.style.use(['science', 'notebook'])\nexcept (ImportError, OSError):\n    pass  # Use matplotlib defaults\nfrom scipy.optimize import minimize\n\n# Define the objective function\ndef objective(x):\n    return (x[0] - 1)**2 + (x[1] - 2.5)**2\n\n# Define the inequality constraint function\ndef constraint(x):\n    return -(x[0] - 1)**2 - (x[1] - 1)**2 + 1.5\n\n# Define the gradient of the objective function\ndef objective_gradient(x):\n    return np.array([2*(x[0] - 1), 2*(x[1] - 2.5)])\n\n# Define the gradient of the inequality constraint function\ndef constraint_gradient(x):\n    return np.array([-2*(x[0] - 1), -2*(x[1] - 1)])\n\n# Define the sine wave equality constraint function\ndef sine_wave_equality_constraint(x):\n    return x[1] - (0.5 * np.sin(2 * np.pi * x[0]) + 1.5)\n\n# Define the gradient of the sine wave equality constraint function\ndef sine_wave_equality_constraint_gradient(x):\n    return np.array([-np.pi * np.cos(2 * np.pi * x[0]), 1])\n\n# Define the constraints including the sine wave equality constraint\nsine_wave_constraints = [{'type': 'ineq', 'fun': constraint, 'jac': constraint_gradient},  # Inequality constraint\n                         {'type': 'eq', 'fun': sine_wave_equality_constraint, 'jac': sine_wave_equality_constraint_gradient}]  # Sine wave equality constraint\n\n# Define only the inequality constraint\ninequality_constraints = [{'type': 'ineq', 'fun': constraint, 'jac': constraint_gradient}]\n\n# Initial guess\nx0 = [1.25, 1.5]\n\n# Solve the optimization problem with the sine wave equality constraint\nres_sine_wave_constraint = minimize(objective, x0, method='SLSQP', jac=objective_gradient, \n                                    constraints=sine_wave_constraints, options={'disp': False})\n\nx_opt_sine_wave_constraint = res_sine_wave_constraint.x\n\n# Solve the optimization problem with only the inequality constraint\nres_inequality_only = minimize(objective, x0, method='SLSQP', jac=objective_gradient, \n                               constraints=inequality_constraints, options={'disp': False})\n\nx_opt_inequality_only = res_inequality_only.x\n\n# Solve the unconstrained optimization problem for reference\nres_unconstrained = minimize(objective, x0, method='SLSQP', jac=objective_gradient, options={'disp': False})\nx_opt_unconstrained = res_unconstrained.x\n\n# Generate data for visualization\nx = np.linspace(-1, 4, 400)\ny = np.linspace(-1, 4, 400)\nX, Y = np.meshgrid(x, y)\nZ = (X - 1)**2 + (Y - 2.5)**2  # Objective function values\nconstraint_values = (X - 1)**2 + (Y - 1)**2\n\n# Data for sine wave constraint\nx_sine = np.linspace(-1, 4, 400)\ny_sine = 0.5 * np.sin(2 * np.pi * x_sine) + 1.5\n\n# Visualization with Improved Color Scheme\nplt.figure(figsize=(8, 6))\nplt.contourf(X, Y, Z, levels=100, cmap='viridis', alpha=0.6)  # Heatmap for the objective function\n\n# Plot all the optimal points\nplt.plot(x_opt_inequality_only[0], x_opt_inequality_only[1], 'ro', label='Optimal Solution (Inequality Only)', markersize=8, markeredgecolor='black')\nplt.plot(x_opt_sine_wave_constraint[0], x_opt_sine_wave_constraint[1], 'mo', label='Optimal Solution (Sine Wave Equality & Inequality)', markersize=8, markeredgecolor='black')\nplt.plot(x_opt_unconstrained[0], x_opt_unconstrained[1], 'co', label='Unconstrained Minimum', markersize=8, markeredgecolor='black')\n\n# Adjust constraint boundary colors\nplt.contour(X, Y, constraint_values, levels=[1.5], colors='navy', linewidths=2, linestyles='dashed')\nplt.contourf(X, Y, constraint_values, levels=[0, 1.5], colors='skyblue', alpha=0.3)\n\n# Plot the sine wave equality constraint with a high contrast color\nplt.plot(x_sine, y_sine, 'lime', linestyle='--', linewidth=2, label='Sine Wave Equality Constraint')\n\nplt.xlim([-1, 4])\nplt.ylim([-1, 4])\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.title('Example NLP')\nplt.legend(loc='upper left', fontsize='small', edgecolor='black', fancybox=True)\nplt.grid(True)\n# Set the aspect ratio to be equal so the circle appears correctly\nplt.gca().set_aspect('equal', adjustable='box')\n\n","type":"content","url":"/appendix-nlp","position":1},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl3":"Karush-Kuhn-Tucker (KKT) conditions"},"type":"lvl3","url":"/appendix-nlp#karush-kuhn-tucker-kkt-conditions","position":2},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl3":"Karush-Kuhn-Tucker (KKT) conditions"},"content":"While this example is simple enough to convince ourselves visually of the solution to this particular problem, it falls short of providing us with actionable chracterization of what constitutes and optimal solution in general.\nThe Karush-Kuhn-Tucker (KKT) conditions provide us with an answer to this problem by generalizing the first-order optimality conditions in unconstrained optimization to problems involving both equality and inequality constraints.\nThis result relies on the construction of an auxiliary function called the Lagrangian, defined as:\\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\mu}, \\boldsymbol{\\lambda})=f(\\mathbf{x})+\\boldsymbol{\\mu}^{\\top} \\mathbf{g}(\\mathbf{x})+\\boldsymbol{\\lambda}^{\\top} \\mathbf{h}(\\mathbf{x})\n\nwhere \\boldsymbol{\\mu} \\in \\mathbb{R}^m and \\boldsymbol{\\lambda} \\in \\mathbb{R}^\\ell are known as Lagrange multipliers. The first-order optimality conditions then state that if \\mathbf{x}^*, then there must exist corresponding Lagrange multipliers \\boldsymbol{\\mu}^* and \\boldsymbol{\\lambda}^* such that:\n\nThe gradient of the Lagrangian with respect to \\mathbf{x} must be zero at the optimal point (stationarity):\\nabla_x \\mathcal{L}(\\mathbf{x}^*, \\boldsymbol{\\mu}^*, \\boldsymbol{\\lambda}^*) = \\nabla f(\\mathbf{x}^*) + \\sum_{i=1}^m \\mu_i^* \\nabla g_i(\\mathbf{x}^*) + \\sum_{j=1}^\\ell \\lambda_j^* \\nabla h_j(\\mathbf{x}^*) = \\mathbf{0}\n\nIn the case where we only have equality constraints, this means that the gradient of the objective and that of constraint are parallel to each other at the optimum but point in opposite directions.\n\nA valid solution of a NLP is one which satisfies all the constraints (primal feasibility)\\begin{aligned}\n   \\mathbf{g}(\\mathbf{x}^*) &\\leq \\mathbf{0}, \\enspace \\text{and} \\enspace \\mathbf{h}(\\mathbf{x}^*) &= \\mathbf{0}\n   \\end{aligned}\n\nFurthermore, the Lagrange multipliers for inequality constraints must be non-negative (dual feasibility)\\boldsymbol{\\mu}^* \\geq \\mathbf{0}\n\nThis condition stems from the fact that the inequality constraints can only push the solution in one direction.\n\nFinally, for each inequality constraint, either the constraint is active (equality holds) or its corresponding Lagrange multiplier is zero at an optimal solution (complementary slackness)\\mu_i^* g_i(\\mathbf{x}^*) = 0, \\quad \\forall i = 1,\\ldots,m\n\nLet’s now solve our example problem above, this time using \n\nIpopt via the \n\nPyomo interface so that we can access the Lagrange multipliers found by the solver.\n\n#  label: appendix_nlp-cell-02\n#  caption: Rendered output from the preceding code cell.\n\nfrom pyomo.environ import *\nfrom pyomo.opt import SolverFactory\nimport math\n\n# Define the Pyomo model\nmodel = ConcreteModel()\n\n# Define the variables\nmodel.x1 = Var(initialize=1.25)\nmodel.x2 = Var(initialize=1.5)\n\n# Define the objective function\ndef objective_rule(model):\n    return (model.x1 - 1)**2 + (model.x2 - 2.5)**2\nmodel.obj = Objective(rule=objective_rule, sense=minimize)\n\n# Define the inequality constraint (circle)\ndef inequality_constraint_rule(model):\n    return (model.x1 - 1)**2 + (model.x2 - 1)**2 <= 1.5\nmodel.ineq_constraint = Constraint(rule=inequality_constraint_rule)\n\n# Define the equality constraint (sine wave) using Pyomo's math functions\ndef equality_constraint_rule(model):\n    return model.x2 == 0.5 * sin(2 * math.pi * model.x1) + 1.5\nmodel.eq_constraint = Constraint(rule=equality_constraint_rule)\n\n# Create a suffix component to capture dual values\nmodel.dual = Suffix(direction=Suffix.IMPORT)\n\n# Create a solver\nsolver=SolverFactory('ipopt')\n\n# Solve the problem\nresults = solver.solve(model, tee=False)\n\n# Check if the solver found an optimal solution\nif (results.solver.status == SolverStatus.ok and \n    results.solver.termination_condition == TerminationCondition.optimal):\n    \n    # Print the results\n    print(f\"x1: {value(model.x1)}\")\n    print(f\"x2: {value(model.x2)}\")\n    \n    # Print the objective value\n    print(f\"Objective value: {value(model.obj)}\")\n\n    # Print the Lagrange multipliers (dual values)\n    print(\"\\nLagrange multipliers:\")\n    ineq_lambda = None\n    eq_lambda = None\n    for c in model.component_objects(Constraint, active=True):\n        for index in c:\n            dual_val = model.dual[c[index]]\n            print(f\"{c.name}[{index}]: {dual_val}\")\n            if c.name == \"ineq_constraint\":\n                ineq_lambda = dual_val\n            elif c.name == \"eq_constraint\":\n                eq_lambda = dual_val\nelse:\n    print(\"Solver did not find an optimal solution.\")\n    print(f\"Solver Status: {results.solver.status}\")\n    print(f\"Termination Condition: {results.solver.termination_condition}\")\n\nAfter running the code above, we can observe the Lagrange multipliers. The Lagrange multiplier associated with the inequality constraint is very small (close to zero), suggesting that the inequality constraint is not active at the optimal solution—meaning that the solution point lies inside the circle defined by this constraint. This can be verified visually in the figure above. As for the equality constraint, its corresponding Lagrange multiplier is non-zero, indicating that this constraint is active at the optimal solution. In general, when we find a Lagrange multiplier close to zero (like the one for the inequality constraint), it means that constraint is not “binding”—the optimal solution does not lie on the boundary defined by this constraint. In contrast, a non-zero Lagrange multiplier, such as the one for the equality constraint, indicates that the constraint is active and that any relaxation would directly affect the objective function’s value, as required by the stationarity condition.","type":"content","url":"/appendix-nlp#karush-kuhn-tucker-kkt-conditions","position":3},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl3":"Lagrange Multiplier Theorem"},"type":"lvl3","url":"/appendix-nlp#lagrange-multiplier-theorem","position":4},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl3":"Lagrange Multiplier Theorem"},"content":"The KKT conditions introduced above characterize the solution structure of constrained optimization problems with equality constraints. In this particular context, these conditions are referred to as the first-order optimality conditions, as part of the Lagrange multiplier theorem. Let’s just re-state them in that simpler setting:\n\nLagrange Multiplier Theorem\n\nConsider the constrained optimization problem:\\begin{aligned}\n\\min_{\\mathbf{x}} \\quad & f(\\mathbf{x}) \\\\\n\\text{subject to} \\quad & h_i(\\mathbf{x}) = 0, \\quad i = 1, \\ldots, m\n\\end{aligned}\n\nwhere \\mathbf{x} \\in \\mathbb{R}^n, f: \\mathbb{R}^n \\to \\mathbb{R}, and h_i: \\mathbb{R}^n \\to \\mathbb{R} for i = 1, \\ldots, m.\n\nAssume that:\n\nf and h_i are continuously differentiable functions.\n\nThe gradients \\nabla h_i(\\mathbf{x}^*) are linearly independent at the optimal point \\mathbf{x}^*.\n\nThen, there exist unique Lagrange multipliers \\lambda_i^* \\in \\mathbb{R}, i = 1, \\ldots, m, such that the following first-order optimality conditions hold:\n\nStationarity: \\nabla f(\\mathbf{x}^*) + \\sum_{i=1}^m \\lambda_i^* \\nabla h_i(\\mathbf{x}^*) = \\mathbf{0}\n\nPrimal feasibility: h_i(\\mathbf{x}^*) = 0, for i = 1, \\ldots, m\n\nNote that both the stationarity and primal feasibility statements are simply saying that the derivative of the Lagrangian in either the primal or dual variables must be zero at an optimal constrained solution. In other words:\\nabla_{\\mathbf{x}, \\boldsymbol{\\lambda}} L(\\mathbf{x}^*, \\boldsymbol{\\lambda}^*) = \\mathbf{0}\n\nLetting \\mathbf{F}(\\mathbf{x}, \\boldsymbol{\\lambda}) stand for \\nabla_{\\mathbf{x}, \\boldsymbol{\\lambda}} L(\\mathbf{x}, \\boldsymbol{\\lambda}), the Lagrange multipliers theorem tells us that an optimal primal-dual pair is actually a zero of that function \\mathbf{F}: the derivative of the Lagrangian. Therefore, we can use this observation to craft a solution method for solving equality constrained optimization using Newton’s method, which is a numerical procedure for finding zeros of a nonlinear function.","type":"content","url":"/appendix-nlp#lagrange-multiplier-theorem","position":5},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl3":"Newton’s Method"},"type":"lvl3","url":"/appendix-nlp#newtons-method","position":6},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl3":"Newton’s Method"},"content":"Newton’s method is a numerical procedure for solving root-finding problems. These are nonlinear systems of equations of the form:\n\nFind \\mathbf{z}^* \\in \\mathbb{R}^n such that \\mathbf{F}(\\mathbf{z}^*) = \\mathbf{0}\n\nwhere \\mathbf{F}: \\mathbb{R}^n \\to \\mathbb{R}^n is a continuously differentiable function. Newton’s method then consists in applying the following sequence of iterates:\\mathbf{z}^{k+1} = \\mathbf{z}^k - [\\nabla \\mathbf{F}(\\mathbf{z}^k)]^{-1} \\mathbf{F}(\\mathbf{z}^k)\n\nwhere \\mathbf{z}^k is the k-th iterate, and \\nabla \\mathbf{F}(\\mathbf{z}^k) is the Jacobian matrix of \\mathbf{F} evaluated at \\mathbf{z}^k.\n\nNewton’s method exhibits local quadratic convergence: if the initial guess \\mathbf{z}^0 is sufficiently close to the true solution \\mathbf{z}^*, and \\nabla \\mathbf{F}(\\mathbf{z}^*) is nonsingular, the method converges quadratically to \\mathbf{z}^* \n\nOrtega & Rheinboldt (1970). However, the method is sensitive to the initial guess; if it’s too far from the desired solution, Newton’s method might fail to converge or converge to a different root. To mitigate this problem, a set of techniques known as numerical continuation methods \n\nAllgower & Georg (1990) have been developed. These methods effectively enlarge the basin of attraction of Newton’s method by solving a sequence of related problems, progressing from an easy one to the target problem. This approach is reminiscent of several concepts in machine learning and statistical inference: curriculum learning in machine learning, where models are trained on increasingly complex data; tempering in Markov Chain Monte Carlo (MCMC) samplers, which gradually adjusts the target distribution to improve mixing; and modern diffusion models, which use a similar concept of gradually transforming noise into structured data.","type":"content","url":"/appendix-nlp#newtons-method","position":7},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl4":"Efficient Implementation of Newton’s Method","lvl3":"Newton’s Method"},"type":"lvl4","url":"/appendix-nlp#efficient-implementation-of-newtons-method","position":8},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl4":"Efficient Implementation of Newton’s Method","lvl3":"Newton’s Method"},"content":"Note that each step of Newton’s method involves computing the inverse of a Jacobian matrix. However, a cardinal rule in numerical linear algebra is to avoid computing matrix inverses explicitly: rarely, if ever, should there be a np.lindex.inv in your code. Instead, the numerically stable and computationally efficient approach is to solve a linear system of equations at each step.\nGiven the Newton’s method iterate:\\mathbf{z}^{k+1} = \\mathbf{z}^k - [\\nabla \\mathbf{F}(\\mathbf{z}^k)]^{-1} \\mathbf{F}(\\mathbf{z}^k)\n\nWe can reformulate this as a two-step procedure:\n\nSolve the linear system: \\underbrace{[\\nabla \\mathbf{F}(\\mathbf{z}^k)]}_{\\mathbf{A}} \\Delta \\mathbf{z}^k = -\\mathbf{F}(\\mathbf{z}^k)\n\nUpdate: \\mathbf{z}^{k+1} = \\mathbf{z}^k + \\Delta \\mathbf{z}^k\n\nThe structure of the linear system in step 1 often allows for specialized solution methods. In the context of automatic differentiation, matrix-free linear solvers are particularly useful. These solvers can find a solution without explicitly forming the matrix A, requiring only the ability to evaluate matrix-vector or vector-matrix products. Typical examples of such methods include classical matrix-splitting methods (e.g., Richardson iteration) or conjugate gradient methods through \n\nsparse.linalg.cg for example. Another useful method is the Generalized Minimal Residual method (GMRES) implemented in SciPy via \n\nsparse.linalg.gmres, which is useful when facing non-symmetric and indefinite systems.\n\nBy inspecting the structure of matrix \\mathbf{A} in the specific application where the function \\mathbf{F} is the derivative of the Lagrangian, we will also uncover an important structure known as the KKT matrix. This structure will then allow us to derive a Quadratic Programming (QP) sub-problem as part of a larger iterative procedure for solving equality and inequality constrained problems via Sequential Quadratic Programming (SQP).","type":"content","url":"/appendix-nlp#efficient-implementation-of-newtons-method","position":9},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl3":"Solving Equality Constrained Programs with Newton’s Method"},"type":"lvl3","url":"/appendix-nlp#solving-equality-constrained-programs-with-newtons-method","position":10},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl3":"Solving Equality Constrained Programs with Newton’s Method"},"content":"To solve equality-constrained optimization problems using Newton’s method, we begin by recognizing that the problem reduces to finding a zero of the function \\mathbf{F}(\\mathbf{z}) = \\nabla_{\\mathbf{x}, \\boldsymbol{\\lambda}} L(\\mathbf{x}, \\boldsymbol{\\lambda}). Here, \\mathbf{F} represents the derivative of the Lagrangian function, and \\mathbf{z} = (\\mathbf{x}, \\boldsymbol{\\lambda}) combines both the primal variables \\mathbf{x} and the dual variables (Lagrange multipliers) \\boldsymbol{\\lambda}. Explicitly, we have:\\mathbf{F}(\\mathbf{z}) = \\begin{bmatrix} \\nabla_{\\mathbf{x}} L(\\mathbf{x}, \\boldsymbol{\\lambda}) \\\\ \\mathbf{h}(\\mathbf{x}) \\end{bmatrix} = \\begin{bmatrix} \\nabla f(\\mathbf{x}) + \\sum_{i=1}^m \\lambda_i \\nabla h_i(\\mathbf{x}) \\\\ \\mathbf{h}(\\mathbf{x}) \\end{bmatrix}.\n\nNewton’s method involves linearizing \\mathbf{F}(\\mathbf{z}) around the current iterate \\mathbf{z}^k = (\\mathbf{x}^k, \\boldsymbol{\\lambda}^k) and then solving the resulting linear system. At each iteration k, Newton’s method updates the current estimate by solving the linear system:\\mathbf{z}^{k+1} = \\mathbf{z}^k - [\\nabla \\mathbf{F}(\\mathbf{z}^k)]^{-1} \\mathbf{F}(\\mathbf{z}^k).\n\nHowever, instead of explicitly inverting the Jacobian matrix \\nabla \\mathbf{F}(\\mathbf{z}^k), we solve the linear system:\\underbrace{\\nabla \\mathbf{F}(\\mathbf{z}^k)}_{\\mathbf{A}} \\Delta \\mathbf{z}^k = -\\mathbf{F}(\\mathbf{z}^k),\n\nwhere \\Delta \\mathbf{z}^k = (\\Delta \\mathbf{x}^k, \\Delta \\boldsymbol{\\lambda}^k) represents the Newton step for the primal and dual variables. Substituting the expression for \\mathbf{F}(\\mathbf{z}) and its Jacobian, the system becomes:\\begin{bmatrix}\n\\nabla^2_{\\mathbf{x}\\mathbf{x}} L(\\mathbf{x}^k, \\boldsymbol{\\lambda}^k) & \\nabla \\mathbf{h}(\\mathbf{x}^k)^T \\\\\n\\nabla \\mathbf{h}(\\mathbf{x}^k) & \\mathbf{0}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\Delta \\mathbf{x}^k \\\\\n\\Delta \\boldsymbol{\\lambda}^k\n\\end{bmatrix}\n=\n-\n\\begin{bmatrix}\n\\nabla f(\\mathbf{x}^k) + \\nabla \\mathbf{h}(\\mathbf{x}^k)^T \\boldsymbol{\\lambda}^k \\\\\n\\mathbf{h}(\\mathbf{x}^k)\n\\end{bmatrix}.\n\nThe matrix on the left-hand side is known as the KKT matrix, as it stems from the Karush-Kuhn-Tucker conditions for this optimization problem\nThe solution of this system provides the updates \\Delta \\mathbf{x}^k and \\Delta \\boldsymbol{\\lambda}^k, which are then used to update the primal and dual variables:\\mathbf{x}^{k+1} = \\mathbf{x}^k + \\Delta \\mathbf{x}^k, \\quad \\boldsymbol{\\lambda}^{k+1} = \\boldsymbol{\\lambda}^k + \\Delta \\boldsymbol{\\lambda}^k.","type":"content","url":"/appendix-nlp#solving-equality-constrained-programs-with-newtons-method","position":11},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl4":"Demonstration","lvl3":"Solving Equality Constrained Programs with Newton’s Method"},"type":"lvl4","url":"/appendix-nlp#demonstration","position":12},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl4":"Demonstration","lvl3":"Solving Equality Constrained Programs with Newton’s Method"},"content":"The following code demonstates how we can implement this idea in Jax. In this demonstration, we are minimizing a quadratic objective function subject to a single equality constraint, a problem formally stated as follows:\\begin{aligned}\n\\min_{x \\in \\mathbb{R}^2} \\quad & f(x) = (x_1 - 2)^2 + (x_2 - 1)^2 \\\\\n\\text{subject to} \\quad & h(x) = x_1^2 + x_2^2 - 1 = 0\n\\end{aligned}\n\nGeometrically speaking, the constraint h(x) describes a unit circle centered at the origin. To solve this problem using the method of Lagrange multipliers, we form the Lagrangian:L(x, \\lambda) = f(x) + \\lambda h(x) = (x_1 - 2)^2 + (x_2 - 1)^2 + \\lambda(x_1^2 + x_2^2 - 1)\n\nFor this particular problem, it happens so that we can also find an analytical without even having to use Newton’s method. From the first-order optimality conditions, we obtain the following linear system of equations:\\begin{align*}\n   2(x_1 - 2) + 2\\lambda x_1 &= 0 \\\\\n   2(x_2 - 1) + 2\\lambda x_2 &= 0 \\\\\n   x_1^2 + x_2^2 - 1 &= 0\\\\\n\\end{align*}\n\nFrom the first two equations, we then get:x_1 = \\frac{2}{1 + \\lambda}, \\quad x_2 = \\frac{1}{1 + \\lambda}\n\nwhich we can substitute these into the 3rd constraint equation to obtain:(\\frac{2}{1 + \\lambda})^2 + (\\frac{1}{1 + \\lambda})^2 = 1 \\Leftrightarrow \\lambda = \\sqrt{5} - 1\n\nThis value of the Lagrange multiplier can then be backsubstituted into the above equations to obtain x_1 = \\frac{2}{\\sqrt{5}} and x_2 =  \\frac{1}{\\sqrt{5}}.\nWe can verify numerically (and visually on the following graph) that the point (2/\\sqrt{5}, 1/\\sqrt{5}) is indeed the point on the unit circle closest to (2, 1).\n\n#  label: appendix_nlp-cell-03\n#  caption: Rendered output from the preceding code cell.\n\n%config InlineBackend.figure_format = 'retina'\nimport jax\nimport jax.numpy as jnp\nfrom jax import grad, jit, jacfwd\nimport matplotlib.pyplot as plt\n\n# Apply book style\ntry:\n    import scienceplots\n    plt.style.use(['science', 'notebook'])\nexcept (ImportError, OSError):\n    pass  # Use matplotlib defaults\n\n# Define the objective function and constraint\ndef f(x):\n    return (x[0] - 2)**2 + (x[1] - 1)**2\n\ndef g(x):\n    return x[0]**2 + x[1]**2 - 1\n\n# Lagrangian\ndef L(x, lambda_):\n    return f(x) + lambda_ * g(x)\n\n# Gradient and Hessian of Lagrangian\ngrad_L_x = jit(grad(L, argnums=0))\ngrad_L_lambda = jit(grad(L, argnums=1))\nhess_L_xx = jit(jacfwd(grad_L_x, argnums=0))\nhess_L_xlambda = jit(jacfwd(grad_L_x, argnums=1))\n\n# Newton's method\n@jit\ndef newton_step(x, lambda_):\n    grad_x = grad_L_x(x, lambda_)\n    grad_lambda = grad_L_lambda(x, lambda_)\n    hess_xx = hess_L_xx(x, lambda_)\n    hess_xlambda = hess_L_xlambda(x, lambda_).reshape(-1)\n    \n    # Construct the full KKT matrix\n    kkt_matrix = jnp.block([\n        [hess_xx, hess_xlambda.reshape(-1, 1)],\n        [hess_xlambda, jnp.array([[0.0]])]\n    ])\n    \n    # Construct the right-hand side\n    rhs = jnp.concatenate([-grad_x, -jnp.array([grad_lambda])])\n    \n    # Solve the KKT system\n    delta = jnp.linalg.solve(kkt_matrix, rhs)\n    \n    return x + delta[:2], lambda_ + delta[2]\n\ndef solve_constrained_optimization(x0, lambda0, max_iter=100, tol=1e-6):\n    x, lambda_ = x0, lambda0\n    \n    for i in range(max_iter):\n        x_new, lambda_new = newton_step(x, lambda_)\n        if jnp.linalg.norm(jnp.concatenate([x_new - x, jnp.array([lambda_new - lambda_])])) < tol:\n            break\n        x, lambda_ = x_new, lambda_new\n    \n    return x, lambda_, i+1\n\n# Analytical solution\ndef analytical_solution():\n    x1 = 2 / jnp.sqrt(5)\n    x2 = 1 / jnp.sqrt(5)\n    lambda_opt = jnp.sqrt(5) - 1\n    return jnp.array([x1, x2]), lambda_opt\n\n# Solve the problem numerically\nx0 = jnp.array([0.5, 0.5])\nlambda0 = 0.0\nx_opt_num, lambda_opt_num, iterations = solve_constrained_optimization(x0, lambda0)\n\n# Compute analytical solution\nx_opt_ana, lambda_opt_ana = analytical_solution()\n\n# Verify the result\nprint(\"\\nNumerical Solution:\")\nprint(f\"Constraint violation: {g(x_opt_num):.6f}\")\nprint(f\"Objective function value: {f(x_opt_num):.6f}\")\n\nprint(\"\\nAnalytical Solution:\")\nprint(f\"Constraint violation: {g(x_opt_ana):.6f}\")\nprint(f\"Objective function value: {f(x_opt_ana):.6f}\")\n\nprint(\"\\nComparison:\")\nx_diff = jnp.linalg.norm(x_opt_num - x_opt_ana)\nlambda_diff = jnp.abs(lambda_opt_num - lambda_opt_ana)\nprint(f\"Difference in x: {x_diff}\")\nprint(f\"Difference in lambda: {lambda_diff}\")\n\n# Precision test\nrtol = 1e-5  # relative tolerance\natol = 1e-8  # absolute tolerance\n\nx_close = jnp.allclose(x_opt_num, x_opt_ana, rtol=rtol, atol=atol)\nlambda_close = jnp.isclose(lambda_opt_num, lambda_opt_ana, rtol=rtol, atol=atol)\n\nprint(\"\\nPrecision Test:\")\nprint(f\"x values are close: {x_close}\")\nprint(f\"lambda values are close: {lambda_close}\")\n\nif x_close and lambda_close:\n    print(\"The numerical solution matches the analytical solution within the specified tolerance.\")\nelse:\n    print(\"The numerical solution differs from the analytical solution more than the specified tolerance.\")\n\n# Visualize the result\nplt.figure(figsize=(12, 10))\n\n# Create a mesh for the contour plot\nx1_range = jnp.linspace(-1.5, 2.5, 100)\nx2_range = jnp.linspace(-1.5, 2.5, 100)\nX1, X2 = jnp.meshgrid(x1_range, x2_range)\nZ = jnp.array([[f(jnp.array([x1, x2])) for x1 in x1_range] for x2 in x2_range])\n\n# Plot filled contours\ncontour = plt.contourf(X1, X2, Z, levels=50, cmap='viridis', alpha=0.7, extent=[-1.5, 2.5, -1.5, 2.5])\nplt.colorbar(contour, label='Objective Function Value')\n\n# Plot the constraint\ntheta = jnp.linspace(0, 2*jnp.pi, 100)\nx1 = jnp.cos(theta)\nx2 = jnp.sin(theta)\nplt.plot(x1, x2, color='red', linewidth=2, label='Constraint')\n\n# Plot the optimal points (numerical and analytical) and initial point\nplt.scatter(x_opt_num[0], x_opt_num[1], color='red', s=100, edgecolor='white', linewidth=2, label='Numerical Optimal Point')\nplt.scatter(x_opt_ana[0], x_opt_ana[1], color='blue', s=100, edgecolor='white', linewidth=2, label='Analytical Optimal Point')\nplt.scatter(x0[0], x0[1], color='green', s=100, edgecolor='white', linewidth=2, label='Initial Point')\n\n# Add labels and title\nplt.xlabel('x1', fontsize=12)\nplt.ylabel('x2', fontsize=12)\nplt.title('Constrained Optimization: Numerical vs Analytical Solution', fontsize=14)\nplt.legend(fontsize=10)\nplt.grid(True, linestyle='--', alpha=0.7)\n\n# Set the axis limits explicitly\nplt.xlim(-1.5, 2.5)\nplt.ylim(-1.5, 2.5)\n\nplt.tight_layout()\n\n","type":"content","url":"/appendix-nlp#demonstration","position":13},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl2":"The SQP Approach: Taylor Expansion and Quadratic Approximation"},"type":"lvl2","url":"/appendix-nlp#the-sqp-approach-taylor-expansion-and-quadratic-approximation","position":14},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl2":"The SQP Approach: Taylor Expansion and Quadratic Approximation"},"content":"Sequential Quadratic Programming (SQP) tackles the problem of solving constrained programs by iteratively solving a sequence of simpler subproblems. Specifically, these subproblems are quadratic programs (QPs) that approximate the original problem around the current iterate by using a quadratic model of the objective function and a linear model of the constraints. Suppose we have the following optimization problem with equality constraints:\\begin{aligned}\n\\min_{\\mathbf{x}} \\quad & f(\\mathbf{x}) \\\\\n\\text{subject to} \\quad & \\mathbf{h}(\\mathbf{x}) = \\mathbf{0}.\n\\end{aligned}\n\nAt each iteration k, we approximate the objective function f(\\mathbf{x}) using a second-order Taylor expansion around the current iterate \\mathbf{x}^k. The standard Taylor expansion for f would be:\\begin{align*}\nf(\\mathbf{x}) \\approx f(\\mathbf{x}^k) + \\nabla f(\\mathbf{x}^k)^T (\\mathbf{x} - \\mathbf{x}^k) + \\frac{1}{2} (\\mathbf{x} - \\mathbf{x}^k)^T \\nabla^2 f(\\mathbf{x}^k) (\\mathbf{x} - \\mathbf{x}^k).\n\\end{align*}\n\nThis expansion uses the Hessian of the objective function \\nabla^2 f(\\mathbf{x}^k) to capture the curvature of f. However, in the context of constrained optimization, we also need to account for the effect of the constraints on the local behavior of the solution. If we were to use only \\nabla^2 f(\\mathbf{x}^k), we would not capture the influence of the constraints on the curvature of the feasible region. The resulting subproblem might then lead to steps that violate the constraints or are less effective in achieving convergence. The choice that we make instead is to use the Hessian of the Lagrangian, \\nabla^2_{\\mathbf{x}\\mathbf{x}} L(\\mathbf{x}^k, \\boldsymbol{\\lambda}^k), leading to the following quadratic model:f(\\mathbf{x}) \\approx f(\\mathbf{x}^k) + \\nabla f(\\mathbf{x}^k)^T (\\mathbf{x} - \\mathbf{x}^k) + \\frac{1}{2} (\\mathbf{x} - \\mathbf{x}^k)^T \\nabla^2_{\\mathbf{x}\\mathbf{x}} L(\\mathbf{x}^k, \\boldsymbol{\\lambda}^k) (\\mathbf{x} - \\mathbf{x}^k).\n\nSimilarly, the equality constraints \\mathbf{h}(\\mathbf{x}) are linearized around \\mathbf{x}^k:\\mathbf{h}(\\mathbf{x}) \\approx \\mathbf{h}(\\mathbf{x}^k) + \\nabla \\mathbf{h}(\\mathbf{x}^k) (\\mathbf{x} - \\mathbf{x}^k).\n\nCombining these approximations, we obtain a Quadratic Programming (QP) subproblem, which approximates our original problem locally at \\mathbf{x}^k but is easier to solve:\\begin{aligned}\n\\text{Minimize} \\quad & \\nabla f(\\mathbf{x}^k)^T \\Delta \\mathbf{x} + \\frac{1}{2} \\Delta \\mathbf{x}^T \\nabla^2_{\\mathbf{x}\\mathbf{x}} L(\\mathbf{x}^k, \\boldsymbol{\\lambda}^k) \\Delta \\mathbf{x} \\\\\n\\text{subject to} \\quad & \\nabla \\mathbf{h}(\\mathbf{x}^k) \\Delta \\mathbf{x} + \\mathbf{h}(\\mathbf{x}^k) = \\mathbf{0},\n\\end{aligned}\n\nwhere \\Delta \\mathbf{x} = \\mathbf{x} - \\mathbf{x}^k. The QP subproblem solved at each iteration focuses on finding the optimal step direction \\Delta \\mathbf{x} for the primal variables.\nWhile solving this QP, we obtain not only the step \\Delta \\mathbf{x} but also the associated Lagrange multipliers for the QP subproblem, which correspond to an updated dual variable vector \\boldsymbol{\\lambda}^{k+1}. More specifically, after solving the QP, we use \\Delta \\mathbf{x}^k to update the primal variables:\\begin{align*}\n\\mathbf{x}^{k+1} = \\mathbf{x}^k + \\Delta \\mathbf{x}^k.\n\\end{align*}\n\nSimultaneously, the Lagrange multipliers from the QP provide the updated dual variables \\boldsymbol{\\lambda}^{k+1}.\nWe summarize the SQP algorithm in the following pseudo-code:\n\nSequential Quadratic Programming (SQP)\n\nInput: Initial estimate \\mathbf{x}^0, initial Lagrange multipliers \\boldsymbol{\\lambda}^0, tolerance \\epsilon > 0.\n\nOutput: Solution \\mathbf{x}^*, Lagrange multipliers \\boldsymbol{\\lambda}^*.\n\nProcedure:\n\nCompute the QP Solution: Solve the QP subproblem to obtain \\Delta \\mathbf{x}^k. The QP solver also provides the updated Lagrange multipliers \\boldsymbol{\\lambda}^{k+1} associated with the constraints.\n\nUpdate the Estimates: Update the primal variables:\\mathbf{x}^{k+1} = \\mathbf{x}^k + \\Delta \\mathbf{x}^k.\n\nSet the dual variables to the updated values \\boldsymbol{\\lambda}^{k+1} from the QP solution.\n\nRepeat Until Convergence: Continue iterating until \\|\\Delta \\mathbf{x}^k\\| < \\epsilon and the KKT conditions are satisfied.","type":"content","url":"/appendix-nlp#the-sqp-approach-taylor-expansion-and-quadratic-approximation","position":15},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl3":"Connection to Newton’s Method in the Equality-Constrained Case","lvl2":"The SQP Approach: Taylor Expansion and Quadratic Approximation"},"type":"lvl3","url":"/appendix-nlp#connection-to-newtons-method-in-the-equality-constrained-case","position":16},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl3":"Connection to Newton’s Method in the Equality-Constrained Case","lvl2":"The SQP Approach: Taylor Expansion and Quadratic Approximation"},"content":"The QP subproblem in SQP is directly related to applying Newton’s method for equality-constrained optimization. To see this, note that the KKT matrix of the QP subproblem is:\\begin{align*}\n\\begin{bmatrix}\n\\nabla^2_{\\mathbf{x}\\mathbf{x}} L(\\mathbf{x}^k, \\boldsymbol{\\lambda}^k) & \\nabla \\mathbf{h}(\\mathbf{x}^k)^T \\\\\n\\nabla \\mathbf{h}(\\mathbf{x}^k) & \\mathbf{0}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\Delta \\mathbf{x}^k \\\\\n\\Delta \\boldsymbol{\\lambda}^k\n\\end{bmatrix}\n=\n-\n\\begin{bmatrix}\n\\nabla f(\\mathbf{x}^k) + \\nabla \\mathbf{h}(\\mathbf{x}^k)^T \\boldsymbol{\\lambda}^k \\\\\n\\mathbf{h}(\\mathbf{x}^k)\n\\end{bmatrix}\n\\end{align*}\n\nThis is exactly the same linear system that have to solve when applying Newton’s method to the KKT conditions of the original program! Thus, solving the QP subproblem at each iteration of SQP is equivalent to taking a Newton step on the KKT conditions of the original nonlinear problem.","type":"content","url":"/appendix-nlp#connection-to-newtons-method-in-the-equality-constrained-case","position":17},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl2":"SQP for Inequality-Constrained Optimization"},"type":"lvl2","url":"/appendix-nlp#sqp-for-inequality-constrained-optimization","position":18},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl2":"SQP for Inequality-Constrained Optimization"},"content":"So far, we’ve applied the ideas behind Sequential Quadratic Programming (SQP) to problems with only equality constraints. Now, let’s extend this framework to handle optimization problems that also include inequality constraints.\nConsider a general nonlinear optimization problem that includes both equality and inequality constraints:\\begin{align*}\n\\min_{\\mathbf{x}} \\quad & f(\\mathbf{x}) \\\\\n\\text{subject to} \\quad & \\mathbf{g}(\\mathbf{x}) \\leq \\mathbf{0}, \\\\\n& \\mathbf{h}(\\mathbf{x}) = \\mathbf{0}.\n\\end{align*}\n\nAs we did earlier, we approximate this problem by constructing a quadratic approximation to the objective and a linearization of the constraints. QP subproblem at each iteration is then formulated as:\\begin{align*}\n\\text{Minimize} \\quad & \\nabla f(\\mathbf{x}^k)^T \\Delta \\mathbf{x} + \\frac{1}{2} \\Delta \\mathbf{x}^T \\nabla^2_{\\mathbf{x}\\mathbf{x}} L(\\mathbf{x}^k, \\boldsymbol{\\lambda}^k, \\boldsymbol{\\nu}^k) \\Delta \\mathbf{x} \\\\\n\\text{subject to} \\quad & \\nabla \\mathbf{g}(\\mathbf{x}^k) \\Delta \\mathbf{x} + \\mathbf{g}(\\mathbf{x}^k) \\leq \\mathbf{0}, \\\\\n& \\nabla \\mathbf{h}(\\mathbf{x}^k) \\Delta \\mathbf{x} + \\mathbf{h}(\\mathbf{x}^k) = \\mathbf{0},\n\\end{align*}\n\nwhere \\Delta \\mathbf{x} = \\mathbf{x} - \\mathbf{x}^k represents the step direction for the primal variables. The following pseudocode outlines the steps involved in applying SQP to a problem with both equality and inequality constraints:\n\nSequential Quadratic Programming (SQP) with Inequality Constraints\n\nInput: Initial estimate \\mathbf{x}^0, initial multipliers \\boldsymbol{\\lambda}^0, \\boldsymbol{\\nu}^0, tolerance \\epsilon > 0.\n\nOutput: Solution \\mathbf{x}^*, Lagrange multipliers \\boldsymbol{\\lambda}^*, \\boldsymbol{\\nu}^*.\n\nProcedure:\n\nInitialization:\nSet k = 0.\n\nRepeat:\n\na. Construct the QP Subproblem:\nFormulate the QP subproblem using the current iterate \\mathbf{x}^k, \\boldsymbol{\\lambda}^k, and \\boldsymbol{\\nu}^k:\\begin{aligned}\n   \\text{Minimize} \\quad & \\nabla f(\\mathbf{x}^k)^T \\Delta \\mathbf{x} + \\frac{1}{2} \\Delta \\mathbf{x}^T \\nabla^2_{\\mathbf{x}\\mathbf{x}} L(\\mathbf{x}^k, \\boldsymbol{\\lambda}^k, \\boldsymbol{\\nu}^k) \\Delta \\mathbf{x} \\\\\n   \\text{subject to} \\quad & \\nabla \\mathbf{g}(\\mathbf{x}^k) \\Delta \\mathbf{x} + \\mathbf{g}(\\mathbf{x}^k) \\leq \\mathbf{0}, \\\\\n   & \\nabla \\mathbf{h}(\\mathbf{x}^k) \\Delta \\mathbf{x} + \\mathbf{h}(\\mathbf{x}^k) = \\mathbf{0}.\n   \\end{aligned}\n\nb. Solve the QP Subproblem:\nSolve for \\Delta \\mathbf{x}^k and obtain the updated Lagrange multipliers \\boldsymbol{\\lambda}^{k+1} and \\boldsymbol{\\nu}^{k+1}.\n\nc. Update the Estimates:\nUpdate the primal variables and multipliers:\\mathbf{x}^{k+1} = \\mathbf{x}^k + \\Delta \\mathbf{x}^k.\n\nd. Check for Convergence:\nIf \\|\\Delta \\mathbf{x}^k\\| < \\epsilon and the KKT conditions are satisfied, stop. Otherwise, set k = k + 1 and repeat.\n\nReturn:\n\\mathbf{x}^* = \\mathbf{x}^{k+1}, \\boldsymbol{\\lambda}^* = \\boldsymbol{\\lambda}^{k+1}, \\boldsymbol{\\nu}^* = \\boldsymbol{\\nu}^{k+1}.","type":"content","url":"/appendix-nlp#sqp-for-inequality-constrained-optimization","position":19},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl3":"Demonstration with JAX and CVXPy","lvl2":"SQP for Inequality-Constrained Optimization"},"type":"lvl3","url":"/appendix-nlp#demonstration-with-jax-and-cvxpy","position":20},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl3":"Demonstration with JAX and CVXPy","lvl2":"SQP for Inequality-Constrained Optimization"},"content":"Consider the following equality and inequality-constrained problem:\\begin{align*}\n\\min_{x \\in \\mathbb{R}^2} \\quad & f(x) = (x_1 - 2)^2 + (x_2 - 1)^2 \\\\\n\\text{subject to} \\quad & g(x) = x_1^2 - x_2 \\leq 0  \\\\\n& h(x) = x_1^2 + x_2^2 - 1 = 0\n\\end{align*}\n\nThis example builds on our previous one but adds a parabola-shaped inequality constraint. We require our solution to lie not only on the circle defining our equality constraint but also below the parabola. To solve the QP subproblem, we will be using the \n\nCVXPY package. While the Lagrangian and derivatives could be computed easily by hand, we use \n\nJAX for generality:\n\n#  label: appendix_nlp-cell-04\n#  caption: Rendered output from the preceding code cell.\n\n%config InlineBackend.figure_format = 'retina'\nimport jax\nimport jax.numpy as jnp\nfrom jax import grad, jit, jacfwd, hessian\nimport numpy as np\nimport cvxpy as cp\nimport matplotlib.pyplot as plt\n\n# Apply book style\ntry:\n    import scienceplots\n    plt.style.use(['science', 'notebook'])\nexcept (ImportError, OSError):\n    pass  # Use matplotlib defaults\n\n# Define the objective function and constraints\n@jit\ndef f(x):\n    return (x[0] - 2)**2 + (x[1] - 1)**2\n\n@jit\ndef g(x):\n    return jnp.array([x[0]**2 + x[1]**2 - 1])\n\n@jit\ndef h(x):\n    return jnp.array([x[0]**2 - x[1]])  # Corrected inequality constraint: x[1] <= x[0]^2\n\n# Compute gradients and Jacobians using JAX\ngrad_f = jit(grad(f))\nhess_f = jit(hessian(f))\njac_g = jit(jacfwd(g))\njac_h = jit(jacfwd(h))\n\n@jit\ndef lagrangian(x, lambda_, nu):\n    return f(x) + jnp.dot(lambda_, g(x)) + jnp.dot(nu, h(x))\n\nhess_L = jit(hessian(lagrangian, argnums=0))\n\ndef solve_qp_subproblem(x, lambda_, nu):\n    n = len(x)\n    delta_x = cp.Variable(n)\n    \n    # Convert JAX arrays to numpy for cvxpy\n    grad_f_np = np.array(grad_f(x))\n    hess_L_np = np.array(hess_L(x, lambda_, nu))\n    jac_g_np = np.array(jac_g(x))\n    jac_h_np = np.array(jac_h(x))\n    g_np = np.array(g(x))\n    h_np = np.array(h(x))\n    \n    obj = cp.Minimize(grad_f_np.T @ delta_x + 0.5 * cp.quad_form(delta_x, hess_L_np))\n    \n    constraints = [\n        jac_g_np @ delta_x + g_np == 0,\n        jac_h_np @ delta_x + h_np <= 0\n    ]\n    \n    prob = cp.Problem(obj, constraints)\n    prob.solve()\n    \n    return delta_x.value, prob.constraints[0].dual_value, prob.constraints[1].dual_value\n\ndef sqp(x0, max_iter=100, tol=1e-6):\n    x = x0\n    lambda_ = jnp.zeros(1)\n    nu = jnp.zeros(1)\n    \n    for i in range(max_iter):\n        delta_x, new_lambda, new_nu = solve_qp_subproblem(x, lambda_, nu)\n        \n        if jnp.linalg.norm(delta_x) < tol:\n            break\n        \n        x = x + delta_x\n        lambda_ = new_lambda\n        nu = new_nu\n        \n    return x, lambda_, nu, i+1\n\n# Initial point\nx0 = jnp.array([0.5, 0.5])\n\n# Solve using SQP\nx_opt, lambda_opt, nu_opt, iterations = sqp(x0)\n\nprint(f\"Optimal x: {x_opt}\")\nprint(f\"Optimal lambda: {lambda_opt}\")\nprint(f\"Optimal nu: {nu_opt}\")\nprint(f\"Iterations: {iterations}\")\n\n# Visualize the result\nplt.figure(figsize=(12, 10))\n\n# Create a mesh for the contour plot\nx1_range = jnp.linspace(-1.5, 2.5, 100)\nx2_range = jnp.linspace(-1.5, 2.5, 100)\nX1, X2 = jnp.meshgrid(x1_range, x2_range)\nZ = jnp.array([[f(jnp.array([x1, x2])) for x1 in x1_range] for x2 in x2_range])\n\n# Plot filled contours\ncontour = plt.contourf(X1, X2, Z, levels=50, cmap='viridis', alpha=0.7)\nplt.colorbar(contour, label='Objective Function Value')\n\n# Plot the equality constraint\ntheta = jnp.linspace(0, 2*jnp.pi, 100)\nx1_eq = jnp.cos(theta)\nx2_eq = jnp.sin(theta)\nplt.plot(x1_eq, x2_eq, color='red', linewidth=2, label='Equality Constraint')\n\n# Plot the inequality constraint and shade the feasible region\nx1_ineq = jnp.linspace(-1.5, 2.5, 100)\nx2_ineq = x1_ineq**2\nplt.plot(x1_ineq, x2_ineq, color='orange', linewidth=2, label='Inequality Constraint')\n\n# Shade the feasible region for the inequality constraint\nx2_lower = jnp.minimum(x2_ineq, 2.5)\nplt.fill_between(x1_ineq, -1.5, x2_lower, color='gray', alpha=0.2, hatch='\\\\/...', label='Feasible Region')\n\n# Plot the optimal and initial points\nplt.scatter(x_opt[0], x_opt[1], color='red', s=100, edgecolor='white', linewidth=2, label='Optimal Point')\nplt.scatter(x0[0], x0[1], color='green', s=100, edgecolor='white', linewidth=2, label='Initial Point')\n\n# Add labels and title\nplt.xlabel('x1', fontsize=12)\nplt.ylabel('x2', fontsize=12)\nplt.title('SQP for Inequality Constraints with CVXPY and JAX', fontsize=14)\nplt.legend(fontsize=10, loc='upper center')\nplt.grid(True, linestyle='--', alpha=0.7)\n\n# Set the axis limits explicitly\nplt.xlim(-1.5, 2.5)\nplt.ylim(-1.5, 2.5)\n\nplt.tight_layout()\n\n# Verify the result\nprint(f\"\\nEquality constraint violation: {g(x_opt)[0]:.6f}\")\nprint(f\"Inequality constraint violation: {h(x_opt)[0]:.6f}\")\nprint(f\"Objective function value: {f(x_opt):.6f}\")\n\n","type":"content","url":"/appendix-nlp#demonstration-with-jax-and-cvxpy","position":21},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl2":"The Arrow-Hurwicz-Uzawa algorithm"},"type":"lvl2","url":"/appendix-nlp#the-arrow-hurwicz-uzawa-algorithm","position":22},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl2":"The Arrow-Hurwicz-Uzawa algorithm"},"content":"While the SQP method addresses constrained optimization problems by sequentially solving quadratic subproblems, an alternative approach follows from viewing constrained optimization as a min-max problem. This perspective leads to a simpler algorithm, originally introduced by the Arrow-Hurwicz-Uzawa \n\nArrow et al. (1958). Consider the following general constrained optimization problem encompassing both equality and inequality constraints:\\begin{aligned}\n\\min_{\\mathbf{x}} \\quad & f(\\mathbf{x}) \\\\\n\\text{subject to} \\quad & \\mathbf{g}(\\mathbf{x}) \\leq \\mathbf{0} \\\\\n& \\mathbf{h}(\\mathbf{x}) = \\mathbf{0}\n\\end{aligned}\n\nUsing the Lagrangian function L(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu}) = f(\\mathbf{x}) + \\boldsymbol{\\mu}^T \\mathbf{g}(\\mathbf{x}) + \\boldsymbol{\\lambda}^T \\mathbf{h}(\\mathbf{x}), we can reformulate this problem as the following min-max problem:\\min_{\\mathbf{x}} \\max_{\\boldsymbol{\\lambda}, \\boldsymbol{\\mu} \\geq 0} L(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\mu})\n\nThe role of each component in this min-max structure can be understood as follows:\n\nThe outer minimization over \\mathbf{x} finds the feasible point that minimizes the objective function f(\\mathbf{x}).\n\nThe maximization over \\boldsymbol{\\mu} \\geq 0 ensures that inequality constraints \\mathbf{g}(\\mathbf{x}) \\leq \\mathbf{0} are satisfied. If any inequality constraint is violated, the corresponding term in \\boldsymbol{\\mu}^T \\mathbf{g}(\\mathbf{x}) can be made arbitrarily large by choosing a large enough \\mu_i.\n\nThe maximization over \\boldsymbol{\\lambda} ensures that equality constraints \\mathbf{h}(\\mathbf{x}) = \\mathbf{0} are satisfied.\n\nUsing this observation, we can devise an algorithm which, like SQP, will update both the primal and dual variables at every step. But rather than using second-order optimization, we will simply use a first-order gradient update step: a descent step in the primal variable, and an ascent step in the dual one. The corresponding procedure, when implemented by gradient descent, is called Gradient Ascent Descent in the learning and optimization communities. In the case of equality constraints only, the algorithm looks like the following:\n\nArrow-Hurwicz-Uzawa for equality constraints only\n\nInput: Initial guess \\mathbf{x}^0, \\boldsymbol{\\lambda}^0, step sizes \\alpha, \\beta\nOutput: Optimal \\mathbf{x}^*, \\boldsymbol{\\lambda}^*\n\n1: for k = 0, 1, 2, \\ldots until convergence do\n\n2:     \\mathbf{x}^{k+1} = \\mathbf{x}^k - \\alpha \\nabla_{\\mathbf{x}} L(\\mathbf{x}^k, \\boldsymbol{\\lambda}^k)  (Primal update)\n\n3:     \\boldsymbol{\\lambda}^{k+1} = \\boldsymbol{\\lambda}^k + \\beta \\nabla_{\\boldsymbol{\\lambda}} L(\\mathbf{x}^{k+1}, \\boldsymbol{\\lambda}^k)  (Dual update)\n\n4: end for\n\n5: return \\mathbf{x}^k, \\boldsymbol{\\lambda}^k\n\nNow to account for the fact that the Lagrange multiplier needs to be non-negative for inequality constraints, we can use our previous idea from projected gradient descent for bound constraints and consider a projection, or clipping step to ensure that this condition is satisfied throughout. In this case, the algorithm looks like the following:\n\nArrow-Hurwicz-Uzawa for equality and inequality constraints\n\nInput: Initial guess \\mathbf{x}^0, \\boldsymbol{\\lambda}^0, \\boldsymbol{\\mu}^0 \\geq 0, step sizes \\alpha, \\beta, \\gamma\nOutput: Optimal \\mathbf{x}^*, \\boldsymbol{\\lambda}^*, \\boldsymbol{\\mu}^*\n\n1: for k = 0, 1, 2, \\ldots until convergence do\n\n2:     \\mathbf{x}^{k+1} = \\mathbf{x}^k - \\alpha \\nabla_{\\mathbf{x}} L(\\mathbf{x}^k, \\boldsymbol{\\lambda}^k, \\boldsymbol{\\mu}^k)  (Primal update)\n\n3:     \\boldsymbol{\\lambda}^{k+1} = \\boldsymbol{\\lambda}^k + \\beta \\, \\nabla_{\\boldsymbol{\\lambda}} L(\\mathbf{x}^{k+1}, \\boldsymbol{\\lambda}^k, \\boldsymbol{\\mu}^k)  (Dual update for equality constraints)\n\n4:     \\boldsymbol{\\mu}^{k+1} = [\\boldsymbol{\\mu}^k + \\gamma \\nabla_{\\boldsymbol{\\mu}} L(\\mathbf{x}^{k+1}, \\boldsymbol{\\lambda}^k, \\boldsymbol{\\mu}^k)]_+  (Dual update with clipping for inequality constraints)\n\n5: end for\n\n6: return \\mathbf{x}^k, \\boldsymbol{\\lambda}^k, \\boldsymbol{\\mu}^k\n\nHere, [\\cdot]_+ denotes the projection onto the non-negative orthant, ensuring that \\boldsymbol{\\mu} remains non-negative.\n\nHowever, as it is widely known from the lessons of GAN (Generative Adversarial Network) training \n\nGoodfellow et al. (2014), Gradient Descent Ascent (GDA) can fail to converge or suffer from instability. The Arrow-Hurwicz-Uzawa algorithm, also known as the first-order Lagrangian method, is known to converge only locally, in the vicinity of an optimal primal-dual pair.\n\n#  label: appendix_nlp-cell-05\n#  caption: Rendered output from the preceding code cell.\n\n%config InlineBackend.figure_format = 'retina'\nimport jax\nimport jax.numpy as jnp\nfrom jax import grad, jit, value_and_grad\nimport optax\nimport matplotlib.pyplot as plt\n\n# Apply book style\ntry:\n    import scienceplots\n    plt.style.use(['science', 'notebook'])\nexcept (ImportError, OSError):\n    pass  # Use matplotlib defaults\n\n# Define the objective function and constraints\n@jit\ndef f(x):\n    return (x[0] - 2)**2 + (x[1] - 1)**2\n\n@jit\ndef g(x):\n    return jnp.array([x[0]**2 + x[1]**2 - 1])\n\n@jit\ndef h(x):\n    return jnp.array([x[0]**2 - x[1]])  # Inequality constraint: x[1] <= x[0]^2\n\n# Define the Lagrangian\n@jit\ndef lagrangian(x, lambda_, mu):\n    return f(x) + jnp.dot(lambda_, g(x)) + jnp.dot(mu, h(x))\n\n# Compute gradients of the Lagrangian\ngrad_L_x = jit(grad(lagrangian, argnums=0))\ngrad_L_lambda = jit(grad(lagrangian, argnums=1))\ngrad_L_mu = jit(grad(lagrangian, argnums=2))\n\n# Define the Arrow-Hurwicz-Uzawa update step\n@jit\ndef update(carry, t):\n    x, lambda_, mu, opt_state_x, opt_state_lambda, opt_state_mu = carry\n    \n    # Compute gradients\n    grad_x = grad_L_x(x, lambda_, mu)\n    grad_lambda = grad_L_lambda(x, lambda_, mu)\n    grad_mu = grad_L_mu(x, lambda_, mu)\n    \n    # Update primal variables (minimization)\n    updates_x, opt_state_x = optimizer_x.update(grad_x, opt_state_x)\n    x = optax.apply_updates(x, updates_x)\n    \n    # Update dual variables (maximization)\n    updates_lambda, opt_state_lambda = optimizer_lambda.update(grad_lambda, opt_state_lambda)\n    lambda_ = optax.apply_updates(lambda_, -updates_lambda)  # Positive update for maximization\n    \n    updates_mu, opt_state_mu = optimizer_mu.update(grad_mu, opt_state_mu)\n    mu = optax.apply_updates(mu, -updates_mu)  # Positive update for maximization\n    \n    # Project mu onto the non-negative orthant\n    mu = jnp.maximum(mu, 0)\n    \n    return (x, lambda_, mu, opt_state_x, opt_state_lambda, opt_state_mu), x\n\ndef arrow_hurwicz_uzawa(x0, lambda0, mu0, max_iter=1000):\n    # Initialize optimizers\n    global optimizer_x, optimizer_lambda, optimizer_mu\n    optimizer_x = optax.adam(learning_rate=0.01)\n    optimizer_lambda = optax.adam(learning_rate=0.01)\n    optimizer_mu = optax.adam(learning_rate=0.01)\n    \n    opt_state_x = optimizer_x.init(x0)\n    opt_state_lambda = optimizer_lambda.init(lambda0)\n    opt_state_mu = optimizer_mu.init(mu0)\n    \n    init_carry = (x0, lambda0, mu0, opt_state_x, opt_state_lambda, opt_state_mu)\n    \n    # Use jax.lax.scan for the optimization loop\n    (x, lambda_, mu, _, _, _), trajectory = jax.lax.scan(update, init_carry, jnp.arange(max_iter))\n    \n    return x, lambda_, mu, trajectory\n\n# Initial point\nx0 = jnp.array([0.5, 0.5])\nlambda0 = jnp.zeros(1)\nmu0 = jnp.zeros(1)\n\n# Solve using Arrow-Hurwicz-Uzawa\nx_opt, lambda_opt, mu_opt, trajectory = arrow_hurwicz_uzawa(x0, lambda0, mu0, max_iter=1000)\n\nprint(f\"Final x: {x_opt}\")\nprint(f\"Final lambda: {lambda_opt}\")\nprint(f\"Final mu: {mu_opt}\")\n\n# Visualize the result\nplt.figure(figsize=(12, 10))\n\n# Create a mesh for the contour plot\nx1_range = jnp.linspace(-1.5, 2.5, 100)\nx2_range = jnp.linspace(-1.5, 2.5, 100)\nX1, X2 = jnp.meshgrid(x1_range, x2_range)\nZ = jnp.array([[f(jnp.array([x1, x2])) for x1 in x1_range] for x2 in x2_range])\n\n# Plot filled contours\ncontour = plt.contourf(X1, X2, Z, levels=50, cmap='viridis', alpha=0.7)\nplt.colorbar(contour, label='Objective Function Value')\n\n# Plot the equality constraint\ntheta = jnp.linspace(0, 2*jnp.pi, 100)\nx1_eq = jnp.cos(theta)\nx2_eq = jnp.sin(theta)\nplt.plot(x1_eq, x2_eq, color='red', linewidth=2, label='Equality Constraint')\n\n# Plot the inequality constraint and shade the feasible region\nx1_ineq = jnp.linspace(-1.5, 2.5, 100)\nx2_ineq = x1_ineq**2\nplt.plot(x1_ineq, x2_ineq, color='orange', linewidth=2, label='Inequality Constraint')\n\n# Shade the feasible region for the inequality constraint\nx2_lower = jnp.minimum(x2_ineq, 2.5)\nplt.fill_between(x1_ineq, -1.5, x2_lower, color='gray', alpha=0.2, hatch='\\\\/...', label='Feasible Region')\n\n# Plot the optimal and initial points\nplt.scatter(x_opt[0], x_opt[1], color='red', s=100, edgecolor='white', linewidth=2, label='Final Point')\nplt.scatter(x0[0], x0[1], color='green', s=100, edgecolor='white', linewidth=2, label='Initial Point')\n\n# Plot the optimization trajectory using scatter plot\nscatter = plt.scatter(trajectory[:, 0], trajectory[:, 1], c=jnp.arange(len(trajectory)), \n                      cmap='cool', s=10, alpha=0.5)\nplt.colorbar(scatter, label='Iteration')\n\n# Add labels and title\nplt.xlabel('x1', fontsize=12)\nplt.ylabel('x2', fontsize=12)\nplt.title('Arrow-Hurwicz-Uzawa Algorithm with JAX and Adam (Corrected Min/Max)', fontsize=14)\nplt.legend(fontsize=10, loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=3)\nplt.grid(True, linestyle='--', alpha=0.7)\n\n# Set the axis limits explicitly\nplt.xlim(-1.5, 2.5)\nplt.ylim(-1.5, 2.5)\n\nplt.tight_layout()\n\n\n# Verify the result\nprint(f\"\\nEquality constraint violation: {g(x_opt)[0]:.6f}\")\nprint(f\"Inequality constraint violation: {h(x_opt)[0]:.6f}\")\nprint(f\"Objective function value: {f(x_opt):.6f}\")\n\n","type":"content","url":"/appendix-nlp#the-arrow-hurwicz-uzawa-algorithm","position":23},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl2":"Projected Gradient Descent"},"type":"lvl2","url":"/appendix-nlp#projected-gradient-descent","position":24},{"hierarchy":{"lvl1":"Nonlinear Programming","lvl2":"Projected Gradient Descent"},"content":"The Arrow-Hurwicz-Uzawa algorithm provided a way to handle constraints through dual variables and a primal-dual update scheme. Another commonly used approach for constrained optimization is Projected Gradient Descent (PGD). The idea is simple: take a gradient descent step as if the problem were unconstrained, then project the result back onto the feasible set. Formally:\\mathbf{x}_{k+1} = \\mathcal{P}_C\\big(\\mathbf{x}_k - \\alpha \\nabla f(\\mathbf{x}_k)\\big),\n\nwhere \\mathcal{P}_C is the projection onto the feasible set C, \\alpha is the step size, and f(\\mathbf{x}) is the objective function.\n\nPGD is particularly effective when the projection is computationally cheap. A common example is box constraints (or bound constraints), where the feasible set is a hyperrectangle:C = \\{ \\mathbf{x} \\mid \\mathbf{x}_{\\mathrm{lb}} \\leq \\mathbf{x} \\leq \\mathbf{x}_{\\mathrm{ub}} \\}.\n\nIn this case, the projection reduces to an element-wise clipping operation:[\\mathcal{P}_C(\\mathbf{x})]_i = \\max\\big(\\min([\\mathbf{x}]_i, [\\mathbf{x}_{\\mathrm{ub}}]_i), [\\mathbf{x}_{\\mathrm{lb}}]_i\\big).\n\nFor bound-constrained problems, PGD is almost as easy to implement as standard gradient descent because the projection step is just a clipping operation. For more general constraints, however, the projection may require solving a separate optimization problem, which can be as hard as the original task. Here is the algorithm for a problem of the form:\\begin{aligned}\n\\min_{\\mathbf{x}} \\quad & f(\\mathbf{x}) \\\\\n\\text{subject to} \\quad & \\mathbf{x}_{\\mathrm{lb}} \\leq \\mathbf{x} \\leq \\mathbf{x}_{\\mathrm{ub}}.\n\\end{aligned}\n\nProjected Gradient Descent for Bound Constraints\n\nInput: Initial point \\mathbf{x}_0, step size \\alpha, bounds \\mathbf{x}_{\\mathrm{lb}}, \\mathbf{x}_{\\mathrm{ub}},\nmaximum iterations \\max_\\text{iter}, tolerance \\varepsilon\n\nInitialize k = 0\n\nWhile k < \\max_\\text{iter} and not converged:\n\nCompute gradient: \\mathbf{g}_k = \\nabla f(\\mathbf{x}_k)\n\nUpdate: \\mathbf{x}_{k+1} = \\text{clip}(\\mathbf{x}_k - \\alpha \\mathbf{g}_k,\\; \\mathbf{x}_{\\mathrm{lb}}, \\mathbf{x}_{\\mathrm{ub}})\n\nCheck convergence: if \\|\\mathbf{x}_{k+1} - \\mathbf{x}_k\\| < \\varepsilon, stop\n\nk = k + 1\n\nReturn \\mathbf{x}_k\n\nThe clipping function is defined as:\\text{clip}(x, x_{\\mathrm{lb}}, x_{\\mathrm{ub}}) = \\max\\big(\\min(x, x_{\\mathrm{ub}}), x_{\\mathrm{lb}}\\big).\n\nUnder mild conditions such as Lipschitz continuity of the gradient, PGD converges to a stationary point of the constrained problem. Its simplicity and low cost make it a common choice whenever the projection can be computed efficiently.","type":"content","url":"/appendix-nlp#projected-gradient-descent","position":25},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time"},"type":"lvl1","url":"/collocation","position":0},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time"},"content":"As in the discrete-time setting, we work with three continuous-time variants that differ only in how the objective is written while sharing the same dynamics, path constraints, and bounds. The path constraints \\mathbf{g}(\\mathbf{x}(t),\\mathbf{u}(t))\\le \\mathbf{0} are pointwise in time, and the bounds \\mathbf{x}_{\\min}\\le \\mathbf{x}(t)\\le \\mathbf{x}_{\\max} and \\mathbf{u}_{\\min}\\le \\mathbf{u}(t)\\le \\mathbf{u}_{\\max} are understood in the same pointwise sense.\n\nMayer Problem\\begin{aligned}\n    \\text{minimize} \\quad & c(\\mathbf{x}(t_f)) \\\\\n    \\text{subject to} \\quad & \\dot{\\mathbf{x}}(t) = \\mathbf{f}(\\mathbf{x}(t), \\mathbf{u}(t)) \\\\\n                            & \\mathbf{g}(\\mathbf{x}(t), \\mathbf{u}(t)) \\leq \\mathbf{0} \\\\\n                            & \\mathbf{x}_{\\text{min}} \\leq \\mathbf{x}(t) \\leq \\mathbf{x}_{\\text{max}} \\\\\n                            & \\mathbf{u}_{\\text{min}} \\leq \\mathbf{u}(t) \\leq \\mathbf{u}_{\\text{max}} \\\\\n    \\text{given} \\quad & \\mathbf{x}(t_0) = \\mathbf{x}_0 \\enspace .\n\\end{aligned}\n\nLagrange Problem\\begin{aligned}\n    \\text{minimize} \\quad & \\int_{t_0}^{t_f} c(\\mathbf{x}(t), \\mathbf{u}(t)) \\, dt \\\\\n    \\text{subject to} \\quad & \\dot{\\mathbf{x}}(t) = \\mathbf{f}(\\mathbf{x}(t), \\mathbf{u}(t)) \\\\\n                            & \\mathbf{g}(\\mathbf{x}(t), \\mathbf{u}(t)) \\leq \\mathbf{0} \\\\\n                            & \\mathbf{x}_{\\text{min}} \\leq \\mathbf{x}(t) \\leq \\mathbf{x}_{\\text{max}} \\\\\n                            & \\mathbf{u}_{\\text{min}} \\leq \\mathbf{u}(t) \\leq \\mathbf{u}_{\\text{max}} \\\\\n    \\text{given} \\quad & \\mathbf{x}(t_0) = \\mathbf{x}_0 \\enspace .\n\\end{aligned}\n\nBolza Problem\\begin{aligned}\n    \\text{minimize} \\quad & c(\\mathbf{x}(t_f)) + \\int_{t_0}^{t_f} c(\\mathbf{x}(t), \\mathbf{u}(t)) \\, dt \\\\\n    \\text{subject to} \\quad & \\dot{\\mathbf{x}}(t) = \\mathbf{f}(\\mathbf{x}(t), \\mathbf{u}(t)) \\\\\n                            & \\mathbf{g}(\\mathbf{x}(t), \\mathbf{u}(t)) \\leq \\mathbf{0} \\\\\n                            & \\mathbf{x}_{\\text{min}} \\leq \\mathbf{x}(t) \\leq \\mathbf{x}_{\\text{max}} \\\\\n                            & \\mathbf{u}_{\\text{min}} \\leq \\mathbf{u}(t) \\leq \\mathbf{u}_{\\text{max}} \\\\\n    \\text{given} \\quad & \\mathbf{x}(t_0) = \\mathbf{x}_0 \\enspace .\n\\end{aligned}\n\nThese three forms are different lenses on the same task. Bolza contains both terminal and running terms. Lagrange can be turned into Mayer by augmenting the state with an accumulator:\\dot{z}(t)=c(\\mathbf{x}(t),\\mathbf{u}(t)),\\quad z(t_0)=0,\\quad \\text{minimize } z(t_f),\n\nwith the original dynamics left unchanged. Mayer is a special case of Bolza with zero running cost. We will use these equivalences freely, since a numerical method cares only about what must be evaluated and where those evaluations are taken.\n\nWith this catalog in place, we now pass from functions to finite representations.","type":"content","url":"/collocation","position":1},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl2":"Direct Transcription Methods"},"type":"lvl2","url":"/collocation#direct-transcription-methods","position":2},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl2":"Direct Transcription Methods"},"content":"The discrete-time problems of the previous chapter already suggested how to proceed: we convert a continuous problem into one over finitely many numbers by deciding where to look at the trajectories and how to interpolate between those looks. We place a mesh t_0<t_1<\\cdots<t_N=t_f and, inside each window [t_k,t_{k+1}], select a small set of interior fractions \\{\\tau_j\\} on the reference interval [0,1]. The running cost is additive over windows, so we write it as a sum of local integrals, map each window to [0,1], and approximate each local integral by a quadrature rule with nodes \\tau_j and weights w_j. This produces\\int_{t_0}^{t_f} c(\\mathbf{x}(t),\\mathbf{u}(t))\\,dt\n\\;\\approx\\;\n\\sum_{k=0}^{N-1} h_k \\sum_{j=1}^q w_j\\, c\\!\\big(\\mathbf{x}(t_k+h_k\\tau_j),\\,\\mathbf{u}(t_k+h_k\\tau_j)\\big),\n\nwith h_k=t_{k+1}-t_k. The dynamics are treated in the same way by the fundamental theorem of calculus,\\mathbf{x}(t_{k+1})-\\mathbf{x}(t_k)=\\int_{t_k}^{t_{k+1}} \\mathbf{f}(\\mathbf{x}(t),\\mathbf{u}(t))\\,dt\n\\;\\approx\\;\nh_k \\sum_{j=1}^q b_j\\, \\mathbf{f}\\!\\big(\\mathbf{x}(t_k+h_k\\tau_j),\\,\\mathbf{u}(t_k+h_k\\tau_j)\\big),\n\nso the places where we “pay” running cost are the same places where we “account” for state changes. Path constraints and bounds are then enforced at the same interior times. In the infinite-horizon discounted case, the same formulas apply with an extra factor e^{-\\rho(t_k+h_k\\tau_j)} multiplying the weights in the cost.\n\nThe values \\mathbf{x}(t_k+h_k\\tau_j) and \\mathbf{u}(t_k+h_k\\tau_j) do not exist a priori. We create them by a finite representation. One option is shooting: parameterize \\mathbf{u} on the mesh, integrate the ODE across each window with a chosen numerical step, and read interior values from that step. Another is collocation: represent \\mathbf{x} inside each window by a local polynomial and choose its coefficients so that the ODE holds at the interior nodes. Both constructions lead to the same structure: a nonlinear program whose objective is a composite quadrature of the running term (plus any terminal term in the Bolza case) and whose constraints are algebraic relations that encode the ODE and the pointwise inequalities at the selected nodes.\n\nSpecific choices recover familiar schemes. If we use the left endpoint as the single interior node, we obtain the forward Euler transcription. If we use both endpoints with equal weights, we obtain the trapezoidal transcription. Higher-order rules arise when we include interior nodes and richer polynomials for \\mathbf{x}. What matters here is the unifying picture: choose nodes, translate integrals into weighted sums, and couple those evaluations to a finite trajectory representation so that cost and physics are enforced at the same places. This is the organizing idea that will guide the rest of the chapter.","type":"content","url":"/collocation#direct-transcription-methods","position":3},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl3":"Discretizing cost and dynamics together","lvl2":"Direct Transcription Methods"},"type":"lvl3","url":"/collocation#discretizing-cost-and-dynamics-together","position":4},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl3":"Discretizing cost and dynamics together","lvl2":"Direct Transcription Methods"},"content":"In a continuous-time OCP, integrals appear twice: in the objective, which accumulates running cost over time, and implicitly in the dynamics, since state changes over any interval are the integral of the vector field. To compute, we must approximate both the integrals and the unknown functions \\mathbf{x}(t) and \\mathbf{u}(t) with finitely many numbers that an optimizer can manipulate.\n\nA natural way to do this is to lay down a finite set of time points (a mesh) over the horizon. You can think of the mesh as a grid we overlay on the “true” trajectories that exist as mathematical objects but are not directly accessible. Our aim is to approximate those trajectories and their integrals using values and simple models tied to the mesh. Using the same mesh for both the cost and the dynamics keeps the representation coherent: we evaluate what we pay and how the state changes at consistent times.\n\nConcretely, we begin by choosing a mesht_0<t_1<\\cdots<t_N=t_f,\\qquad h_k:=t_{k+1}-t_k .\n\nThe running cost is additive over disjoint intervals. When the horizon [t_0,t_f] is partitioned by the mesh, additivity (linearity) of the integral gives\\int_{t_0}^{t_f} c(\\mathbf{x}(t),\\mathbf{u}(t))\\,dt\n\\;=\\; \\sum_{k=0}^{N-1} \\int_{t_k}^{t_{k+1}} c(\\mathbf{x}(t),\\mathbf{u}(t))\\,dt .\n\nThis identity is exact: it is just the additivity (linearity) of the Lebesgue/Riemann integral over a partition. No approximation has been made yet. Approximations enter only when we later replace each window integral by a quadrature rule: a finite set of nodes and positive weights prescribing an integral approximation. This sets the table for three important ingredients that we will use throughout the chapter.\n\nFirst, it turns a global object into local contributions that live on each [t_k,t_{k+1}]. Numerical integration is most effective when it is composite: we approximate each small interval integral and then sum the results. Doing so controls error uniformly, because the global quadrature error is the accumulation of local errors that shrink with the step size. It also allows non-uniform steps h_k=t_{k+1}-t_k, which we will use later for mesh refinement.\n\nSecond, the split aligns the cost with the local dynamics constraints. On each interval the ODE can be written, by the fundamental theorem of calculus, as\\mathbf{x}(t_{k+1})-\\mathbf{x}(t_k)=\\int_{t_k}^{t_{k+1}} \\mathbf{f}(\\mathbf{x}(t),\\mathbf{u}(t))\\,dt.\n\nWhen we approximate this integral, we introduce interior evaluation points t_{k,j}\\in[t_k,t_{k+1}]. Using the same points in the cost and in the dynamics ties \\mathbf{x} and \\mathbf{u} together coherently: the places where we “pay” for running cost are also the places where we enforce the ODE. This avoids a mismatch between where we approximate the objective and where we impose feasibility.\n\nThird, the decomposition yields a nonlinear program with sparse structure. Each interval contributes a small block to the objective and constraints that depends only on variables from that interval (and its endpoints). Modern solvers exploit this banded sparsity to scale to long horizons.\n\nWith the split justified, we standardize the approximation. Map each interval to a reference domain via t=t_k+h_k\\tau with \\tau\\in[0,1] and dt=h_k\\,d\\tau. A quadrature rule on [0,1] is specified by evaluation points \\{\\tau_j\\}_{j=1}^q \\subset [0,1] and positive weights \\{w_j\\}_{j=1}^q such that, for a smooth \\phi,\\int_0^1 \\phi(\\tau)\\,d\\tau \\;\\approx\\; \\sum_{j=1}^q w_j\\,\\phi(\\tau_j).\n\nApplying it on each interval gives\\int_{t_k}^{t_{k+1}} c(\\mathbf{x}(t),\\mathbf{u}(t))\\,dt\n\\;\\approx\\;\nh_k\\sum_{j=1}^q w_j\\, c\\!\\big(\\mathbf{x}(t_k+h_k\\tau_j),\\,\\mathbf{u}(t_k+h_k\\tau_j)\\big).\n\nSumming these window contributions gives a composite approximation of the integral over [t_0,t_f]:\\int_{t_0}^{t_f} c(\\mathbf{x}(t),\\mathbf{u}(t))\\,dt\n\\;\\approx\\;\n\\sum_{k=0}^{N-1} h_k \\sum_{j=1}^q w_j\\, c\\!\\big(\\mathbf{x}(t_{k,j}),\\mathbf{u}(t_{k,j})\\big).\n\nThe outer index k indicates the window; the inner index i indicates the samples within that window; the factor h_k appears from the change of variables.\n\nThe dynamics admit the same treatment. By the fundamental theorem of calculus,\\mathbf{x}(t_{k+1})-\\mathbf{x}(t_k)\n=\\int_{t_k}^{t_{k+1}} \\dot{\\mathbf{x}}(t)\\,dt\n=\\int_{t_k}^{t_{k+1}} \\mathbf{f}(\\mathbf{x}(t),\\mathbf{u}(t))\\,dt .\n\nReplacing this integral by a quadrature rule that uses the same nodes produces the window defect relation\\mathbf{x}_{k+1}-\\mathbf{x}_k\n\\;\\approx\\;\nh_k\\sum_{j=1}^q b_j\\, \\mathbf{f}\\!\\big(\\mathbf{x}(t_{k,j}),\\mathbf{u}(t_{k,j})\\big),\n\nwhere \\{b_j\\} are the weights used for the ODE. Path constraints \\mathbf{g}(\\mathbf{x}(t),\\mathbf{u}(t))\\le 0 are imposed at selected nodes t_{k,j} in the same spirit. Using the same evaluation points for cost and dynamics keeps the representation coherent: we “pay” running cost and “account” for state changes at the same times.","type":"content","url":"/collocation#discretizing-cost-and-dynamics-together","position":5},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl4":"On the choice of interior points","lvl3":"Discretizing cost and dynamics together","lvl2":"Direct Transcription Methods"},"type":"lvl4","url":"/collocation#on-the-choice-of-interior-points","position":6},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl4":"On the choice of interior points","lvl3":"Discretizing cost and dynamics together","lvl2":"Direct Transcription Methods"},"content":"Once we select a mesh t_0<\\cdots<t_N and interior fractions \\{\\tau_j\\}_{j=1}^q per window [t_k,t_{k+1}], we need \\mathbf{x}(t_{k,j}) and \\mathbf{u}(t_{k,j}) at the evaluation times t_{k,j} := t_k + h_k\\tau_j. These values do not preexist. They come from one of two constructions that align with the standard quadrature taxonomy: step-function based and interpolating-function based rules.","type":"content","url":"/collocation#on-the-choice-of-interior-points","position":7},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl5":"Step-function based construction (piecewise constants; rectangle or midpoint)","lvl4":"On the choice of interior points","lvl3":"Discretizing cost and dynamics together","lvl2":"Direct Transcription Methods"},"type":"lvl5","url":"/collocation#step-function-based-construction-piecewise-constants-rectangle-or-midpoint","position":8},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl5":"Step-function based construction (piecewise constants; rectangle or midpoint)","lvl4":"On the choice of interior points","lvl3":"Discretizing cost and dynamics together","lvl2":"Direct Transcription Methods"},"content":"Here we approximate the relevant time functions by step functions on each window. For controls, a common choice is piecewise-constant:\\mathbf{u}(t)=\\mathbf{u}_k\\quad\\text{for }t\\in[t_k,t_{k+1}].\n\nFor the running cost and the vector field, the corresponding quadrature is a rectangle rule on [t_k,t_{k+1}]. Using the left endpoint gives\\int_{t_k}^{t_{k+1}} c(\\mathbf{x}(t),\\mathbf{u}(t))\\,dt\n\\;\\approx\\; h_k\\,c(\\mathbf{x}_k,\\mathbf{u}_k),\n\nand replacing the dynamics integral by the same step-function idea yields the forward Euler relation\\mathbf{x}_{k+1}=\\mathbf{x}_k+h_k\\,\\mathbf{f}(\\mathbf{x}_k,\\mathbf{u}_k,t_k).\n\nIf we prefer the midpoint rectangle rule, we sample at t_{k+\\frac12}=t_k+\\tfrac{h_k}{2}. In practice we then generate \\mathbf{x}_{k+\\frac12} by a half-step of the chosen integrator, and set \\mathbf{u}_{k+\\frac12}=\\mathbf{u}_k (piecewise-constant) or an average if we allow a short linear segment. Either way, interior values come from integrating forward given a step-function model for \\mathbf{u} and a rectangle-rule view of the integrals. This is the shooting viewpoint. Single shooting keeps only control parameters as decision variables; multiple shooting adds the window-start states and enforces step consistency.","type":"content","url":"/collocation#step-function-based-construction-piecewise-constants-rectangle-or-midpoint","position":9},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl5":"Interpolating-function based construction (low-order polynomials; trapezoid, Simpson, Gauss/Radau/Lobatto)","lvl4":"On the choice of interior points","lvl3":"Discretizing cost and dynamics together","lvl2":"Direct Transcription Methods"},"type":"lvl5","url":"/collocation#interpolating-function-based-construction-low-order-polynomials-trapezoid-simpson-gauss-radau-lobatto","position":10},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl5":"Interpolating-function based construction (low-order polynomials; trapezoid, Simpson, Gauss/Radau/Lobatto)","lvl4":"On the choice of interior points","lvl3":"Discretizing cost and dynamics together","lvl2":"Direct Transcription Methods"},"content":"Here we approximate time functions by polynomials on each window. If we interpolate \\mathbf{x}(t) linearly between endpoints, the cost naturally uses the trapezoidal rule\\int_{t_k}^{t_{k+1}} c\\,dt\\;\\approx\\;\\tfrac{h_k}{2}\\big[c(\\mathbf{x}_k,\\mathbf{u}_k)+c(\\mathbf{x}_{k+1},\\mathbf{u}_{k+1})\\big],\n\nand the dynamics use the matched trapezoidal defect\\mathbf{x}_{k+1}=\\mathbf{x}_k+\\tfrac{h_k}{2}\\Big[\\mathbf{f}(\\mathbf{x}_k,\\mathbf{u}_k,t_k)+\\mathbf{f}(\\mathbf{x}_{k+1},\\mathbf{u}_{k+1},t_{k+1})\\Big].\n\nWith a quadratic interpolation that includes the midpoint, Simpson’s rule appears in the cost and the Hermite–Simpson relations tie \\mathbf{x}_{k+\\frac12} to endpoint values and slopes. More generally, collocation chooses interior nodes on [t_k,t_{k+1}] (equally spaced gives Newton–Cotes like trapezoid or Simpson; Gaussian points give Gauss, Radau, or Lobatto schemes) and enforces the ODE at those nodes:\\frac{d}{dt}\\mathbf{x}(t_{k,j})=\\mathbf{f}\\!\\big(\\mathbf{x}(t_{k,j}),\\mathbf{u}(t_{k,j}),t_{k,j}\\big),\n\nwith continuity at endpoints. The interior values \\mathbf{x}(t_{k,j}) are evaluations of the decision polynomials; \\mathbf{u}(t_{k,j}) follows from the chosen control interpolation (constant, linear, or quadratic). The running cost is evaluated by the same interpolatory quadrature at the same nodes, which keeps “where we pay” aligned with “where we enforce.”","type":"content","url":"/collocation#interpolating-function-based-construction-low-order-polynomials-trapezoid-simpson-gauss-radau-lobatto","position":11},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl2":"Polynomial Interpolation"},"type":"lvl2","url":"/collocation#polynomial-interpolation","position":12},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl2":"Polynomial Interpolation"},"content":"We often want to construct a function that passes through a given set of points. For example, suppose we know a function should satisfy:f(x_0) = y_0, \\quad f(x_1) = y_1, \\quad \\dots, \\quad f(x_m) = y_m.\n\nThese are called interpolation constraints. Our goal is to find a function f(x) that satisfies all of them exactly.\n\nTo make the problem tractable, we restrict ourselves to a class of functions. In polynomial interpolation, we assume that f(x) is a polynomial of degree at most N. That means we are trying to find coefficients c_0, \\dots, c_N such thatf(x) = \\sum_{n=0}^N c_n \\, \\phi_n(x),\n\nwhere the functions \\phi_n(x) form a basis for the space of polynomials. The most common choice is the monomial basis, where \\phi_n(x) = x^n. This gives:f(x) = c_0 + c_1 x + c_2 x^2 + \\dots + c_N x^N.\n\nOther valid bases include Legendre, Chebyshev, and Lagrange polynomials, each chosen for specific numerical properties. But all span the same function space.\n\nTo find a unique solution, we need the number of unknowns (the c_n) to match the number of constraints. Since a degree-N polynomial has N+1 coefficients, we need:N + 1 = m + 1 \\quad \\Rightarrow \\quad N = m.\n\nSo if we want a function that passes through 4 points, we need a cubic polynomial (N = 3). Choosing a higher degree than necessary would give us infinitely many solutions; a lower degree may make the problem unsolvable.","type":"content","url":"/collocation#polynomial-interpolation","position":13},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl4":"Solving for the Coefficients (Monomial Basis)","lvl2":"Polynomial Interpolation"},"type":"lvl4","url":"/collocation#solving-for-the-coefficients-monomial-basis","position":14},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl4":"Solving for the Coefficients (Monomial Basis)","lvl2":"Polynomial Interpolation"},"content":"If we fix the basis functions to be monomials, we can build a system of equations by plugging in each x_i into f(x). This gives:\\begin{aligned}\nf(x_0) &= c_0 + c_1 x_0 + c_2 x_0^2 + \\dots + c_N x_0^N = y_0 \\\\\nf(x_1) &= c_0 + c_1 x_1 + c_2 x_1^2 + \\dots + c_N x_1^N = y_1 \\\\\n&\\vdots \\\\\nf(x_m) &= c_0 + c_1 x_m + c_2 x_m^2 + \\dots + c_N x_m^N = y_m\n\\end{aligned}\n\nThis system can be written in matrix form as:\\begin{bmatrix}\n1 & x_0 & x_0^2 & \\cdots & x_0^N \\\\\n1 & x_1 & x_1^2 & \\cdots & x_1^N \\\\\n\\vdots & \\vdots & \\vdots & & \\vdots \\\\\n1 & x_m & x_m^2 & \\cdots & x_m^N\n\\end{bmatrix}\n\\begin{bmatrix}\nc_0 \\\\ c_1 \\\\ \\vdots \\\\ c_N\n\\end{bmatrix}\n=\n\\begin{bmatrix}\ny_0 \\\\ y_1 \\\\ \\vdots \\\\ y_m\n\\end{bmatrix}\n\nThe matrix on the left is called the Vandermonde matrix. Solving this system gives the coefficients c_n that define the interpolating polynomial.","type":"content","url":"/collocation#solving-for-the-coefficients-monomial-basis","position":15},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl4":"Using a Different Basis","lvl2":"Polynomial Interpolation"},"type":"lvl4","url":"/collocation#using-a-different-basis","position":16},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl4":"Using a Different Basis","lvl2":"Polynomial Interpolation"},"content":"We don’t have to use monomials. We can pick any set of basis functions \\phi_n(x), such as Chebyshev or Fourier modes, and follow the same steps. The interpolating function becomes:f(x) = \\sum_{n=0}^N c_n \\, \\phi_n(x),\n\nand each interpolation constraint becomes:f(x_i) = \\sum_{n=0}^N c_n \\, \\phi_n(x_i) = y_i.\n\nAssembling these into a system gives:\\begin{bmatrix}\n\\phi_0(x_0) & \\phi_1(x_0) & \\dots & \\phi_N(x_0) \\\\\n\\phi_0(x_1) & \\phi_1(x_1) & \\dots & \\phi_N(x_1) \\\\\n\\vdots & \\vdots & & \\vdots \\\\\n\\phi_0(x_m) & \\phi_1(x_m) & \\dots & \\phi_N(x_m)\n\\end{bmatrix}\n\\begin{bmatrix}\nc_0 \\\\ c_1 \\\\ \\vdots \\\\ c_N\n\\end{bmatrix}\n=\n\\begin{bmatrix}\ny_0 \\\\ y_1 \\\\ \\vdots \\\\ y_m\n\\end{bmatrix}\n\nFrom here, we solve as before and reconstruct f(x).","type":"content","url":"/collocation#using-a-different-basis","position":17},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl4":"Derivative Constraints","lvl2":"Polynomial Interpolation"},"type":"lvl4","url":"/collocation#derivative-constraints","position":18},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl4":"Derivative Constraints","lvl2":"Polynomial Interpolation"},"content":"Sometimes, instead of a value constraint f(x_i) = y_i, we want to impose a slope constraint f'(x_i) = s_i. This is common in applications like spline interpolation or collocation methods, where derivative information is available from an ODE.\n\nSincef(x) = \\sum_{n=0}^N c_n \\phi_n(x) \\quad \\Rightarrow \\quad f'(x) = \\sum_{n=0}^N c_n \\phi_n'(x),\n\nwe can directly write the slope constraint:f'(x_i) = \\sum_{n=0}^N c_n \\phi_n'(x_i) = s_i.\n\nTo enforce this, we replace one of the interpolation equations in our system with this slope constraint. The resulting system still has m+1 equations and N+1 = m+1 unknowns.\n\nConcretely, suppose we have k+1 value constraints at nodes X=\\{x_0,\\ldots,x_k\\} with values Y=\\{y_0,\\ldots,y_k\\} and r slope constraints at nodes Z=\\{z_1,\\ldots,z_r\\} with slopes S=\\{s_1,\\ldots,s_r\\}, with k+1+r=N+1. The linear system for the coefficients \\mathbf{c}=[c_0,\\ldots,c_N]^\\top is\\begin{bmatrix}\n\\phi_0(x_0) & \\phi_1(x_0) & \\cdots & \\phi_N(x_0) \\\\\n\\vdots & \\vdots & & \\vdots \\\\\n\\phi_0(x_k) & \\phi_1(x_k) & \\cdots & \\phi_N(x_k) \\\\\n\\phi_0'(z_1) & \\phi_1'(z_1) & \\cdots & \\phi_N'(z_1) \\\\\n\\vdots & \\vdots & & \\vdots \\\\\n\\phi_0'(z_r) & \\phi_1'(z_r) & \\cdots & \\phi_N'(z_r)\n\\end{bmatrix}\n\\begin{bmatrix}\nc_0 \\\\ c_1 \\\\ \\vdots \\\\ c_N\n\\end{bmatrix}\n=\n\\begin{bmatrix}\ny_0 \\\\ \\vdots \\\\ y_k \\\\ s_1 \\\\ \\vdots \\\\ s_r\n\\end{bmatrix}.\n\nIf a value and a slope are imposed at the same node, take z_j=x_i and include both the value row and the derivative row; the system remains square. In the monomial basis, the top block is the Vandermonde matrix and the derivative block has entries n\\,x^{\\,n-1}. Once solved for \\mathbf{c}, reconstructf(x)=\\sum_{n=0}^N c_n\\,\\phi_n(x).","type":"content","url":"/collocation#derivative-constraints","position":19},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl4":"Interpolating ODE Trajectories (Collocation)","lvl2":"Polynomial Interpolation"},"type":"lvl4","url":"/collocation#interpolating-ode-trajectories-collocation","position":20},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl4":"Interpolating ODE Trajectories (Collocation)","lvl2":"Polynomial Interpolation"},"content":"Having established the general framework of interpolation, we now apply these concepts to the specific context of approximating trajectories governed by ordinary differential equations.  The idea of applying polynomial interpolation with derivative constraints yields a method known as “collocation”. More precisely, a degree-s collocation method is a way to discretize an ordinary differential equation (ODE) by approximating the solution on each time interval with a polynomial of degree s, and then enforcing that this polynomial satisfies the ODE exactly at s carefully chosen points (the collocation nodes).\n\nConsider a dynamical system described by the ordinary differential equation:\\dot{\\mathbf{x}}(t) = \\mathbf{f}(\\mathbf{x}(t),\\mathbf{u}(t),t)\n\nwhere \\mathbf{x}(t) represents the state trajectory and \\mathbf{u}(t) denotes the control input.\nLet us focus on a single mesh interval [t_k,t_{k+1}] with step size h_k := t_{k+1}-t_k. To work with a standardized domain, we introduce the transformation t = t_k + h_k\\tau that maps the physical interval [t_k,t_{k+1}] to the reference interval [0,1]. On this reference interval, we select a set of collocation nodes \\{\\tau_j\\}_{j=0}^K \\subset [0,1].\n\nOur goal is now to approximate the unknown trajectory using a polynomial of degree K. Using a monomial basis, we represent (parameterize) our trajectory as:\\mathbf{x}_h(\\tau) := \\sum_{n=0}^K \\mathbf{a}_n\\,\\tau^n\n\nwhere \\mathbf{a}_n \\in \\mathbb{R}^d are coefficient vectors to be determined.\nCollocation enforces the differential equation at a chosen set of nodes on [0,1]. Depending on the node family, these nodes may be interior-only or may include one or both endpoints. With the polynomial state model, we can differentiate analytically. Using the change of variables t=t_k+h_k\\,\\tau, we obtain:\\dot{\\mathbf{x}}_h(t_k+h_k\\,\\tau_j) = \\frac{1}{h_k} \\sum_{n=1}^K n\\,\\mathbf{a}_n\\,\\tau_j^{n-1}\n\nThe collocation condition requires that this polynomial derivative equals the right-hand side of the ODE at each collocation node \\tau_j:\\frac{1}{h_k} \\sum_{n=1}^K n\\,\\mathbf{a}_n\\,\\tau_j^{n-1} = \\mathbf{f}\\left( \\sum_{n=0}^K \\mathbf{a}_n\\,\\tau_j^{n},\\ \\mathbf{u}_j,\\ t_k+h_k\\,\\tau_j \\right), \\quad \\text{for each collocation node } \\tau_j.\n\nwhere \\mathbf{u}_j represents the control value at node \\tau_j.","type":"content","url":"/collocation#interpolating-ode-trajectories-collocation","position":21},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl5":"Boundary Conditions and Node Families","lvl4":"Interpolating ODE Trajectories (Collocation)","lvl2":"Polynomial Interpolation"},"type":"lvl5","url":"/collocation#boundary-conditions-and-node-families","position":22},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl5":"Boundary Conditions and Node Families","lvl4":"Interpolating ODE Trajectories (Collocation)","lvl2":"Polynomial Interpolation"},"content":"\n\n#  label: cocp-cell-01\n#  caption: Rendered output from the preceding code cell.\n\n%config InlineBackend.figure_format = 'retina'\nimport matplotlib.pyplot as plt\n\n# Apply book style\ntry:\n    import scienceplots\n    plt.style.use(['science', 'notebook'])\nexcept (ImportError, OSError):\n    pass  # Use matplotlib defaults\nimport numpy as np\nfrom matplotlib.patches import Rectangle\nimport matplotlib.patches as mpatches\nfrom scipy.interpolate import CubicSpline\n\n# Set up the figure with subplots\nfig = plt.figure(figsize=(16, 12))\n\n# Create a 2x2 subplot layout\ngs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n\ndef create_collocation_plot(ax, title, node_positions, slope_nodes, endpoint_types):\n    \"\"\"\n    Create a collocation method illustration\n    \n    Parameters:\n    - ax: matplotlib axis\n    - title: plot title\n    - node_positions: list of x-positions for nodes (normalized 0-1)\n    - slope_nodes: list of booleans indicating which nodes enforce slopes\n    - endpoint_types: tuple of (left_type, right_type) where type is 'slope', 'eval', or 'continuity'\n    \"\"\"\n    \n    # Time interval\n    t_start, t_end = 0, 1\n    \n    # Create a smooth trajectory curve for illustration\n    x = np.linspace(t_start, t_end, 100)\n    # Create an S-shaped curve to represent state trajectory\n    y = 0.3 + 0.4 * np.sin(3 * np.pi * x) * np.exp(-2 * x)\n    \n    # Build a cubic spline of the trajectory for consistent slope evaluation\n    spline = CubicSpline(x, y, bc_type='natural')\n    \n    # Plot the trajectory\n    ax.plot(x, y, 'k-', linewidth=2, label='State trajectory x(t)')\n    \n    # Plot collocation points\n    for i, pos in enumerate(node_positions):\n        t_node = t_start + pos * (t_end - t_start)\n        y_node = 0.3 + 0.4 * np.sin(3 * np.pi * t_node) * np.exp(-2 * t_node)\n        \n        # Endpoints are rendered in the endpoint section (as squares). Skip here.\n        if np.isclose(t_node, 0.0) or np.isclose(t_node, 1.0):\n            continue\n\n        if slope_nodes[i]:\n            # Blue dot for slope constraint nodes\n            ax.plot(t_node, y_node, 'bo', markersize=8, markerfacecolor='blue', \n                   markeredgecolor='darkblue', linewidth=1.5)\n            # Add tangent line to show slope constraint (centered on node)\n            dt = 0.08  # Half-length for symmetric extension\n            t_prev = max(0, t_node - dt)\n            t_next = min(1, t_node + dt)\n            \n            # Calculate slope from the spline derivative (matches plotted curve)\n            slope = spline.derivative()(t_node)\n            \n            # Create symmetric tangent line centered on the node\n            y_prev = y_node + slope * (t_prev - t_node)\n            y_next = y_node + slope * (t_next - t_node)\n            ax.plot([t_prev, t_next], [y_prev, y_next], 'r--', alpha=0.8, linewidth=2)\n        else:\n            # Green dot for evaluation-only nodes\n            ax.plot(t_node, y_node, 'go', markersize=8, markerfacecolor='lightgreen', \n                   markeredgecolor='darkgreen', linewidth=1.5)\n    \n    # Handle endpoints specially (always render as squares if applicable)\n    endpoints = [(0, 'left'), (1, 'right')]\n    for pos, side in endpoints:\n        y_end = 0.3 + 0.4 * np.sin(3 * np.pi * pos) * np.exp(-2 * pos)\n        end_type = endpoint_types[0] if side == 'left' else endpoint_types[1]\n        \n        if end_type == 'slope':\n            ax.plot(pos, y_end, 'bs', markersize=10, markerfacecolor='blue', \n                   markeredgecolor='darkblue', linewidth=2)\n            # Add tangent line (centered on endpoint)\n            dt = 0.08  # Half-length for symmetric extension\n            \n            # Calculate slope from the spline derivative (matches plotted curve)\n            slope = spline.derivative()(pos)\n            \n            t_prev = pos - dt\n            t_next = pos + dt\n            y_prev = y_end + slope * (t_prev - pos)\n            y_next = y_end + slope * (t_next - pos)\n            ax.plot([t_prev, t_next], [y_prev, y_next], 'r--', alpha=0.8, linewidth=2)\n        elif end_type == 'eval':\n            ax.plot(pos, y_end, 'gs', markersize=10, markerfacecolor='lightgreen', \n                   markeredgecolor='darkgreen', linewidth=2)\n        elif end_type == 'continuity':\n            ax.plot(pos, y_end, 'ms', markersize=10, markerfacecolor='orange', \n                   markeredgecolor='darkorange', linewidth=2)\n    \n    # Add time markers\n    ax.axvline(x=0, color='gray', linestyle=':', alpha=0.5)\n    ax.axvline(x=1, color='gray', linestyle=':', alpha=0.5)\n    ax.text(0, -0.15, r'$t_k$', ha='center', va='top', fontsize=12)\n    ax.text(1, -0.15, r'$t_{k+1}$', ha='center', va='top', fontsize=12)\n    \n    # Formatting\n    ax.set_xlim(-0.1, 1.1)\n    ax.set_ylim(-0.2, 0.8)\n    ax.set_xlabel('Time', fontsize=12)\n    ax.set_ylabel('State', fontsize=12)\n    ax.set_title(title, fontsize=14, fontweight='bold')\n    ax.grid(True, alpha=0.3)\n\n# Define node positions for each method (normalized to [0,1])\n# Using 4 nodes total for fair comparison\n\n# Lobatto IIIA nodes (includes both endpoints)\nlobatto_nodes = [0.0, 0.276, 0.724, 1.0]\nlobatto_slopes = [True, True, True, True]\nlobatto_endpoints = ('slope', 'slope')\n\n# Radau IA nodes (includes left endpoint)\nradau1_nodes = [0.0, 0.155, 0.645, 0.955]\nradau1_slopes = [True, True, True, True]  # Collocation at left endpoint and interior nodes\nradau1_endpoints = ('slope', 'eval')  # Left: slope, Right: evaluation-only\n\n# Radau IIA nodes (includes right endpoint)\nradau2_nodes = [0.045, 0.355, 0.845, 1.0]\nradau2_slopes = [True, True, True, True]  # Collocation at interior nodes and right endpoint\nradau2_endpoints = ('continuity', 'slope')  # Left: continuity, Right: slope\n\n# Gauss nodes (no endpoints)\ngauss_nodes = [0.113, 0.387, 0.613, 0.887]\ngauss_slopes = [True, True, True, True]\ngauss_endpoints = ('eval', 'eval')\n\n# Create subplots\nax1 = fig.add_subplot(gs[0, 0])\ncreate_collocation_plot(ax1, 'Lobatto IIIA Method', lobatto_nodes, lobatto_slopes, lobatto_endpoints)\n\nax2 = fig.add_subplot(gs[0, 1])\ncreate_collocation_plot(ax2, 'Radau IA Method', radau1_nodes, radau1_slopes, radau1_endpoints)\n\nax3 = fig.add_subplot(gs[1, 0])\ncreate_collocation_plot(ax3, 'Radau IIA Method', radau2_nodes, radau2_slopes, radau2_endpoints)\n\nax4 = fig.add_subplot(gs[1, 1])\ncreate_collocation_plot(ax4, 'Gauss Method', gauss_nodes, gauss_slopes, gauss_endpoints)\n\n# Create legend\nlegend_elements = [\n    mpatches.Patch(color='blue', label='Slope constraint (f = dynamics)'),\n    mpatches.Patch(color='lightgreen', label='Polynomial evaluation only'),\n    mpatches.Patch(color='orange', label='Continuity constraint'),\n    plt.Line2D([0], [0], color='red', linestyle='--', alpha=0.7, label='Tangent (slope direction)'),\n    plt.Line2D([0], [0], color='black', linewidth=2, label='State trajectory')\n]\n\nfig.legend(handles=legend_elements, loc='lower center', bbox_to_anchor=(0.5, 0.03), \n           ncol=3, fontsize=12, frameon=True, fancybox=True, shadow=False)\n\n# Add main title\nfig.suptitle('Collocation Methods for Optimal Control\\n(Illustration of Node Types and Constraints)', \n             fontsize=16, fontweight='bold', y=0.95)\n\nplt.tight_layout(rect=[0.04, 0.10, 0.98, 0.93])\n\nThe choice of collocation nodes determines how boundary conditions are handled and affects the resulting discretization properties. Three standard families are commonly used: Labatto, Randau and and Gauss.\n\nLet’s consider these three setup with more generality over any given basis. We start again by taking a mesh interval [t_k,t_{k+1}] of length h_k=t_{k+1}-t_k and reparametrize time byt = t_k + h_k\\,\\tau,\\qquad \\tau\\in[0,1].\n\nWe then choose to represent the (unknown) state by a degree–K polynomial\\mathbf{x}_h(\\tau) \\;=\\; \\sum_{n=0}^{K}\\mathbf{a}_n\\,\\phi_n(\\tau),\n\nwhere \\{\\phi_n\\}_{n=0}^K is any linearly independent basis of polynomials of degree \\le K, and \\mathbf{a}_0,\\dots,\\mathbf{a}_K are vector coefficients to be determined.\n\nThe collocation condition  mean that we require for the chosen K collocation points that:\\frac{d}{dt}\\mathbf{x}_h\\bigl(\\tau_j\\bigr) \\;=\\; \\mathbf{f}\\bigl(\\mathbf{x}_h(\\tau_j),\\mathbf{u}_h(\\tau_j),t_k+h_k\\tau_j\\bigr),\n\\qquad j=0,1,\\dots,K,\n\nwhich, using \\tfrac{d}{dt}=(1/h_k)\\tfrac{d}{d\\tau}, becomes\\underbrace{\\tfrac{1}{h_k}\\mathbf{x}_h'(\\tau_j)}_{\\text{polynomial slope at node}}\n\\;=\\;\n\\underbrace{\\mathbf{f}\\!\\bigl(\\mathbf{x}_h(\\tau_j),\\mathbf{u}_h(\\tau_j),t_k+h_k\\tau_j\\bigr)}_{\\text{ODE slope at same point}},\n\\qquad j=0,\\dots,K.\n\nPut simply: choose some nodes inside the interval, and at each of those nodes force the slope of the polynomial approximation to match the slope prescribed by the ODE. What we mean by the expression “collocation conditions” is simply to say that we want to satisfy a set of “slope-matching equations” at the chosen nodes.\n\nBy definition of the mesh variables,\\mathbf{x}_k := \\mathbf{x}_h(0),\\qquad \\mathbf{x}_{k+1} := \\mathbf{x}_h(1),\n\nand (if you sample the control at endpoints)\\mathbf{u}_k := \\mathbf{u}_h(0),\\qquad \\mathbf{u}_{k+1} := \\mathbf{u}_h(1).\n\nWith the monomial basis,\\phi_n(0)=\\delta_{n0}\\ \\Rightarrow\\ \\mathbf{x}_h(0)=\\sum_{n=0}^K \\mathbf{a}_n \\phi_n(0)=\\mathbf{a}_0=\\mathbf{x}_k,\\phi_n(1)=1\\ \\Rightarrow\\ \\mathbf{x}_h(1)=\\sum_{n=0}^K \\mathbf{a}_n=\\mathbf{x}_{k+1}.\n\nFor the derivative, \\phi_n'(\\tau)=n\\,\\tau^{n-1}, so\\mathbf{x}_h'(0)=\\sum_{n=0}^K \\mathbf{a}_n\\,\\phi_n'(0)=\\mathbf{a}_1,\n\\qquad\n\\mathbf{x}_h'(1)=\\sum_{n=1}^K n\\,\\mathbf{a}_n.\n\nWhen chaining intervals into a global trajectory, direct collocation enforces state continuity by construction: the variable \\mathbf{x}_{k+1} at the end of one interval is the same as the starting variable of the next. What is not enforced automatically is slope continuity; the derivative at the end of one interval generally does not match the derivative at the start of the next. Different collocation methods may have different slope continuity properties depending on the chosen collocation nodes.\n\nLobatto Nodes (endpoints included):\n\nThe family of Labotto methods correspond to any set of so-called Lobatto nodes \\{\\tau_j\\}_{j=0}^K with the specificity that we require \\tau_0=0 and \\tau_K=1. Let’s assume that we work with the power (monomial) basis \\phi_n(\\tau)=\\tau^n, so that\\mathbf{x}_h(\\tau)=\\sum_{n=0}^{K}\\mathbf{a}_n\\,\\tau^n,\n\nDifferentiating \\mathbf{x}_h with respect to \\tau gives\\frac{d\\mathbf{x}_h}{d\\tau}(\\tau)=\\sum_{n=0}^{K}\\mathbf{a}_n\\,\\phi_n'(\\tau),\n\nSince we have the chain rule \\frac{d}{dt} = \\frac{1}{h_k}\\frac{d}{d\\tau} from the time transformation t = t_k + h_k\\tau, the time derivative becomes\\frac{d\\mathbf{x}_h}{dt}(t_k + h_k\\tau) = \\frac{1}{h_k}\\frac{d\\mathbf{x}_h}{d\\tau}(\\tau) = \\frac{1}{h_k}\\sum_{n=0}^{K}\\mathbf{a}_n\\,\\phi_n'(\\tau).\n\nso the collocation equations at Lobatto nodes are\\frac{1}{h_k}\\sum_{n=0}^{K}\\mathbf{a}_n\\,\\phi_n'(\\tau_j)\n\\;=\\;\n\\mathbf{f}\\!\\Bigl(\\sum_{n=0}^{K}\\mathbf{a}_n\\,\\phi_n(\\tau_j),\\ \\mathbf{u}_h(\\tau_j),\\ t_k+h_k\\tau_j\\Bigr),\n\\qquad j=0,1,\\dots,K.\n\nFor j=0 and j=K, these conditions become:\\frac{1}{h_k}\\sum_{n=0}^{K}\\mathbf{a}_n\\,\\phi_n'(0)\n\\;=\\;\n\\mathbf{f}\\!\\Bigl(\\sum_{n=0}^{K}\\mathbf{a}_n\\,\\phi_n(0),\\ \\mathbf{u}_h(0),\\ t_k\\Bigr),\n\\qquad \\text{(left endpoint)}\\frac{1}{h_k}\\sum_{n=0}^{K}\\mathbf{a}_n\\,\\phi_n'(1)\n\\;=\\;\n\\mathbf{f}\\!\\Bigl(\\sum_{n=0}^{K}\\mathbf{a}_n\\,\\phi_n(1),\\ \\mathbf{u}_h(1),\\ t_{k+1}\\Bigr),\n\\qquad \\text{(right endpoint)}\n\nWith the monomial basis \\phi_n(\\tau)=\\tau^n, we have \\phi_n'(0)=n\\delta_{n,1} (only \\phi_1'=1, others vanish) and \\phi_n'(1)=n. Also, \\phi_n(0)=\\delta_{n,0} and \\phi_n(1)=1 for all n. This simplifies the endpoint conditions to:\\frac{\\mathbf{a}_1}{h_k} = \\mathbf{f}(\\mathbf{a}_0, \\mathbf{u}_h(0), t_k) = \\mathbf{f}(\\mathbf{x}_k, \\mathbf{u}_k, t_k),\n\\qquad \\text{(left endpoint slope)}\\frac{1}{h_k}\\sum_{n=1}^{K}n\\,\\mathbf{a}_n = \\mathbf{f}\\!\\Bigl(\\sum_{n=0}^{K}\\mathbf{a}_n, \\mathbf{u}_h(1), t_{k+1}\\Bigr) = \\mathbf{f}(\\mathbf{x}_{k+1}, \\mathbf{u}_{k+1}, t_{k+1}),\n\\qquad \\text{(right endpoint slope)}\n\nThese equations enforce that the polynomial’s slope at both endpoints matches the ODE’s prescribed slope, which is why the figure shows red tangent lines at both endpoints for Lobatto methods.\n\nRadau Nodes (one endpoint included):\nRadau points include only one endpoint. Radau-I includes the left endpoint (\\tau_0 = 0) while Radau-II includes the right endpoint (\\tau_K = 1). This means that a radau collocation is defined by any set of collocation nodes such that \\tau_K = 1. This translates into requireing that we match the ODE over the mesh only at the right endpoiunt in addition to the interior nodes.  As a consequence, we leave the solution unconstrained to take any value on the left endpoint. When chaining up multiple intervals across a global solution, this may pose some complication as we will no longer be able to ensure continuity as the slope at one endpoitn need not match that of the next endpoint. (But could you have a situation where slopes match but the states don’t line up?)\n\nAt the included endpoint the ODE is enforced (slope shown in the figure), while at the other endpoint continuity links adjacent intervals. For Radau-I with K+1 points:\\mathbf{x}_k = \\mathbf{x}_h(0) = \\mathbf{a}_0\n\nThe endpoint \\mathbf{x}_{k+1} = \\mathbf{x}_h(1) = \\sum_{n=0}^K \\mathbf{a}_n is not directly constrained by a collocation condition, requiring separate continuity enforcement between intervals.\n\nGauss Nodes (endpoints excluded):\nGauss points exclude both endpoints, using only interior points \\tau_j \\in (0,1) for j = 1,\\ldots,K. The ODE is enforced only at interior nodes; both endpoints are handled through separate continuity constraints:\\mathbf{x}_k = \\mathbf{x}_h(0) = \\mathbf{a}_0\\mathbf{x}_{k+1} = \\mathbf{x}_h(1) = \\sum_{n=0}^K \\mathbf{a}_n\n\nOrigins and Selection Criteria:\nThese node families derive from orthogonal polynomial theory. Gauss nodes correspond to roots of Legendre polynomials and provide optimal quadrature accuracy for smooth integrands. Radau nodes are roots of modified Legendre polynomials with one endpoint constraint, while Lobatto nodes include both endpoints and correspond to roots of derivatives of Legendre polynomials.\n\nFor optimal control applications, Radau-II nodes are often preferred because they provide implicit time-stepping behavior and good stability properties. Lobatto nodes simplify boundary condition handling but may require smaller time steps. Gauss nodes offer highest quadrature accuracy but complicate endpoint treatment.","type":"content","url":"/collocation#boundary-conditions-and-node-families","position":23},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl5":"Control Parameterization and Cost Integration","lvl4":"Interpolating ODE Trajectories (Collocation)","lvl2":"Polynomial Interpolation"},"type":"lvl5","url":"/collocation#control-parameterization-and-cost-integration","position":24},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl5":"Control Parameterization and Cost Integration","lvl4":"Interpolating ODE Trajectories (Collocation)","lvl2":"Polynomial Interpolation"},"content":"The control inputs can be handled with similar polynomial approximations. We may use piecewise-constant controls, piecewise-linear controls, or higher-order polynomial parameterizations of the form:\\mathbf{u}_h(\\tau) = \\sum_{n=0}^{K_u} \\mathbf{b}_n\\,\\tau^n\n\nwhere \\mathbf{u}_j = \\mathbf{u}_h(\\tau_j) represents the control values at each collocation point. This polynomial framework extends to cost function evaluation, where running costs are integrated using the same quadrature nodes and weights:\\int_{t_k}^{t_{k+1}} c\\,dt \\approx h_k\\sum_{j=0}^K w_j\\, c\\big(\\mathbf{x}_h(\\tau_j),\\mathbf{u}_h(\\tau_j), t_k+h_k\\,\\tau_j\\big)","type":"content","url":"/collocation#control-parameterization-and-cost-integration","position":25},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl2":"A Compendium of Direct Transcription Methods in Trajectory Optimization"},"type":"lvl2","url":"/collocation#a-compendium-of-direct-transcription-methods-in-trajectory-optimization","position":26},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl2":"A Compendium of Direct Transcription Methods in Trajectory Optimization"},"content":"The mesh and interior nodes are the common scaffold. What distinguishes one transcription from another is how we obtain values at those nodes and how we approximate the two integrals that appear implicitly and explicitly: the integral of the running cost and the integral that carries the state forward. In other words, we now commit to two design choices that mirror the previous section: a finite representation for \\mathbf{x}(t) and \\mathbf{u}(t) over each interval [t_i,t_{i+1}], and a quadrature rule whose nodes and weights are used consistently for both cost and dynamics. The result is always a sparse nonlinear program; the differences are in where we sample and how we tie samples together.\n\nBelow, each transcription should be read as “same grid, same interior points, same evaluations for cost and physics,” with only the local representation changing.","type":"content","url":"/collocation#a-compendium-of-direct-transcription-methods-in-trajectory-optimization","position":27},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl3":"Euler Collocation","lvl2":"A Compendium of Direct Transcription Methods in Trajectory Optimization"},"type":"lvl3","url":"/collocation#euler-collocation","position":28},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl3":"Euler Collocation","lvl2":"A Compendium of Direct Transcription Methods in Trajectory Optimization"},"content":"Work on one interval [t_k,t_{k+1}] of length h_k with the reparametrization t=t_k+h_k\\,\\tau, \\tau\\in[0,1]. Assume a degree 1 polynomial:\\mathbf{x}_h(\\tau)=\\sum_{n=0}^{1}\\mathbf{a}_n\\,\\phi_n(\\tau),\n\nfor any basis \\{\\phi_0,\\phi_1\\} of linear polynomials.\nEndpoint conditions give\\mathbf{x}_h(0)=\\mathbf{x}_k\\Rightarrow \\mathbf{a}_0=\\mathbf{x}_k,\\qquad\n\\mathbf{x}_h(1)=\\mathbf{x}_{k+1}\\Rightarrow \\mathbf{a}_1=\\mathbf{x}_{k+1}-\\mathbf{x}_k.\n\nby backsubstitution and because\\mathbf{x}_h(\\tau)=\\mathbf{a}_0+\\mathbf{a}_1\\,\\tau.\n\nFurthermore, the derivative with respect to \\tau is:\\frac{d}{d\\tau}\\mathbf{x}_h(\\tau)=\\mathbf{a}_1=\\mathbf{x}_{k+1}-\\mathbf{x}_k,\n\\qquad\n\\frac{d}{dt}=\\frac{1}{h_k}\\frac{d}{d\\tau}\n\\Rightarrow\n\\frac{d}{dt}\\mathbf{x}_h=\\frac{1}{h_k}\\,(\\mathbf{x}_{k+1}-\\mathbf{x}_k).\n\nBecause from the mapping t(\\tau)=t_k+h_k\\tau we can invert:\n\\tau(t)=\\frac{t-t_k}{h_k} and differentiating gives\\frac{d\\tau}{dt}=\\frac{1}{h_k}.\n\nThe collocation condition at a single Radau-II node \\tau=1:\\frac{1}{h_k}\\,\\mathbf{x}_h'(\\tau)\\Big|_{\\tau=1}\n\\;=\\;\n\\mathbf{f}\\!\\big(\\mathbf{x}_h(1),\\mathbf{u}_h(1),t_{k+1}\\big)\n\\;=\\;\n\\mathbf{f}\\!\\big(\\mathbf{x}_{k+1},\\mathbf{u}_{k+1},t_{k+1}\\big).\n\nBecause \\mathbf{x}_h is linear, \\mathbf{x}_h'(\\tau) is constant in \\tau, so \\mathbf{x}_h'(1)=\\mathbf{x}_h'(\\tau) for all \\tau. Moreover, linear interpolation between the two endpoints gives\\mathbf{x}_h'(\\tau)=\\mathbf{x}_{k+1}-\\mathbf{x}_k.\n\nSubstitute this into the collocation condition:\\frac{1}{h_k}\\big(\\mathbf{x}_{k+1}-\\mathbf{x}_k\\big) \\;=\\; \\mathbf{f}\\!\\big(\\mathbf{x}_{k+1},\\mathbf{u}_{k+1},t_{k+1}\\big),\n\nwhich is exactly the implicit Euler step{\\ \\mathbf{x}_{k+1}=\\mathbf{x}_k + h_k\\,\\mathbf{f}\\!\\big(\\mathbf{x}_{k+1},\\mathbf{u}_{k+1},t_{k+1}\\big)\\ }.\n\nThe overall direct transcription is then:\n\nImplicit–Euler Collocation (Radau-II, degree 1)\n\nLet t_0<\\cdots<t_N with h_i:=t_{i+1}-t_i. Decision variables are \\{\\mathbf{x}_i\\}_{i=0}^N, \\{\\mathbf{u}_i\\}_{i=0}^N. Solve\\begin{aligned}\n\\min\\ & c_T(\\mathbf{x}_N)\\;+\\;\\sum_{i=0}^{N-1} h_i\\,c(\\mathbf{x}_{i+1},\\mathbf{u}_{i+1})\\\\\n\\text{s.t.}\\ & \\mathbf{x}_{i+1}-\\mathbf{x}_i - h_i\\,\\mathbf{f}(\\mathbf{x}_{i+1},\\mathbf{u}_{i+1})=\\mathbf{0},\\quad i=0,\\ldots,N-1,\\\\\n& \\mathbf{g}(\\mathbf{x}_{i+1},\\mathbf{u}_{i+1})\\le \\mathbf{0},\\\\\n& \\mathbf{x}_{\\min}\\le \\mathbf{x}_i\\le \\mathbf{x}_{\\max},\\quad \\mathbf{u}_{\\min}\\le \\mathbf{u}_i\\le \\mathbf{u}_{\\max},\\\\\n& \\mathbf{x}_0=\\mathbf{x}(t_0).\n\\end{aligned}\n\nNote that:\n\nThe running cost and path constraints are evaluated at the same right-endpoint where the dynamics are enforced, keeping “where we pay” aligned with “where we enforce.”\n\nState continuity is automatic because \\mathbf{x}_{i+1} is a shared variable between adjacent intervals; slope continuity is not enforced unless you add it.\nHere’s an updated subsection that explicitly says what collocation nodes are chosen and why the trapezoidal defect uses them the way it does.\n\nSide remark. If you instead collocate at the left endpoint (Radau-I with \\tau=0) with the same linear model, you obtain \\frac{1}{h_k}(\\mathbf{x}_{k+1}-\\mathbf{x}_k)=\\mathbf{f}(\\mathbf{x}_k,\\mathbf{u}_k,t_k), i.e., the explicit Euler step. In that very precise sense, explicit Euler can be viewed as a (left-endpoint) degree-1 collocation scheme.\n\nExplicit–Euler Collocation (Radau-I, degree 1)\n\nLet t_0<\\cdots<t_N with h_i:=t_{i+1}-t_i. Decision variables are \\{\\mathbf{x}_i\\}_{i=0}^N and \\{\\mathbf{u}_i\\}_{i=0}^N. Solve\\begin{aligned}\n\\min\\ & c_T(\\mathbf{x}_N)\\;+\\;\\sum_{i=0}^{N-1} h_i\\,c(\\mathbf{x}_i,\\mathbf{u}_i)\\\\\n\\text{s.t.}\\ & \\mathbf{x}_{i+1}-\\mathbf{x}_i - h_i\\,\\mathbf{f}(\\mathbf{x}_i,\\mathbf{u}_i)=\\mathbf{0},\\quad i=0,\\ldots,N-1,\\\\\n& \\mathbf{g}(\\mathbf{x}_i,\\mathbf{u}_i)\\le \\mathbf{0},\\quad i=0,\\ldots,N-1,\\\\\n& \\mathbf{x}_{\\min}\\le \\mathbf{x}_i\\le \\mathbf{x}_{\\max},\\quad \\mathbf{u}_{\\min}\\le \\mathbf{u}_i\\le \\mathbf{u}_{\\max},\\\\\n& \\mathbf{x}_0=\\mathbf{x}(t_0).\n\\end{aligned}","type":"content","url":"/collocation#euler-collocation","position":29},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl3":"Trapezoidal collocation","lvl2":"A Compendium of Direct Transcription Methods in Trajectory Optimization"},"type":"lvl3","url":"/collocation#trapezoidal-collocation","position":30},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl3":"Trapezoidal collocation","lvl2":"A Compendium of Direct Transcription Methods in Trajectory Optimization"},"content":"In this scheme we take the two endpoints as the nodes on each interval:\\tau_0=0,\\qquad \\tau_1=1\\quad(\\text{``Lobatto with }K=1\").\n\nWe approximate \\mathbf{x} linearly over [t_i,t_{i+1}], and we evaluate both the running cost and the dynamics at these two nodes with equal weights. Because a linear polynomial has a constant derivative, we do not try to match the ODE’s slope at both endpoints (that would overconstrain a linear function). Instead, we enforce the ODE in its integral form over the interval and approximate the integral of \\mathbf{f} by the trapezoid rule using those two nodes. This makes the cost quadrature and the state-update (“defect”) use the same nodes and weights.\n\nTrapezoidal Collocation\n\nLet t_0<\\cdots<t_N with h_i:=t_{i+1}-t_i. Decision variables are \\{\\mathbf{x}_i\\}_{i=0}^N, \\{\\mathbf{u}_i\\}_{i=0}^N. Solve\\begin{aligned}\n\\min_{\\{\\mathbf{x}_i,\\mathbf{u}_i\\}}\\ & c_T(\\mathbf{x}_N)\\; +\\; \\sum_{i=0}^{N-1} \\tfrac{h_i}{2}\\,\\Big[c(\\mathbf{x}_i,\\mathbf{u}_i)+c(\\mathbf{x}_{i+1},\\mathbf{u}_{i+1})\\Big]\\\\\n\\text{s.t.}\\ & \\mathbf{x}_{i+1}-\\mathbf{x}_i \\;-\\; \\tfrac{h_i}{2}\\Big[\\mathbf{f}(\\mathbf{x}_i,\\mathbf{u}_i)+\\mathbf{f}(\\mathbf{x}_{i+1},\\mathbf{u}_{i+1})\\Big] \\;=\\; \\mathbf{0},\\quad i=0,\\ldots,N-1,\\\\\n& \\mathbf{g}(\\mathbf{x}_i,\\mathbf{u}_i) \\le \\mathbf{0},\\ \\ \\mathbf{g}(\\mathbf{x}_{i+1},\\mathbf{u}_{i+1}) \\le \\mathbf{0},\\\\\n& \\mathbf{x}_{\\min} \\le \\mathbf{x}_i \\le \\mathbf{x}_{\\max},\\ \\ \\mathbf{u}_{\\min} \\le \\mathbf{u}_i \\le \\mathbf{u}_{\\max},\\\\\n& \\mathbf{x}_0 = \\mathbf{x}(t_0).\n\\end{aligned}\n\nSummary: the collocation nodes for trapezoidal are the two endpoints; the state is linear on each interval; and the dynamics are enforced via the integrated ODE with the trapezoid rule at those two nodes, yielding the familiar trapezoidal defect.","type":"content","url":"/collocation#trapezoidal-collocation","position":31},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl3":"Hermite–Simpson (quadratic interpolation; midpoint included)","lvl2":"A Compendium of Direct Transcription Methods in Trajectory Optimization"},"type":"lvl3","url":"/collocation#hermite-simpson-quadratic-interpolation-midpoint-included","position":32},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl3":"Hermite–Simpson (quadratic interpolation; midpoint included)","lvl2":"A Compendium of Direct Transcription Methods in Trajectory Optimization"},"content":"On each interval [t_i,t_{i+1}] we pick three collocation nodes on the reference domain \\tau\\in[0,1]:\\tau_0=0,\\qquad \\tau_{1/2}=\\tfrac12,\\qquad \\tau_1=1.\n\nSo we evaluate at left, midpoint, right. These are the same three nodes used by Simpson’s rule (weights 1{:}4{:}1) for numerical quadrature. We let \\mathbf{x}_h be quadratic in \\tau. Two things then happen:\n\nIntegral (defect) enforcement with Simpson’s rule.\nWe enforce the ODE in integral form over the interval and approximate the integral of \\mathbf{f} with Simpson’s rule using the three nodes above. This yields the first constraint (the “Simpson defect”), which uses \\mathbf{f} evaluated at left, midpoint, and right.\n\nSlope matching at the midpoint (collocation).\nBecause a quadratic has limited shape, we don’t try to match slopes at both endpoints. Instead, we introduce midpoint variables (\\mathbf{x}_{i+\\frac12},\\mathbf{u}_{i+\\frac12}) and match the ODE at the midpoint. The second constraint below is exactly the midpoint collocation condition written in an equivalent Hermite form: it pins the midpoint state to the average of the endpoints plus a correction based on endpoint slopes, ensuring that the polynomial’s derivative is consistent with the ODE at \\tau=\\tfrac12.\n\nThis way, where we pay (Simpson quadrature) and where we enforce (midpoint collocation + Simpson defect) are aligned at the same three nodes, which is why the method is both accurate and well conditioned on smooth problems.\n\nHermite–Simpson Transcription\n\nLet t_0<\\cdots<t_N with h_i:=t_{i+1}-t_i and midpoints t_{i+\\frac12}. Decision variables are \\{\\mathbf{x}_i\\}_{i=0}^N, \\{\\mathbf{u}_i\\}_{i=0}^N, plus midpoint variables \\{\\mathbf{x}_{i+\\frac12},\\mathbf{u}_{i+\\frac12}\\}_{i=0}^{N-1}. Solve\\begin{aligned}\n\\min\\ & c_T(\\mathbf{x}_N)\\; +\\; \\sum_{i=0}^{N-1} \\tfrac{h_i}{6}\\Big[ c(\\mathbf{x}_i,\\mathbf{u}_i) + 4\\,c(\\mathbf{x}_{i+\\frac12},\\mathbf{u}_{i+\\frac12}) + c(\\mathbf{x}_{i+1},\\mathbf{u}_{i+1}) \\Big]\\\\\n\\text{s.t.}\\ & \\underbrace{\\mathbf{x}_{i+1}-\\mathbf{x}_i - \\tfrac{h_i}{6}\\Big[\\mathbf{f}(\\mathbf{x}_i,\\mathbf{u}_i) + 4\\,\\mathbf{f}(\\mathbf{x}_{i+\\frac12},\\mathbf{u}_{i+\\frac12}) + \\mathbf{f}(\\mathbf{x}_{i+1},\\mathbf{u}_{i+1})\\Big]}_{\\text{Simpson defect over }[t_i,t_{i+1}]} = \\mathbf{0},\\\\\n& \\underbrace{\\mathbf{x}_{i+\\frac12} - \\tfrac{\\mathbf{x}_i+\\mathbf{x}_{i+1}}{2} - \\tfrac{h_i}{8}\\Big[\\mathbf{f}(\\mathbf{x}_i,\\mathbf{u}_i) - \\mathbf{f}(\\mathbf{x}_{i+1},\\mathbf{u}_{i+1})\\Big]}_{\\text{midpoint collocation (slope matching at }t_{i+\\frac12}\\text{)}} = \\mathbf{0},\\\\\n& \\mathbf{g}(\\mathbf{x}_i,\\mathbf{u}_i) \\le \\mathbf{0},\\ \\ \\mathbf{g}(\\mathbf{x}_{i+\\frac12},\\mathbf{u}_{i+\\frac12}) \\le \\mathbf{0},\\ \\ \\mathbf{g}(\\mathbf{x}_{i+1},\\mathbf{u}_{i+1}) \\le \\mathbf{0},\\\\\n& \\mathbf{x}_{\\min} \\le \\mathbf{x}_i,\\mathbf{x}_{i+\\frac12} \\le \\mathbf{x}_{\\max},\\ \\ \\mathbf{u}_{\\min} \\le \\mathbf{u}_i,\\mathbf{u}_{i+\\frac12} \\le \\mathbf{u}_{\\max},\\\\\n& \\mathbf{x}_0 = \\mathbf{x}(t_0),\\quad i=0,\\ldots,N-1.\n\\end{aligned}\n\nCollocation nodes recap: \\tau=0,\\ \\tfrac12,\\ 1.\n\nThe midpoint is where we explicitly match the ODE slope (collocation).\n\nThe three nodes together are used for the Simpson integral of \\mathbf{f} (state update) and of c (cost), keeping physics and objective synchronized.","type":"content","url":"/collocation#hermite-simpson-quadratic-interpolation-midpoint-included","position":33},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl2":"Examples"},"type":"lvl2","url":"/collocation#examples","position":34},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl2":"Examples"},"content":"","type":"content","url":"/collocation#examples","position":35},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl3":"Compressor Surge Problem","lvl2":"Examples"},"type":"lvl3","url":"/collocation#compressor-surge-problem","position":36},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl3":"Compressor Surge Problem","lvl2":"Examples"},"content":"A compressor is a machine that raises the pressure of a gas by squeezing it into a smaller volume. You find them in natural gas pipelines, jet engines, and factories. But compressors can run into trouble if the flow of gas becomes too small. In that case, the machine can “stall” much like an airplane wing at too high an angle. Instead of moving forward, the gas briefly pushes back, creating strong pressure oscillations that can damage the compressor and anything connected to it.\n\nTo prevent this, engineers often add a close-coupled valve (CCV) at the outlet. The valve can quickly adjust the flow to keep the compressor away from these unstable conditions. Our goal is to design a control strategy for operating this valve so that the compressor never enters surge.\n\nFollowing  \n\nGravdahl & Egeland, 1997 and \n\nGrancharova & Johansen (2012), we model the compressor using a simplified second-order representation:\\begin{aligned}\n\\dot{x}_1 &= B(\\Psi_e(x_1) - x_2 - u) \\\\\n\\dot{x}_2 &= \\frac{1}{B}(x_1 - \\Phi(x_2))\n\\end{aligned}\n\nHere, \\mathbf{x} = [x_1, x_2]^T represents the state variables:\n\nx_1 is the normalized mass flow through the compressor.\n\nx_2 is the normalized pressure ratio across the compressor.\n\nThe control input u denotes the normalized mass flow through a CCV.\nThe functions \\Psi_e(x_1) and \\Phi(x_2) represent the characteristics of the compressor and valve, respectively:\\begin{aligned}\n\\Psi_e(x_1) &= \\psi_{c0} + H\\left(1 + 1.5\\left(\\frac{x_1}{W} - 1\\right) - 0.5\\left(\\frac{x_1}{W} - 1\\right)^3\\right) \\\\\n\\Phi(x_2) &= \\gamma \\operatorname{sign}(x_2) \\sqrt{|x_2|}\n\\end{aligned}\n\nThe system parameters are given as \\gamma = 0.5, B = 1, H = 0.18, \\psi_{c0} = 0.3, and W = 0.25.\n\nOne possible way to pose the problem \n\nGrancharova & Johansen (2012) is by penalizing deviations from the setpoints using a quadratic penalty in the instantaneous cost function as well as in the terminal one. Furthermore, we also penalize taking large actions (which are energy hungry and potentially unsafe) within the integral term. The idea of penalizing deviations throughout is a natural way of posing the problem when solving it via single shooting. Another alternative, which we will explore below, is to set the desired setpoint as a hard terminal constraint.\n\nThe control objective is to stabilize the system and prevent surge, formulated as a continuous-time optimal control problem (COCP) in the Bolza form:\\begin{aligned}\n\\text{minimize} \\quad & \\left[ \\int_0^T \\alpha(\\mathbf{x}(t) - \\mathbf{x}^*)^T(\\mathbf{x}(t) - \\mathbf{x}^*) + \\kappa u(t)^2 \\, dt\\right] + \\beta(\\mathbf{x}(T) - \\mathbf{x}^*)^T(\\mathbf{x}(T) - \\mathbf{x}^*) + R v^2  \\\\\n\\text{subject to} \\quad & \\dot{x}_1(t) = B(\\Psi_e(x_1(t)) - x_2(t) - u(t)) \\\\\n& \\dot{x}_2(t) = \\frac{1}{B}(x_1(t) - \\Phi(x_2(t))) \\\\\n& u_{\\text{min}} \\leq u(t) \\leq u_{\\text{max}} \\\\\n& -x_2(t) + 0.4 \\leq v \\\\\n& -v \\leq 0 \\\\\n& \\mathbf{x}(0) = \\mathbf{x}_0\n\\end{aligned}\n\nThe parameters \\alpha, \\beta, \\kappa, and R are non-negative weights that allow the designer to prioritize different aspects of performance (e.g., tight setpoint tracking vs. smooth control actions). We also constraint the control input to be within 0 \\leq u(t) \\leq 0.3 due to the physical limitations of the valve.\n\nThe authors in \n\nGrancharova & Johansen (2012) also add a soft path constraint x_2(t) \\geq 0.4 to ensure that we maintain a minimum pressure at all time. This is implemented as a soft constraint using slack variables. The reason that we have the term R v^2 in the objective is to penalizes violations of the soft constraint: we allow for deviations, but don’t want to do it too much.\n\nIn the experiment below, we choose the setpoint \\mathbf{x}^* = [0.40, 0.60]^T as it corresponds to an unstable equilibrium point. If we were to run the system without applying any control, we would see that the system starts to oscillate.\n\n#  label: cocp-cell-02\n#  caption: Rendered output from the preceding code cell.\n\n%config InlineBackend.figure_format = 'retina'\nimport numpy as np\nfrom scipy.optimize import minimize\nimport matplotlib.pyplot as plt\n\n# Apply book style\ntry:\n    import scienceplots\n    plt.style.use(['science', 'notebook'])\nexcept (ImportError, OSError):\n    pass  # Use matplotlib defaults\n\n# System parameters\ngamma, B, H, psi_c0, W = 0.5, 1, 0.18, 0.3, 0.25\nalpha, beta, kappa, R = 1, 0, 0.08, 0\nT, N = 12, 60\ndt = T / N\nx1_star, x2_star = 0.40, 0.60\n\ndef psi_e(x1):\n    return psi_c0 + H * (1 + 1.5 * ((x1 / W) - 1) - 0.5 * ((x1 / W) - 1)**3)\n\ndef phi(x2):\n    return gamma * np.sign(x2) * np.sqrt(np.abs(x2))\n\ndef system_dynamics(x, u):\n    x1, x2 = x\n    dx1dt = B * (psi_e(x1) - x2 - u)\n    dx2dt = (1 / B) * (x1 - phi(x2))\n    return np.array([dx1dt, dx2dt])\n\ndef euler_step(x, u, dt):\n    return x + dt * system_dynamics(x, u)\n\ndef instantenous_cost(x, u):\n    return (alpha * np.sum((x - np.array([x1_star, x2_star]))**2) + kappa * u**2)\n\ndef terminal_cost(x):\n    return beta * np.sum((x - np.array([x1_star, x2_star]))**2)\n\ndef objective_and_constraints(z):\n    u, v = z[:-1], z[-1]\n    x = np.zeros((N+1, 2))\n    x[0] = x0\n    obj = 0\n    cons = []\n    for i in range(N):\n        x[i+1] = euler_step(x[i], u[i], dt)\n        obj += dt * instantenous_cost(x[i], u[i])\n        cons.append(0.4 - x[i+1, 1] - v)\n    obj += terminal_cost(x[-1]) + R * v**2\n    return obj, np.array(cons)\n\ndef solve_trajectory_optimization(x0, u_init):\n    z0 = np.zeros(N + 1)\n    z0[:-1] = u_init\n    bounds = [(0, 0.3)] * N + [(0, None)]\n    result = minimize(\n        lambda z: objective_and_constraints(z)[0],\n        z0,\n        method='SLSQP',\n        bounds=bounds,\n        constraints={'type': 'ineq', 'fun': lambda z: -objective_and_constraints(z)[1]},\n        options={'disp': True, 'maxiter': 1000, 'ftol': 1e-6}\n    )\n    return result.x, result\n\ndef simulate_trajectory(x0, u):\n    x = np.zeros((N+1, 2))\n    x[0] = x0\n    for i in range(N):\n        x[i+1] = euler_step(x[i], u[i], dt)\n    return x\n\n# Run optimizations and simulations\nx0 = np.array([0.25, 0.25])\nt = np.linspace(0, T, N+1)\n\n# Optimized control starting from zero\nz_single_shooting, _ = solve_trajectory_optimization(x0, np.zeros(N))\nu_opt_shoot, v_opt_shoot = z_single_shooting[:-1], z_single_shooting[-1]\nx_opt_shoot = simulate_trajectory(x0, u_opt_shoot)\n\n# Do-nothing control (u = 0)\nu_nothing = np.zeros(N)\nx_nothing = simulate_trajectory(x0, u_nothing)\n\n# Plotting\nplt.figure(figsize=(15, 20))\n\n# State variables over time\nplt.subplot(3, 1, 1)\nplt.plot(t, x_opt_shoot[:, 0], label='x1 (opt from 0)')\nplt.plot(t, x_opt_shoot[:, 1], label='x2 (opt from 0)')\nplt.plot(t, x_nothing[:, 0], ':', label='x1 (do-nothing)')\nplt.plot(t, x_nothing[:, 1], ':', label='x2 (do-nothing)')\nplt.axhline(y=x1_star, color='r', linestyle='--', label='x1 setpoint')\nplt.axhline(y=x2_star, color='g', linestyle='--', label='x2 setpoint')\nplt.xlabel('Time')\nplt.ylabel('State variables')\nplt.title('State variables over time')\nplt.legend()\nplt.grid(True)\n\n# Phase portrait\nplt.subplot(3, 1, 2)\nplt.plot(x_opt_shoot[:, 0], x_opt_shoot[:, 1], label='Optimized from 0')\nplt.plot(x_nothing[:, 0], x_nothing[:, 1], ':', label='Do-nothing')\nplt.plot(x1_star, x2_star, 'r*', markersize=10, label='Setpoint')\nplt.xlabel('x1 (mass flow)')\nplt.ylabel('x2 (pressure)')\nplt.title('Phase portrait')\nplt.legend()\nplt.grid(True)\n\n# Control inputs\nplt.subplot(3, 1, 3)\nplt.plot(t[:-1], u_opt_shoot, label='Optimized from 0')\nplt.plot(t[:-1], u_nothing, ':', label='Do-nothing')\nplt.xlabel('Time')\nplt.ylabel('Control input (u)')\nplt.title('Control input over time')\nplt.legend()\nplt.grid(True)\n\n","type":"content","url":"/collocation#compressor-surge-problem","position":37},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl4":"Solution by Trapezoidal Collocation","lvl3":"Compressor Surge Problem","lvl2":"Examples"},"type":"lvl4","url":"/collocation#solution-by-trapezoidal-collocation","position":38},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl4":"Solution by Trapezoidal Collocation","lvl3":"Compressor Surge Problem","lvl2":"Examples"},"content":"Another way to pose the problem is by imposing a terminal state constraint on the system rather than through a penalty in the integral term. In the following experiment, we use a problem formulation of the form:\\begin{aligned}\n\\text{minimize} \\quad & \\left[ \\int_0^T \\kappa u(t)^2 \\, dt\\right] \\\\\n\\text{subject to} \\quad & \\dot{x}_1(t) = B(\\Psi_e(x_1(t)) - x_2(t) - u(t)) \\\\\n& \\dot{x}_2(t) = \\frac{1}{B}(x_1(t) - \\Phi(x_2(t))) \\\\\n& u_{\\text{min}} \\leq u(t) \\leq u_{\\text{max}} \\\\\n& \\mathbf{x}(0) = \\mathbf{x}_0 \\\\\n& \\mathbf{x}(T) = \\mathbf{x}^\\star\n\\end{aligned}\n\nWe then find a control function u(t) and state trajectory x(t) using the trapezoidal collocation method.\n\n#  label: cocp-cell-03\n#  caption: Rendered output from the preceding code cell.\n\n%config InlineBackend.figure_format = 'retina'\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.integrate import solve_ivp\nfrom scipy.interpolate import interp1d\nimport matplotlib.pyplot as plt\n\n# Apply book style\ntry:\n    import scienceplots\n    plt.style.use(['science', 'notebook'])\nexcept (ImportError, OSError):\n    pass  # Use matplotlib defaults\n\n# System parameters\ngamma, B, H, psi_c0, W = 0.5, 1, 0.18, 0.3, 0.25\nkappa = 0.08\nT, N = 12, 20  # Number of collocation points\nt = np.linspace(0, T, N)\ndt = T / (N - 1)\nx1_star, x2_star = 0.40, 0.60\n\ndef psi_e(x1):\n    return psi_c0 + H * (1 + 1.5 * ((x1 / W) - 1) - 0.5 * ((x1 / W) - 1)**3)\n\ndef phi(x2):\n    return gamma * np.sign(x2) * np.sqrt(np.abs(x2))\n\ndef system_dynamics(t, x, u_func):\n    x1, x2 = x\n    u = u_func(t)\n    dx1dt = B * (psi_e(x1) - x2 - u)\n    dx2dt = (1 / B) * (x1 - phi(x2))\n    return [dx1dt, dx2dt]\n\ndef objective(z):\n    x = z[:2*N].reshape((N, 2))\n    u = z[2*N:]\n    \n    # Trapezoidal rule for the cost function\n    cost = 0\n    for i in range(N-1):\n        cost += 0.5 * dt * (kappa * u[i]**2 + kappa * u[i+1]**2)\n    \n    return cost\n\ndef constraints(z):\n    x = z[:2*N].reshape((N, 2))\n    u = z[2*N:]\n    \n    cons = []\n    \n    # Dynamics constraints (trapezoidal rule)\n    for i in range(N-1):\n        f_i = system_dynamics(t[i], x[i], lambda t: u[i])\n        f_ip1 = system_dynamics(t[i+1], x[i+1], lambda t: u[i+1])\n        cons.extend(x[i+1] - x[i] - 0.5 * dt * (np.array(f_i) + np.array(f_ip1)))\n    \n    # Terminal constraint\n    cons.extend([x[-1, 0] - x1_star, x[-1, 1] - x2_star])\n    \n    # Initial condition constraint\n    cons.extend([x[0, 0] - x0[0], x[0, 1] - x0[1]])\n    \n    return np.array(cons)\n\ndef solve_trajectory_optimization(x0):\n    # Initial guess\n    x_init = np.linspace(x0, [x1_star, x2_star], N)\n    u_init = np.zeros(N)\n    z0 = np.concatenate([x_init.flatten(), u_init])\n    \n    # Bounds\n    bounds = [(None, None)] * (2*N)  # State variables\n    bounds += [(0, 0.3)] * N  # Control inputs\n    \n    # Constraints\n    cons = {'type': 'eq', 'fun': constraints}\n    \n    result = minimize(\n        objective,\n        z0,\n        method='SLSQP',\n        bounds=bounds,\n        constraints=cons,\n        options={'disp': True, 'maxiter': 1000, 'ftol': 1e-6}\n    )\n    return result.x, result\n\n# Run optimization\nx0 = np.array([0.5, 0.5])\nz_opt, result = solve_trajectory_optimization(x0)\nx_opt_coll = z_opt[:2*N].reshape((N, 2))\nu_opt_coll = z_opt[2*N:]\n\nprint(f\"Optimization successful: {result.success}\")\nprint(f\"Final objective value: {result.fun}\")\nprint(f\"Final state: x1 = {x_opt_coll[-1, 0]:.4f}, x2 = {x_opt_coll[-1, 1]:.4f}\")\nprint(f\"Target state: x1 = {x1_star:.4f}, x2 = {x2_star:.4f}\")\n\n# Create interpolated control function\nu_func = interp1d(t, u_opt_coll, kind='linear', bounds_error=False, fill_value=(u_opt_coll[0], u_opt_coll[-1]))\n\n# Solve IVP with the optimized control\nsol = solve_ivp(lambda t, x: system_dynamics(t, x, u_func), [0, T], x0, dense_output=True)\n\n# Generate solution points\nt_dense = np.linspace(0, T, 200)\nx_ivp = sol.sol(t_dense).T\n\n# Plotting\nplt.figure(figsize=(15, 20))\n\n# State variables over time\nplt.subplot(3, 1, 1)\nplt.plot(t, x_opt_coll[:, 0], 'bo-', label='x1 (collocation)')\nplt.plot(t, x_opt_coll[:, 1], 'ro-', label='x2 (collocation)')\nplt.plot(t_dense, x_ivp[:, 0], 'b--', label='x1 (integrated)')\nplt.plot(t_dense, x_ivp[:, 1], 'r--', label='x2 (integrated)')\nplt.axhline(y=x1_star, color='b', linestyle=':', label='x1 setpoint')\nplt.axhline(y=x2_star, color='r', linestyle=':', label='x2 setpoint')\nplt.xlabel('Time')\nplt.ylabel('State variables')\nplt.title('State variables over time')\nplt.legend()\nplt.grid(True)\n\n# Phase portrait\nplt.subplot(3, 1, 2)\nplt.plot(x_opt_coll[:, 0], x_opt_coll[:, 1], 'go-', label='Collocation')\nplt.plot(x_ivp[:, 0], x_ivp[:, 1], 'm--', label='Integrated')\nplt.plot(x1_star, x2_star, 'r*', markersize=10, label='Setpoint')\nplt.xlabel('x1 (mass flow)')\nplt.ylabel('x2 (pressure)')\nplt.title('Phase portrait')\nplt.legend()\nplt.grid(True)\n\n# Control inputs\nplt.subplot(3, 1, 3)\nplt.step(t, u_opt_coll, 'g-', where='post', label='Collocation')\nplt.plot(t_dense, u_func(t_dense), 'm--', label='Interpolated')\nplt.xlabel('Time')\nplt.ylabel('Control input (u)')\nplt.title('Control input over time')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\n\nYou can try to vary the number of collocation points in the code and observe how the state trajectory progressively matches the ground truth (the line denoted “integrated solution”). Note that this version of the code also lacks bound constraints on the variable x_2 to ensure a minimum pressure, as we did earlier. Consider this a good exercise to try on your own.","type":"content","url":"/collocation#solution-by-trapezoidal-collocation","position":39},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl4":"System Identification as Trajectory Optimization (Compressor Surge)","lvl3":"Compressor Surge Problem","lvl2":"Examples"},"type":"lvl4","url":"/collocation#system-identification-as-trajectory-optimization-compressor-surge","position":40},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl4":"System Identification as Trajectory Optimization (Compressor Surge)","lvl3":"Compressor Surge Problem","lvl2":"Examples"},"content":"We now turn the compressor surge model into a simple system identification task: estimate unknown parameters (here, the scalar B) from measured trajectories. This can be viewed as a trajectory optimization problem: choose parameters (and optionally states) to minimize reconstruction error while enforcing the dynamics.\n\nGiven time-aligned data \\{(\\mathbf{u}_k,\\mathbf{y}_k)\\}_{k=0}^{N}, model states \\mathbf{x}_k\\in\\mathbb{R}^d, outputs \\mathbf{y}_k\\approx \\mathbf{h}(\\mathbf{x}_k;\\boldsymbol{\\theta}), step \\Delta t, and dynamics \\mathbf{f}(\\mathbf{x},\\mathbf{u};\\boldsymbol{\\theta}), the simultaneous (full-discretization) viewpoint is\\begin{aligned}\n\\min_{\\boldsymbol{\\theta},\\,\\{\\mathbf{x}_k\\}} \\quad & \\sum_{k\\in K}\\;\\big\\|\\mathbf{y}_k - \\mathbf{h}(\\mathbf{x}_k;\\boldsymbol{\\theta})\\big\\|_2^2 \\\\\n\\text{s.t.}\\quad & \\mathbf{x}_{k+1} - \\mathbf{x}_k - \\Delta t\\,\\mathbf{f}(\\mathbf{x}_k,\\mathbf{u}_k;\\boldsymbol{\\theta}) = \\mathbf{0},\\quad k=0,\\ldots,N-1, \\\\\n& \\mathbf{x}_0 \\;\\text{given},\n\\end{aligned}\n\nwhile the single-shooting (recursive elimination) variant eliminates the states by simulating forward from \\mathbf{x}_0:J(\\boldsymbol{\\theta}) := \\sum_{k\\in K}\\;\\big\\|\\mathbf{y}_k - \\mathbf{h}(\\boldsymbol{\\phi}_k(\\boldsymbol{\\theta};\\mathbf{x}_0,\\mathbf{u}_{0:N-1})\\big\\|_2^2,\\quad \\min_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}),\n\nwhere \\boldsymbol{\\phi}_k denotes the state reached at step k by an RK4 rollout under parameter \\boldsymbol{\\theta}. In our demo the data grid and rollout grid coincide, so \\boldsymbol{\\phi}_k = \\mathbf{x}_k and no interpolation is required. We will identify B by fitting the model to data generated from the ground-truth B=1 system under randomized initial conditions and small input perturbations.\n\n#  label: cocp-cell-04\n#  caption: Rendered output from the preceding code cell.\n\n%config InlineBackend.figure_format = 'retina'\nimport numpy as np\nfrom scipy.integrate import solve_ivp\nimport matplotlib.pyplot as plt\n\n# Apply book style\ntry:\n    import scienceplots\n    plt.style.use(['science', 'notebook'])\nexcept (ImportError, OSError):\n    pass  # Use matplotlib defaults\n\n# System parameters\ngamma, B, H, psi_c0, W = 0.5, 1, 0.18, 0.3, 0.25\n\n# Simulation parameters\nT = 50  # Total simulation time\ndt = 0.1  # Time step\nt = np.arange(0, T + dt, dt)\nN = len(t)\n\n# Number of trajectories\nnum_trajectories = 10\n\ndef psi_e(x1):\n    return psi_c0 + H * (1 + 1.5 * ((x1 / W) - 1) - 0.5 * ((x1 / W) - 1)**3)\n\ndef phi(x2):\n    return gamma * np.sign(x2) * np.sqrt(np.abs(x2))\n\ndef system_dynamics(t, x, u):\n    x1, x2 = x\n    dx1dt = B * (psi_e(x1) - x2 - u)\n    dx2dt = (1 / B) * (x1 - phi(x2))\n    return [dx1dt, dx2dt]\n\n# \"Do nothing\" controller with small random noise\ndef u_func(t):\n    return np.random.normal(0, 0.01)  # Mean 0, standard deviation 0.01\n\n# Function to simulate a single trajectory\ndef simulate_trajectory(x0):\n    sol = solve_ivp(lambda t, x: system_dynamics(t, x, u_func(t)), [0, T], x0, t_eval=t, method='RK45')\n    return sol.y[0], sol.y[1]\n\n# Generate multiple trajectories\ntrajectories = []\ninitial_conditions = []\n\nfor i in range(num_trajectories):\n    # Randomize initial conditions around [0.5, 0.5]\n    x0 = np.array([0.5, 0.5]) + np.random.normal(0, 0.05, 2)\n    initial_conditions.append(x0)\n    x1, x2 = simulate_trajectory(x0)\n    trajectories.append((x1, x2))\n\n# Calculate control inputs (small random noise)\nu = np.array([u_func(ti) for ti in t])\n\n# Plotting\nplt.figure(figsize=(15, 15))\n\n# State variables over time\nplt.subplot(3, 1, 1)\nfor i, (x1, x2) in enumerate(trajectories):\n    plt.plot(t, x1, label=f'x1 (Traj {i+1})' if i == 0 else \"_nolegend_\")\n    plt.plot(t, x2, label=f'x2 (Traj {i+1})' if i == 0 else \"_nolegend_\")\nplt.xlabel('Time')\nplt.ylabel('State variables')\nplt.title('State variables over time (Multiple Trajectories)')\nplt.legend()\nplt.grid(True)\n\n# Phase portrait\nplt.subplot(3, 1, 2)\nfor x1, x2 in trajectories:\n    plt.plot(x1, x2)\n    plt.plot(x1[0], x2[0], 'bo', markersize=5)\n    plt.plot(x1[-1], x2[-1], 'ro', markersize=5)\nplt.xlabel('x1 (mass flow)')\nplt.ylabel('x2 (pressure)')\nplt.title('Phase portrait (Multiple Trajectories)')\nplt.grid(True)\n\n# Control input (small random noise)\nplt.subplot(3, 1, 3)\nplt.plot(t, u, 'k-')\nplt.xlabel('Time')\nplt.ylabel('Control input (u)')\nplt.title('Control input over time (Small random noise)')\nplt.grid(True)\n\nplt.tight_layout()\n\n# Save the data\nnp.savez('_static/compressor_surge_data_multi.npz', t=t, trajectories=trajectories, u=u, initial_conditions=initial_conditions)\n\nprint(\"Data collection complete. Results saved to 'compressor_surge_data_multi.npz'\")\nprint(f\"Data shape: {num_trajectories} trajectories, each with {N} time steps\")\nprint(f\"Time range: 0 to {T} seconds\")\nprint(\"Initial conditions:\")\nfor i, x0 in enumerate(initial_conditions):\n    print(f\"  Trajectory {i+1}: x1 = {x0[0]:.4f}, x2 = {x0[1]:.4f}\")\n\n#  label: cocp-cell-05\n#  caption: Rendered output from the preceding code cell.\n\n%config InlineBackend.figure_format = 'retina'\nimport numpy as np\nfrom scipy.optimize import minimize\nimport matplotlib.pyplot as plt\n\n# Apply book style\ntry:\n    import scienceplots\n    plt.style.use(['science', 'notebook'])\nexcept (ImportError, OSError):\n    pass  # Use matplotlib defaults\n\n# Load the data\ndata = np.load('_static/compressor_surge_data_multi.npz', allow_pickle=True)\nt = data['t']\ntrajectories = data['trajectories']\nu = data['u']\ninitial_conditions = data['initial_conditions']\n\n# Known system parameters\ngamma, H, psi_c0, W = 0.5, 0.18, 0.3, 0.25\n# B is the parameter we want to identify\nB_true = 1.0  # True value, used for comparison\n\ndef psi_e(x1):\n    return psi_c0 + H * (1 + 1.5 * ((x1 / W) - 1) - 0.5 * ((x1 / W) - 1)**3)\n\ndef phi(x2):\n    return gamma * np.sign(x2) * np.sqrt(np.abs(x2))\n\ndef system_dynamics(t, x, u, B):\n    x1, x2 = x\n    dx1dt = B * (psi_e(x1) - x2 - u)\n    dx2dt = (1 / B) * (x1 - phi(x2))\n    return np.array([dx1dt, dx2dt])\n\ndef rk4_step(f, t, x, u, dt, B):\n    k1 = f(t, x, u, B)\n    k2 = f(t + 0.5*dt, x + 0.5*dt*k1, u, B)\n    k3 = f(t + 0.5*dt, x + 0.5*dt*k2, u, B)\n    k4 = f(t + dt, x + dt*k3, u, B)\n    return x + (dt/6) * (k1 + 2*k2 + 2*k3 + k4)\n\ndef simulate_trajectory(x0, B):\n    x = np.zeros((len(t), 2))\n    x[0] = x0\n    for i in range(1, len(t)):\n        x[i] = rk4_step(system_dynamics, t[i-1], x[i-1], u[i-1], t[i] - t[i-1], B)\n    return x\n\ndef objective(B):\n    error = 0\n    for i, (x1_obs, x2_obs) in enumerate(trajectories):\n        x_sim = simulate_trajectory(initial_conditions[i], B[0])\n        error += np.sum((x_sim[:, 0] - x1_obs)**2 + (x_sim[:, 1] - x2_obs)**2)\n    return error\n\n# Perform optimization\nresult = minimize(objective, x0=[1.5], method='Nelder-Mead', options={'disp': True})\n\nB_identified = result.x[0]\n\nprint(f\"True B: {B_true}\")\nprint(f\"Identified B: {B_identified}\")\nprint(f\"Relative error: {abs(B_identified - B_true) / B_true * 100:.2f}%\")\n\n# Plot results\nplt.figure(figsize=(15, 10))\n\n# Plot one trajectory for comparison\ntraj_index = 0\nx1_obs, x2_obs = trajectories[traj_index]\nx_sim = simulate_trajectory(initial_conditions[traj_index], B_identified)\n\nplt.subplot(2, 1, 1)\nplt.plot(t, x1_obs, 'b-', label='Observed x1')\nplt.plot(t, x2_obs, 'r-', label='Observed x2')\nplt.plot(t, x_sim[:, 0], 'b--', label='Simulated x1')\nplt.plot(t, x_sim[:, 1], 'r--', label='Simulated x2')\nplt.xlabel('Time')\nplt.ylabel('State variables')\nplt.title('Observed vs Simulated Trajectory')\nplt.legend()\nplt.grid(True)\n\nplt.subplot(2, 1, 2)\nplt.plot(x1_obs, x2_obs, 'g-', label='Observed')\nplt.plot(x_sim[:, 0], x_sim[:, 1], 'm--', label='Simulated')\nplt.xlabel('x1 (mass flow)')\nplt.ylabel('x2 (pressure)')\nplt.title('Phase Portrait: Observed vs Simulated')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\n\n","type":"content","url":"/collocation#system-identification-as-trajectory-optimization-compressor-surge","position":41},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl3":"Flight Trajectory Optimization","lvl2":"Examples"},"type":"lvl3","url":"/collocation#flight-trajectory-optimization","position":42},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl3":"Flight Trajectory Optimization","lvl2":"Examples"},"content":"We consider a concrete task: computing a fuel-optimal trajectory between Montréal–Trudeau (CYUL) and Toronto Pearson (CYYZ), taking into account both aircraft dynamics and wind conditions along the route. For this demo, we leverage the excellent library \n\nOpenAP.top which provides direct transcription methods and airplane dynamics models \n\nSun, 2022. Furthermore, it allows us to import a a wind field comes from ERA5 \n\nC3S, 2018, a global atmospheric dataset. It combines historical observations from satellites, aircraft, and surface stations with a weather model to reconstruct the state of the atmosphere across space and time. In climate science, this is called a reanalysis.\n\nERA5 data is stored in GRIB files, a compact format widely used in meteorology. Each file contains a gridded field: values of wind and other variables arranged on a regular 4D lattice over longitude, latitude, altitude, and time. Since the aircraft rarely sits exactly on a grid point, we interpolate the wind components it sees as it moves.\n\nThe aircraft is modeled as a point mass with state\\mathbf{x}(t) = (x(t), y(t), h(t), m(t)),\n\nwhere (x, y) is horizontal position, h is altitude, and m is remaining mass. Controls are Mach number M(t), vertical speed v_s(t), and heading angle \\psi(t). The equations of motion combine airspeed and wind:\\begin{aligned}\n\\dot x &= v(M,h)\\cos\\psi\\cos\\gamma + u_w(x,y,h,t), \\\\\n\\dot y &= v(M,h)\\sin\\psi\\cos\\gamma + v_w(x,y,h,t), \\\\\n\\dot h &= v_s, \\\\\n\\dot m &= -\\,\\mathrm{FF}(T(h,M,v_s), h, M, v_s),\n\\end{aligned}\n\nwhere \\gamma = \\arcsin(v_s / v) is the flight path angle and \\mathrm{FF} is the fuel flow rate based on current conditions. The wind terms u_w and v_w are taken from ERA5 and interpolated in space and time.\n\nThe optimization minimizes fuel burn over the CYUL–CYYZ leg. But the same setup could be used to minimize arrival time, or some weighted combination of time, cost, and emissions.\n\nWe use OpenAP.top, which solves the problem using direct collocation at Legendre–Gauss–Lobatto (LGL) points. Each trajectory segment is mapped to the unit interval, the state is interpolated by Lagrange polynomials at nonuniform LGL nodes, and the dynamics are enforced at those points. Integration is done with matching quadrature weights.\n\nThis setup lets us optimize trajectories under realistic conditions by feeding in the appropriate ERA5 GRIB file (e.g., era5_mtl_20230601_12.grib). The result accounts for wind patterns (eg. headwinds, tailwinds, shear) along the corridor between Montréal and Toronto.\n\n#  label: cocp-cell-06\n#  caption: Rendered output from the preceding code cell.\n\n%config InlineBackend.figure_format = 'retina'\n# OpenAP.top demo with optional wind overlay – docs: https://github.com/junzis/openap-top\nfrom openap import top\nimport matplotlib.pyplot as plt\n\n# Apply book style\ntry:\n    import scienceplots\n    plt.style.use(['science', 'notebook'])\nexcept (ImportError, OSError):\n    pass  # Use matplotlib defaults\nimport os\n\n# Montreal region route (Canada): CYUL (Montréal–Trudeau) → CYYZ (Toronto)\nopt = top.CompleteFlight(\"A320\", \"CYUL\", \"CYYZ\", m0=0.85)\n\n# Optional: point to a local ERA5/GRIB file to enable wind (set env var OPENAP_WIND_GRIB)\n# If not set, look for a default small file produced by `_static/openap_fetch_era5.py`.\nfgrib = os.environ.get(\"OPENAP_WIND_GRIB\", \"_static/era5_mtl_20230601_12.grib\")\nwindfield = None\nif fgrib and os.path.exists(fgrib):\n    try:\n        windfield = top.tools.read_grids(fgrib)\n        opt.enable_wind(windfield)\n    except Exception:\n        windfield = None  # fall back silently if GRIB reading deps are missing\n\n# Solve for a fuel-optimal trajectory (CasADi direct collocation under the hood)\nflight = opt.trajectory(objective=\"fuel\")\n\n# Visualize; overlay wind barbs if windfield available\nif windfield is not None:\n    ax = top.vis.trajectory(flight, windfield=windfield, barb_steps=15)\nelse:\n    ax = top.vis.trajectory(flight)\n\ntitle = \"OpenAP.top fuel-optimal trajectory (A320: CYUL → CYYZ)\"\nif hasattr(ax, \"set_title\"):\n    ax.set_title(title)\nelse:\n    plt.title(title)\n\n","type":"content","url":"/collocation#flight-trajectory-optimization","position":43},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl3":"Hydro Cascade Scheduling with Physical Routing and Multiple Shooting","lvl2":"Examples"},"type":"lvl3","url":"/collocation#hydro-cascade-scheduling-with-physical-routing-and-multiple-shooting","position":44},{"hierarchy":{"lvl1":"Trajectory Optimization in Continuous Time","lvl3":"Hydro Cascade Scheduling with Physical Routing and Multiple Shooting","lvl2":"Examples"},"content":"Earlier in the book, we introduced a simplified view of hydro reservoir control, where the water level evolves in discrete time by accounting for inflow and outflow, with precipitation treated as a noisy input. While useful for learning and control design, this model abstracts away much of the physical behavior of actual rivers and dams.\n\nIn this chapter, we move toward a more realistic setup inspired by \n\nSavorgnan et al., 2011. We consider a series of dams arranged in a cascade, where the actions taken upstream influence downstream levels with a delay. The amount of power produced depends not only on how much water flows through the turbines, but also on the head (the vertical distance between the reservoir surface and the turbine outlet). The larger the head, the more potential energy is available for conversion into electricity, and the higher the power output.\n\nTo capture these effects, we follow a modeling approach inspired by the Saint-Venant equations, which describe how water levels and flows evolve in open channels. Instead of solving the full PDEs, we use a reduced model that approximates each dammed section of river (called a reach) as a lumped system governed by an ordinary differential equation. The main variable of interest is the water level h_r(t), which changes over time depending on how much water enters, how much is discharged through the turbines q_r(t), and how much is spilled s_r(t). The mass balance for reach r is written as:\\frac{d h_r(t)}{dt} = \\frac{1}{A_r} \\left( z_r(t) - q_r(t) - s_r(t) \\right),\n\nwhere A_r is the surface area of the reservoir, assumed constant. The inflow z_r(t) to a reach either comes from nature (for the first dam), or from the upstream turbine and spill discharge, delayed by a travel time \\tau_{r-1}:z_1(t) = \\text{inflow}(t), \\qquad\nz_r(t) = q_{r-1}(t - \\tau_{r-1}) + s_{r-1}(t - \\tau_{r-1}), \\quad \\text{for } r > 1.\n\nPower generation at each reach depends on how much water is discharged and the available head:P_r(t) = \\rho g \\eta \\, q_r(t) \\, H_r(h_r(t)),\n\nwhere \\rho is water density, g is gravitational acceleration, \\eta is turbine efficiency, and H_r(h_r(t)) denotes the head as a function of the water level. In some models, the head is approximated as the difference between the current level and a fixed tailwater height (the water level downstream of the dam, after it has passed through the turbine).\n\nThe operator’s goal is to meet a target generation profile P^\\text{ref}(t), such as one dictated by a market dispatch or load-following constraint. This leads to an objective that minimizes the deviation from the target over the full horizon:\\min_{\\{q_r(t), s_r(t)\\}} \\int_0^T \\left( \\sum_{r=1}^R P_r(t) - P^\\text{ref}(t) \\right)^2 dt.\n\nIn practice, this is combined with operational constraints: turbine capacity 0 \\le q_r(t) \\le \\bar{q}_r, spillway limits 0 \\le s_r(t) \\le \\bar{s}_r, and safe level bounds h_r^{\\min} \\le h_r(t) \\le h_r^{\\max}. Depending on the use case, one may also penalize spill to encourage water conservation, or penalize fast changes in levels for ecological reasons.\n\nWhat makes this problem particularly interesting is the coupling across space and time. An upstream reach cannot simply act in isolation: if the operator wants reach r to produce power at a specific time, the water must be released by reach r-1 sufficiently in advance. This coordination is further complicated by delays, nonlinearities in head-dependent power, and limited storage capacity.\n\nWe solve the problem using multiple shooting. Each reach is divided into local simulation segments over short time windows. Within each segment, the dynamics are integrated forward using the ODEs, and continuity constraints are added to ensure that the water levels match across segment boundaries. At the same time, the inflows passed from upstream reaches must arrive at the right time and be consistent with previous decisions. In discrete time, this gives rise to a set of state-update equations:h_r^{k+1} = h_r^k + \\Delta t \\cdot \\frac{1}{A_r}(z_r^k - q_r^k - s_r^k),\n\nwith delays handled by shifting z_r^k according to the appropriate travel time. These constraints are enforced as part of a nonlinear program, alongside the power tracking objective and control bounds.\n\nCompared to the earlier inflow-outflow model, this richer setup introduces more structure, but also more opportunity. The cascade now behaves like a coordinated team: upstream reservoirs can store water in anticipation of future needs, while downstream dams adjust their output to match arrivals and avoid overflows. The optimization produces not just a schedule, but a strategy for how the entire system should act together to meet demand.\n\n#  label: cocp-cell-07\n#  caption: Rendered output from the preceding code cell.\n\n%config InlineBackend.figure_format = 'retina'\n# Instrumented MSD hydro demo with heterogeneity + diagnostics\n# - Breaks symmetry to avoid trivial identical plots\n# - Adds rich diagnostics to explain flat levels and equalities\n#\n# This cell runs end-to-end and shows plots + tables.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Apply book style\ntry:\n    import scienceplots\n    plt.style.use(['science', 'notebook'])\nexcept (ImportError, OSError):\n    pass  # Use matplotlib defaults\nfrom dataclasses import dataclass\nfrom typing import Tuple\nfrom scipy.optimize import minimize\nfrom math import sqrt\nimport warnings\n\n# ---------- Model ----------\n\ng = 9.81  # m/s^2\n\n@dataclass\nclass ReachParams:\n    L: float\n    W: float\n    k_b: float\n    S_b: float\n    k_t: float\n    @property\n    def A_surf(self) -> float:\n        return self.L * self.W\n\ndef smooth_relu(x, eps=1e-9):\n    return 0.5*(x + np.sqrt(x*x + eps))\n\ndef q_bypass(H, rp: ReachParams):\n    H_eff = smooth_relu(H)\n    return rp.k_b * rp.S_b * np.sqrt(2*g*H_eff)\n\ndef muskingum_coeffs(K: float, X: float, dt: float) -> Tuple[float, float, float]:\n    D  = 2.0*K*(1.0 - X) + dt\n    C0 = (dt - 2.0*K*X) / D\n    C1 = (dt + 2.0*K*X) / D\n    C2 = (2.0*K*(1.0 - X) - dt) / D\n    return C0, C1, C2\n\ndef integrate_interval(H0, u, z, dt, nsub, rp: ReachParams):\n    \"\"\"Forward Euler. Returns Hend, avg_qout.\"\"\"\n    h = dt/nsub\n    H = H0\n    qsum = 0.0\n    for _ in range(nsub):\n        qb = q_bypass(H, rp)\n        qout = u + qb\n        dHdt = (z - qout) / rp.A_surf\n        H += h*dHdt\n        qsum += qout\n    return H, qsum/nsub\n\ndef shapes(M,N): return (M*(N+1), M*N, M*N)\n\ndef unpack(x, M, N):\n    nH, nu, nz = shapes(M,N)\n    H = x[:nH].reshape(M,N+1)\n    u = x[nH:nH+nu].reshape(M,N)\n    z = x[nH+nu:nH+nu+nz].reshape(M,N)\n    return H,u,z\n\ndef pack(H,u,z): return np.concatenate([H.ravel(), u.ravel(), z.ravel()])\n\n# ---------- Problem builder ----------\n\ndef make_params_hetero(M):\n    \"\"\"Heterogeneous reaches to break symmetry.\"\"\"\n    # Widths, spillway areas, and power coeffs vary by reach\n    W_list = np.linspace(80, 140, M)         # m\n    L_list = np.full(M, 4000.0)              # m\n    S_b_list = np.linspace(14.0, 20.0, M)    # m^2\n    k_t_list = np.linspace(7.5, 8.5, M)      # power coeff\n    k_b_list = np.linspace(0.55, 0.65, M)    # spill coeff\n    return [ReachParams(L=float(L_list[i]), W=float(W_list[i]),\n                        k_b=float(k_b_list[i]), S_b=float(S_b_list[i]),\n                        k_t=float(k_t_list[i])) for i in range(M)]\n\ndef build_demo(M=3, N=12, dt=900.0, seed=0, hetero=True):\n    rng = np.random.default_rng(seed)\n    params = make_params_hetero(M) if hetero else [ReachParams(4000.0, 100.0, 0.6, 18.26, 8.0) for _ in range(M)]\n\n    # initial levels (heterogeneous)\n    H0 = np.array([17.0, 16.7, 17.3][:M])\n\n    H_ref = np.array([17.0, 16.9, 17.1][:M]) if hetero else np.full(M, 17.0)\n    H_bounds = (16.0, 18.5)\n    u_bounds = (40.0, 160.0)\n\n    Qin_base = 300.0\n    Qin_ext = Qin_base + 30.0*np.sin(2*np.pi*np.arange(N)/N)  # stronger swing\n\n    Pref_raw = 60.0 + 15.0*np.sin(2*np.pi*(np.arange(N)-2)/N)\n\n    # default Muskingum parameters per link (M-1 links)\n    if M > 1:\n        K_list = list(np.linspace(1800.0, 2700.0, M-1))\n        X_list = [0.2]*(M-1)\n    else:\n        K_list = []\n        X_list = []\n\n    return dict(params=params, H0=H0, H_ref=H_ref, H_bounds=H_bounds,\n                u_bounds=u_bounds, Qin_ext=Qin_ext, Pref_raw=Pref_raw,\n                dt=dt, N=N, M=M, nsub=10,\n                muskingum=dict(K=K_list, X=X_list))\n\n# ---------- Objective / constraints / helpers ----------\n\ndef compute_total_power(H,u,params):\n    M,N = u.shape\n    Pn = np.zeros(N)\n    for n in range(N):\n        for i in range(M):\n            Pn[n] += params[i].k_t * u[i,n] * H[i,n]\n    return Pn\n\ndef decompose_objective(x, data, Pref, wP, wH, wDu):\n    H,u,z = unpack(x, data[\"M\"], data[\"N\"])\n    params, H_ref = data[\"params\"], data[\"H_ref\"]\n    track = np.sum((compute_total_power(H,u,params)-Pref)**2)\n    lvl   = np.sum((H[:,:-1]-H_ref[:,None])**2)\n    du    = np.sum((u[:,1:]-u[:,:-1])**2)\n    return dict(track=wP*track, lvl=wH*lvl, du=wDu*du, raw=dict(track=track,lvl=lvl,du=du))\n\ndef make_objective(data, Pref, wP=8.0, wH=0.02, wDu=1e-4):\n    params, H_ref, N, M = data[\"params\"], data[\"H_ref\"], data[\"N\"], data[\"M\"]\n    def obj(x):\n        H,u,z = unpack(x,M,N)\n        return (\n            wP*np.sum((compute_total_power(H,u,params)-Pref)**2)\n            + wH*np.sum((H[:,:-1]-H_ref[:,None])**2)\n            + wDu*np.sum((u[:,1:]-u[:,:-1])**2)\n        )\n    return obj, dict(wP=wP,wH=wH,wDu=wDu)\n\ndef make_constraints(data):\n    params, H0, Qin_ext, dt, N, M, nsub = (\n        data[\"params\"], data[\"H0\"], data[\"Qin_ext\"], data[\"dt\"], data[\"N\"], data[\"M\"], data[\"nsub\"]\n    )\n    cons = []\n    def init_fun(x):\n        H,u,z = unpack(x,M,N); return H[:,0]-H0\n    cons.append({'type':'eq','fun':init_fun})\n    def dyn_fun(x):\n        H,u,z = unpack(x,M,N)\n        res=[]\n        for i in range(M):\n            for n in range(N):\n                Hend, _ = integrate_interval(H[i,n], u[i,n], z[i,n], dt, nsub, params[i])\n                res.append(H[i,n+1]-Hend)\n        return np.array(res)\n    cons.append({'type':'eq','fun':dyn_fun})\n    def coup_fun(x):\n        H,u,z = unpack(x,M,N)\n        res=[]\n        # First reach is exogenous inflow per interval\n        for n in range(N):\n            res.append(z[0,n]-Qin_ext[n])\n        # Downstream links: Muskingum routing\n        K_list = data.get(\"muskingum\", {}).get(\"K\", [])\n        X_list = data.get(\"muskingum\", {}).get(\"X\", [])\n        for i in range(1,M):\n            # Seed condition for z[i,0]\n            _, I0 = integrate_interval(H[i-1,0], u[i-1,0], z[i-1,0], dt, nsub, params[i-1])\n            res.append(z[i,0] - I0)\n            # Coefficients\n            Ki = K_list[i-1] if i-1 < len(K_list) else 1800.0\n            Xi = X_list[i-1] if i-1 < len(X_list) else 0.2\n            C0, C1, C2 = muskingum_coeffs(Ki, Xi, dt)\n            # Recursion over intervals\n            for n in range(N-1):\n                # upstream interval-average outflows for n and n+1\n                _, I_n   = integrate_interval(H[i-1,n],   u[i-1,n],   z[i-1,n],   dt, nsub, params[i-1])\n                _, I_np1 = integrate_interval(H[i-1,n+1], u[i-1,n+1], z[i-1,n+1], dt, nsub, params[i-1])\n                res.append(z[i,n+1] - (C0*I_np1 + C1*I_n + C2*z[i,n]))\n        return np.array(res)\n    cons.append({'type':'eq','fun':coup_fun})\n    return cons\n\ndef make_bounds(data):\n    Hmin,Hmax = data[\"H_bounds\"]\n    umin,umax = data[\"u_bounds\"]\n    M,N = data[\"M\"], data[\"N\"]\n    nH,nu,nz = shapes(M,N)\n    lb = np.empty(nH+nu+nz); ub = np.empty_like(lb)\n    lb[:nH]=Hmin; ub[:nH]=Hmax\n    lb[nH:nH+nu]=umin; ub[nH:nH+nu]=umax\n    lb[nH+nu:]=0.0; ub[nH+nu:]=2000.0\n    return list(zip(lb,ub))\n\ndef residuals(x, data):\n    params, H0, Qin_ext, dt, N, M, nsub = (\n        data[\"params\"], data[\"H0\"], data[\"Qin_ext\"], data[\"dt\"], data[\"N\"], data[\"M\"], data[\"nsub\"]\n    )\n    H,u,z = unpack(x, M, N)\n    dyn = np.zeros((M,N)); coup = np.zeros((M,N))\n    for i in range(M):\n        for n in range(N):\n            Hend, qavg = integrate_interval(H[i,n], u[i,n], z[i,n], dt, nsub, params[i])\n            dyn[i,n] = H[i,n+1] - Hend\n            if i == 0:\n                coup[i,n] = z[i,n] - Qin_ext[n]\n            else:\n                # Muskingum residual, align on current index using n and n-1\n                Ki = data.get(\"muskingum\", {}).get(\"K\", [1800.0]*(M-1))[i-1]\n                Xi = data.get(\"muskingum\", {}).get(\"X\", [0.2]*(M-1))[i-1]\n                C0, C1, C2 = muskingum_coeffs(Ki, Xi, dt)\n                if n == 0:\n                    coup[i,n] = 0.0\n                else:\n                    _, I_nm1 = integrate_interval(H[i-1,n-1], u[i-1,n-1], z[i-1,n-1], dt, nsub, params[i-1])\n                    _, I_n   = integrate_interval(H[i-1,n],   u[i-1,n],   z[i-1,n],   dt, nsub, params[i-1])\n                    coup[i,n] = z[i,n] - (C0*I_n + C1*I_nm1 + C2*z[i,n-1])\n    return dyn, coup\n\n# ---------- Feasible initial guess with hetero controls ----------\n\ndef feasible_initial_guess(data):\n    \"\"\"Feasible x0 with nontrivial u by setting u at mid + per-reach pattern, then integrating to define H,z.\"\"\"\n    M,N,dt,nsub = data[\"M\"], data[\"N\"], data[\"dt\"], data[\"nsub\"]\n    params = data[\"params\"]\n    umin,umax = data[\"u_bounds\"]\n    Qin_ext = data[\"Qin_ext\"]\n\n    # pattern to break symmetry\n    base = 0.5*(umin+umax)\n    phase = np.linspace(0, np.pi/2, M)\n    tgrid = np.arange(N)\n    u_pattern = np.array([base + 25*np.sin(2*np.pi*(tgrid/N) + ph) for ph in phase])\n    u_pattern = np.clip(u_pattern, umin, umax)\n\n    H = np.zeros((M, N+1)); u = np.zeros((M, N)); z = np.zeros((M, N))\n    H[:,0] = data[\"H0\"]\n    # Set controls from pattern first\n    for i in range(M):\n        u[i,:] = u_pattern[i,:]\n\n    # First reach: exogenous inflow, integrate forward and record outflow averages\n    qavg_up = np.zeros((M, N))\n    for n in range(N):\n        z[0,n] = Qin_ext[n]\n        Hend, qavg = integrate_interval(H[0,n], u[0,n], z[0,n], dt, nsub, params[0])\n        H[0,n+1] = Hend\n        qavg_up[0,n] = qavg\n\n    # Downstream reaches with Muskingum routing\n    K_list = data.get(\"muskingum\", {}).get(\"K\", [1800.0]*(M-1))\n    X_list = data.get(\"muskingum\", {}).get(\"X\", [0.2]*(M-1))\n    for i in range(1,M):\n        Ki = K_list[i-1] if i-1 < len(K_list) else 1800.0\n        Xi = X_list[i-1] if i-1 < len(X_list) else 0.2\n        C0, C1, C2 = muskingum_coeffs(Ki, Xi, dt)\n        I = qavg_up[i-1,:]\n        # seed\n        z[i,0] = I[0]\n        # propagate recursively over time\n        for n in range(N-1):\n            z[i,n+1] = C0*I[n+1] + C1*I[n] + C2*z[i,n]\n        # integrate levels for reach i using routed inflow\n        for n in range(N):\n            Hend, qavg = integrate_interval(H[i,n], u[i,n], z[i,n], dt, nsub, params[i])\n            H[i,n+1] = Hend\n            qavg_up[i,n] = qavg\n    return pack(H,u,z)\n\ndef scale_pref(Pref_raw, x0, data):\n    H,u,z = unpack(x0, data[\"M\"], data[\"N\"])\n    P0 = compute_total_power(H,u,data[\"params\"])\n    s = max(np.mean(P0),1e-6)/max(np.mean(Pref_raw),1e-6)\n    return Pref_raw*s, P0\n\ndef run_demo(show: bool = True, save_path: str | None = 'hydro.png', verbose: bool = False):\n    \"\"\"Build, solve, and render the hydro demo.\n\n    Parameters\n    ----------\n    show : bool\n        If True, displays the matplotlib figure via plt.show().\n    save_path : str | None\n        If provided, saves the figure to this path.\n    verbose : bool\n        If True, prints diagnostic information.\n\n    Returns\n    -------\n    matplotlib.figure.Figure | None\n        Returns the Figure when show is False; otherwise returns None.\n    \"\"\"\n    # ---------- Solve ----------\n    data = build_demo(M=3, N=16, dt=900.0, hetero=True)\n    x0 = feasible_initial_guess(data)\n    Pref, P0 = scale_pref(data[\"Pref_raw\"], x0, data)\n\n    objective, weights = make_objective(data, Pref, wP=8.0, wH=0.02, wDu=5e-4)\n    # Suppress noisy SciPy warning about delta_grad during quasi-Newton updates\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\",\n            message=r\"delta_grad == 0.0\",\n            category=UserWarning,\n            module=r\"scipy\\.optimize\\.\\_differentiable_functions\",\n        )\n        res = minimize(\n            fun=objective,\n            x0=x0,\n            method='trust-constr',\n            bounds=make_bounds(data),\n            constraints=make_constraints(data),\n            options=dict(maxiter=1000, disp=verbose),\n        )\n\n    H,u,z = unpack(res.x, data[\"M\"], data[\"N\"])\n    P = compute_total_power(H,u,data[\"params\"])\n    dyn_res, coup_res = residuals(res.x, data)\n\n    # ---------- Diagnostics ----------\n    if verbose:\n        terms = decompose_objective(res.x, data, Pref, **weights)\n        print(\"\\n=== Objective decomposition ===\")\n        print({k: float(v) if not isinstance(v, dict) else {kk: float(vv) for kk,vv in v.items()} for k,v in terms.items()})\n\n        print(\"\\n=== Constraint residuals (max |.|) ===\")\n        print(\"dyn:\", float(np.max(np.abs(dyn_res)))), print(\"coup:\", float(np.max(np.abs(coup_res))))\n\n        # Muskingum coefficient sanity and residuals\n        if data.get(\"M\", 1) > 1:\n            K_list = data.get(\"muskingum\", {}).get(\"K\", [])\n            X_list = data.get(\"muskingum\", {}).get(\"X\", [])\n            coef_checks = []\n            mean_abs_res = []\n            for i in range(1, data[\"M\"]):\n                Ki = K_list[i-1] if i-1 < len(K_list) else 1800.0\n                Xi = X_list[i-1] if i-1 < len(X_list) else 0.2\n                C0, C1, C2 = muskingum_coeffs(Ki, Xi, data[\"dt\"])\n                coef_checks.append(dict(link=i, sum=float(C0+C1+C2), min_coef=float(min(C0,C1,C2))))\n                # compute mean abs residual for this link\n                res_vals = []\n                for n in range(data[\"N\"]-1):\n                    _, I_n   = integrate_interval(H[i-1,n],   u[i-1,n],   z[i-1,n],   data[\"dt\"], data[\"nsub\"], data[\"params\"][i-1])\n                    _, I_np1 = integrate_interval(H[i-1,n+1], u[i-1,n+1], z[i-1,n+1], data[\"dt\"], data[\"nsub\"], data[\"params\"][i-1])\n                    res_vals.append(float(abs(z[i,n+1] - (C0*I_np1 + C1*I_n + C2*z[i,n]))))\n                mean_abs_res.append(dict(link=i, mean_abs=float(np.mean(res_vals))))\n            print(\"\\n=== Muskingum coeff checks (sum, min_coef) ===\")\n            print(coef_checks)\n            print(\"=== Muskingum mean |residual| per link ===\")\n            print(mean_abs_res)\n\n    # Per-interval diagnostic table for each reach (kept for debugging but unused here)\n    def interval_table(i):\n        rp = data[\"params\"][i]\n        rows = []\n        for n in range(data[\"N\"]):\n            qb = q_bypass(H[i,n], rp)\n            net = z[i,n] - (u[i,n] + qb)\n            dH = data[\"dt\"]*net/rp.A_surf\n            rows.append(dict(interval=n, Hn=H[i,n], Hn1=H[i,n+1], u=u[i,n], z=z[i,n], qb=qb, net_flow=net, dH_pred=dH))\n        return pd.DataFrame(rows)\n\n    # summary and tables available to callers if needed\n    tables = [interval_table(i) for i in range(data[\"M\"])]\n    summary = pd.DataFrame([\n        dict(reach=i+1,\n             H_mean=float(np.mean(H[i])), H_std=float(np.std(H[i])),\n             u_mean=float(np.mean(u[i])), u_std=float(np.std(u[i])),\n             z_mean=float(np.mean(z[i])), z_std=float(np.std(z[i])))\n        for i in range(data[\"M\"])\n    ])\n\n    # ---------- Plots ----------\n    M,N = data[\"M\"], data[\"N\"]\n    t_nodes = np.arange(N+1)\n    t = np.arange(N)\n\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    fig.suptitle('Hydroelectric System Optimization Results', fontsize=16)\n\n    ax1 = axes[0, 0]\n    for i in range(M):\n        ax1.plot(t_nodes, H[i], marker='o', label=f'Reach {i+1}')\n    ax1.set_xlabel(\"Node n\"); ax1.set_ylabel(\"H [m]\"); ax1.set_title(\"Water Levels\")\n    ax1.grid(True); ax1.legend()\n\n    ax2 = axes[0, 1]\n    for i in range(M):\n        ax2.step(t, u[i], where='post', label=f'Reach {i+1}')\n    ax2.set_xlabel(\"Interval n\"); ax2.set_ylabel(\"u [m³/s]\"); ax2.set_title(\"Turbine Discharge\")\n    ax2.grid(True); ax2.legend()\n\n    ax3 = axes[1, 0]\n    for i in range(M):\n        ax3.step(t, z[i], where='post', label=f'Reach {i+1}')\n    ax3.set_xlabel(\"Interval n\"); ax3.set_ylabel(\"z [m³/s]\"); ax3.set_title(\"Inflow (Coupling)\")\n    ax3.grid(True); ax3.legend()\n\n    ax4 = axes[1, 1]\n    ax4.plot(t, P0, marker='s', label=\"Power @ x0\")\n    ax4.plot(t, P, marker='o', label=\"Power @ optimum\")\n    ax4.plot(t, Pref, marker='x', label=\"Scaled Pref\")\n    ax4.set_xlabel(\"Interval n\"); ax4.set_ylabel(\"Power units\"); ax4.set_title(\"Power Tracking\")\n    ax4.legend(); ax4.grid(True)\n\n    plt.tight_layout()\n    if save_path:\n        fig.savefig(save_path, bbox_inches='tight')\n    if show:\n        return None\n    return fig\n\n\n# Run the demo directly when loaded in a notebook cell\nrun_demo(show=True, save_path=None, verbose=False)\n\nThe figure shows the result of a multiple-shooting optimization applied to a three-reach hydroelectric cascade. The time horizon is discretized into 16 intervals, and SciPy’s trust-constr solver is used to find a feasible control sequence that satisfies mass balance, turbine and spillway limits, and Muskingum-style routing dynamics. Each reach integrates its own local ODE, with continuity constraints linking the flows between reaches.\n\nThe top-left panel shows the water levels in each reservoir. We observe that upstream reservoirs tend to increase their levels ahead of discharge events, building potential energy before releasing water downstream. The top-right panel shows turbine discharges for each reach. These vary smoothly and are temporally coordinated across the system. The bottom-right panel compares the total generation to a synthetic demand profile, which is generated by a sum of time-shifted sigmoids and normalized to be feasible given turbine capacities. The optimized schedule (orange) tracks this demand closely, while the initial guess (blue) lags behind. The bottom-left panel plots the routed inflows between reaches, which display the expected lag and smoothing effects from Muskingum routing. The interplay between these plots shows how the system anticipates, stores, and routes water to meet time-varying generation targets within physical and operational limits.","type":"content","url":"/collocation#hydro-cascade-scheduling-with-physical-routing-and-multiple-shooting","position":45},{"hierarchy":{"lvl1":"Dynamic Programming"},"type":"lvl1","url":"/dp","position":0},{"hierarchy":{"lvl1":"Dynamic Programming"},"content":"Unlike the methods we’ve discussed so far, dynamic programming takes a step back and considers an entire family of related problems rather than a single optimization problem. This approach, while seemingly more complex at first glance, can often lead to efficient solutions.\n\nDynamic programming leverage the solution structure underlying many control problems that allows for a decomposition it into smaller, more manageable subproblems. Each subproblem is itself an optimization problem, embedded within the larger whole. This recursive structure is the foundation upon which dynamic programming constructs its solutions.\n\nTo ground our discussion, let us return to the domain of discrete-time optimal control problems (DOCPs). These problems frequently arise from the discretization of continuous-time optimal control problems. While the focus here will be on deterministic problems, these concepts extend naturally to stochastic problems by taking the expectation over the random quantities.\n\nConsider a typical DOCP of Bolza type:\\begin{align*}\n\\text{minimize} \\quad & J \\triangleq c_\\mathrm{T}(\\mathbf{x}_T) + \\sum_{t=1}^{T-1} c_t(\\mathbf{x}_t, \\mathbf{u}_t) \\\\\n\\text{subject to} \\quad \n& \\mathbf{x}_{t+1} = \\mathbf{f}_t(\\mathbf{x}_t, \\mathbf{u}_t), \\quad t = 1, \\ldots, T-1, \\\\\n& \\mathbf{u}_{lb} \\leq \\mathbf{u}_t \\leq \\mathbf{u}_{ub}, \\quad t = 1, \\ldots, T, \\\\\n& \\mathbf{x}_{lb} \\leq \\mathbf{x}_t \\leq \\mathbf{x}_{ub}, \\quad t = 1, \\ldots, T, \\\\\n\\text{given} \\quad & \\mathbf{x}_1\n\\end{align*}\n\nRather than considering only the total cost from the initial time to the final time, dynamic programming introduces the concept of cost from an arbitrary point in time to the end. This leads to the definition of the “cost-to-go” or “value function” J_k(\\mathbf{x}_k):J_k(\\mathbf{x}_k) \\triangleq c_\\mathrm{T}(\\mathbf{x}_T) + \\sum_{t=k}^{T-1} c_t(\\mathbf{x}_t, \\mathbf{u}_t)\n\nThis function represents the total cost incurred from stage k onwards to the end of the time horizon, given that the system is initialized in state \\mathbf{x}_k at stage k. Suppose the problem has been solved from stage k+1 to the end, yielding the optimal cost-to-go J_{k+1}^\\star(\\mathbf{x}_{k+1}) for any state \\mathbf{x}_{k+1} at stage k+1. The question then becomes: how does this information inform the decision at stage k?\n\nGiven knowledge of the optimal behavior from k+1 onwards, the task reduces to determining the optimal action \\mathbf{u}_k at stage k. This control should minimize the sum of the immediate cost c_k(\\mathbf{x}_k, \\mathbf{u}_k) and the optimal future cost J_{k+1}^\\star(\\mathbf{x}_{k+1}), where \\mathbf{x}_{k+1} is the resulting state after applying action \\mathbf{u}_k. Mathematically, this is expressed as:J_k^\\star(\\mathbf{x}_k) = \\min_{\\mathbf{u}_k} \\left[ c_k(\\mathbf{x}_k, \\mathbf{u}_k) + J_{k+1}^\\star(\\mathbf{f}_k(\\mathbf{x}_k, \\mathbf{u}_k)) \\right]\n\nThis equation is known as Bellman’s equation, named after Richard Bellman, who formulated the principle of optimality:\n\nAn optimal policy has the property that whatever the previous state and decision, the remaining decisions must constitute an optimal policy with regard to the state resulting from the previous decision.\n\nIn other words, any sub-path of an optimal path, from any intermediate point to the end, must itself be optimal. This principle is the basis for the backward induction procedure which computes the optimal value function and provides closed-loop control capabilities without having to use an explicit NLP solver.\n\nDynamic programming can handle nonlinear systems and non-quadratic cost functions naturally. It provides a global optimal solution, when one exists, and can incorporate state and control constraints with relative ease. However, as the dimension of the state space increases, this approach suffers from what Bellman termed the “curse of dimensionality.” The computational complexity and memory requirements grow exponentially with the state dimension, rendering direct application of dynamic programming intractable for high-dimensional problems.\n\nFortunately, learning-based methods offer efficient tools to combat the curse of dimensionality on two fronts: by using function approximation (e.g., neural networks) to avoid explicit discretization, and by leveraging randomization through Monte Carlo methods inherent in the learning paradigm. Most of this course is dedicated to those ideas.","type":"content","url":"/dp","position":1},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Backward Recursion"},"type":"lvl3","url":"/dp#backward-recursion","position":2},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Backward Recursion"},"content":"The principle of optimality provides a methodology for solving optimal control problems. Beginning at the final time horizon and working backwards, at each stage the local optimization problem given by Bellman’s equation is solved. This process, termed backward recursion or backward induction, constructs the optimal value function stage by stage.\n\nBackward Recursion for Dynamic Programming\n\nInput: Terminal cost function c_\\mathrm{T}(\\cdot), stage cost functions c_t(\\cdot, \\cdot), system dynamics f_t(\\cdot, \\cdot), time horizon \\mathrm{T}\n\nOutput: Optimal value functions J_t^\\star(\\cdot) and optimal control policies \\mu_t^\\star(\\cdot) for t = 1, \\ldots, T\n\nInitialize J_T^\\star(\\mathbf{x}) = c_\\mathrm{T}(\\mathbf{x}) for all \\mathbf{x} in the state space\n\nFor t = T-1, T-2, \\ldots, 1:\n\nFor each state \\mathbf{x} in the state space:\n\nCompute J_t^\\star(\\mathbf{x}) = \\min_{\\mathbf{u}} \\left[ c_t(\\mathbf{x}, \\mathbf{u}) + J_{t+1}^\\star(f_t(\\mathbf{x}, \\mathbf{u})) \\right]\n\nCompute \\mu_t^\\star(\\mathbf{x}) = \\arg\\min_{\\mathbf{u}} \\left[ c_t(\\mathbf{x}, \\mathbf{u}) + J_{t+1}^\\star(f_t(\\mathbf{x}, \\mathbf{u})) \\right]\n\nEnd For\n\nEnd For\n\nReturn J_t^\\star(\\cdot), \\mu_t^\\star(\\cdot) for t = 1, \\ldots, T\n\nUpon completion of this backward pass, we now have access to the optimal control to take at any stage and in any state. Furthermore, we can simulate optimal trajectories from any initial state and applying the optimal policy at each stage to generate the optimal trajectory.\n\nBackward induction solves deterministic Bolza DOCP\n\nSetting. Let \\mathbf{x}_{t+1}=\\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t) for t=1,\\dots,T-1, with admissible action sets \\mathcal{U}_t(\\mathbf{x})\\neq\\varnothing. Let stage costs c_t(\\mathbf{x},\\mathbf{u}) and terminal cost c_\\mathrm{T}(\\mathbf{x}) be real-valued and bounded below. Assume for every (t,\\mathbf{x}) the one-step problem\\min_{\\mathbf{u}\\in\\mathcal{U}_t(\\mathbf{x})}\\big\\{c_t(\\mathbf{x},\\mathbf{u})+J_{t+1}^\\star(\\mathbf{f}_t(\\mathbf{x},\\mathbf{u}))\\big\\}\n\nadmits a minimizer (e.g., compact \\mathcal{U}_t(\\mathbf{x}) and continuity suffice).\n\nDefine J_T^\\star(\\mathbf{x}) \\equiv c_\\mathrm{T}(\\mathbf{x}) and for t=T-1,\\dots,1J_t^\\star(\\mathbf{x}) \\;\\triangleq\\; \\min_{\\mathbf{u}\\in\\mathcal{U}_t(\\mathbf{x})}\n\\Big[c_t(\\mathbf{x},\\mathbf{u})+J_{t+1}^\\star\\big(\\mathbf{f}_t(\\mathbf{x},\\mathbf{u})\\big)\\Big],\n\nand select any minimizer \\boldsymbol{\\mu}_t^\\star(\\mathbf{x})\\in\\arg\\min(\\cdot).\n\nClaim. For every initial state \\mathbf{x}_1, the control sequence\n\\boldsymbol{\\mu}_1^\\star(\\mathbf{x}_1),\\dots,\\boldsymbol{\\mu}_{T-1}^\\star(\\mathbf{x}_{T-1})\ngenerated by these selectors is optimal for the Bolza problem, and\nJ_1^\\star(\\mathbf{x}_1) equals the optimal cost. Moreover, J_t^\\star(\\cdot) is the optimal cost-to-go from stage t for every state, i.e., backward induction recovers the entire value function.\n\nWe give a direct proof by backward induction. The general idea is that any feasible sequence can be improved by replacing its tail with an optimal continuation, so optimal solutions can be built stage by stage. This is sometimes called a “cut-and-paste” argument.\n\nStep 1 (verification of the recursion at a fixed stage).Fix t\\in\\{1,\\dots,T-1\\} and \\mathbf{x}\\in\\mathbb{X}. Consider any admissible control sequence \\mathbf{u}_t,\\dots,\\mathbf{u}_{T-1} starting from \\mathbf{x}_t=\\mathbf{x} and define the induced states \\mathbf{x}_{k+1}=\\mathbf{f}_k(\\mathbf{x}_k,\\mathbf{u}_k). Its total cost from t isc_t(\\mathbf{x}_t,\\mathbf{u}_t)+\\sum_{k=t+1}^{T-1}c_k(\\mathbf{x}_k,\\mathbf{u}_k)+c_\\mathrm{T}(\\mathbf{x}_T).\n\nBy definition of J_{t+1}^\\star, the tail cost satisfies\\sum_{k=t+1}^{T-1}c_k(\\mathbf{x}_k,\\mathbf{u}_k)+c_\\mathrm{T}(\\mathbf{x}_T)\n\\;\\ge\\; J_{t+1}^\\star(\\mathbf{x}_{t+1})\n\\;=\\; J_{t+1}^\\star\\big(\\mathbf{f}_t(\\mathbf{x},\\mathbf{u}_t)\\big).\n\nHence the total cost is bounded below byc_t(\\mathbf{x},\\mathbf{u}_t)+J_{t+1}^\\star\\big(\\mathbf{f}_t(\\mathbf{x},\\mathbf{u}_t)\\big).\n\nTaking the minimum over \\mathbf{u}_t\\in\\mathcal{U}_t(\\mathbf{x}) yields\\text{(any admissible cost from $t$)}\\;\\ge\\;J_t^\\star(\\mathbf{x}).\n\\tag{$\\ast$}\n\nStep 2 (existence of an optimal prefix at stage t).By assumption, there exists \\boldsymbol{\\mu}_t^\\star(\\mathbf{x}) attaining the minimum in the definition of J_t^\\star(\\mathbf{x}). If we now paste to \\boldsymbol{\\mu}_t^\\star(\\mathbf{x}) an optimal tail policy from t+1 (whose existence we will establish inductively), the resulting sequence attains cost exactlyc_t\\big(\\mathbf{x},\\boldsymbol{\\mu}_t^\\star(\\mathbf{x})\\big)\n+J_{t+1}^\\star\\!\\Big(\\mathbf{f}_t\\big(\\mathbf{x},\\boldsymbol{\\mu}_t^\\star(\\mathbf{x})\\big)\\Big)\n=J_t^\\star(\\mathbf{x}),\n\nwhich matches the lower bound (\\ast); hence it is optimal from t.\n\nStep 3 (backward induction over time).Base case t=T. The statement holds because J_T^\\star(\\mathbf{x})=c_\\mathrm{T}(\\mathbf{x}) and there is no control to choose.\n\nInductive step. Assume the tail statement holds for t+1: from any state \\mathbf{x}_{t+1} there exists an optimal control sequence realizing J_{t+1}^\\star(\\mathbf{x}_{t+1}). Then by Steps 1–2, selecting \\boldsymbol{\\mu}_t^\\star(\\mathbf{x}_t) at stage t and concatenating the optimal tail from t+1 yields an optimal sequence from t with value J_t^\\star(\\mathbf{x}_t).\n\nBy backward induction, the claim holds for all t, in particular for t=1 and any initial \\mathbf{x}_1. Therefore the backward recursion both certifies optimality (verification) and constructs an optimal policy (synthesis), while recovering the full family \\{J_t^\\star\\}_{t=1}^T.\n\nNo “big NLP” required\n\nThe Bolza DOCP over the whole horizon couples all controls through the dynamics and is typically posed as a single large nonlinear program. The proof shows you can solve T-1 sequences of one-step problems instead: at each (t,\\mathbf{x}) minimize\\mathbf{u}\\mapsto c_t(\\mathbf{x},\\mathbf{u}) + J_{t+1}^\\star(\\mathbf{f}_t(\\mathbf{x},\\mathbf{u})).\n\nIn finite state–action spaces this becomes pure table lookup and argmin. In continuous spaces you still solve local one-step minimizations, but you avoid a monolithic horizon-coupled NLP.\n\nGraph interpretation (optional intuition)\n\nUnroll time to form a DAG whose nodes are (t,\\mathbf{x}) and whose edges correspond to feasible controls with edge weight c_t(\\mathbf{x},\\mathbf{u}). The terminal node cost is c_\\mathrm{T}(\\cdot). The Bolza problem is a shortest-path problem on this DAG. The equationJ_t^\\star(\\mathbf{x})=\\min_{\\mathbf{u}}\\{c_t(\\mathbf{x},\\mathbf{u})+J_{t+1}^\\star(\\mathbf{f}_t(\\mathbf{x},\\mathbf{u}))\\}\n\nis exactly the dynamic programming recursion for shortest paths on acyclic graphs, hence backward induction is optimal. ```{prf:remark} If minimizers may not exist\nReplace each \"min\" by \"inf\" in the definitions and state that for every $\\varepsilon>0$ there exist $\\varepsilon$-optimal selectors $\\boldsymbol{\\mu}_t^\\varepsilon(\\cdot)$ achieving cost within $\\varepsilon$ of $J_t^\\star(\\cdot)$. The same cut-and-paste and induction go through.\n``` ","type":"content","url":"/dp#backward-recursion","position":3},{"hierarchy":{"lvl1":"Dynamic Programming","lvl4":"Example: Optimal Harvest in Resource Management","lvl3":"Backward Recursion"},"type":"lvl4","url":"/dp#example-optimal-harvest-in-resource-management","position":4},{"hierarchy":{"lvl1":"Dynamic Programming","lvl4":"Example: Optimal Harvest in Resource Management","lvl3":"Backward Recursion"},"content":"Dynamic programming is often used in resource management and conservation biology to devise policies to be implemented by decision makers and stakeholders : for eg. in fishereries, or timber harvesting. Per \n\nConroy & Peterson (2013), we consider a population of a particular species, whose abundance we denote by x_t, where t represents discrete time steps. Our objective is to maximize the cumulative harvest over a finite time horizon, while also considering the long-term sustainability of the population. This optimization problem can be formulated as:\\text{maximize} \\quad \\sum_{t=t_0}^{t_f} F(x_t \\cdot h_t) + F_\\mathrm{T}(x_{t_f})\n\nHere, F(\\cdot) represents the immediate reward function associated with harvesting, h_t is the harvest rate at time t, and F_\\mathrm{T}(\\cdot) denotes a terminal value function that could potentially assign value to the final population state. In this particular problem, we assign no terminal value to the final population state, setting F_\\mathrm{T}(x_{t_f}) = 0 and allowing us to focus solely on the cumulative harvest over the time horizon.\n\nIn our model population model, the abundance of a specicy x ranges from 1 to 100 individuals. The decision variable is the harvest rate h, which can take values from the set D = \\{0, 0.1, 0.2, 0.3, 0.4, 0.5\\}. The population dynamics are governed by a modified logistic growth model:x_{t+1} = x_t + 0.3x_t(1 - x_t/125) - h_tx_t\n\nwhere the 0.3 represents the growth rate and 125 is the carrying capacity (the maximum population size given the available resources). The logistic growth model returns continuous values; however our DP formulation uses a discrete state space. Therefore, we also round the the outcomes to the nearest integer.\n\nApplying the principle of optimality, we can express the optimal value function J^\\star(x_t,t) recursively:J^\\star(x_t, t) = \\max_{h_t \\in D} (F(x, h, t) + J^*(x_{t+1}, t+1))\n\nwith the boundary condition J^*(x_{t_f}) = 0.\n\nIt’s worth noting that while this example uses a relatively simple model, the same principles can be applied to more complex scenarios involving stochasticity, multiple species interactions, or spatial heterogeneity.\n\n#  label: dp-harvest-policy\n#  caption: Dynamic programming harvest example: printed output shows the optimal policy table, resulting population trajectory, and per-period harvests for an initial population of 50 fish.\n\n%config InlineBackend.figure_format = 'retina'\nimport numpy as np\n\n# Parameters\nr_max = 0.3\nK = 125\nT = 20  # Number of time steps\nN_max = 100  # Maximum population size to consider\nh_max = 0.5  # Maximum harvest rate\nh_step = 0.1  # Step size for harvest rate\n\n# Create state and decision spaces\nN_space = np.arange(1, N_max + 1)\nh_space = np.arange(0, h_max + h_step, h_step)\n\n# Initialize value function and policy\nV = np.zeros((T + 1, len(N_space)))\npolicy = np.zeros((T, len(N_space)))\n\n# Terminal value function (F_T)\ndef terminal_value(N):\n    return 0\n\n# State return function (F)\ndef state_return(N, h):\n    return N * h\n\n# State dynamics function\ndef state_dynamics(N, h):\n    return N + r_max * N * (1 - N / K) - N * h\n\n# Backward iteration\nfor t in range(T - 1, -1, -1):\n    for i, N in enumerate(N_space):\n        max_value = float('-inf')\n        best_h = 0\n\n        for h in h_space:\n            if h > 1:  # Ensure harvest rate doesn't exceed 100%\n                continue\n\n            next_N = state_dynamics(N, h)\n            if next_N < 1:  # Ensure population doesn't go extinct\n                continue\n\n            next_N_index = np.searchsorted(N_space, next_N)\n            if next_N_index == len(N_space):\n                next_N_index -= 1\n\n            value = state_return(N, h) + V[t + 1, next_N_index]\n\n            if value > max_value:\n                max_value = value\n                best_h = h\n\n        V[t, i] = max_value\n        policy[t, i] = best_h\n\n# Function to simulate the optimal policy with conversion to Python floats\ndef simulate_optimal_policy(initial_N, T):\n    trajectory = [float(initial_N)]  # Ensure first value is a Python float\n    harvests = []\n\n    for t in range(T):\n        N = trajectory[-1]\n        N_index = np.searchsorted(N_space, N)\n        if N_index == len(N_space):\n            N_index -= 1\n\n        h = policy[t, N_index]\n        harvests.append(float(N * h))  # Ensure harvest is a Python float\n\n        next_N = state_dynamics(N, h)\n        trajectory.append(float(next_N))  # Ensure next population value is a Python float\n\n    return trajectory, harvests\n\n# Example usage\ninitial_N = 50\ntrajectory, harvests = simulate_optimal_policy(initial_N, T)\n\nprint(\"Optimal policy:\")\nprint(policy)\nprint(\"\\nPopulation trajectory:\", trajectory)\nprint(\"Harvests:\", harvests)\nprint(\"Total harvest:\", sum(harvests))\n\n","type":"content","url":"/dp#example-optimal-harvest-in-resource-management","position":5},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Handling Continuous Spaces with Interpolation"},"type":"lvl3","url":"/dp#handling-continuous-spaces-with-interpolation","position":6},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Handling Continuous Spaces with Interpolation"},"content":"In many real-world problems, such as our resource management example, the state space is inherently continuous. Dynamic programming, however, is usually defined on discrete state spaces. To reconcile this, we approximate the value function on a finite grid of points and use interpolation to estimate its value elsewhere.\n\nIn our earlier example, we acted as if population sizes could only be whole numbers: 1 fish, 2 fish, 3 fish. But real measurements don’t fit neatly. What do you do with a survey that reports 42.7 fish? Our reflex in the code example was to round to the nearest integer, effectively saying “let’s just call it 43.” This corresponds to nearest-neighbor interpolation, also known as discretization. It’s the zeroth-order case: you assume the value between grid points is constant and equal to the closest one. In practice, this amounts to overlaying a grid on the continuous landscape and forcing yourself to stand at the intersections. In our demo code, this step was carried out with \n\nnumpy.searchsorted.\n\nWhile easy to implement, nearest-neighbor interpolation can introduce artifacts:\n\nDecisions may change abruptly, even if the state only shifts slightly.\n\nPrecision is lost, especially in regimes where small variations matter.\n\nThe curse of dimensionality forces an impractically fine grid if many state variables are added.\n\nTo address these issues, we can use higher-order interpolation. Instead of taking the nearest neighbor, we estimate the value at off-grid points by leveraging multiple nearby values.","type":"content","url":"/dp#handling-continuous-spaces-with-interpolation","position":7},{"hierarchy":{"lvl1":"Dynamic Programming","lvl4":"Backward Recursion with Interpolation","lvl3":"Handling Continuous Spaces with Interpolation"},"type":"lvl4","url":"/dp#backward-recursion-with-interpolation","position":8},{"hierarchy":{"lvl1":"Dynamic Programming","lvl4":"Backward Recursion with Interpolation","lvl3":"Handling Continuous Spaces with Interpolation"},"content":"Suppose we have computed J_{k+1}^\\star(\\mathbf{x}) only at grid points \\mathbf{x} \\in \\mathcal{X}_\\text{grid}.\nTo evaluate Bellman’s equation at an arbitrary \\mathbf{x}_{k+1}, we interpolate.\nFormally, let I_{k+1}(\\mathbf{x}) be the interpolation operator that extends the value function from \\mathcal{X}_\\text{grid} to the continuous space. Then:J_k^\\star(\\mathbf{x}_k) \n= \\min_{\\mathbf{u}_k} \n\\Big[ c_k(\\mathbf{x}_k, \\mathbf{u}_k) \n+ I_{k+1}\\big(\\mathbf{f}_k(\\mathbf{x}_k, \\mathbf{u}_k)\\big) \\Big].\n\nFor instance, in one dimension, linear interpolation gives:I_{k+1}(x) = J_{k+1}^\\star(x_l) + \\frac{x - x_l}{x_u - x_l} \\big(J_{k+1}^\\star(x_u) - J_{k+1}^\\star(x_l)\\big),\n\nwhere x_l and x_u are the nearest grid points bracketing x. Linear interpolation is often sufficient, but higher-order methods (cubic splines, radial basis functions) can yield smoother and more accurate estimates. The choice of interpolation scheme and grid layout both affect accuracy and efficiency. A finer grid improves resolution but increases computational cost, motivating strategies like adaptive grid refinement or replacing interpolation altogether with parametric function approximation which we are going to see later in this book.\n\nIn higher-dimensional spaces, naive interpolation becomes prohibitively expensive due to the curse of dimensionality. Several approaches such as tensorized multilinear interpolation, radial basis functions, and machine learning models address this challenge by extending a common principle: they approximate the value function at unobserved points using information from a finite set of evaluations. However, as dimensionality continues to grow, even tensor methods face scalability limits, which is why flexible parametric models like neural networks have become essential tools for high-dimensional function approximation.\n\nBackward Recursion with Interpolation\n\nInput:\n\nTerminal cost c_\\mathrm{T}(\\cdot)\n\nStage costs c_t(\\cdot,\\cdot)\n\nDynamics f_t(\\cdot,\\cdot)\n\nTime horizon T\n\nState grid \\mathcal{X}_\\text{grid}\n\nAction set \\mathcal{U}\n\nInterpolation method \\mathcal{I}(\\cdot) (e.g., linear, cubic spline, RBF, neural network)\n\nOutput: Value functions J_t^\\star(\\cdot) and policies \\mu_t^\\star(\\cdot) for all t\n\nInitialize terminal values:\n\nCompute J_T^\\star(\\mathbf{x}) = c_\\mathrm{T}(\\mathbf{x}) for all \\mathbf{x} \\in \\mathcal{X}_\\text{grid}\n\nFit interpolator: I_T \\leftarrow \\mathcal{I}(\\{\\mathbf{x}, J_T^\\star(\\mathbf{x})\\}_{\\mathbf{x} \\in \\mathcal{X}_\\text{grid}})\n\nBackward recursion:For t = T-1, T-2, \\dots, 0:\n\na. Bellman update at grid points:For each \\mathbf{x} \\in \\mathcal{X}_\\text{grid}:\n\nFor each \\mathbf{u} \\in \\mathcal{U}:\n\nCompute next state: \\mathbf{x}_\\text{next} = f_t(\\mathbf{x}, \\mathbf{u})\n\nInterpolate future cost: \\hat{J}_{t+1}(\\mathbf{x}_\\text{next}) = I_{t+1}(\\mathbf{x}_\\text{next})\n\nCompute total cost: J_t(\\mathbf{x}, \\mathbf{u}) = c_t(\\mathbf{x}, \\mathbf{u}) + \\hat{J}_{t+1}(\\mathbf{x}_\\text{next})\n\nMinimize over actions: J_t^\\star(\\mathbf{x}) = \\min_{\\mathbf{u} \\in \\mathcal{U}} J_t(\\mathbf{x}, \\mathbf{u})\n\nStore optimal action: \\mu_t^\\star(\\mathbf{x}) = \\arg\\min_{\\mathbf{u} \\in \\mathcal{U}} J_t(\\mathbf{x}, \\mathbf{u})\n\nb. Fit interpolator for current stage:I_t \\leftarrow \\mathcal{I}(\\{\\mathbf{x}, J_t^\\star(\\mathbf{x})\\}_{\\mathbf{x} \\in \\mathcal{X}_\\text{grid}})\n\nReturn: Interpolated value functions \\{I_t\\}_{t=0}^T and policies \\{\\mu_t^\\star\\}_{t=0}^{T-1}","type":"content","url":"/dp#backward-recursion-with-interpolation","position":9},{"hierarchy":{"lvl1":"Dynamic Programming","lvl5":"Example: Optimal Harvest with Linear Interpolation","lvl4":"Backward Recursion with Interpolation","lvl3":"Handling Continuous Spaces with Interpolation"},"type":"lvl5","url":"/dp#example-optimal-harvest-with-linear-interpolation","position":10},{"hierarchy":{"lvl1":"Dynamic Programming","lvl5":"Example: Optimal Harvest with Linear Interpolation","lvl4":"Backward Recursion with Interpolation","lvl3":"Handling Continuous Spaces with Interpolation"},"content":"Here is a demonstration of the backward recursion procedure using linear interpolation.\n\n#  label: dp-harvest-interp\n#  caption: Backward recursion with linear interpolation: console output summarizes the smoothed optimal policy, state trajectory, and harvest totals for the resource management example.\n\n\nimport numpy as np\n\n# Parameters\nr_max = 0.3\nK = 125\nT = 20  # Number of time steps\nN_max = 100  # Maximum population size to consider\nh_max = 0.5  # Maximum harvest rate\nh_step = 0.1  # Step size for harvest rate\n\n# Create state and decision spaces\nN_space = np.arange(1, N_max + 1)\nh_space = np.arange(0, h_max + h_step, h_step)\n\n# Initialize value function and policy\nV = np.zeros((T + 1, len(N_space)))\npolicy = np.zeros((T, len(N_space)))\n\n# Terminal value function (F_T)\ndef terminal_value(N):\n    return 0\n\n# State return function (F)\ndef state_return(N, h):\n    return N * h\n\n# State dynamics function\ndef state_dynamics(N, h):\n    return N + r_max * N * (1 - N / K) - N * h\n\n# Function to linearly interpolate between grid points in N_space\ndef interpolate_value_function(V, N_space, next_N, t):\n    if next_N <= N_space[0]:\n        return V[t, 0]  # Below or at minimum population, return minimum value\n    if next_N >= N_space[-1]:\n        return V[t, -1]  # Above or at maximum population, return maximum value\n    \n    # Find indices to interpolate between\n    lower_idx = np.searchsorted(N_space, next_N) - 1\n    upper_idx = lower_idx + 1\n    \n    # Linear interpolation\n    N_lower = N_space[lower_idx]\n    N_upper = N_space[upper_idx]\n    weight = (next_N - N_lower) / (N_upper - N_lower)\n    return (1 - weight) * V[t, lower_idx] + weight * V[t, upper_idx]\n\n# Backward iteration with interpolation\nfor t in range(T - 1, -1, -1):\n    for i, N in enumerate(N_space):\n        max_value = float('-inf')\n        best_h = 0\n        \n        for h in h_space:\n            if h > 1:  # Ensure harvest rate doesn't exceed 100%\n                continue\n            \n            next_N = state_dynamics(N, h)\n            if next_N < 1:  # Ensure population doesn't go extinct\n                continue\n            \n            # Interpolate value for next_N\n            value = state_return(N, h) + interpolate_value_function(V, N_space, next_N, t + 1)\n            \n            if value > max_value:\n                max_value = value\n                best_h = h\n        \n        V[t, i] = max_value\n        policy[t, i] = best_h\n\n# Function to simulate the optimal policy using interpolation\ndef simulate_optimal_policy(initial_N, T):\n    trajectory = [initial_N]\n    harvests = []\n\n    for t in range(T):\n        N = trajectory[-1]\n        \n        # Interpolate optimal harvest rate\n        if N <= N_space[0]:\n            h = policy[t, 0]\n        elif N >= N_space[-1]:\n            h = policy[t, -1]\n        else:\n            lower_idx = np.searchsorted(N_space, N) - 1\n            upper_idx = lower_idx + 1\n            weight = (N - N_space[lower_idx]) / (N_space[upper_idx] - N_space[lower_idx])\n            h = (1 - weight) * policy[t, lower_idx] + weight * policy[t, upper_idx]\n        \n        harvests.append(float(N * h))  # Ensure harvest is a Python float\n        next_N = state_dynamics(N, h)\n        trajectory.append(float(next_N))  # Ensure next population value is a Python float\n\n    return trajectory, harvests\n\n# Example usage\ninitial_N = 50\ntrajectory, harvests = simulate_optimal_policy(initial_N, T)\n\nprint(\"Optimal policy:\")\nprint(policy)\nprint(\"\\nPopulation trajectory:\", trajectory)\nprint(\"Harvests:\", harvests)\nprint(\"Total harvest:\", sum(harvests))\n\nDue to pedagogical considerations, this example is using our own implementation of the linear interpolation procedure. However, a more general and practical approach would be to use a built-in interpolation procedure in Numpy. Because our state space has a single dimension, we can simply use \n\nscipy​.interpolate​.interp1d which offers various interpolation methods through its kind argument, which can take values in ‘linear’, ‘nearest’, ‘nearest-up’, ‘zero’, ‘slinear’, ‘quadratic’, ‘cubic’, ‘previous’, or ‘next’. ‘zero’, ‘slinear’, ‘quadratic’ and ‘cubic’.\n\nHere’s a more general implementation which here uses cubic interpolation through the scipy.interpolate.interp1d function:\n\n#  label: dp-harvest-cubic\n#  caption: Cubic interpolation further smooths the optimal harvest policy—this output prints the leading rows of the policy table along with the resulting trajectory and harvest statistics.\n\n\nimport numpy as np\nfrom scipy.interpolate import interp1d\n\n# Parameters\nr_max = 0.3\nK = 125\nT = 20  # Number of time steps\nN_max = 100  # Maximum population size to consider\nh_max = 0.5  # Maximum harvest rate\nh_step = 0.1  # Step size for harvest rate\n\n# Create state and decision spaces\nN_space = np.arange(1, N_max + 1)\nh_space = np.arange(0, h_max + h_step, h_step)\n\n# Initialize value function and policy\nV = np.zeros((T + 1, len(N_space)))\npolicy = np.zeros((T, len(N_space)))\n\n# Terminal value function (F_T)\ndef terminal_value(N):\n    return 0\n\n# State return function (F)\ndef state_return(N, h):\n    return N * h\n\n# State dynamics function\ndef state_dynamics(N, h):\n    return N + r_max * N * (1 - N / K) - N * h\n\n# Function to create interpolation function for a given time step\ndef create_interpolator(V_t, N_space):\n    return interp1d(N_space, V_t, kind='cubic', bounds_error=False, fill_value=(V_t[0], V_t[-1]))\n\n# Backward iteration with interpolation\nfor t in range(T - 1, -1, -1):\n    interpolator = create_interpolator(V[t+1], N_space)\n    \n    for i, N in enumerate(N_space):\n        max_value = float('-inf')\n        best_h = 0\n\n        for h in h_space:\n            if h > 1:  # Ensure harvest rate doesn't exceed 100%\n                continue\n\n            next_N = state_dynamics(N, h)\n            if next_N < 1:  # Ensure population doesn't go extinct\n                continue\n\n            # Use interpolation to get the value for next_N\n            value = state_return(N, h) + interpolator(next_N)\n\n            if value > max_value:\n                max_value = value\n                best_h = h\n\n        V[t, i] = max_value\n        policy[t, i] = best_h\n\n# Function to simulate the optimal policy using interpolation\ndef simulate_optimal_policy(initial_N, T):\n    trajectory = [initial_N]\n    harvests = []\n\n    for t in range(T):\n        N = trajectory[-1]\n        \n        # Create interpolator for the policy at time t\n        policy_interpolator = interp1d(N_space, policy[t], kind='cubic', bounds_error=False, fill_value=(policy[t][0], policy[t][-1]))\n        \n        h = policy_interpolator(N)\n        harvests.append(float(N * h))  # Ensure harvest is a Python float\n\n        next_N = state_dynamics(N, h)\n        trajectory.append(float(next_N))  # Ensure next population value is a Python float\n\n    return trajectory, harvests\n\n# Example usage\ninitial_N = 50\ntrajectory, harvests = simulate_optimal_policy(initial_N, T)\n\nprint(\"Optimal policy (first few rows):\")\nprint(policy[:5])\nprint(\"\\nPopulation trajectory:\", trajectory)\nprint(\"Harvests:\", harvests)\nprint(\"Total harvest:\", sum(harvests))\n\n ## Linear Quadratic Regulator via Dynamic Programming\n\nLet us now consider a special case of our dynamic programming formulation: the discrete-time Linear Quadratic Regulator (LQR) problem. This example will illustrate how the structure of linear dynamics and quadratic costs leads to a particularly clean form of the backward recursion.\n\nConsider a linear time-invariant system with dynamics:\n\n$$\n\\mathbf{x}_{t+1} = A\\mathbf{x}_t + B\\mathbf{u}_t\n$$\n\nwhere $\\mathbf{x}_t \\in \\mathbb{R}^n$ is the state and $\\mathbf{u}_t \\in \\mathbb{R}^m$ is the control input.  \nThe cost function to be minimized is quadratic:\n\n$$\nJ = \\frac{1}{2}\\mathbf{x}_T^\\top S_T \\mathbf{x}_T + \\frac{1}{2}\\sum_{t=0}^{T-1} \\left(\\mathbf{x}_t^\\top Q \\mathbf{x}_t + \\mathbf{u}_t^\\top R \\mathbf{u}_t\\right)\n$$\n\nwhere $S_T \\geq 0$, $Q \\geq 0$, and $R > 0$ are symmetric matrices of appropriate dimensions.  \nOur goal is to find the optimal control sequence $\\mathbf{u}_t^*$ that minimizes $J$ over a fixed time horizon $[0, T]$, given an initial state $\\mathbf{x}_0$.\n\nLet's apply the principle of optimality to derive the backward recursion for this problem. We'll start at the final time step and work our way backward.\n\nAt $t = T$, the terminal cost is given by:\n\n$$\nJ_T^*(\\mathbf{x}_T) = \\frac{1}{2}\\mathbf{x}_T^\\top S_T \\mathbf{x}_T\n$$\n\nAt $t = T-1$, the cost-to-go is:\n\n$$\nJ_{T-1}(\\mathbf{x}_{T-1}, \\mathbf{u}_{T-1}) = \\frac{1}{2}\\mathbf{x}_{T-1}^\\top Q \\mathbf{x}_{T-1} + \\frac{1}{2}\\mathbf{u}_{T-1}^\\top R \\mathbf{u}_{T-1} + J_T^*(\\mathbf{x}_T)\n$$\n\nSubstituting the dynamics equation:\n\n$$\nJ_{T-1} = \\frac{1}{2}\\mathbf{x}_{T-1}^\\top Q \\mathbf{x}_{T-1} + \\frac{1}{2}\\mathbf{u}_{T-1}^\\top R \\mathbf{u}_{T-1} + \\frac{1}{2}(A\\mathbf{x}_{T-1} + B\\mathbf{u}_{T-1})^\\top S_T (A\\mathbf{x}_{T-1} + B\\mathbf{u}_{T-1})\n$$\n\nTo find the optimal control, we differentiate with respect to $\\mathbf{u}_{T-1}$ and set it to zero:\n\n$$\n\\frac{\\partial J_{T-1}}{\\partial \\mathbf{u}_{T-1}} = R\\mathbf{u}_{T-1} + B^\\top S_T (A\\mathbf{x}_{T-1} + B\\mathbf{u}_{T-1}) = 0\n$$\n\nSolving for $\\mathbf{u}_{T-1}^*$:\n\n$$\n\\mathbf{u}_{T-1}^* = -(R + B^\\top S_T B)^{-1}B^\\top S_T A\\mathbf{x}_{T-1}\n$$\n\nWe can define the gain matrix:\n\n$$\nK_{T-1} = (R + B^\\top S_T B)^{-1}B^\\top S_T A\n$$\n\nSo that $\\mathbf{u}_{T-1}^* = -K_{T-1}\\mathbf{x}_{T-1}$. The optimal cost-to-go at $T-1$ is then:\n\n$$\nJ_{T-1}^*(\\mathbf{x}_{T-1}) = \\frac{1}{2}\\mathbf{x}_{T-1}^\\top S_{T-1} \\mathbf{x}_{T-1}\n$$\n\nWhere $S_{T-1}$ is given by:\n\n$$\nS_{T-1} = Q + A^\\top S_T A - A^\\top S_T B(R + B^\\top S_T B)^{-1}B^\\top S_T A\n$$\n\nContinuing this process backward in time, we find that for any $t$:\n\n$$\n\\mathbf{u}_t^* = -K_t\\mathbf{x}_t\n$$\n\nWhere:\n\n$$\nK_t = (R + B^\\top S_{t+1} B)^{-1}B^\\top S_{t+1} A\n$$\n\nAnd the optimal cost-to-go is:\n\n$$\nJ_t^*(\\mathbf{x}_t) = \\frac{1}{2}\\mathbf{x}_t^\\top S_t \\mathbf{x}_t\n$$\n\nWhere $S_t$ satisfies the so-called discrete-time Riccati equation:\n\n$$\nS_t = Q + A^\\top S_{t+1} A - A^\\top S_{t+1} B(R + B^\\top S_{t+1} B)^{-1}B^\\top S_{t+1} A\n$$  \n### Example: Linear Quadratic Regulation of a Liquid Tank \n\nWe are dealing with a liquid-level control system for a storage tank. This system consists of a reservoir connected to a tank via valves. These valves are controlled by a gear train, which is driven by a DC motor. The motor, in turn, is controlled by an electronic amplifier. The goal is to maintain a constant liquid level in the tank, adjusting only when necessary.\n\nThe system is described by a third-order continuous-time model with the following state variables:\n- $x_1(t)$: the height of the liquid in the tank\n- $x_2(t)$: the angular position of the electric motor driving the valves\n- $x_3(t)$: the angular velocity of the motor\n\nThe input to the system, $u(t)$, represents the signal sent to the electronic amplifier connected to the motor.\nThe system dynamics are described by the following differential equations:\n\n$$\n\\begin{aligned}\n& \\dot{x}_1(t) = -2x_1(t) \\\\\n& \\dot{x}_2(t) = x_3(t) \\\\\n& \\dot{x}_3(t) = -10x_3(t) + 9000u(t)\n\\end{aligned}\n$$\n\nTo pose this as a discrete-time LQR problem, we need to discretize the continuous-time system. Let's assume a sampling time of $T_s$ seconds. We can use the forward Euler method for a simple discretization:\n\n$$\n\\begin{aligned}\n& x_1(k+1) = x_1(k) + T_s(-2x_1(k)) \\\\\n& x_2(k+1) = x_2(k) + T_sx_3(k) \\\\\n& x_3(k+1) = x_3(k) + T_s(-10x_3(k) + 9000u(k))\n\\end{aligned}\n$$\n\nThis can be written in the standard discrete-time state-space form:\n\n$x(k+1) = Ax(k) + Bu(k)$\n\nWhere:\n\n$x(k) = \\begin{bmatrix} x_1(k) \\\\ x_2(k) \\\\ x_3(k) \\end{bmatrix}$\n\n$A = \\begin{bmatrix} \n1-2T_s & 0 & 0 \\\\\n0 & 1 & T_s \\\\\n0 & 0 & 1-10T_s\n\\end{bmatrix}$\n\n$B = \\begin{bmatrix} \n0 \\\\\n0 \\\\\n9000T_s\n\\end{bmatrix}$\n\nThe goal of our LQR controller is to maintain the liquid level at a desired reference value while minimizing control effort. We can formulate this as a discrete-time LQR problem with the following cost function:\n\n$J = \\sum_{k=0}^{\\infty} \\left( (x_1(k) - x_{1,ref})^2 + ru^2(k) \\right)$\n\nWhere $x_{1,ref}$ is the reference liquid level and $r$ is a positive weight on the control input.\n\nTo put this in standard discrete-time LQR form, we rewrite the cost function as:\n\n$J = \\sum_{k=0}^{\\infty} \\left( x^\\mathrm{T}(k)Qx(k) + ru^2(k) \\right)$\n\nWhere:\n\n$Q = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{bmatrix}$\n\nThe discrete-time LQR problem is now to find the optimal control law $u^*(k) = -Kx(k)$ that minimizes this cost function, subject to the discrete-time system dynamics $x(k+1) = Ax(k) + Bu(k)$.\n\nThe solution involves solving the discrete-time algebraic Riccati equation:\n\n$P = A^TPA - A^TPB(B^TPB + R)^{-1}B^TPA + Q$\n\nWhere $R = r$ (a scalar in this case), to find the positive definite matrix $P$. Then, the optimal gain matrix $K$ is given by:\n\n$K = (B^TPB + R)^{-1}B^TPA$\n\nThis formulation ensures that:\n1. The liquid level ($x_1(k)$) is maintained close to the reference value.\n2. The system acts primarily when there's a change in the liquid level, as only $x_1(k)$ is directly penalized in the cost function.\n3. The control effort is minimized, ensuring smooth operation of the valves.\n\nBy tuning the weight $r$ and the sampling time $T_s$, we can balance the trade-off between maintaining the desired liquid level, the amount of control effort used, and the responsiveness of the system. ","type":"content","url":"/dp#example-optimal-harvest-with-linear-interpolation","position":11},{"hierarchy":{"lvl1":"Dynamic Programming","lvl2":"Stochastic Dynamic Programming and Markov Decision Processes"},"type":"lvl2","url":"/dp#stochastic-dynamic-programming-and-markov-decision-processes","position":12},{"hierarchy":{"lvl1":"Dynamic Programming","lvl2":"Stochastic Dynamic Programming and Markov Decision Processes"},"content":"While our previous discussion centered on deterministic systems, many real-world problems involve uncertainty. Stochastic Dynamic Programming (SDP) extends our framework to handle stochasticity in both the objective function and system dynamics. This extension naturally leads us to consider more general policy classes and to formalize when simpler policies suffice.","type":"content","url":"/dp#stochastic-dynamic-programming-and-markov-decision-processes","position":13},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Decision Rules and Policies","lvl2":"Stochastic Dynamic Programming and Markov Decision Processes"},"type":"lvl3","url":"/dp#decision-rules-and-policies","position":14},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Decision Rules and Policies","lvl2":"Stochastic Dynamic Programming and Markov Decision Processes"},"content":"Before diving into stochastic systems, we need to establish terminology for the different types of strategies a decision maker might employ. In the deterministic setting, we implicitly used feedback controllers of the form u(\\mathbf{x}, t). In the stochastic setting, we must be more precise about what information policies can use and how they select actions.\n\nA decision rule is a prescription for action selection in each state at a specified decision epoch. These rules can vary in their complexity based on two main criteria:\n\nDependence on history: Markovian or History-dependent\n\nAction selection method: Deterministic or Randomized\n\nMarkovian decision rules depend only on the current state, while history-dependent rules consider the entire sequence of past states and actions. Formally, a history h_t at time t is:h_t = (s_1, a_1, \\ldots, s_{t-1}, a_{t-1}, s_t)\n\nThe set of all possible histories at time t, denoted H_t, grows exponentially with t:\n\nH_1 = \\mathcal{S} (just the initial state)\n\nH_2 = \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S}\n\nH_t = \\mathcal{S} \\times (\\mathcal{A} \\times \\mathcal{S})^{t-1}\n\nDeterministic rules select an action with certainty, while randomized rules specify a probability distribution over the action space.\n\nThese classifications lead to four types of decision rules:\n\nMarkovian Deterministic (MD): \\pi_t: \\mathcal{S} \\rightarrow \\mathcal{A}_s\n\nMarkovian Randomized (MR): \\pi_t: \\mathcal{S} \\rightarrow \\mathcal{P}(\\mathcal{A}_s)\n\nHistory-dependent Deterministic (HD): \\pi_t: H_t \\rightarrow \\mathcal{A}_s\n\nHistory-dependent Randomized (HR): \\pi_t: H_t \\rightarrow \\mathcal{P}(\\mathcal{A}_s)\n\nwhere \\mathcal{P}(\\mathcal{A}_s) denotes the set of probability distributions over \\mathcal{A}_s.\n\nA policy \\boldsymbol{\\pi} is a sequence of decision rules, one for each decision epoch:\\boldsymbol{\\pi} = (\\pi_1, \\pi_2, ..., \\pi_{N-1})\n\nThe set of all policies of class K (where K \\in \\{HR, HD, MR, MD\\}) is denoted as \\Pi^K. These policy classes form a hierarchy:\\Pi^{MD} \\subset \\Pi^{MR} \\subset \\Pi^{HR}, \\quad \\Pi^{MD} \\subset \\Pi^{HD} \\subset \\Pi^{HR}\n\nThe largest set \\Pi^{HR} contains all possible policies. We ask: under what conditions can we restrict attention to the much simpler set \\Pi^{MD} without loss of optimality?\n\nNotation: rules vs. policies\n\nDecision rule (kernel). A map from information to action distributions:\n\nMarkov, deterministic:  \\pi_t:\\mathcal{S}\\to\\mathcal{A}_s\n\nMarkov, randomized:     \\pi_t(\\cdot\\mid s)\\in\\Delta(\\mathcal{A}_s)\n\nHistory-dependent:       \\pi_t(\\cdot\\mid h_t)\\in\\Delta(\\mathcal{A}_{s_t})\n\nPolicy (sequence). \\boldsymbol{\\pi}=(\\pi_1,\\pi_2,\\ldots).\n\nStationary policy. \\boldsymbol{\\pi}=\\mathrm{const}(\\pi) with \\pi_t\\equiv\\pi \\ \\forall t.By convention, we identify \\pi with its stationary policy \\mathrm{const}(\\pi) when no confusion arises.","type":"content","url":"/dp#decision-rules-and-policies","position":15},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Stochastic System Dynamics","lvl2":"Stochastic Dynamic Programming and Markov Decision Processes"},"type":"lvl3","url":"/dp#stochastic-system-dynamics","position":16},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Stochastic System Dynamics","lvl2":"Stochastic Dynamic Programming and Markov Decision Processes"},"content":"In the stochastic setting, our system evolution takes the form:\\mathbf{x}_{t+1} = \\mathbf{f}_t(\\mathbf{x}_t, \\mathbf{u}_t, \\mathbf{w}_t)\n\nHere, \\mathbf{w}_t represents a random disturbance or noise term at time t due to the inherent uncertainty in the system’s behavior. The stage cost function may also incorporate stochastic influences:c_t(\\mathbf{x}_t, \\mathbf{u}_t, \\mathbf{w}_t)\n\nIn this context, our objective shifts from minimizing a deterministic cost to minimizing the expected total cost:\\mathbb{E}\\left[c_\\mathrm{T}(\\mathbf{x}_T) + \\sum_{t=1}^{T-1} c_t(\\mathbf{x}_t, \\mathbf{u}_t, \\mathbf{w}_t)\\right]\n\nwhere the expectation is taken over the distributions of the random variables \\mathbf{w}_t. The principle of optimality still holds in the stochastic case, but Bellman’s optimality equation now involves an expectation:J_k^\\star(\\mathbf{x}_k) = \\min_{\\mathbf{u}_k} \\mathbb{E}_{\\mathbf{w}_k}\\left[c_k(\\mathbf{x}_k, \\mathbf{u}_k, \\mathbf{w}_k) + J_{k+1}^\\star(\\mathbf{f}_k(\\mathbf{x}_k, \\mathbf{u}_k, \\mathbf{w}_k))\\right]\n\nIn practice, this expectation is often computed by discretizing the distribution of \\mathbf{w}_k when the set of possible disturbances is very large or even continuous. Let’s say we approximate the distribution with K discrete values \\mathbf{w}_k^i, each occurring with probability p_k^i. Then our Bellman equation becomes:J_k^\\star(\\mathbf{x}_k) = \\min_{\\mathbf{u}_k} \\sum_{i=1}^K p_k^i \\left(c_k(\\mathbf{x}_k, \\mathbf{u}_k, \\mathbf{w}_k^i) + J_{k+1}^\\star(\\mathbf{f}_k(\\mathbf{x}_k, \\mathbf{u}_k, \\mathbf{w}_k^i))\\right)","type":"content","url":"/dp#stochastic-system-dynamics","position":17},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Optimality Equations in the Stochastic Setting","lvl2":"Stochastic Dynamic Programming and Markov Decision Processes"},"type":"lvl3","url":"/dp#optimality-equations-in-the-stochastic-setting","position":18},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Optimality Equations in the Stochastic Setting","lvl2":"Stochastic Dynamic Programming and Markov Decision Processes"},"content":"When dealing with stochastic systems, a central question arises: what information should our control policy use? In the most general case, a policy might use the entire history of observations and actions. However, as we’ll see, the Markovian structure of our problems allows for dramatic simplifications.\n\nLet h_t = (s_1, a_1, s_2, a_2, \\ldots, s_{t-1}, a_{t-1}, s_t) denote the complete history up to time t. In the stochastic setting, the history-based optimality equations become:u_t(h_t) = \\sup_{a\\in A_{s_t}}\\left\\{ r_t(s_t,a) + \\sum_{j\\in S} p_t(j\\mid s_t,a)\\, u_{t+1}(h_t,a,j) \\right\\},\\quad u_N(h_N)=r_N(s_N)\n\nwhere we now explicitly use the transition probabilities p_t(j|s_t,a) rather than a deterministic dynamics function.\n\nPrinciple of optimality for stochastic systems\n\nLet u_t^* be the optimal expected return from epoch t onward. Then:\n\na. u_t^* satisfies the optimality equations:u_t^*(h_t) = \\sup_{a \\in A_{s_t}} \\left\\{ r_t(s_t, a) + \\sum_{j \\in S} p_t(j|s_t, a) u_{t+1}^*(h_t, a, j) \\right\\}\n\nwith boundary condition u_N^*(h_N) = r_N(s_N).\n\nb. Any policy \\pi^* that selects actions attaining the supremum (or maximum) in the above equation at each history is optimal.\n\nIntuition: This formalizes Bellman’s principle of optimality: “An optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision.” The recursive structure means that optimal local decisions (choosing the best action at each step) lead to global optimality, even with uncertainty captured by the transition probabilities.\n\nA simplification occurs when we examine these history-based equations more closely. The Markov property of our system dynamics and rewards means that the optimal return actually depends on the history only through the current state:\n\nState sufficiency for stochastic MDPs\n\nIn finite-horizon stochastic MDPs with Markovian dynamics and rewards, the optimal return u_t^*(h_t) depends on the history only through the current state s_t. Thus we can write u_t^*(h_t) = v_t^*(s_t) for some function v_t^* that depends only on state and time.\n\nFollowing \n\nPuterman (1994) Theorem 4.4.2. We proceed by backward induction.\n\nBase case: At the terminal time N, we have u_N^*(h_N) = r_N(s_N) by the boundary condition. Since the terminal reward depends only on the final state s_N and not on how we arrived there, u_N^*(h_N) = u_N^*(s_N).\n\nInductive step: Assume u_{t+1}^*(h_{t+1}) depends on h_{t+1} only through s_{t+1} for all t+1, \\ldots, N. Then from the optimality equation:u_t^*(h_t) = \\sup_{a \\in A_{s_t}} \\left\\{ r_t(s_t, a) + \\sum_{j \\in S} p_t(j|s_t, a) u_{t+1}^*(h_t, a, j) \\right\\}\n\nBy the induction hypothesis, u_{t+1}^*(h_t, a, j) depends only on the next state j, so:u_t^*(h_t) = \\sup_{a \\in A_{s_t}} \\left\\{ r_t(s_t, a) + \\sum_{j \\in S} p_t(j|s_t, a) u_{t+1}^*(j) \\right\\}\n\nSince the expression in brackets depends on h_t only through the current state s_t (the rewards and transition probabilities are Markovian), we conclude that u_t^*(h_t) = u_t^*(s_t).\n\nIntuition: The Markov property means that the current state contains all information needed to predict future evolution. The past provides no additional value for decision-making. This result allows us to work with value functions v_t^*(s) indexed only by state and time, dramatically simplifying both theory and computation.\n\nThis state-sufficiency result, combined with the fact that randomization never helps when maximizing expected returns, leads to a dramatic simplification of the policy space:\n\nPolicy reduction for stochastic MDPs\n\nFor finite-horizon stochastic MDPs with finite state and action sets:\\sup_{\\pi \\in \\Pi^{\\mathrm{HR}}} v_\\pi(s,t) = \\max_{\\pi \\in \\Pi^{\\mathrm{MD}}} v_\\pi(s,t)\n\nThat is, there exists an optimal policy that is both deterministic and Markovian.\n\nSketch following \n\nPuterman (1994) Lemma 4.3.1 and Theorem 4.4.2. First, Lemma 4.3.1 shows that for any function w and any distribution q over actions, \\sup_a w(a) \\ge \\sum_a q(a) w(a). Thus randomization cannot improve the expected value over choosing a single maximizing action. Second, by state sufficiency (Proposition \n\nProposition 1 and \n\nPuterman (1994) Thm. 4.4.2(a)), the optimal return depends on the history only through (s_t,t). Therefore, selecting at each (s_t,t) an action that attains the maximum yields a deterministic Markov decision rule which is optimal whenever the maximum is attained. If only a supremum exists, \\varepsilon-optimal selectors exist by choosing actions within \\varepsilon of the supremum (see \n\nPuterman (1994) Thm. 4.3.4).\n\nIntuition: Even in stochastic systems, randomization in the policy doesn’t help when maximizing expected returns: you should always choose the action with the highest expected value. Combined with state sufficiency, this means simple state-to-action mappings are optimal.\n\nThese results justify focusing on deterministic Markov policies and lead to the backward recursion algorithm for stochastic systems:\n\nBackward Recursion for Stochastic Dynamic Programming\n\nInput: Terminal cost function c_\\mathrm{T}(\\cdot), stage cost functions c_t(\\cdot, \\cdot, \\cdot), system dynamics \\mathbf{f}_t(\\cdot, \\cdot, \\cdot), time horizon \\mathrm{T}, disturbance distributions\n\nOutput: Optimal value functions J_t^\\star(\\cdot) and optimal control policies \\mu_t^\\star(\\cdot) for t = 1, \\ldots, T\n\nInitialize J_T^\\star(\\mathbf{x}) = c_\\mathrm{T}(\\mathbf{x}) for all \\mathbf{x} in the state space\n\nFor t = T-1, T-2, \\ldots, 1:\n\nFor each state \\mathbf{x} in the state space:\n\nCompute J_t^\\star(\\mathbf{x}) = \\min_{\\mathbf{u}} \\mathbb{E}_{\\mathbf{w}_t}\\left[c_t(\\mathbf{x}, \\mathbf{u}, \\mathbf{w}_t) + J_{t+1}^\\star(\\mathbf{f}_t(\\mathbf{x}, \\mathbf{u}, \\mathbf{w}_t))\\right]\n\nCompute \\mu_t^\\star(\\mathbf{x}) = \\arg\\min_{\\mathbf{u}} \\mathbb{E}_{\\mathbf{w}_t}\\left[c_t(\\mathbf{x}, \\mathbf{u}, \\mathbf{w}_t) + J_{t+1}^\\star(\\mathbf{f}_t(\\mathbf{x}, \\mathbf{u}, \\mathbf{w}_t))\\right]\n\nEnd For\n\nEnd For\n\nReturn J_t^\\star(\\cdot), \\mu_t^\\star(\\cdot) for t = 1, \\ldots, T\n\nWhile SDP provides us with a framework to for handling uncertainty, it makes the curse of dimensionality even more difficult to handle in practice. Both the state space and the disturbance space must be discretized. This can lead to a combinatorial explosion in the number of scenarios to be evaluated at each stage.\n\nHowever, just as we tackled the challenges of continuous state spaces with discretization and interpolation, we can devise efficient methods to handle the additional complexity of evaluating expectations. This problem essentially becomes one of numerical integration. When the set of disturbances is continuous (as is often the case with continuous state spaces), we enter a domain where numerical quadrature methods could be applied. But these methods tend to scale poorly as the number of dimensions grows. This is where more efficient techniques, often rooted in Monte Carlo methods, come into play. Two ingredients tackle the curse of dimensionality:\n\nFunction approximation (through discretization, interpolation, neural networks, etc.)\n\nMonte Carlo integration (simulation)\n\nThese two elements essentially distill the key ingredients of machine learning, which is the direction we’ll be exploring in this course.","type":"content","url":"/dp#optimality-equations-in-the-stochastic-setting","position":19},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Example: Stochastic Optimal Harvest in Resource Management","lvl2":"Stochastic Dynamic Programming and Markov Decision Processes"},"type":"lvl3","url":"/dp#example-stochastic-optimal-harvest-in-resource-management","position":20},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Example: Stochastic Optimal Harvest in Resource Management","lvl2":"Stochastic Dynamic Programming and Markov Decision Processes"},"content":"Building upon our previous deterministic model, we now introduce stochasticity to more accurately reflect the uncertainties inherent in real-world resource management scenarios \n\nConroy & Peterson, 2013. As before, we consider a population of a particular species, whose abundance we denote by x_t, where t represents discrete time steps. Our objective remains to maximize the cumulative harvest over a finite time horizon, while also considering the long-term sustainability of the population. However, we now account for two sources of stochasticity: partial controllability of harvest and environmental variability affecting growth rates.\nThe optimization problem can be formulated as:\\text{maximize} \\quad \\mathbb{E}\\left[\\sum_{t=t_0}^{t_f} F(x_t \\cdot h_t)\\right]\n\nHere, F(\\cdot) represents the immediate reward function associated with harvesting, and h_t is the realized harvest rate at time t. The expectation \\mathbb{E}[\\cdot] over both harvest and growth rates, which we view as random variables.\nIn our stochastic model, the abundance x still ranges from 1 to 100 individuals. The decision variable is now the desired harvest rate d_t, which can take values from the set D = {0, 0.1, 0.2, 0.3, 0.4, 0.5}. However, the realized harvest rate h_t is stochastic and follows a discrete distribution:h_t = \\begin{cases}\n0.75d_t & \\text{with probability } 0.25 \\\\\nd_t & \\text{with probability } 0.5 \\\\\n1.25d_t & \\text{with probability } 0.25\n\\end{cases}\n\nBy expressing the harvest rate as a random variable, we mean to capture the fact that harvesting is a not completely under our control: we might obtain more or less what we had intended to. Furthermore, we generalize the population dynamics to the stochastic case via:\n\n$$\n\nx_{t+1} = x_t + r_tx_t(1 - x_t/K) - h_tx_t\n$$\n\nwhere K = 125 is the carrying capacity. The growth rate r_t is now stochastic and follows a discrete distribution:r_t = \\begin{cases}\n0.85r_{\\text{max}} & \\text{with probability } 0.25 \\\\\n1.05r_{\\text{max}} & \\text{with probability } 0.5 \\\\\n1.15r_{\\text{max}} & \\text{with probability } 0.25\n\\end{cases}\n\nwhere r_{\\text{max}} = 0.3 is the maximum growth rate.\nApplying the principle of optimality, we can express the optimal value function J^\\star(x_t, t) recursively:J^\\star(x_t, t) = \\max_{d(t) \\in D} \\mathbb{E}\\left[F(x_t \\cdot h_t) + J^\\star(x_{t+1}, t+1)\\right]\n\nwhere the expectation is taken over the harvest and growth rate random variables. The boundary condition remains J^*(x_{t_f}) = 0. We can now adapt our previous code to account for the stochasticity in our model. One important difference is that simulating a solution in this context requires multiple realizations of our process. This is an important consideration when evaluating reinforcement learning methods in practice, as success cannot be claimed based on a single successful trajectory.\n\n#  label: dp-harvest-stochastic\n#  caption: Stochastic resource management simulation: the cell reports the optimal policy sample, average trajectory, and visualizes ensemble trajectories plus the distribution of total harvest.\n\nimport numpy as np\nfrom scipy.interpolate import interp1d\n\n# Parameters\nr_max = 0.3\nK = 125\nT = 30  # Number of time steps\nN_max = 100  # Maximum population size to consider\nh_max = 0.5  # Maximum harvest rate\nh_step = 0.1  # Step size for harvest rate\n\n# Create state and decision spaces\nN_space = np.linspace(1, N_max, 100)  # Using more granular state space\nh_space = np.arange(0, h_max + h_step, h_step)\n\n# Stochastic parameters\nh_outcomes = np.array([0.75, 1.0, 1.25])\nh_probs = np.array([0.25, 0.5, 0.25])\nr_outcomes = np.array([0.85, 1.05, 1.15]) * r_max\nr_probs = np.array([0.25, 0.5, 0.25])\n\n# Initialize value function and policy\nV = np.zeros((T + 1, len(N_space)))\npolicy = np.zeros((T, len(N_space)))\n\n# State return function (F)\ndef state_return(N, h):\n    return N * h\n\n# State dynamics function (stochastic)\ndef state_dynamics(N, h, r):\n    return N + r * N * (1 - N / K) - h * N\n\n# Function to create interpolation function for a given time step\ndef create_interpolator(V_t, N_space):\n    return interp1d(N_space, V_t, kind='linear', bounds_error=False, fill_value=(V_t[0], V_t[-1]))\n\n# Backward iteration with stochastic dynamics\nfor t in range(T - 1, -1, -1):\n    interpolator = create_interpolator(V[t+1], N_space)\n    \n    for i, N in enumerate(N_space):\n        max_value = float('-inf')\n        best_h = 0\n\n        for h in h_space:\n            if h > 1:  # Ensure harvest rate doesn't exceed 100%\n                continue\n\n            expected_value = 0\n            for h_factor, h_prob in zip(h_outcomes, h_probs):\n                for r_factor, r_prob in zip(r_outcomes, r_probs):\n                    realized_h = h * h_factor\n                    realized_r = r_factor\n\n                    next_N = state_dynamics(N, realized_h, realized_r)\n                    if next_N < 1:  # Ensure population doesn't go extinct\n                        continue\n\n                    # Use interpolation to get the value for next_N\n                    value = state_return(N, realized_h) + interpolator(next_N)\n                    expected_value += value * h_prob * r_prob\n\n            if expected_value > max_value:\n                max_value = expected_value\n                best_h = h\n\n        V[t, i] = max_value\n        policy[t, i] = best_h\n\n# Function to simulate the optimal policy using interpolation (stochastic version)\ndef simulate_optimal_policy(initial_N, T, num_simulations=100):\n    all_trajectories = []\n    all_harvests = []\n\n    for _ in range(num_simulations):\n        trajectory = [initial_N]\n        harvests = []\n\n        for t in range(T):\n            N = trajectory[-1]\n            \n            # Create interpolator for the policy at time t\n            policy_interpolator = interp1d(N_space, policy[t], kind='linear', bounds_error=False, fill_value=(policy[t][0], policy[t][-1]))\n            \n            intended_h = policy_interpolator(N)\n            \n            # Apply stochasticity\n            h_factor = np.random.choice(h_outcomes, p=h_probs)\n            r_factor = np.random.choice(r_outcomes, p=r_probs)\n            \n            realized_h = intended_h * h_factor\n            harvests.append(N * realized_h)\n\n            next_N = state_dynamics(N, realized_h, r_factor)\n            trajectory.append(next_N)\n\n        all_trajectories.append(trajectory)\n        all_harvests.append(harvests)\n\n    return all_trajectories, all_harvests\n\n# Example usage\ninitial_N = 50\ntrajectories, harvests = simulate_optimal_policy(initial_N, T)\n\n# Calculate average trajectory and total harvest\navg_trajectory = np.mean(trajectories, axis=0)\navg_total_harvest = np.mean([sum(h) for h in harvests])\n\nprint(\"Optimal policy (first few rows):\")\nprint(policy[:5])\nprint(\"\\nAverage population trajectory:\", avg_trajectory)\nprint(\"Average total harvest:\", avg_total_harvest)\n\n# Plot results\n%config InlineBackend.figure_format = 'retina'\nimport matplotlib.pyplot as plt\n\n# Apply book style\ntry:\n    import scienceplots\n    plt.style.use(['science', 'notebook'])\nexcept (ImportError, OSError):\n    pass  # Use matplotlib defaults\n\nplt.figure(figsize=(12, 6))\nplt.subplot(121)\nfor traj in trajectories[:20]:  # Plot first 20 trajectories\n    plt.plot(range(T+1), traj, alpha=0.3)\nplt.plot(range(T+1), avg_trajectory, 'r-', linewidth=2)\nplt.title('Population Trajectories')\nplt.xlabel('Time')\nplt.ylabel('Population')\n\nplt.subplot(122)\nplt.hist([sum(h) for h in harvests], bins=20)\nplt.title('Distribution of Total Harvest')\nplt.xlabel('Total Harvest')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\n\n","type":"content","url":"/dp#example-stochastic-optimal-harvest-in-resource-management","position":21},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Linear Quadratic Regulator via Dynamic Programming","lvl2":"Stochastic Dynamic Programming and Markov Decision Processes"},"type":"lvl3","url":"/dp#linear-quadratic-regulator-via-dynamic-programming","position":22},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Linear Quadratic Regulator via Dynamic Programming","lvl2":"Stochastic Dynamic Programming and Markov Decision Processes"},"content":"We now examine a special case where the backward recursion admits a closed-form solution. When the system dynamics are linear and the cost function is quadratic, the optimization at each stage can be solved analytically. Moreover, the value function itself maintains a quadratic structure throughout the recursion, and the optimal policy reduces to a simple linear feedback law. This result eliminates the need for discretization, interpolation, or any function approximation. The infinite-dimensional problem collapses to tracking a finite set of matrices.\n\nConsider a discrete-time linear system:\\mathbf{x}_{t+1} = A_t\\mathbf{x}_t + B_t\\mathbf{u}_t\n\nwhere \\mathbf{x}_t \\in \\mathbb{R}^n is the state and \\mathbf{u}_t \\in \\mathbb{R}^m is the control input. The matrices A_t \\in \\mathbb{R}^{n \\times n} and B_t \\in \\mathbb{R}^{n \\times m} describe the system dynamics at time t.\n\nThe cost function to be minimized is quadratic:J = \\frac{1}{2}\\mathbf{x}_T^\\top Q_T \\mathbf{x}_T + \\frac{1}{2}\\sum_{t=0}^{T-1} \\left(\\mathbf{x}_t^\\top Q_t \\mathbf{x}_t + \\mathbf{u}_t^\\top R_t \\mathbf{u}_t\\right)\n\nwhere Q_T \\succeq 0 (positive semidefinite), Q_t \\succeq 0, and R_t \\succ 0 (positive definite) are symmetric matrices of appropriate dimensions. The positive definiteness of R_t ensures the minimization problem is well-posed.\n\nWhat we now have to observe is that if the terminal cost is quadratic, then the value function at every earlier stage remains quadratic. This is not immediately obvious, but it follows from the structure of Bellman’s equation combined with the linearity of the dynamics.\n\nWe claim that the optimal cost-to-go from any stage t takes the form:J_t^\\star(\\mathbf{x}_t) = \\frac{1}{2}\\mathbf{x}_t^\\top P_t \\mathbf{x}_t\n\nfor some positive semidefinite matrix P_t. At the terminal time, this is true by definition: P_T = Q_T.\n\nLet’s verify this structure and derive the recursion for P_t using backward induction. Suppose we’ve established that J_{t+1}^\\star(\\mathbf{x}_{t+1}) = \\frac{1}{2}\\mathbf{x}_{t+1}^\\top P_{t+1} \\mathbf{x}_{t+1}. Bellman’s equation at stage t states:J_t^\\star(\\mathbf{x}_t) = \\min_{\\mathbf{u}_t} \\left[ \\frac{1}{2}\\mathbf{x}_t^\\top Q_t \\mathbf{x}_t + \\frac{1}{2}\\mathbf{u}_t^\\top R_t \\mathbf{u}_t + J_{t+1}^\\star(\\mathbf{x}_{t+1}) \\right]\n\nSubstituting the dynamics \\mathbf{x}_{t+1} = A_t\\mathbf{x}_t + B_t\\mathbf{u}_t and the quadratic form for J_{t+1}^\\star:J_t^\\star(\\mathbf{x}_t) = \\min_{\\mathbf{u}_t} \\left[ \\frac{1}{2}\\mathbf{x}_t^\\top Q_t \\mathbf{x}_t + \\frac{1}{2}\\mathbf{u}_t^\\top R_t \\mathbf{u}_t + \\frac{1}{2}(A_t\\mathbf{x}_t + B_t\\mathbf{u}_t)^\\top P_{t+1} (A_t\\mathbf{x}_t + B_t\\mathbf{u}_t) \\right]\n\nExpanding the last term:(A_t\\mathbf{x}_t + B_t\\mathbf{u}_t)^\\top P_{t+1} (A_t\\mathbf{x}_t + B_t\\mathbf{u}_t) = \\mathbf{x}_t^\\top A_t^\\top P_{t+1} A_t \\mathbf{x}_t + 2\\mathbf{x}_t^\\top A_t^\\top P_{t+1} B_t \\mathbf{u}_t + \\mathbf{u}_t^\\top B_t^\\top P_{t+1} B_t \\mathbf{u}_t\n\nThe expression inside the minimization becomes:\\frac{1}{2}\\mathbf{x}_t^\\top Q_t \\mathbf{x}_t + \\frac{1}{2}\\mathbf{u}_t^\\top R_t \\mathbf{u}_t + \\frac{1}{2}\\mathbf{x}_t^\\top A_t^\\top P_{t+1} A_t \\mathbf{x}_t + \\mathbf{x}_t^\\top A_t^\\top P_{t+1} B_t \\mathbf{u}_t + \\frac{1}{2}\\mathbf{u}_t^\\top B_t^\\top P_{t+1} B_t \\mathbf{u}_t\n\nCollecting terms involving \\mathbf{u}_t:= \\frac{1}{2}\\mathbf{x}_t^\\top (Q_t + A_t^\\top P_{t+1} A_t) \\mathbf{x}_t + \\mathbf{x}_t^\\top A_t^\\top P_{t+1} B_t \\mathbf{u}_t + \\frac{1}{2}\\mathbf{u}_t^\\top (R_t + B_t^\\top P_{t+1} B_t) \\mathbf{u}_t\n\nThis is a quadratic function of \\mathbf{u}_t. To find the minimizer, we take the gradient with respect to \\mathbf{u}_t and set it to zero:\\frac{\\partial}{\\partial \\mathbf{u}_t} = (R_t + B_t^\\top P_{t+1} B_t) \\mathbf{u}_t + B_t^\\top P_{t+1} A_t \\mathbf{x}_t = 0\n\nSince R_t + B_t^\\top P_{t+1} B_t is positive definite (both R_t and P_{t+1} are positive semidefinite with R_t strictly positive), we can solve for the optimal control:\\mathbf{u}_t^\\star = -(R_t + B_t^\\top P_{t+1} B_t)^{-1} B_t^\\top P_{t+1} A_t \\mathbf{x}_t\n\nDefine the gain matrix:K_t = (R_t + B_t^\\top P_{t+1} B_t)^{-1} B_t^\\top P_{t+1} A_t\n\nso that \\mathbf{u}_t^\\star = -K_t\\mathbf{x}_t. This is a linear feedback policy: the optimal control is simply a linear function of the current state.\n\nSubstituting \\mathbf{u}_t^\\star back into the cost-to-go expression and simplifying (by completing the square), we obtain:J_t^\\star(\\mathbf{x}_t) = \\frac{1}{2}\\mathbf{x}_t^\\top P_t \\mathbf{x}_t\n\nwhere P_t satisfies the discrete-time Riccati equation:P_t = Q_t + A_t^\\top P_{t+1} A_t - A_t^\\top P_{t+1} B_t (R_t + B_t^\\top P_{t+1} B_t)^{-1} B_t^\\top P_{t+1} A_t\n\nPutting everything together, the backward induction procedure under the LQR setting then becomes:\n\nBackward Recursion for LQR\n\nInput: System matrices A_t, B_t, cost matrices Q_t, R_t, Q_T, time horizon T\n\nOutput: Cost matrices P_t and gain matrices K_t for t = 0, \\ldots, T-1\n\nInitialize: P_T = Q_T\n\nFor t = T-1, T-2, \\ldots, 0:\n\nCompute the gain matrix:K_t = (R_t + B_t^\\top P_{t+1} B_t)^{-1} B_t^\\top P_{t+1} A_t\n\nCompute the cost matrix via the Riccati equation:P_t = Q_t + A_t^\\top P_{t+1} A_t - A_t^\\top P_{t+1} B_t (R_t + B_t^\\top P_{t+1} B_t)^{-1} B_t^\\top P_{t+1} A_t\n\nEnd For\n\nReturn: \\{P_0, \\ldots, P_T\\} and \\{K_0, \\ldots, K_{T-1}\\}\n\nOptimal policy: \\mathbf{u}_t^\\star = -K_t\\mathbf{x}_t\n\nOptimal cost-to-go: J_t^\\star(\\mathbf{x}_t) = \\frac{1}{2}\\mathbf{x}_t^\\top P_t \\mathbf{x}_t","type":"content","url":"/dp#linear-quadratic-regulator-via-dynamic-programming","position":23},{"hierarchy":{"lvl1":"Dynamic Programming","lvl2":"Markov Decision Process Formulation"},"type":"lvl2","url":"/dp#markov-decision-process-formulation","position":24},{"hierarchy":{"lvl1":"Dynamic Programming","lvl2":"Markov Decision Process Formulation"},"content":"Rather than expressing the stochasticity in our system through a disturbance term as a parameter to a deterministic difference equation, we often work with an alternative representation (more common in operations research) which uses the Markov Decision Process formulation. The idea is that when we model our system in this way with the disturbance term being drawn indepently of the previous stages, the induced trajectory are those of a Markov chain. Hence, we can re-cast our control problem in that language, leading to the so-called Markov Decision Process framework in which we express the system dynamics in terms of transition probabilities rather than explicit state equations. In this framework, we express the probability that the system is in a given state using the transition probability function:p_t(\\mathbf{x}_{t+1} | \\mathbf{x}_t, \\mathbf{u}_t)\n\nThis function gives the probability of transitioning to state \\mathbf{x}_{t+1} at time t+1, given that the system is in state \\mathbf{x}_t and action \\mathbf{u}_t is taken at time t. Therefore, p_t specifies a conditional probability distribution over the next states: namely, the sum (for discrete state spaces) or integral over the next state should be 1.\n\nGiven the control theory formulation of our problem via a deterministic dynamics function and a noise term, we can derive the corresponding transition probability function through the following relationship:\\begin{aligned}\np_t(\\mathbf{x}_{t+1} | \\mathbf{x}_t, \\mathbf{u}_t) &= \\mathbb{P}(\\mathbf{W}_t \\in \\left\\{\\mathbf{w} \\in \\mathbf{W}: \\mathbf{x}_{t+1} = f_t(\\mathbf{x}_t, \\mathbf{u}_t, \\mathbf{w})\\right\\}) \\\\\n&= \\sum_{\\left\\{\\mathbf{w} \\in \\mathbf{W}: \\mathbf{x}_{t+1} = f_t(\\mathbf{x}_t, \\mathbf{u}_t, \\mathbf{w})\\right\\}} q_t(\\mathbf{w})\n\\end{aligned}\n\nHere, q_t(\\mathbf{w}) represents the probability density or mass function of the disturbance \\mathbf{W}_t (assuming discrete state spaces). When dealing with continuous spaces, the above expression simply contains an integral rather than a summation.\n\nFor a system with deterministic dynamics and no disturbance, the transition probabilities become much simpler and be expressed using the indicator function. Given a deterministic system with dynamics:\\mathbf{x}_{t+1} = f_t(\\mathbf{x}_t, \\mathbf{u}_t)\n\nThe transition probability function can be expressed as:p_t(\\mathbf{x}_{t+1} | \\mathbf{x}_t, \\mathbf{u}_t) = \\begin{cases}\n1 & \\text{if } \\mathbf{x}_{t+1} = f_t(\\mathbf{x}_t, \\mathbf{u}_t) \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\nWith this transition probability function, we can recast our Bellman optimality equation:J_t^\\star(\\mathbf{x}_t) = \\max_{\\mathbf{u}_t \\in \\mathbf{U}} \\left\\{ c_t(\\mathbf{x}_t, \\mathbf{u}_t) + \\sum_{\\mathbf{x}_{t+1}} p_t(\\mathbf{x}_{t+1} | \\mathbf{x}_t, \\mathbf{u}_t) J_{t+1}^\\star(\\mathbf{x}_{t+1}) \\right\\}\n\nHere, {c}(\\mathbf{x}_t, \\mathbf{u}_t) represents the expected immediate reward (or negative cost) when in state \\mathbf{x}_t and taking action \\mathbf{u}_t at time t. The summation term computes the expected optimal value for the future states, weighted by their transition probabilities.\n\nThis formulation offers several advantages:\n\nIt makes the Markovian nature of the problem explicit: the future state depends only on the current state and action, not on the history of states and actions.\n\nFor discrete-state problems, the entire system dynamics can be specified by a set of transition matrices, one for each possible action.\n\nIt allows us to bridge the gap with the wealth of methods in the field of probabilistic graphical models and statistical machine learning techniques for modelling and analysis.","type":"content","url":"/dp#markov-decision-process-formulation","position":25},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Notation in Operations Reseach","lvl2":"Markov Decision Process Formulation"},"type":"lvl3","url":"/dp#notation-in-operations-reseach","position":26},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Notation in Operations Reseach","lvl2":"Markov Decision Process Formulation"},"content":"The presentation above was intended to bridge the gap between the control-theoretic perspective and the world of closed-loop control through the idea of determining the value function of a parametric optimal control problem. We then saw how the backward induction procedure was applicable to both the deterministic and stochastic cases by taking the expectation over the disturbance variable. We then said that we can alternatively work with a representation of our system where instead of writing our model as a deterministic dynamics function taking a disturbance as an input, we would rather work directly via its transition probability function, which gives rise to the Markov chain interpretation of our system in simulation.\n\nNote that the notation used in control theory tends to differ from that found in operations research communities, in which the field of dynamic programming flourished. We summarize those (purely notational) differences in this section.\n\nIn operations research, the system state at each decision epoch is typically denoted by s \\in \\mathcal{S}, where S is the set of possible system states. When the system is in state s, the decision maker may choose an action a from the set of allowable actions \\mathcal{A}_s. The union of all action sets is denoted as \\mathcal{A} = \\bigcup_{s \\in \\mathcal{S}} \\mathcal{A}_s.\n\nThe dynamics of the system are described by a transition probability function p_t(j | s, a), which represents the probability of transitioning to state j \\in \\mathcal{S} at time t+1, given that the system is in state s at time t and action a \\in \\mathcal{A}_s is chosen. This transition probability function satisfies:\\sum_{j \\in \\mathcal{S}} p_t(j | s, a) = 1\n\nIt’s worth noting that in operations research, we typically work with reward maximization rather than cost minimization, which is more common in control theory. However, we can easily switch between these perspectives by simply negating the quantity. That is, maximizing a reward function is equivalent to minimizing its negative, which we would then call a cost function.\n\nThe reward function is denoted by r_t(s, a), representing the reward received at time t when the system is in state s and action a is taken. In some cases, the reward may also depend on the next state, in which case it is denoted as r_t(s, a, j). The expected reward can then be computed as:r_t(s, a) = \\sum_{j \\in \\mathcal{S}} r_t(s, a, j) p_t(j | s, a)\n\nCombined together, these elemetns specify a Markov decision process, which is fully described by the tuple:\\{T, S, \\mathcal{A}_s, p_t(\\cdot | s, a), r_t(s, a)\\}\n\nwhere \\mathrm{T} represents the set of decision epochs (the horizon).","type":"content","url":"/dp#notation-in-operations-reseach","position":27},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"What is an Optimal Policy?","lvl2":"Markov Decision Process Formulation"},"type":"lvl3","url":"/dp#what-is-an-optimal-policy","position":28},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"What is an Optimal Policy?","lvl2":"Markov Decision Process Formulation"},"content":"Let’s go back to the starting point and define what it means for a policy to be optimal in a Markov Decision Problem. For this, we will be considering different possible search spaces (policy classes) and compare policies based on the ordering of their value from any possible start state. The value of a policy \\boldsymbol{\\pi} (optimal or not) is defined as the expected total reward obtained by following that policy from a given starting state. Formally, for a finite-horizon MDP with N decision epochs, we define the value function v^{\\boldsymbol{\\pi}}(s, t) as:v^{\\boldsymbol{\\pi}}(s, t) \\triangleq \\mathbb{E}\\left[\\sum_{k=t}^{N-1} r_t(S_k, A_k) + r_N(S_N) \\mid S_t = s\\right]\n\nwhere S_t is the state at time t, A_t is the action taken at time t, and r_t is the reward function. For simplicity, we write v^{\\boldsymbol{\\pi}}(s) to denote v^{\\boldsymbol{\\pi}}(s, 1), the value of following policy \\boldsymbol{\\pi} from state s at the first stage over the entire horizon N.\n\nIn finite-horizon MDPs, our goal is to identify an optimal policy, denoted by \\boldsymbol{\\pi}^*, that maximizes total expected reward over the horizon N. Specifically:v^{\\boldsymbol{\\pi}^*}(s) \\geq v^{\\boldsymbol{\\pi}}(s), \\quad \\forall s \\in \\mathcal{S}, \\quad \\forall \\boldsymbol{\\pi} \\in \\Pi^{\\text{HR}}\n\nWe call \\boldsymbol{\\pi}^* an optimal policy because it yields the highest possible value across all states and all policies within the policy class \\Pi^{\\text{HR}}. We denote by v^* the maximum value achievable by any policy:v^*(s) = \\max_{\\boldsymbol{\\pi} \\in \\Pi^{\\text{HR}}} v^{\\boldsymbol{\\pi}}(s), \\quad \\forall s \\in \\mathcal{S}\n\nIn reinforcement learning literature, v^* is typically referred to as the “optimal value function,” while in some operations research references, it might be called the “value of an MDP.” An optimal policy \\boldsymbol{\\pi}^* is one for which its value function equals the optimal value function:v^{\\boldsymbol{\\pi}^*}(s) = v^*(s), \\quad \\forall s \\in \\mathcal{S}\n\nThis notion of optimality applies to every state. Policies optimal in this sense are sometimes called “uniformly optimal policies.” A weaker notion of optimality, often encountered in reinforcement learning practice, is optimality with respect to an initial distribution of states. In this case, we seek a policy \\boldsymbol{\\pi} \\in \\Pi^{\\text{HR}} that maximizes:\\sum_{s \\in \\mathcal{S}} v^{\\boldsymbol{\\pi}}(s) P_1(S_1 = s)\n\nwhere P_1(S_1 = s) is the probability of starting in state s.\n\nThe maximum value can be achieved by searching over the space of deterministic Markovian Policies. Consequently:v^*(s) = \\max_{\\boldsymbol{\\pi} \\in \\Pi^{\\mathrm{HR}}} v^{\\boldsymbol{\\pi}}(s) = \\max _{\\boldsymbol{\\pi} \\in \\Pi^{M D}} v^{\\boldsymbol{\\pi}}(s), \\quad s \\in S\n\nThis equality significantly simplifies the computational complexity of our algorithms, as the search problem can now be decomposed into N sub-problems in which we only have to search over the set of possible actions. This is the backward induction algorithm, which we present a second time, but departing this time from the control-theoretic notation and using the MDP formalism:\n\nBackward Induction\n\nInput: State space S, Action space A, Transition probabilities p_t, Reward function r_t, Time horizon N\n\nOutput: Optimal value functions v^*\n\nInitialize:\n\nSet t = N\n\nFor all s_N \\in S:v^*(s_N, N) = r_N(s_N)\n\nFor t = N-1 to 1:\n\nFor each s_t \\in S:\na. Compute the optimal value function:v^*(s_t, t) = \\max_{a \\in A_{s_t}} \\left\\{r_t(s_t, a) + \\sum_{j \\in S} p_t(j | s_t, a) v^*(j, t+1)\\right\\}\n\nb. Determine the set of optimal actions:A_{s_t,t}^* = \\arg\\max_{a \\in A_{s_t}} \\left\\{r_t(s_t, a) + \\sum_{j \\in S} p_t(j | s_t, a) v^*(j, t+1)\\right\\}\n\nReturn the optimal value functions u_t^* and optimal action sets A_{s_t,t}^* for all t and s_t\n\nNote that the same procedure can also be used for finding the value of a policy with minor changes;\n\nPolicy Evaluation\n\nInput:\n\nState space S\n\nAction space A\n\nTransition probabilities p_t\n\nReward function r_t\n\nTime horizon N\n\nA markovian deterministic policy \\boldsymbol{\\pi} = (\\pi_1, \\ldots, \\pi_{N-1})\n\nOutput: Value function v^{\\boldsymbol{\\pi}} for policy \\boldsymbol{\\pi}\n\nInitialize:\n\nSet t = N\n\nFor all s_N \\in S:v^{\\boldsymbol{\\pi}}(s_N, N) = r_N(s_N)\n\nFor t = N-1 to 1:\n\nFor each s_t \\in S:\na. Compute the value function for the given policy:v^{\\boldsymbol{\\pi}}(s_t, t) = r_t(s_t, \\pi_t(s_t)) + \\sum_{j \\in S} p_t(j | s_t, \\pi_t(s_t)) v^{\\boldsymbol{\\pi}}(j, t+1)\n\nReturn the value function v^{\\boldsymbol{\\pi}}(s_t, t) for all t and s_t\n\nThis code could also finally be adapted to support randomized policies using:v^{\\boldsymbol{\\pi}}(s_t, t) = \\sum_{a_t \\in \\mathcal{A}_{s_t}} \\pi_t(a_t \\mid s_t) \\left( r_t(s_t, a_t) + \\sum_{j \\in S} p_t(j | s_t, a_t) v^{\\boldsymbol{\\pi}}(j, t+1) \\right)","type":"content","url":"/dp#what-is-an-optimal-policy","position":29},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Example: Sample Size Determination in Pharmaceutical Development","lvl2":"Markov Decision Process Formulation"},"type":"lvl3","url":"/dp#example-sample-size-determination-in-pharmaceutical-development","position":30},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Example: Sample Size Determination in Pharmaceutical Development","lvl2":"Markov Decision Process Formulation"},"content":"Pharmaceutical development is the process of bringing a new drug from initial discovery to market availability. This process is lengthy, expensive, and risky, typically involving several stages:\n\nDrug Discovery: Identifying a compound that could potentially treat a disease.\n\nPreclinical Testing: Laboratory and animal testing to assess safety and efficacy.\n. Clinical Trials: Testing the drug in humans, divided into phases:\n\nPhase I: Testing for safety in a small group of healthy volunteers.\n\nPhase II: Testing for efficacy and side effects in a larger group with the target condition.\n\nPhase III: Large-scale testing to confirm efficacy and monitor side effects.\n\nRegulatory Review: Submitting a New Drug Application (NDA) for approval.\n\nPost-Market Safety Monitoring: Continuing to monitor the drug’s effects after market release.\n\nThis process can take 10-15 years and cost over $1 billion \n\nAdams & Brantner (2009). The high costs and risks involved call for a principled approach to decision making. We’ll focus on the clinical trial phases and NDA approval, per the MDP model presented by \n\nChang (2010):\n\nStates (S): Our state space is S = \\{s_1, s_2, s_3, s_4\\}, where:\n\ns_1: Phase I clinical trial\n\ns_2: Phase II clinical trial\n\ns_3: Phase III clinical trial\n\ns_4: NDA approval\n\nActions (A): At each state, the action is choosing the sample size n_i for the corresponding clinical trial. The action space is A = \\{10, 11, ..., 1000\\}, representing possible sample sizes.\n\nTransition Probabilities (P): The probability of moving from one state to the next depends on the chosen sample size and the inherent properties of the drug.\nWe define:\n\nP(s_2|s_1, n_1) = p_{12}(n_1) = \\sum_{i=0}^{\\lfloor\\eta_1 n_1\\rfloor} \\binom{n_1}{i} p_0^i (1-p_0)^{n_1-i}\nwhere p_0 is the true toxicity rate and \\eta_1 is the toxicity threshold for Phase I.\n\nOf particular interest is the transition from Phase II to Phase III which we model as:\n\nP(s_3|s_2, n_2) = p_{23}(n_2) = \\Phi\\left(\\frac{\\sqrt{n_2}}{2}\\delta - z_{1-\\eta_2}\\right)\n\nwhere \\Phi is the cumulative distribution function (CDF) of the standard normal distribution:\n\n\\Phi(x) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^x e^{-t^2/2} dt\n\nThis is giving us the probability that we would observe a treatment effect this large or larger if the null hypothesis (no treatment effect) were true. A higher probability indicates stronger evidence of a treatment effect, making it more likely that the drug will progress to Phase III.\n\nIn this expression, \\delta is called the “normalized treatment effect”. In clinical trials, we’re often interested in the difference between the treatment and control groups. The “normalized” part means we’ve adjusted this difference for the variability in the data. Specifically \\delta = \\frac{\\mu_t - \\mu_c}{\\sigma} where \\mu_t is the mean outcome in the treatment group, \\mu_c is the mean outcome in the control group, and \\sigma is the standard deviation of the outcome. A larger \\delta indicates a stronger treatment effect.\n\nFurthermore, the term z_{1-\\eta_2} is the (1-\\eta_2)-quantile of the standard normal distribution. In other words, it’s the value where the probability of a standard normal random variable being greater than this value is \\eta_2. For example, if \\eta_2 = 0.05, then z_{1-\\eta_2} \\approx 1.645. A smaller \\eta_2 makes the trial more conservative, requiring stronger evidence to proceed to Phase III.\n\nFinally, n_2 is the sample size for Phase II. The \\sqrt{n_2} term reflects that the precision of our estimate of the treatment effect improves with the square root of the sample size.\n\nP(s_4|s_3, n_3) = p_{34}(n_3) = \\Phi\\left(\\frac{\\sqrt{n_3}}{2}\\delta - z_{1-\\eta_3}\\right)\nwhere \\eta_3 is the significance level for Phase III.\n\nRewards (R): The reward function captures the costs of running trials and the potential profit from a successful drug:\n\nr(s_i, n_i) = -c_i(n_i) for i = 1, 2, 3, where c_i(n_i) is the cost of running a trial with sample size n_i.\n\nr(s_4) = g_4, where g_4 is the expected profit from a successful drug.\n\nDiscount Factor (\\gamma): We use a discount factor 0 < \\gamma \\leq 1 to account for the time value of money and risk preferences.\n\n#  label: dp-clinical-trials\n#  caption: Clinical trial phase-sizing via backward induction: the console output lists the phase values, recommended enrollment for each phase, and basic sanity checks on the resulting policy.\n\nimport numpy as np\nfrom scipy.stats import binom\nfrom scipy.stats import norm\n\ndef binomial_pmf(k, n, p):\n    return binom.pmf(k, n, p)\n\ndef transition_prob_phase1(n1, eta1, p0):\n    return np.sum([binomial_pmf(i, n1, p0) for i in range(int(eta1 * n1) + 1)])\n\ndef transition_prob_phase2(n2, eta2, delta):\n    return norm.cdf((np.sqrt(n2) / 2) * delta - norm.ppf(1 - eta2))\n\ndef transition_prob_phase3(n3, eta3, delta):\n    return norm.cdf((np.sqrt(n3) / 2) * delta - norm.ppf(1 - eta3))\n\ndef immediate_reward(n):\n    return -n  # Negative to represent cost\n\ndef backward_induction(S, A, gamma, g4, p0, delta, eta1, eta2, eta3):\n    V = np.zeros(len(S))\n    V[3] = g4  # Value for NDA approval state\n    optimal_n = [None] * 3  # Store optimal n for each phase\n\n    # Backward induction\n    for i in range(2, -1, -1):  # Iterate backwards from Phase III to Phase I\n        max_value = -np.inf\n        for n in A:\n            if i == 0:  # Phase I\n                p = transition_prob_phase1(n, eta1, p0)\n            elif i == 1:  # Phase II\n                p = transition_prob_phase2(n, eta2, delta)\n            else:  # Phase III\n                p = transition_prob_phase3(n, eta3, delta)\n            value = immediate_reward(n) + gamma * p * V[i+1]\n            if value > max_value:\n                max_value = value\n                optimal_n[i] = n\n        V[i] = max_value\n\n    return V, optimal_n\n\n# Set up the problem parameters\nS = ['Phase I', 'Phase II', 'Phase III', 'NDA approval']\nA = range(10, 1001)\ngamma = 0.95\ng4 = 10000\np0 = 0.1  # Example toxicity rate for Phase I\ndelta = 0.5  # Example normalized treatment difference\neta1, eta2, eta3 = 0.2, 0.1, 0.025\n\n# Run the backward induction algorithm\nV, optimal_n = backward_induction(S, A, gamma, g4, p0, delta, eta1, eta2, eta3)\n\n# Print results\nfor i, state in enumerate(S):\n    print(f\"Value for {state}: {V[i]:.2f}\")\nprint(f\"Optimal sample sizes: Phase I: {optimal_n[0]}, Phase II: {optimal_n[1]}, Phase III: {optimal_n[2]}\")\n\n# Sanity checks\nprint(\"\\nSanity checks:\")\nprint(f\"1. NDA approval value: {V[3]}\")\nprint(f\"2. All values non-positive and <= NDA value: {all(v <= V[3] for v in V)}\")\nprint(f\"3. Optimal sample sizes in range: {all(10 <= n <= 1000 for n in optimal_n if n is not None)}\")\n\n","type":"content","url":"/dp#example-sample-size-determination-in-pharmaceutical-development","position":31},{"hierarchy":{"lvl1":"Dynamic Programming","lvl2":"Infinite-Horizon MDPs"},"type":"lvl2","url":"/dp#infinite-horizon-mdps","position":32},{"hierarchy":{"lvl1":"Dynamic Programming","lvl2":"Infinite-Horizon MDPs"},"content":"It often makes sense to model control problems over infinite horizons. We extend the previous setting and define the expected total reward of policy \\boldsymbol{\\pi} \\in \\Pi^{\\mathrm{HR}}, v^{\\boldsymbol{\\pi}} as:v^{\\boldsymbol{\\pi}}(s) = \\mathbb{E}\\left[\\sum_{t=1}^{\\infty} r(S_t, A_t)\\right]\n\nOne drawback of this model is that we could easily encounter values that are +\\infty or -\\infty, even in a setting as simple as a single-state MDP which loops back into itself and where the accrued reward is nonzero.\n\nTherefore, it is often more convenient to work with an alternative formulation which guarantees the existence of a limit: the expected total discounted reward of policy \\boldsymbol{\\pi} \\in \\Pi^{\\mathrm{HR}} is defined to be:v_\\gamma^{\\boldsymbol{\\pi}}(s) \\equiv \\lim_{N \\rightarrow \\infty} \\mathbb{E}\\left[\\sum_{t=1}^N \\gamma^{t-1} r(S_t, A_t)\\right]\n\nfor 0 \\leq \\gamma < 1 and when \\max_{s \\in \\mathcal{S}} \\max_{a \\in \\mathcal{A}_s}|r(s, a)| = R_{\\max} < \\infty, in which case, |v_\\gamma^{\\boldsymbol{\\pi}}(s)| \\leq (1-\\gamma)^{-1} R_{\\max}.\n\nFinally, another possibility for the infinite-horizon setting is the so-called average reward or gain of policy \\boldsymbol{\\pi} \\in \\Pi^{\\mathrm{HR}} defined as:g^{\\boldsymbol{\\pi}}(s) \\equiv \\lim_{N \\rightarrow \\infty} \\frac{1}{N} \\mathbb{E}\\left[\\sum_{t=1}^N r(S_t, A_t)\\right]\n\nWe won’t be working with this formulation in this course due to its inherent practical and theoretical complexities.\n\nExtending the previous notion of optimality from finite-horizon models, a policy \\boldsymbol{\\pi}^* is said to be discount optimal for a given \\gamma if:v_\\gamma^{\\boldsymbol{\\pi}^*}(s) \\geq v_\\gamma^{\\boldsymbol{\\pi}}(s) \\quad \\text { for each } s \\in S \\text { and all } \\boldsymbol{\\pi} \\in \\Pi^{\\mathrm{HR}}\n\nFurthermore, the value of a discounted MDP v_\\gamma^*(s), is defined by:v_\\gamma^*(s) \\equiv \\max _{\\boldsymbol{\\pi} \\in \\Pi^{\\mathrm{HR}}} v_\\gamma^{\\boldsymbol{\\pi}}(s)\n\nMore often, we refer to v_\\gamma by simply calling it the optimal value function.\n\nAs for the finite-horizon setting, the infinite horizon discounted model does not require history-dependent policies, since for any \\boldsymbol{\\pi} \\in \\Pi^{HR} there exists a \\boldsymbol{\\pi}^{\\prime} \\in \\Pi^{MR} with identical total discounted reward:v_\\gamma^*(s) \\equiv \\max_{\\boldsymbol{\\pi} \\in \\Pi^{HR}} v_\\gamma^{\\boldsymbol{\\pi}}(s)=\\max_{\\boldsymbol{\\pi} \\in \\Pi^{MR}} v_\\gamma^{\\boldsymbol{\\pi}}(s) .","type":"content","url":"/dp#infinite-horizon-mdps","position":33},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Random Horizon Interpretation of Discounting","lvl2":"Infinite-Horizon MDPs"},"type":"lvl3","url":"/dp#random-horizon-interpretation-of-discounting","position":34},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Random Horizon Interpretation of Discounting","lvl2":"Infinite-Horizon MDPs"},"content":"The use of discounting can be motivated both from a modeling perspective and as a means to ensure that the total reward remains bounded. From the modeling perspective, we can view discounting as a way to weight more or less importance on the immediate rewards vs. the long-term consequences. There is also another interpretation which stems from that of a finite horizon model but with an uncertain end time. More precisely:\n\nLet v_\\nu^{\\boldsymbol{\\pi}}(s) denote the expected total reward obtained by using policy \\boldsymbol{\\pi} when the horizon length \\nu is random. We define it by:v_\\nu^{\\boldsymbol{\\pi}}(s) \\equiv \\mathbb{E}_s^{\\boldsymbol{\\pi}}\\left[\\mathbb{E}_\\nu\\left\\{\\sum_{t=1}^\\nu r(S_t, A_t)\\right\\}\\right]\n\nRandom horizon interpretation of discounting\n\nSuppose that the horizon \\nu follows a geometric distribution with parameter \\gamma, 0 \\leq \\gamma < 1, independent of the policy such that\nP(\\nu=n) = (1-\\gamma) \\gamma^{n-1}, \\, n=1,2, \\ldots, then v_\\nu^{\\boldsymbol{\\pi}}(s) = v_\\gamma^{\\boldsymbol{\\pi}}(s) for all s \\in \\mathcal{S} .\n\nSee proposition 5.3.1 in \n\nPuterman (1994).\n\nBy definition of the finite-horizon value function and the law of total expectation:v_\\nu^{\\boldsymbol{\\pi}}(s) = \\sum_{n=1}^{\\infty} P(\\nu=n) \\cdot v_n^{\\boldsymbol{\\pi}}(s) = \\sum_{n=1}^{\\infty} (1-\\gamma) \\gamma^{n-1} \\cdot E_s^{\\boldsymbol{\\pi}} \\left\\{\\sum_{t=1}^n r(S_t, A_t)\\right\\}.\n\nCombining the expectation with the sum over n:v_\\nu^{\\boldsymbol{\\pi}}(s) = E_s^{\\boldsymbol{\\pi}} \\left\\{\\sum_{n=1}^{\\infty} (1-\\gamma) \\gamma^{n-1} \\sum_{t=1}^n r(S_t, A_t)\\right\\}.\n\nReordering the summations: Under the bounded reward assumption |r(s,a)| \\leq R_{\\max} and \\gamma < 1, we haveE_s^{\\boldsymbol{\\pi}} \\left\\{\\sum_{n=1}^{\\infty} \\sum_{t=1}^n |r(S_t, A_t)| \\cdot (1-\\gamma) \\gamma^{n-1}\\right\\} \\leq R_{\\max} \\sum_{n=1}^{\\infty} n (1-\\gamma) \\gamma^{n-1} = \\frac{R_{\\max}}{1-\\gamma} < \\infty,\n\nwhich justifies exchanging the order of summation by Fubini’s theorem.\n\nTo reverse the order, note that the pair (n,t) with 1 \\leq t \\leq n can be reindexed by fixing t first and letting n range from t to \\infty:\\sum_{n=1}^{\\infty} \\sum_{t=1}^n = \\sum_{t=1}^{\\infty} \\sum_{n=t}^{\\infty}.\n\nTherefore:\\begin{align*}\nv_\\nu^{\\boldsymbol{\\pi}}(s) &= E_s^{\\boldsymbol{\\pi}} \\left\\{\\sum_{t=1}^{\\infty} r(S_t, A_t) \\sum_{n=t}^{\\infty} (1-\\gamma) \\gamma^{n-1}\\right\\}.\n\\end{align*}\n\nEvaluating the inner sum: Using the substitution m = n - t + 1 (so n = m + t - 1):\\begin{align*}\n\\sum_{n=t}^{\\infty} (1-\\gamma) \\gamma^{n-1} &= \\sum_{m=1}^{\\infty} (1-\\gamma) \\gamma^{m+t-2} \\\\\n&= \\gamma^{t-1} (1-\\gamma) \\sum_{m=1}^{\\infty} \\gamma^{m-1} \\\\\n&= \\gamma^{t-1} (1-\\gamma) \\cdot \\frac{1}{1-\\gamma} = \\gamma^{t-1}.\n\\end{align*}\n\nSubstituting back:v_\\nu^{\\boldsymbol{\\pi}}(s) = E_s^{\\boldsymbol{\\pi}} \\left\\{\\sum_{t=1}^{\\infty} \\gamma^{t-1} r(S_t, A_t)\\right\\} = v_\\gamma^{\\boldsymbol{\\pi}}(s).","type":"content","url":"/dp#random-horizon-interpretation-of-discounting","position":35},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Vector Representation in Markov Decision Processes","lvl2":"Infinite-Horizon MDPs"},"type":"lvl3","url":"/dp#vector-representation-in-markov-decision-processes","position":36},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Vector Representation in Markov Decision Processes","lvl2":"Infinite-Horizon MDPs"},"content":"Let V be the set of bounded real-valued functions on a discrete state space S. This means any function  f \\in V  satisfies the condition:\\|f\\| = \\max_{s \\in S} |f(s)| < \\infty.\n\nwhere notation  \\|f\\|  represents the sup-norm (or  \\ell_\\infty -norm) of the function  f .\n\nWhen working with discrete state spaces, we can interpret elements of V as vectors and linear operators on V as matrices, allowing us to leverage tools from linear algebra. The sup-norm (\\ell_\\infty norm) of matrix \\mathbf{H} is defined as:\\|\\mathbf{H}\\| \\equiv \\max_{s \\in S} \\sum_{j \\in S} |\\mathbf{H}_{s,j}|\n\nwhere \\mathbf{H}_{s,j} represents the (s, j)-th component of the matrix \\mathbf{H}.\n\nFor a Markovian decision rule \\pi \\in \\Pi^{MD}, we define:\\begin{align*}\n\\mathbf{r}_\\pi(s) &\\equiv r(s, \\pi(s)), \\quad \\mathbf{r}_\\pi \\in \\mathbb{R}^{|S|}, \\\\\n[\\mathbf{P}_\\pi]_{s,j} &\\equiv p(j \\mid s, \\pi(s)), \\quad \\mathbf{P}_\\pi \\in \\mathbb{R}^{|S| \\times |S|}.\n\\end{align*}\n\nFor a randomized decision rule \\pi \\in \\Pi^{MR}, these definitions extend to:\\begin{align*}\n\\mathbf{r}_\\pi(s) &\\equiv \\sum_{a \\in A_s} \\pi(a \\mid s) \\, r(s, a), \\\\\n[\\mathbf{P}_\\pi]_{s,j} &\\equiv \\sum_{a \\in A_s} \\pi(a \\mid s) \\, p(j \\mid s, a).\n\\end{align*}\n\nIn both cases, \\mathbf{r}_\\pi denotes a reward vector in \\mathbb{R}^{|S|}, with each component \\mathbf{r}_\\pi(s) representing the reward associated with state s. Similarly, \\mathbf{P}_\\pi is a transition probability matrix in \\mathbb{R}^{|S| \\times |S|}, capturing the transition probabilities under decision rule \\pi.\n\nFor a nonstationary Markovian policy \\boldsymbol{\\pi} = (\\pi_1, \\pi_2, \\ldots) \\in \\Pi^{MR}, the expected total discounted reward is given by:\\mathbf{v}_\\gamma^{\\boldsymbol{\\pi}}(s)=\\mathbb{E}\\left[\\sum_{t=1}^{\\infty} \\gamma^{t-1} r\\left(S_t, A_t\\right) \\,\\middle|\\, S_1 = s\\right].\n\nUsing vector notation, this can be expressed as:\\begin{aligned}\n\\mathbf{v}_\\gamma^{\\boldsymbol{\\pi}} &= \\sum_{t=1}^{\\infty} \\gamma^{t-1} \\mathbf{P}_{\\boldsymbol{\\pi}}^{t-1} \\mathbf{r}_{\\pi_1} \\\\\n&= \\mathbf{r}_{\\pi_1} + \\gamma \\mathbf{P}_{\\pi_1} \\mathbf{r}_{\\pi_2} + \\gamma^2 \\mathbf{P}_{\\pi_1} \\mathbf{P}_{\\pi_2} \\mathbf{r}_{\\pi_3} + \\cdots \\\\\n&= \\mathbf{r}_{\\pi_1} + \\gamma \\mathbf{P}_{\\pi_1} \\left( \\mathbf{r}_{\\pi_2} + \\gamma \\mathbf{P}_{\\pi_2} \\mathbf{r}_{\\pi_3} + \\gamma^2 \\mathbf{P}_{\\pi_2} \\mathbf{P}_{\\pi_3} \\mathbf{r}_{\\pi_4} + \\cdots \\right).\n\\end{aligned}\n\nThis formulation leads to a recursive relationship:\\begin{align*}\n\\mathbf{v}_\\gamma^{\\boldsymbol{\\pi}} &= \\mathbf{r}_{\\pi_1} + \\gamma \\mathbf{P}_{\\pi_1} \\mathbf{v}_\\gamma^{\\boldsymbol{\\pi}^{\\prime}}\\\\\n&=\\sum_{t=1}^{\\infty} \\gamma^{t-1} \\mathbf{P}_{\\boldsymbol{\\pi}}^{t-1} \\mathbf{r}_{\\pi_t}\n\\end{align*}\n\nwhere \\boldsymbol{\\pi}^{\\prime} = (\\pi_2, \\pi_3, \\ldots).\n\nFor a stationary policy \\boldsymbol{\\pi} = \\mathrm{const}(\\pi) with constant decision rule \\pi, the total expected reward simplifies to:\\begin{align*}\n\\mathbf{v}_\\gamma^{\\pi} &= \\mathbf{r}_\\pi+ \\gamma \\mathbf{P}_\\pi \\mathbf{v}_\\gamma^{\\pi} \\\\\n&=\\sum_{t=1}^{\\infty} \\gamma^{t-1} \\mathbf{P}_\\pi^{t-1} \\mathbf{r}_{\\pi}\n\\end{align*}\n\nThis last expression is called a Neumann series expansion, and it’s guaranteed to exists under the assumptions of bounded reward and discount factor strictly less than one.\n\nNeumann Series and Invertibility\n\nThe spectral radius of a matrix \\mathbf{H} is defined as:\\rho(\\mathbf{H}) \\equiv \\max_{i} |\\lambda_i(\\mathbf{H})|\n\nwhere \\lambda_i(\\mathbf{H}) are the eigenvalues of \\mathbf{H}.\n\nNeumann Series Existence: For any matrix \\mathbf{H}, the Neumann series\\sum_{t=0}^{\\infty} \\mathbf{H}^t = \\mathbf{I} + \\mathbf{H} + \\mathbf{H}^2 + \\cdots\n\nconverges if and only if \\rho(\\mathbf{H}) < 1. When this condition holds, the matrix (\\mathbf{I} - \\mathbf{H}) is invertible and(\\mathbf{I} - \\mathbf{H})^{-1} = \\sum_{t=0}^{\\infty} \\mathbf{H}^t.\n\nNote that for any induced matrix norm \\|\\cdot\\| (i.e., a norm satisfying \\|\\mathbf{H}\\mathbf{v}\\| \\leq \\|\\mathbf{H}\\| \\cdot \\|\\mathbf{v}\\| for all vectors \\mathbf{v}) and any matrix \\mathbf{H}, the spectral radius is bounded by:\\rho(\\mathbf{H}) \\leq \\|\\mathbf{H}\\|.\n\nThis inequality provides a practical way to verify the convergence condition \\rho(\\mathbf{H}) < 1 by checking the simpler condition \\|\\mathbf{H}\\| < 1 rather than trying to compute the eigenvalues directly.\n\nWe can now verify that (\\mathbf{I} - \\gamma \\mathbf{P}_d) is invertible and the Neumann series converges.\n\nNorm of the transition matrix: Since \\mathbf{P}_d is a stochastic matrix (each row sums to 1 and all entries are non-negative), its \\ell_\\infty-norm is:\\|\\mathbf{P}_d\\| = \\max_{s \\in S} \\sum_{j \\in S} [\\mathbf{P}_d]_{s,j} = \\max_{s \\in S} 1 = 1.\n\nNorm of the scaled matrix: Using the homogeneity property of norms, we have:\\|\\gamma \\mathbf{P}_d\\| = |\\gamma| \\cdot \\|\\mathbf{P}_d\\| = |\\gamma| \\cdot 1 = |\\gamma|.\n\nBounding the spectral radius: Since the spectral radius is bounded by the matrix norm:\\rho(\\gamma \\mathbf{P}_d) \\leq \\|\\gamma \\mathbf{P}_d\\| = |\\gamma|.\n\nVerifying convergence: Since 0 \\leq \\gamma < 1 by assumption, we have:\\rho(\\gamma \\mathbf{P}_d) \\leq |\\gamma| < 1.\n\nThis strict inequality guarantees that (\\mathbf{I} - \\gamma \\mathbf{P}_d) is invertible and the Neumann series converges.\n\nTherefore, the Neumann series expansion converges and yields:\\mathbf{v}_\\gamma^{d^\\infty} = (\\mathbf{I} - \\gamma \\mathbf{P}_d)^{-1} \\mathbf{r}_d = \\sum_{t=0}^{\\infty} (\\gamma \\mathbf{P}_d)^t \\mathbf{r}_d = \\sum_{t=1}^{\\infty} \\gamma^{t-1} \\mathbf{P}_d^{t-1} \\mathbf{r}_d.\n\nConsequently, for a stationary policy, \\mathbf{v}_\\gamma^{d^\\infty} can be determined as the solution to the linear equation:\\mathbf{v} = \\mathbf{r}_d+ \\gamma \\mathbf{P}_d\\mathbf{v},\n\nwhich can be rearranged to:(\\mathbf{I} - \\gamma \\mathbf{P}_d) \\mathbf{v} = \\mathbf{r}_d.\n\nWe can also characterize \\mathbf{v}_\\gamma^{d^\\infty} as the solution to an operator equation. More specifically, define the transformation \\mathrm{L}_d by\\mathrm{L}_d \\mathbf{v} \\equiv \\mathbf{r}_d+\\gamma \\mathbf{P}_d\\mathbf{v}\n\nfor any \\mathbf{v} \\in V. Intuitively, \\mathrm{L}_d takes a value function \\mathbf{v} as input and returns a new value function that combines immediate rewards (\\mathbf{r}_d) with discounted future values (\\gamma \\mathbf{P}_d\\mathbf{v}).\n\nNote\n\nWhile we often refer to \\mathrm{L}_d as a “linear operator” in the RL literature, it is technically an affine operator (or affine transformation), not a linear operator in the strict sense. To see why, recall that a linear operator \\mathcal{T} must satisfy:\n\nAdditivity: \\mathcal{T}(\\mathbf{v}_1 + \\mathbf{v}_2) = \\mathcal{T}(\\mathbf{v}_1) + \\mathcal{T}(\\mathbf{v}_2)\n\nHomogeneity: \\mathcal{T}(\\alpha \\mathbf{v}) = \\alpha \\mathcal{T}(\\mathbf{v}) for all scalars \\alpha\n\nHowever, \\mathrm{L}_d fails the additivity test:\\mathrm{L}_d(\\mathbf{v}_1 + \\mathbf{v}_2) = \\mathbf{r}_d + \\gamma \\mathbf{P}_d(\\mathbf{v}_1 + \\mathbf{v}_2) = \\mathbf{r}_d + \\gamma \\mathbf{P}_d\\mathbf{v}_1 + \\gamma \\mathbf{P}_d\\mathbf{v}_2\n\nwhile\\mathrm{L}_d(\\mathbf{v}_1) + \\mathrm{L}_d(\\mathbf{v}_2) = (\\mathbf{r}_d + \\gamma \\mathbf{P}_d\\mathbf{v}_1) + (\\mathbf{r}_d + \\gamma \\mathbf{P}_d\\mathbf{v}_2) = 2\\mathbf{r}_d + \\gamma \\mathbf{P}_d\\mathbf{v}_1 + \\gamma \\mathbf{P}_d\\mathbf{v}_2.\n\nThe presence of the constant term \\mathbf{r}_d makes \\mathrm{L}_d affine rather than linear. An affine operator has the form \\mathcal{A}(\\mathbf{v}) = \\mathbf{b} + \\mathcal{T}(\\mathbf{v}), where \\mathbf{b} is a constant vector and \\mathcal{T} is a linear operator. In our case, \\mathbf{b} = \\mathbf{r}_d and \\mathcal{T}(\\mathbf{v}) = \\gamma \\mathbf{P}_d\\mathbf{v}.\n\nDespite this technical distinction, the term “linear operator” is commonly used in the reinforcement learning literature when referring to \\mathrm{L}_d, following a slight abuse of terminology.\n\nTherefore, we view \\mathrm{L}_d as an operator mapping elements of V to V: i.e., \\mathrm{L}_d: V \\rightarrow V. The fact that the value function of a policy is the solution to a fixed-point equation can then be expressed with the statement:\\mathbf{v}_\\gamma^{d^\\infty}=\\mathrm{L}_d \\mathbf{v}_\\gamma^{d^\\infty}.\n\nThis is a fixed-point equation: the value function \\mathbf{v}_\\gamma^{d^\\infty} is a fixed point of the operator \\mathrm{L}_d.","type":"content","url":"/dp#vector-representation-in-markov-decision-processes","position":37},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Solving Operator Equations","lvl2":"Infinite-Horizon MDPs"},"type":"lvl3","url":"/dp#solving-operator-equations","position":38},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Solving Operator Equations","lvl2":"Infinite-Horizon MDPs"},"content":"The operator equation we encountered in MDPs, \\mathbf{v}_\\gamma^{d^\\infty} = \\mathrm{L}_d \\mathbf{v}_\\gamma^{d^\\infty}, is a specific instance of a more general class of problems known as operator equations. These equations appear in various fields of mathematics and applied sciences, ranging from differential equations to functional analysis.\n\nOperator equations can take several forms, each with its own characteristics and solution methods:\n\nFixed Point Form: x = \\mathrm{T}(x), where \\mathrm{T}: X \\rightarrow X.\nCommon in fixed-point problems, such as our MDP equation, we seek a fixed point x^* such that x^* = \\mathrm{T}(x^*).\n\nGeneral Operator Equation: \\mathrm{T}(x) = y, where \\mathrm{T}: X \\rightarrow Y.\nHere, X and Y can be different spaces. We seek an x \\in X that satisfies the equation for a given y \\in Y.\n\nNonlinear Equation: \\mathrm{T}(x) = 0, where \\mathrm{T}: X \\rightarrow Y.\nA special case of the general operator equation where we seek roots or zeros of the operator.\n\nVariational Inequality: Find x^* \\in K such that \\langle \\mathrm{T}(x^*), x - x^* \\rangle \\geq 0 for all x \\in K.\nHere, K is a closed convex subset of X, and \\mathrm{T}: K \\rightarrow X^* (the dual space of X). These problems often arise in optimization, game theory, and partial differential equations.","type":"content","url":"/dp#solving-operator-equations","position":39},{"hierarchy":{"lvl1":"Dynamic Programming","lvl4":"Successive Approximation Method","lvl3":"Solving Operator Equations","lvl2":"Infinite-Horizon MDPs"},"type":"lvl4","url":"/dp#successive-approximation-method","position":40},{"hierarchy":{"lvl1":"Dynamic Programming","lvl4":"Successive Approximation Method","lvl3":"Solving Operator Equations","lvl2":"Infinite-Horizon MDPs"},"content":"For equations in fixed point form, a common numerical solution method is successive approximation, also known as fixed-point iteration:\n\nSuccessive Approximation\n\nInput: An operator \\mathrm{T}: X \\rightarrow X, an initial guess x_0 \\in X, and a tolerance \\epsilon > 0Output: An approximate fixed point x^* such that \\|x^* - \\mathrm{T}(x^*)\\| < \\epsilon\n\nInitialize n = 0\n\nrepeat3. Compute x_{n+1} = \\mathrm{T}(x_n)4. If \\|x_{n+1} - x_n\\| < \\epsilon, return x_{n+1}5. Set n = n + 1\n\nuntil convergence or maximum iterations reached\n\nThe convergence of successive approximation depends on the properties of the operator \\mathrm{T}. In the simplest and most common setting, we assume \\mathrm{T} is a contraction mapping. The Banach Fixed-Point Theorem then guarantees that \\mathrm{T} has a unique fixed point, and the successive approximation method will converge to this fixed point from any starting point. Specifically, \\mathrm{T} is a contraction if there exists a constant q \\in [0,1) such that for all x,y \\in X:d(\\mathrm{T}(x), \\mathrm{T}(y)) \\leq q \\cdot d(x,y)\n\nwhere d is the metric on X. In this case, the rate of convergence is linear, with error bound:d(x_n, x^*) \\leq \\frac{q^n}{1-q} d(x_1, x_0)\n\nHowever, the contraction mapping condition is not the only one that can lead to convergence. For instance, if \\mathrm{T} is nonexpansive (i.e., Lipschitz continuous with Lipschitz constant 1) and X is a Banach space with certain geometrical properties (e.g., uniformly convex), then under additional conditions (e.g., \\mathrm{T} has at least one fixed point), the successive approximation method can still converge, albeit potentially more slowly than in the contraction case.\n\nIn practice, when dealing with specific problems like MDPs or differential equations, the properties of the operator often naturally align with one of these convergence conditions. For example, in discounted MDPs, the Bellman operator is a contraction in the supremum norm, which guarantees the convergence of value iteration.","type":"content","url":"/dp#successive-approximation-method","position":41},{"hierarchy":{"lvl1":"Dynamic Programming","lvl4":"Newton-Kantorovich Method","lvl3":"Solving Operator Equations","lvl2":"Infinite-Horizon MDPs"},"type":"lvl4","url":"/dp#newton-kantorovich-method","position":42},{"hierarchy":{"lvl1":"Dynamic Programming","lvl4":"Newton-Kantorovich Method","lvl3":"Solving Operator Equations","lvl2":"Infinite-Horizon MDPs"},"content":"The Newton-Kantorovich method is a generalization of Newton’s method from finite dimensional vector spaces to infinite dimensional function spaces: rather than iterating in the space of vectors, we are iterating in the space of functions.\n\nNewton’s method is often written as the familiar update:x_{k+1} = x_k - [DF(x_k)]^{-1} F(x_k),\n\nwhich makes it look as though the essence of the method is “take a derivative and invert it.” But the real workhorse behind Newton’s method (both in finite and infinite dimensions) is linearization.\n\nAt each step, the idea is to replace the nonlinear operator F:X \\to Y by a local surrogate model of the formF(x+h) \\approx F(x) + Lh,\n\nwhere L is a linear map capturing how small perturbations in the input propagate to changes in the output. This is a Taylor-like expansion in Banach spaces: the role of the derivative is precisely to provide the correct notion of such a linear operator.\n\nTo find a root of F, we impose the condition that the surrogate vanishes at the next iterate:0 = F(x+h) \\approx F(x) + Lh.\n\nSolving this linear equation gives the increment h. In finite dimensions, L is the Jacobian matrix; in Banach spaces, it must be the Fréchet derivative.\n\nBut what exactly is a Fréchet derivative in infinite dimensions? To understand this, we need to generalize the concept of derivative from finite-dimensional calculus. In infinite-dimensional spaces, there are several notions of differentiability, each with different strengths and requirements:\n\n1. Gâteaux (Directional) Derivative\n\nWe say that the Gâteaux derivative of F at x in a specific direction h is:F'(x; h) = \\lim_{t \\to 0} \\frac{F(x + th) - F(x)}{t}\n\nThis quantity measures how the function F changes along the ray x + th. While this limit may exist for each direction h separately, it doesn’t guarantee that the derivative is linear in h. This is a key limitation: the Gâteaux derivative can exist in all directions but still fail to provide a good linear approximation.\n\n2. Hadamard Directional Derivative\n\nRather than considering a single direction of perturbation, we now consider a bundle of perturbations around h. We ask how the function changes as we approach the target direction from nearby directions. We say that F has a Hadamard directional derivative if:F'(x; h) = \\lim_{\\substack{t \\downarrow 0 \\\\ h' \\to h}} \\frac{F(x + t h') - F(x)}{t}\n\nThis is a stronger condition than Gâteaux differentiability because it requires the limit to be uniform over nearby directions. However, it still doesn’t guarantee linearity in h.\n\n3. Fréchet Derivative\n\nThe strongest and most natural notion: F is Fréchet differentiable at x if there exists a bounded linear operator L such that:\\lim_{h \\to 0} \\frac{\\|F(x + h) - F(x) - Lh\\|}{\\|h\\|} = 0\n\nThis definition directly addresses the inadequacy of the previous notions. Unlike Gâteaux and Hadamard derivatives, the Fréchet derivative explicitly requires the existence of a linear operator L that provides a good approximation. Key properties:\n\nL must be linear in h (unlike the directional derivatives above)\n\nThe approximation error is o(\\|h\\|), uniform in all directions\n\nThis is the “true” derivative: it generalizes the Jacobian matrix to infinite dimensions\n\nNotation: L = F'(x) or DF(x)\n\nRelationship:\\text{Fréchet differentiable} \\Rightarrow \\text{Hadamard directionally diff.} \\Rightarrow \\text{Gâteaux directionally diff.}\n\nIn the context of the Newton-Kantorovich method, we work with an operator F: X \\to Y where both X and Y are Banach spaces. The Fréchet derivative F'(x) is the best linear approximation of F near x, and it’s exactly this linear operator L that we use in our linearization F(x+h) \\approx F(x) + F'(x)h.\n\nNow apart from those mathematical technicalities, Newton-Kantorovich has in essence the same structure as that of the original Newton’s method. That is, it applies the following sequence of steps:\n\nLinearize the Operator:\nGiven an approximation  x_n , we consider the Fréchet derivative of  F , denoted by  F'(x_n) . This derivative is a linear operator that provides a local approximation of  F  near  x_n .\n\nSet Up the Newton Step:\nThe method then solves the linearized equation for a correction  h_n :F'(x_n) h_n = -F(x_n).\n\nThis equation represents a linear system where  h_n  is chosen so that the linearized operator  F(x_n) + F'(x_n)h_n  equals zero.\n\nUpdate the Solution:\nThe new approximation  x_{n+1}  is then given by:x_{n+1} = x_n + h_n.\n\nThis correction step refines  x_n , bringing it closer to the true solution.\n\nRepeat Until Convergence:\nWe repeat the linearization and update steps until the solution  x_n  converges to the desired tolerance, which can be verified by checking that  \\|F(x_n)\\|  is sufficiently small, or by monitoring the norm  \\|x_{n+1} - x_n\\| .\n\nThe convergence of Newton-Kantorovich does not hinge on  F  being a contraction over the entire domain (as it could be the case for successive approximation). The convergence properties of the Newton-Kantorovich method are as follows:\n\nLocal Convergence: Under mild conditions (e.g., F is Fréchet differentiable and F'(x) is invertible near the solution), the method converges locally. This means that if the initial guess is sufficiently close to the true solution, the method will converge.\n\nGlobal Convergence: Global convergence is not guaranteed in general. However, under stronger conditions (e.g., F is analytic and satisfies certain bounds), the method can converge globally.\n\nRate of Convergence: When the method converges, it typically exhibits quadratic convergence. This means that the error at each step is proportional to the square of the error at the previous step:\\|x_{n+1} - x^*\\| \\leq C\\|x_n - x^*\\|^2\n\nwhere x^* is the true solution and C is some constant. This quadratic convergence is significantly faster than the linear convergence typically seen in methods like successive approximation.","type":"content","url":"/dp#newton-kantorovich-method","position":43},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Optimality Equations for Infinite-Horizon MDPs","lvl2":"Infinite-Horizon MDPs"},"type":"lvl3","url":"/dp#optimality-equations-for-infinite-horizon-mdps","position":44},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Optimality Equations for Infinite-Horizon MDPs","lvl2":"Infinite-Horizon MDPs"},"content":"Recall that in the finite-horizon setting, the optimality equations are:v_n(s) = \\max_{a \\in A_s} \\left\\{r(s, a) + \\gamma \\sum_{j \\in S} p(j | s, a) v_{n+1}(j)\\right\\}\n\nwhere v_n(s) is the value function at time step n for state s, A_s is the set of actions available in state s, r(s, a) is the reward function, \\gamma is the discount factor, and p(j | s, a) is the transition probability from state s to state j given action a.\n\nIntuitively, we would expect that by taking the limit of n to infinity, we might get the nonlinear equations:v(s) = \\max_{a \\in A_s} \\left\\{r(s, a) + \\gamma \\sum_{j \\in S} p(j | s, a) v(j)\\right\\}\n\nwhich are called the optimality equations or Bellman equations for infinite-horizon MDPs.\n\nWe can adopt an operator-theoretic perspective by defining operators on the space V of bounded real-valued functions on the state space S. For a deterministic Markov rule \\pi \\in \\Pi^{MD}, define the policy-evaluation operator:(\\BellmanPi v)(s) = r(s,\\pi(s)) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,\\pi(s)) v(j)\n\nThe Bellman optimality operator is then:\\Bellman \\mathbf{v} \\equiv \\max_{\\pi \\in \\Pi^{MD}} \\left\\{\\mathbf{r}_\\pi + \\gamma \\mathbf{P}_\\pi \\mathbf{v}\\right\\}\n\nwhere \\Pi^{MD} is the set of Markov deterministic decision rules, \\mathbf{r}_\\pi is the reward vector under decision rule \\pi, and \\mathbf{P}_\\pi is the transition probability matrix under decision rule \\pi.\n\nNote that while we write \\max_{\\pi \\in \\Pi^{MD}}, we do not implement the above operator by enumerating all decision rules. Rather, the fact that we compare policies based on their value functions in a componentwise fashion means that maximizing over the space of Markovian deterministic rules reduces to the following update in component form:(\\Bellman \\mathbf{v})(s) = \\max_{a \\in \\mathcal{A}_s} \\left\\{r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j)\\right\\}\n\nFor convenience, we define the greedy selector \\mathrm{Greedy}(v) \\in \\Pi^{MD} that extracts an optimal decision rule from a value function:\\mathrm{Greedy}(v)(s) \\in \\arg\\max_{a \\in \\mathcal{A}_s} \\left\\{r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j)\\right\\}\n\nIn Puterman’s terminology, such a greedy selector is called v-improving (or conserving when it achieves the maximum). This operator will be useful for expressing algorithms succinctly:\n\nValue iteration: v_{k+1} = \\Bellman v_k, then extract \\pi = \\mathrm{Greedy}(v^*)\n\nPolicy iteration: \\pi_{k+1} = \\mathrm{Greedy}(v^{\\pi_k}) with v^{\\pi_k} solving v = \\mathrm{L}_{\\pi_k}v\n\nThe equivalence between these two forms can be shown mathematically, as demonstrated in the following proposition and proof.\n\nThe operator \\Bellman defined as a maximization over Markov deterministic decision rules:(\\Bellman \\mathbf{v})(s) = \\max_{\\pi \\in \\Pi^{MD}} \\left\\{r(s,\\pi(s)) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,\\pi(s)) v(j)\\right\\}\n\nis equivalent to the componentwise maximization over actions:(\\Bellman \\mathbf{v})(s) = \\max_{a \\in \\mathcal{A}_s} \\left\\{r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j)\\right\\}\n\nFix s. LetQ_v(s,a) \\triangleq r(s,a)+\\gamma\\sum_{j}p(j\\mid s,a)\\,v(j).\n\nFor any rule \\pi \\in \\Pi^{MD}, we have (\\BellmanPi v)(s)=Q_v(s,\\pi(s))\\le \\max_{a\\in\\mathcal{A}_s}Q_v(s,a).\n\nTaking the maximum over \\pi gives\\max_{\\pi\\in\\Pi^{MD}}(\\BellmanPi v)(s) \\le \\max_{a\\in\\mathcal{A}_s}Q_v(s,a).\n\nConversely, choose a greedy selector \\pi^v\\in\\Pi^{MD} such that for each s,\\pi^v(s)\\in\\arg\\max_{a\\in\\mathcal{A}_s}Q_v(s,a)\n\n(possible since \\mathcal{A}_s is finite; otherwise use a measurable \\varepsilon-greedy selector). Then(\\Bellman _{\\pi^v}v)(s)=Q_v(s,\\pi^v(s))=\\max_{a\\in\\mathcal{A}_s}Q_v(s,a),\n\nso \\max_{\\pi}(\\BellmanPi v)(s)\\ge \\max_{a}Q_v(s,a). Combining both inequalities yields equality.","type":"content","url":"/dp#optimality-equations-for-infinite-horizon-mdps","position":45},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Algorithms for Solving the Optimality Equations","lvl2":"Infinite-Horizon MDPs"},"type":"lvl3","url":"/dp#algorithms-for-solving-the-optimality-equations","position":46},{"hierarchy":{"lvl1":"Dynamic Programming","lvl3":"Algorithms for Solving the Optimality Equations","lvl2":"Infinite-Horizon MDPs"},"content":"The optimality equations are operator equations. Therefore, we can apply general numerical methods to solve them. Applying the successive approximation method to the Bellman optimality equation yields a method known as “value iteration” in dynamic programming. A direct application of the blueprint for successive approximation yields the following algorithm:\n\nValue Iteration\n\nInput Given an MDP (S, A, P, R, \\gamma) and tolerance \\varepsilon > 0\n\nOutput Compute an \\varepsilon-optimal value function v and policy \\pi\n\nInitialize v_0(s) = 0 for all s \\in S\n\nn \\leftarrow 0\n\nrepeat\n\nFor each s \\in S:\n\nv_{n+1}(s) \\leftarrow (\\Bellman v_n)(s) = \\max_{a \\in A} \\left\\{r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a)v_n(j)\\right\\}\n\n\\delta \\leftarrow \\|v_{n+1} - v_n\\|_\\infty\n\nn \\leftarrow n + 1\n\nuntil \\delta < \\frac{\\varepsilon(1-\\gamma)}{2\\gamma}\n\nExtract greedy policy: \\pi \\leftarrow \\mathrm{Greedy}(v_n) where\\mathrm{Greedy}(v)(s) \\in \\arg\\max_{a \\in \\mathcal{A}_s} \\left\\{r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a)v(j)\\right\\}\n\nreturn v_n, \\pi\n\nThe termination criterion in this algorithm is based on a specific bound that provides guarantees on the quality of the solution. This is in contrast to supervised learning, where we often use arbitrary termination criteria based on computational budget or early stopping when the learning curve flattens. This is because establishing implementable generalization bounds in supervised learning is challenging.\n\nHowever, in the dynamic programming context, we can derive various bounds that can be implemented in practice. These bounds help us terminate our procedure with a guarantee on the precision of our value function and, correspondingly, on the optimality of the resulting policy.\n\nConvergence of Value Iteration\n\n(Adapted from \n\nPuterman (1994) theorem 6.3.1)\n\nLet v_0 be any initial value function, \\varepsilon > 0 a desired accuracy, and let \\{v_n\\} be the sequence of value functions generated by value iteration, i.e., v_{n+1} = \\Bellman v_n for n \\geq 0, where \\Bellman is the Bellman optimality operator. Then:\n\nv_n converges to the optimal value function v^*_\\gamma,\n\nThe algorithm terminates in finite time,\n\nThe resulting policy \\pi_\\varepsilon is \\varepsilon-optimal, and\n\nWhen the algorithm terminates, v_{n+1} is within \\varepsilon/2 of v^*_\\gamma.\n\nParts 1 and 2 follow directly from the fact that \\Bellman is a contraction mapping. Hence, by Banach’s fixed-point theorem, it has a unique fixed point (which is v^*_\\gamma), and repeated application of \\Bellman will converge to this fixed point. Moreover, this convergence happens at a geometric rate, which ensures that we reach the termination condition in finite time.\n\nTo show that the Bellman optimality operator \\Bellman is a contraction mapping, we need to prove that for any two value functions v and u:\\|\\Bellman v - \\Bellman u\\|_\\infty \\leq \\gamma \\|v - u\\|_\\infty\n\nwhere \\gamma \\in [0,1) is the discount factor and \\|\\cdot\\|_\\infty is the supremum norm.\n\nLet’s start by writing out the definition of \\Bellman v and \\Bellman u:\\begin{align*}\n(\\Bellman v)(s) &= \\max_{a \\in A} \\left\\{r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a)v(j)\\right\\}\\\\\n(\\Bellman u)(s) &= \\max_{a \\in A} \\left\\{r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a)u(j)\\right\\}\n\\end{align*}\n\nFor any state s, let a_v be the action that achieves the maximum for (\\Bellman v)(s), and a_u be the action that achieves the maximum for (\\Bellman u)(s). By the definition of these maximizers:\\begin{align*}\n(\\Bellman v)(s) &\\geq r(s,a_u) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a_u)v(j)\\\\\n(\\Bellman u)(s) &\\geq r(s,a_v) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a_v)u(j)\n\\end{align*}\n\nSubtracting these inequalities:\\begin{align*}\n(\\Bellman v)(s) - (\\Bellman u)(s) &\\leq \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a_v)(v(j) - u(j))\\\\\n(\\Bellman u)(s) - (\\Bellman v)(s) &\\leq \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a_u)(u(j) - v(j))\n\\end{align*}\n\nTaking the absolute value and using the fact that \\sum_{j \\in \\mathcal{S}} p(j|s,a) = 1:|(\\Bellman v)(s) - (\\Bellman u)(s)| \\leq \\gamma \\max_{j \\in \\mathcal{S}} |v(j) - u(j)| = \\gamma \\|v - u\\|_\\infty\n\nSince this holds for all s \\in \\mathcal{S}, taking the supremum over s gives:\\|\\Bellman v - \\Bellman u\\|_\\infty \\leq \\gamma \\|v - u\\|_\\infty\n\nThus, \\Bellman is a contraction mapping with contraction factor \\gamma.\n\nNow, let’s prove parts 3 and 4. Suppose the algorithm has just terminated, i.e., \\|v_{n+1} - v_n\\|_\\infty < \\frac{\\varepsilon(1-\\gamma)}{2\\gamma} for some n. We want to show that our current value function v_{n+1} and the policy \\pi_\\varepsilon derived from it are close to optimal.\n\nBy the triangle inequality:\\|v^{\\pi_\\varepsilon}_\\gamma - v^*_\\gamma\\|_\\infty \\leq \\|v^{\\pi_\\varepsilon}_\\gamma - v_{n+1}\\|_\\infty + \\|v_{n+1} - v^*_\\gamma\\|_\\infty\n\nFor the first term, since v^{\\pi_\\varepsilon}_\\gamma is the fixed point of \\mathrm{L}_{\\pi_\\varepsilon} and \\pi_\\varepsilon is greedy with respect to v_{n+1} (i.e., \\mathrm{L}_{\\pi_\\varepsilon}v_{n+1} = \\Bellman v_{n+1}):\\begin{aligned}\n\\|v^{\\pi_\\varepsilon}_\\gamma - v_{n+1}\\|_\\infty &= \\|\\mathrm{L}_{\\pi_\\varepsilon}v^{\\pi_\\varepsilon}_\\gamma - v_{n+1}\\|_\\infty \\\\\n&\\leq \\|\\mathrm{L}_{\\pi_\\varepsilon}v^{\\pi_\\varepsilon}_\\gamma - \\mathrm{L}_{\\pi_\\varepsilon}v_{n+1}\\|_\\infty + \\|\\mathrm{L}_{\\pi_\\varepsilon}v_{n+1} - v_{n+1}\\|_\\infty \\\\\n&= \\|\\mathrm{L}_{\\pi_\\varepsilon}v^{\\pi_\\varepsilon}_\\gamma - \\mathrm{L}_{\\pi_\\varepsilon}v_{n+1}\\|_\\infty + \\|\\Bellman v_{n+1} - v_{n+1}\\|_\\infty \\\\\n&\\leq \\gamma\\|v^{\\pi_\\varepsilon}_\\gamma - v_{n+1}\\|_\\infty + \\gamma\\|v_{n+1} - v_n\\|_\\infty\n\\end{aligned}\n\nwhere we used that both \\Bellman and \\mathrm{L}_{\\pi_\\varepsilon} are contractions with factor \\gamma, and that v_{n+1} = \\Bellman v_n.\n\nRearranging:\\|v^{\\pi_\\varepsilon}_\\gamma - v_{n+1}\\|_\\infty \\leq \\frac{\\gamma}{1-\\gamma}\\|v_{n+1} - v_n\\|_\\infty\n\nSimilarly, since v^*_\\gamma is the fixed point of \\Bellman:\\|v_{n+1} - v^*_\\gamma\\|_\\infty = \\|\\Bellman v_n - \\Bellman v^*_\\gamma\\|_\\infty \\leq \\gamma\\|v_n - v^*_\\gamma\\|_\\infty \\leq \\frac{\\gamma}{1-\\gamma}\\|v_{n+1} - v_n\\|_\\infty\n\nSince \\|v_{n+1} - v_n\\|_\\infty < \\frac{\\varepsilon(1-\\gamma)}{2\\gamma}:\\|v^{\\pi_\\varepsilon}_\\gamma - v_{n+1}\\|_\\infty \\leq \\frac{\\gamma}{1-\\gamma} \\cdot \\frac{\\varepsilon(1-\\gamma)}{2\\gamma} = \\frac{\\varepsilon}{2}\\|v_{n+1} - v^*_\\gamma\\|_\\infty \\leq \\frac{\\gamma}{1-\\gamma} \\cdot \\frac{\\varepsilon(1-\\gamma)}{2\\gamma} = \\frac{\\varepsilon}{2}\n\nCombining these:\\|v^{\\pi_\\varepsilon}_\\gamma - v^*_\\gamma\\|_\\infty \\leq \\frac{\\varepsilon}{2} + \\frac{\\varepsilon}{2} = \\varepsilon\n\nThis completes the proof, showing that v_{n+1} is within \\varepsilon/2 of v^*_\\gamma (part 4) and \\pi_\\varepsilon is \\varepsilon-optimal (part 3).","type":"content","url":"/dp#algorithms-for-solving-the-optimality-equations","position":47},{"hierarchy":{"lvl1":"Dynamic Programming","lvl4":"Newton-Kantorovich Applied to Bellman Optimality","lvl3":"Algorithms for Solving the Optimality Equations","lvl2":"Infinite-Horizon MDPs"},"type":"lvl4","url":"/dp#newton-kantorovich-applied-to-bellman-optimality","position":48},{"hierarchy":{"lvl1":"Dynamic Programming","lvl4":"Newton-Kantorovich Applied to Bellman Optimality","lvl3":"Algorithms for Solving the Optimality Equations","lvl2":"Infinite-Horizon MDPs"},"content":"We now apply the Newton-Kantorovich framework to the Bellman optimality equation. Let(\\Bellman v)(s) = \\max_{a \\in A(s)} \\left\\{ r(s,a) + \\gamma \\sum_{s'} p(s' \\mid s,a) v(s') \\right\\}.\n\nThe problem is to find v such that \\Bellman v = v, or equivalently \\mathrm{B}(v) := \\Bellman v - v = 0. The operator \\Bellman is piecewise affine, hence not globally differentiable, but it is directionally differentiable everywhere in the Hadamard sense and Fréchet differentiable at points where the maximizer is unique.\n\nWe consider three complementary perspectives for understanding and computing its derivative.","type":"content","url":"/dp#newton-kantorovich-applied-to-bellman-optimality","position":49},{"hierarchy":{"lvl1":"Dynamic Programming","lvl5":"Perspective 1: Max of Affine Maps","lvl4":"Newton-Kantorovich Applied to Bellman Optimality","lvl3":"Algorithms for Solving the Optimality Equations","lvl2":"Infinite-Horizon MDPs"},"type":"lvl5","url":"/dp#perspective-1-max-of-affine-maps","position":50},{"hierarchy":{"lvl1":"Dynamic Programming","lvl5":"Perspective 1: Max of Affine Maps","lvl4":"Newton-Kantorovich Applied to Bellman Optimality","lvl3":"Algorithms for Solving the Optimality Equations","lvl2":"Infinite-Horizon MDPs"},"content":"In tabular form, for finite state and action spaces, the Bellman operator can be written as a pointwise maximum of affine maps:(\\Bellman v)(s) = \\max_{a \\in A(s)} \\left\\{ r(s,a) + \\gamma (P_a v)(s) \\right\\},\n\nwhere P_a \\in \\mathbb{R}^{|S| \\times |S|} is the transition matrix associated with action a. Each Q_a v := r^a + \\gamma P_a v is affine in v. The operator \\Bellman therefore computes the upper envelope of a finite set of affine functions at each state.\n\nAt any v, let the active set at state s be\\mathcal{A}^*(s; v) := \\arg\\max_{a \\in A(s)} (Q_a v)(s).\n\nThen the Hadamard directional derivative exists and is given by(\\Bellman '(v; h))(s) = \\max_{a \\in \\mathcal{A}^*(s; v)} \\gamma (P_a h)(s).\n\nIf the active set is a singleton, this expression becomes linear in h, and \\Bellman is Fréchet differentiable at v, with\\Bellman'(v) = \\gamma P_{\\pi_v},\n\nwhere \\pi_v(s) := a^*(s) is the greedy policy at v. In the presence of ties, the derivative becomes set-valued: the Clarke subdifferential consists of stochastic matrices whose rows are convex combinations of the $\\gamma P_a$ over $a \\in \\mathcal{A}^*(s; v)$. ","type":"content","url":"/dp#perspective-1-max-of-affine-maps","position":51},{"hierarchy":{"lvl1":"Dynamic Programming","lvl5":"Perspective 2: Envelope Theorem","lvl4":"Newton-Kantorovich Applied to Bellman Optimality","lvl3":"Algorithms for Solving the Optimality Equations","lvl2":"Infinite-Horizon MDPs"},"type":"lvl5","url":"/dp#perspective-2-envelope-theorem","position":52},{"hierarchy":{"lvl1":"Dynamic Programming","lvl5":"Perspective 2: Envelope Theorem","lvl4":"Newton-Kantorovich Applied to Bellman Optimality","lvl3":"Algorithms for Solving the Optimality Equations","lvl2":"Infinite-Horizon MDPs"},"content":"Consider now a value function approximated as a linear combination of basis functions:v_c(s) = \\sum_j c_j \\phi_j(s).\n\nAt a node s_i, define the parametric maximizationv_i(c) := (\\Bellman v_c)(s_i) = \\max_{a \\in A(s_i)} \\left\\{ r(s_i,a) + \\gamma \\sum_j c_j \\mathbb{E}_{s' \\mid s_i, a}[\\phi_j(s')] \\right\\}.\n\nDefineF_i(a, c) := r(s_i,a) + \\gamma \\sum_j c_j \\mathbb{E}_{s' \\mid s_i, a}[\\phi_j(s')],\n\nso that v_i(c) = \\max_a F_i(a, c). Since F_i is linear in c, we can apply the envelope theorem (Danskin’s theorem): if the optimizer a_i^*(c) is unique or selected measurably, then\\frac{\\partial v_i}{\\partial c_j}(c) = \\gamma \\mathbb{E}_{s' \\mid s_i, a_i^*(c)}[\\phi_j(s')].\n\nWe do not need to differentiate the optimizer a_i^*(c) itself. The result extends to the subdifferential case when ties occur, where the Jacobian becomes set-valued.\n\nThis result is useful when solving the collocation equation \\Phi c = v(c). Newton’s method requires the Jacobian v'(c), and this expression allows us to compute it without involving any derivatives of the optimal action.","type":"content","url":"/dp#perspective-2-envelope-theorem","position":53},{"hierarchy":{"lvl1":"Dynamic Programming","lvl5":"Perspective 3: The Implicit Function Theorem","lvl4":"Newton-Kantorovich Applied to Bellman Optimality","lvl3":"Algorithms for Solving the Optimality Equations","lvl2":"Infinite-Horizon MDPs"},"type":"lvl5","url":"/dp#perspective-3-the-implicit-function-theorem","position":54},{"hierarchy":{"lvl1":"Dynamic Programming","lvl5":"Perspective 3: The Implicit Function Theorem","lvl4":"Newton-Kantorovich Applied to Bellman Optimality","lvl3":"Algorithms for Solving the Optimality Equations","lvl2":"Infinite-Horizon MDPs"},"content":"The third perspective applies the implicit function theorem to understand when the Bellman operator is differentiable despite containing a max operator. The maximization problem defines an implicit relationship between the value function and the optimal action, and the implicit function theorem tells us when this relationship is smooth enough to differentiate through.\n\nThe Bellman operator is defined as(\\Bellman v)(s) = \\max_{a} \\left\\{ r(s,a) + \\gamma \\sum_j p(j \\mid s,a) v(j) \\right\\}.\n\nThe difficulty is that the max operator encodes a discrete selection: which action achieves the maximum. To apply the implicit function theorem, we reformulate this as follows. For each action a, define the action-value function:Q_a(v, s) := r(s,a) + \\gamma \\sum_j p(j \\mid s,a) v(j).\n\nThe optimal action at v satisfies the optimality condition:Q_{a^*(s)}(v, s) \\geq Q_a(v, s) \\quad \\text{for all } a.\n\nNow suppose that at a particular v, action a^*(s) is a strict local maximizer in the sense that there exists \\delta > 0 such thatQ_{a^*(s)}(v, s) > Q_a(v, s) + \\delta \\quad \\text{for all } a \\neq a^*(s).\n\nThis strict inequality is the regularity condition needed for the implicit function theorem. It ensures that the optimal action is unique at v and remains so in a neighborhood of v.\n\nTo see why, consider any perturbation v + h with \\|h\\| small. Since Q_a is linear in v, we have:Q_a(v+h, s) = Q_a(v, s) + \\gamma \\sum_j p(j \\mid s,a) h(j).\n\nThe perturbation term is bounded: |\\gamma \\sum_j p(j \\mid s,a) h(j)| \\leq \\gamma \\|h\\|. Therefore, for \\|h\\| < \\delta/\\gamma, the strict gap ensures thatQ_{a^*(s)}(v+h, s) > Q_a(v+h, s) \\quad \\text{for all } a \\neq a^*(s).\n\nThus a^*(s) remains the unique maximizer throughout the neighborhood \\{v + h : \\|h\\| < \\delta/\\gamma\\}.\n\nThe implicit function theorem now applies: in this neighborhood, the mapping v \\mapsto a^*(s; v) is constant (and hence smooth), taking the value a^*(s). This allows us to write(\\Bellman v)(s) = Q_{a^*(s)}(v, s) = r(s,a^*(s)) + \\gamma \\sum_j p(j \\mid s,a^*(s)) v(j)\n\nas an explicit formula that holds throughout the neighborhood. Since Q_{a^*(s)}(\\cdot, s) is an affine (hence smooth) function of v, we can differentiate it:\\frac{d}{dv} (\\Bellman v)(s) = \\gamma P_{a^*(s)}.\n\nMore precisely, for any perturbation h:(\\Bellman (v+h))(s) = (\\Bellman v)(s) + \\gamma \\sum_j p(j \\mid s,a^*(s)) h(j) + o(\\|h\\|).\n\nThis is the Fréchet derivative:\\Bellman'(v) = \\gamma P_{\\pi_v},\n\nwhere \\pi_v(s) = a^*(s) is the greedy policy.\n\nThe role of the implicit function theorem: It guarantees that when the maximizer is unique with a strict gap (the regularity condition), the argmax function v \\mapsto a^*(s; v) is locally constant, which removes the non-differentiability of the max operator. Without this regularity condition (specifically, at points where multiple actions tie for optimality), the implicit function theorem does not apply, and the operator is not Fréchet differentiable. The active set perspective (Perspective 1) and the envelope theorem (Perspective 2) provide the tools to handle these non-smooth points.","type":"content","url":"/dp#perspective-3-the-implicit-function-theorem","position":55},{"hierarchy":{"lvl1":"Dynamic Programming","lvl4":"Connection to Policy Iteration","lvl3":"Algorithms for Solving the Optimality Equations","lvl2":"Infinite-Horizon MDPs"},"type":"lvl4","url":"/dp#connection-to-policy-iteration","position":56},{"hierarchy":{"lvl1":"Dynamic Programming","lvl4":"Connection to Policy Iteration","lvl3":"Algorithms for Solving the Optimality Equations","lvl2":"Infinite-Horizon MDPs"},"content":"We return to the Newton-Kantorovich step:(I - \\Bellman'(v_n)) h_n = v_n - \\Bellman v_n,\n\\quad\nv_{n+1} = v_n - h_n.\n\nSuppose \\Bellman'(v_n) = \\gamma P_{\\pi_{v_n}} for the greedy policy \\pi_{v_n}. Then(I - \\gamma P_{\\pi_{v_n}}) v_{n+1} = r^{\\pi_{v_n}},\n\nwhich is exactly policy evaluation for \\pi_{v_n}. Recomputing the greedy policy from v_{n+1} yields the next iterate.\n\nThus, policy iteration is Newton-Kantorovich applied to the Bellman optimality equation. At points of nondifferentiability (when ties occur), the operator is still semismooth, and policy iteration corresponds to a semismooth Newton method. The envelope theorem is what justifies the simplification of the Jacobian to \\gamma P_{\\pi_v}, bypassing the need to differentiate through the optimizer. This completes the equivalence.","type":"content","url":"/dp#connection-to-policy-iteration","position":57},{"hierarchy":{"lvl1":"Dynamic Programming","lvl4":"The Semismooth Newton Perspective","lvl3":"Algorithms for Solving the Optimality Equations","lvl2":"Infinite-Horizon MDPs"},"type":"lvl4","url":"/dp#the-semismooth-newton-perspective","position":58},{"hierarchy":{"lvl1":"Dynamic Programming","lvl4":"The Semismooth Newton Perspective","lvl3":"Algorithms for Solving the Optimality Equations","lvl2":"Infinite-Horizon MDPs"},"content":"The three perspectives we developed above (the active set view, the envelope theorem, and the implicit function theorem) all point toward a deeper framework for understanding Newton-type methods on non-smooth operators. This framework, known as semismooth Newton methods, was developed precisely to handle operators like the Bellman operator that are piecewise smooth but not globally differentiable. The connection between policy iteration and semismooth Newton methods has been rigorously developed in recent work \n\nGargiani et al. (2022).\n\nThe classical Newton-Kantorovich method assumes the operator is Fréchet differentiable everywhere. The derivative exists, is unique, and varies continuously with the base point. But the Bellman operator \\Bellman violates this assumption at any value function where multiple actions tie for optimality at some state. At such points, the implicit function theorem fails, and there is no unique Fréchet derivative.\n\nSemismooth Newton methods address this by replacing the notion of a single Jacobian with a generalized derivative that captures the behavior of the operator near non-smooth points. The most commonly used generalized derivative is the Clarke subdifferential, which we can think of as the convex hull of all possible “candidate Jacobians” that arise from limits approaching the non-smooth point from different directions.\n\nFor the Bellman residual \\mathrm{B}(v) = \\Bellman v - v, the Clarke subdifferential at a point v can be characterized explicitly using our first perspective. Recall that at each state s, we defined the active set \\mathcal{A}^*(s; v) = \\arg\\max_a Q_a(v, s). When this set contains multiple actions, the operator is not Fréchet differentiable. However, it remains directionally differentiable in all directions, and the Clarke subdifferential consists of all matrices of the form\\partial \\mathrm{B}(v) = \\left\\{ I - \\gamma P_\\pi : \\pi(s) \\in \\mathcal{A}^*(s; v) \\text{ for all } s \\right\\}.\n\nIn words, the generalized Jacobian is the set of all matrices I - \\gamma P_\\pi where \\pi is any policy that selects an action from the active set at each state. When the maximizer is unique everywhere, this set reduces to a singleton, and we recover the classical Fréchet derivative. When ties occur, the set has multiple elements: precisely the convex combinations mentioned in Perspective 1.\n\nThe semismooth Newton method for solving \\mathrm{B}(v) = 0 proceeds by selecting an element J_k \\in \\partial \\mathrm{B}(v_k) at each iteration and solvingJ_k h_k = -\\mathrm{B}(v_k), \\quad v_{k+1} = v_k + h_k.\n\nWhat this tells us is that any choice from the Clarke subdifferential yields a valid Newton-like update. In the context of the Bellman equation, choosing J_k = I - \\gamma P_{\\pi_k} where \\pi_k is any greedy policy corresponds exactly to the policy evaluation step in policy iteration. The freedom in selecting which action to choose when ties occur translates to the freedom in selecting which element of the subdifferential to use.\n\nUnder appropriate regularity conditions (specifically, when the residual function is BD-regular or CD-regular), the semismooth Newton method converges locally at a quadratic rate \n\nGargiani et al. (2022). This means that near the solution, the error decreases quadratically:\\|v_{k+1} - v^*\\| \\leq C \\|v_k - v^*\\|^2.\n\nThis theoretical result explains an empirical observation that has long been noted in practice: policy iteration typically converges in very few iterations, often just a handful, even when the state and action spaces are enormous and the space of possible policies is exponentially large.\n\nThe semismooth Newton framework also suggests a spectrum of methods interpolating between value iteration and policy iteration. Value iteration can be interpreted as a Newton-like method where we choose J_k = I at every iteration, ignoring the dependence of \\Bellman on v entirely. This choice guarantees global convergence through the contraction property but sacrifices the quadratic local convergence rate. Policy iteration, at the other extreme, uses the full generalized Jacobian J_k = I - \\gamma P_{\\pi_k}, achieving quadratic convergence but at the cost of solving a linear system at each iteration.\n\nBetween these extremes lie methods that use approximate Jacobians. One natural variant is to choose J_k = \\alpha I for some scalar \\alpha > 1. This leads to the updatev_{k+1} = \\frac{\\alpha - 1}{\\alpha} v_k + \\frac{1}{\\alpha} \\Bellman v_k.\n\nThis is known as \\alpha-value iteration or successive over-relaxation when \\alpha > 1. For appropriate choices of \\alpha, this method retains global convergence while achieving better local rates than standard value iteration, and it requires only pointwise operations rather than solving a linear system. The Newton perspective thus unifies existing algorithms and generates new ones by systematically exploring different approximations to the generalized Jacobian.\n\nThe connection to semismooth Newton methods places policy iteration within a broader mathematical framework that extends far beyond dynamic programming. Semismooth Newton methods are used in optimization (for complementarity problems and variational inequalities), in PDE-constrained optimization (for problems with control constraints), and in economics (for equilibrium problems). The Bellman equation, viewed through this lens, is simply one instance of a piecewise smooth equation, and the tools developed for such equations apply directly.","type":"content","url":"/dp#the-semismooth-newton-perspective","position":59},{"hierarchy":{"lvl1":"Dynamic Programming","lvl4":"Policy Iteration","lvl3":"Algorithms for Solving the Optimality Equations","lvl2":"Infinite-Horizon MDPs"},"type":"lvl4","url":"/dp#policy-iteration","position":60},{"hierarchy":{"lvl1":"Dynamic Programming","lvl4":"Policy Iteration","lvl3":"Algorithms for Solving the Optimality Equations","lvl2":"Infinite-Horizon MDPs"},"content":"While we derived policy iteration-like steps from the Newton-Kantorovich method, it’s worth examining policy iteration as a standalone algorithm, as it has been traditionally presented in the field of dynamic programming.\n\nThe policy iteration algorithm for discounted Markov decision problems is as follows:\n\nPolicy Iteration\n\nInput: MDP (S, A, P, R, \\gamma)\nOutput: Optimal policy \\pi^*\n\nInitialize: n = 0, select an arbitrary decision rule \\pi_0 \\in \\Pi^{MD}\n\nrepeat\n3. (Policy evaluation) Obtain \\mathbf{v}^n by solving:(\\mathbf{I}-\\gamma \\mathbf{P}_{\\pi_n}) \\mathbf{v} = \\mathbf{r}_{\\pi_n}\n\n(Policy improvement) Choose \\pi_{n+1} = \\mathrm{Greedy}(\\mathbf{v}^n) where:\\pi_{n+1} \\in \\arg\\max_{\\pi \\in \\Pi^{MD}}\\left\\{\\mathbf{r}_\\pi+\\gamma \\mathbf{P}_\\pi \\mathbf{v}^n\\right\\}\n\nequivalently, for each s:\\pi_{n+1}(s) \\in \\arg\\max_{a \\in \\mathcal{A}_s}\\left\\{r(s,a)+\\gamma \\sum_j p(j|s,a) \\mathbf{v}^n(j)\\right\\}\n\nSet \\pi_{n+1} = \\pi_n if possible.\n\nIf \\pi_{n+1} = \\pi_n, return \\pi^* = \\pi_n\n\nn = n + 1\n\nuntil convergence\n\nAs opposed to value iteration, this algorithm produces a sequence of both deterministic Markovian decision rules \\{\\pi_n\\} and value functions \\{\\mathbf{v}^n\\}. We recognize in this algorithm the linearization step of the Newton-Kantorovich procedure, which takes place here in the policy evaluation step 3 where we solve the linear system (\\mathbf{I}-\\gamma \\mathbf{P}_{\\pi_n}) \\mathbf{v} = \\mathbf{r}_{\\pi_n}. In practice, this linear system could be solved either using direct methods (eg. Gaussian elimination), using simple iterative methods such as the successive approximation method for policy evaluation, or more sophisticated methods such as GMRES.","type":"content","url":"/dp#policy-iteration","position":61},{"hierarchy":{"lvl1":"Dynamics Models for Decision Making"},"type":"lvl1","url":"/dynamics","position":0},{"hierarchy":{"lvl1":"Dynamics Models for Decision Making"},"content":"","type":"content","url":"/dynamics","position":1},{"hierarchy":{"lvl1":"Dynamics Models for Decision Making"},"type":"lvl1","url":"/dynamics#dynamics-models-for-decision-making","position":2},{"hierarchy":{"lvl1":"Dynamics Models for Decision Making"},"content":"The kind of model we need here is a dynamics model. It does not just describe correlations. It tells us how a system evolves in time and, most importantly for control, how that evolution responds to inputs we choose.\n\nA dynamics model earns its keep by answering counterfactuals of the form: given an initial condition and an input schedule, what trajectory should I expect? That ability to roll a trajectory forward under different candidate inputs is the backbone of planning, policy evaluation, and learning from interaction.\n\nAt this level, we can think of the model as a trajectory generator:(\\mathbf{x}_0,\\ \\{\\mathbf{u}_t\\},\\ \\{\\mathbf{d}_t\\}) \\ \\longmapsto\\ \\{\\mathbf{x}_t,\\ \\mathbf{y}_t\\}_{t=0:T},\n\nwhere \\mathbf{u}_t are controls we set, \\mathbf{d}_t are exogenous drivers we do not control (weather, inflow, demand), \\mathbf{x}_t are internal system variables, and \\mathbf{y}_t are observations. The split between \\mathbf{u} and \\mathbf{d} is practical: it separates what we can act on from what we must accommodate.\n\nTwo design pressures shape such models:\n\nResponsiveness to inputs. The model must expose the levers that matter for the decision problem, even if everything underneath is approximate.\n\nMemory management. To simulate step by step, we need a compact summary of the past that is sufficient to predict the next step once an input arrives. That summary is what we will call the state.\n\nThis brings us to a standard but powerful representation. Rather than carry the full history, we look for a variable \\mathbf{x}_t that captures “what matters so far” for predicting what comes next under a given input. With that variable in hand, the model advances in small increments and can be composed with estimators and controllers.\n\nWith this motivation in place, we can now introduce the formalism.","type":"content","url":"/dynamics#dynamics-models-for-decision-making","position":3},{"hierarchy":{"lvl1":"The State‑Space Perspective"},"type":"lvl1","url":"/dynamics#the-state-space-perspective","position":4},{"hierarchy":{"lvl1":"The State‑Space Perspective"},"content":"Most dynamics models, whether derived from physics or learned from data, can be cast into state‑space form. The state \\mathbf{x} is the compact memory that summarizes the past for prediction and control. Inputs \\mathbf{u} perturb that state, exogenous drivers \\mathbf{d} push it around, and outputs \\mathbf{y} are what we can measure. The equations look the same whether time is treated in discrete steps or as a continuous variable.","type":"content","url":"/dynamics#the-state-space-perspective","position":5},{"hierarchy":{"lvl1":"The State‑Space Perspective","lvl2":"Discrete versus continuous time"},"type":"lvl2","url":"/dynamics#discrete-versus-continuous-time","position":6},{"hierarchy":{"lvl1":"The State‑Space Perspective","lvl2":"Discrete versus continuous time"},"content":"How we represent time is dictated by how we sense and actuate: digital controllers sample and apply inputs in steps; the underlying physics evolve continuously.\n\nTime can be represented in two complementary ways, depending on how the system is sensed, actuated, or modelled.\n\nIn discrete time, we treat time as an integer counter, t = 0, 1, 2, \\dots, advancing in fixed steps. This matches how digital systems operate: sensors are sampled periodically, decisions are made at regular intervals, and most logged data takes this form.\n\nContinuous time treats time as a real variable, t \\in \\mathbb{R}_{\\ge 0}. Many physical systems (mechanical, thermal, chemical) are most naturally expressed this way, using differential equations to describe how state changes.\n\nThe two views are interchangeable to some extent. A continuous-time model can be discretized through numerical integration, although this involves approximation. The degree of approximation depends on both the step size \\Delta t and the integration algorithm used. Conversely, a discrete-time policy can be extended to continuous time by holding inputs constant over time intervals (a zeroth-order hold), or by interpolating between values.\n\nIn physical systems, this hybrid setup is almost always present. Control software sends discrete commands to hardware (say, the output of a PID controller) which are then processed by a DAC (digital-to-analog converter) and applied to the plant through analog signals. The hardware might hold a voltage constant, ramp it, or apply some analog shaping. On the sensing side, continuous signals are sampled via ADCs before reaching a digital controller. So in practice, even systems governed by continuous dynamics end up interfacing with the digital world through discrete-time approximations.\n\nThis raises a natural question: if everything eventually gets discretized anyway, why not just model everything in discrete time from the start?\n\nIn many cases, we do. But continuous-time models can still be useful, sometimes even necessary. They often make physical assumptions more explicit, connect more naturally to domain knowledge (e.g. differential equations in mechanics or thermodynamics), and expose invariances or conserved quantities that get obscured by time discretization. They also make it easier to model systems at different time scales, or to reason about how behaviors change as resolution increases. So while implementation happens in discrete time, thinking in continuous time can clarify the structure of the model.\n\nStill, it’s helpful to see how both representations look in mathematical form. The state-space equations are nearly identical with different notations depending on how time is represented.\n\nDiscrete time\n\nHaving defined state as the summary we carry forward, a step of prediction applies the chosen input and advances the state.\n\n\\mathbf{x}_{t+1} = f_t(\\mathbf{x}_t, \\mathbf{u}_t), \\qquad \\mathbf{y}_t = h_t(\\mathbf{x}_t, \\mathbf{u}_t).\n\nContinuous time\n\n\\dot{\\mathbf{x}}(t) = f(\\mathbf{x}(t), \\mathbf{u}(t)), \\qquad \\mathbf{y}(t) = h(\\mathbf{x}(t), \\mathbf{u}(t)).\n\nThe dot denotes a derivative with respect to real time; everything else (state, control, observation) remains the same.\n\nWhen the functions f and h are linear we obtain\n\nLinearity is not a belief about the world, it is a modeling choice that trades fidelity for transparency and speed.\n\n\\dot{\\mathbf{x}} = A\\mathbf{x} + B\\mathbf{u}, \\qquad \\mathbf{y} = C\\mathbf{x} + D\\mathbf{u}.\n\nThe matrices A, B, C, D may vary with t.  Readers with an ML background will recognise the parallel with recurrent neural networks: the state is the hidden vector, the control the input, and the output the read‑out layer.\n\nClassical control often moves to the frequency domain, using Laplace and Z‑transforms to turn differential and difference equations into algebraic ones. That is invaluable for stability analysis of linear time‑invariant systems, but the time‑domain state‑space view is more flexible for learning and simulation, so we will keep our primary focus there.","type":"content","url":"/dynamics#discrete-versus-continuous-time","position":7},{"hierarchy":{"lvl1":"Examples of Deterministic Dynamics: HVAC Control"},"type":"lvl1","url":"/dynamics#examples-of-deterministic-dynamics-hvac-control","position":8},{"hierarchy":{"lvl1":"Examples of Deterministic Dynamics: HVAC Control"},"content":"Imagine you’re in Montréal, in the middle of February. Outside it’s -20°C, but inside your home, a thermostat tries to keep things comfortable. When the indoor temperature drops below your setpoint, the heating system kicks in. That system (a small building, a heater, the surrounding weather) can be modeled mathematically.\n\nWe start with a very simple approximation: treat the entire room as a single “thermal mass,” like a big air-filled box that heats up or cools down depending on how much heat flows in or out.\n\nLet \\mathbf{x}(t) be the indoor air temperature at time t, and \\mathbf{u}(t) be the heating power supplied by the HVAC system. The outside air temperature, denoted \\mathbf{d}(t), affects the system too, acting as a known disturbance. Then the rate of change of indoor temperature is:\\dot{\\mathbf{x}}(t) = -\\frac{1}{RC}\\mathbf{x}(t) + \\frac{1}{RC}\\mathbf{d}(t) + \\frac{1}{C}\\mathbf{u}(t).\n\nHere:\n\nR is a thermal resistance: how well the walls insulate.\n\nC is a thermal capacitance: how much energy it takes to heat the air.\n\nThis is a continuous-time linear system, and we can write it in standard state-space form:\\dot{\\mathbf{x}}(t) = \\mathbf{A}\\mathbf{x}(t) + \\mathbf{B}\\mathbf{u}(t) + \\mathbf{E}\\mathbf{d}(t), \\quad \\mathbf{y}(t) = \\mathbf{C}\\mathbf{x}(t),\n\nwith:\n\n\\mathbf{x}(t): indoor air temperature (the state)\n\n\\mathbf{u}(t): heater input (the control)\n\n\\mathbf{d}(t): outdoor temperature (disturbance)\n\n\\mathbf{y}(t): observed indoor temperature (output)\n\n\\mathbf{A} = -\\frac{1}{RC}\n\n\\mathbf{B} = \\frac{1}{C}\n\n\\mathbf{E} = \\frac{1}{RC}\n\n\\mathbf{C} = 1\n\nThis model is simple, but too simplistic. It ignores the fact that the walls themselves store heat and release it slowly. This kind of delay is called thermal inertia: even if you turn the heater off, the walls might continue to warm the room for a while.\n\nTo capture this effect, we need to expand our state to include the wall temperature. We now model two coupled thermal masses: one for the air, and one for the wall. Heat can flow from the heater into the air, from the air into the wall, and from the wall out to the environment. This gives a more realistic description of how heat moves through a building envelope.\n\nWe write down an energy balance for each mass:\n\nFor the air:C_{\\text{air}} \\frac{dT_{\\text{in}}}{dt} = \\frac{T_{\\text{wall}} - T_{\\text{in}}}{R_{\\text{ia}}} + u(t),\n\nFor the wall:C_{\\text{wall}} \\frac{dT_{\\text{wall}}}{dt} = \\frac{T_{\\text{out}} - T_{\\text{wall}}}{R_{\\text{wo}}} - \\frac{T_{\\text{wall}} - T_{\\text{in}}}{R_{\\text{ia}}}.\n\nEach term on the right-hand side corresponds to a flow of heat: the air gains heat from the wall and the heater, and the wall exchanges heat with both the air and the outside.\n\nNow define the state vector:\\mathbf{x}(t) = \\begin{bmatrix} T_{\\text{in}}(t) \\\\ T_{\\text{wall}}(t) \\end{bmatrix},\n\\quad \\mathbf{u}(t) = u(t),\n\\quad \\mathbf{d}(t) = T_{\\text{out}}(t).\n\nDividing both equations by their respective capacitances and rearranging terms, we arrive at the coupled system:\\dot{\\mathbf{x}}(t) = \\mathbf{A}\\mathbf{x}(t) + \\mathbf{B}\\mathbf{u}(t) + \\mathbf{E}\\mathbf{d}(t), \\quad \\mathbf{y}(t) = \\mathbf{C}\\mathbf{x}(t),\n\nwith:\\mathbf{A} = \\begin{bmatrix}\n-\\frac{1}{R_{\\text{ia}}C_{\\text{air}}} & \\frac{1}{R_{\\text{ia}}C_{\\text{air}}} \\\\\n\\frac{1}{R_{\\text{ia}}C_{\\text{wall}}} & -\\left(\\frac{1}{R_{\\text{ia}}} + \\frac{1}{R_{\\text{wo}}}\\right) \\frac{1}{C_{\\text{wall}}}\n\\end{bmatrix},\n\\quad\n\\mathbf{B} = \\begin{bmatrix} \\frac{1}{C_{\\text{air}}} \\\\ 0 \\end{bmatrix},\n\\quad\n\\mathbf{E} = \\begin{bmatrix} 0 \\\\ \\frac{1}{R_{\\text{wo}}C_{\\text{wall}}} \\end{bmatrix},\n\\quad\n\\mathbf{C} = \\begin{bmatrix} 1 & 0 \\end{bmatrix}.\n\nEach entry in \\mathbf{A} has a physical interpretation:\n\nA_{11}: heat loss from the air to the wall\n\nA_{12}: heat gain by the air from the wall\n\nA_{21}: heat gain by the wall from the air\n\nA_{22}: net loss from the wall to both the air and the outside\n\nThe temperatures are now dynamically coupled: any change in one affects the other. The wall acts as a buffer that absorbs and releases heat over time.\n\nThis is still a linear system, just with a 2D state. But already it behaves differently. The walls absorb and release heat, smoothing out fluctuations and slowing down the system’s response.\n\nAs we add more rooms, walls, or building elements, the system grows. Each new temperature adds a new state. The equations still have the same structure, and their sparsity follows the building layout. Nodes represent temperatures; edges encode how heat flows between them.","type":"content","url":"/dynamics#examples-of-deterministic-dynamics-hvac-control","position":9},{"hierarchy":{"lvl1":"Examples of Deterministic Dynamics: HVAC Control","lvl2":"What Do We Control?"},"type":"lvl2","url":"/dynamics#what-do-we-control","position":10},{"hierarchy":{"lvl1":"Examples of Deterministic Dynamics: HVAC Control","lvl2":"What Do We Control?"},"content":"This network of states is what we control. What we mean by “control input” \\mathbf{u}(t) depends on both what we want to achieve and what we can implement in practice.\n\nThe most direct interpretation is to let \\mathbf{u}(t) represent the actual heating power delivered to the system, measured in watts. This makes sense when modeling from physical principles or simulating a system with fine-grained actuation.\n\nIn many real buildings, however, thermostats don’t issue power commands. They activate a relay, turning the heater on or off based on whether the measured temperature crosses a setpoint. Some systems allow for modulated control—such as varying fan speed or partially opening a valve—but those details are often hidden behind firmware or closed controllers.\n\nA common implementation involves a PID control loop that compares the measured temperature to a setpoint and adjusts the control signal accordingly. While the actual logic might be simple, the resulting behavior appears smoothed or delayed from the perspective of the building.\n\nDepending on the abstraction level, we might:\n\nTreat \\mathbf{u}(t) as continuous power input, if designing the full control logic.\n\nUse it as a setpoint input, assuming a lower-level controller handles the rest.\n\nOr reduce it to a binary signal—heater on or off—when working with logged behavior from a smart thermostat.\n\nEach perspective shapes the kind of model we build and the kind of control problem we pose. If we’re aiming to design a controller from scratch, it may be worth modeling the full closed-loop dynamics. If the goal is to tune setpoints or learn policies from data, a coarser abstraction might be not only sufficient, but more robust.","type":"content","url":"/dynamics#what-do-we-control","position":11},{"hierarchy":{"lvl1":"Examples of Deterministic Dynamics: HVAC Control","lvl2":"Why This Model?"},"type":"lvl2","url":"/dynamics#why-this-model","position":12},{"hierarchy":{"lvl1":"Examples of Deterministic Dynamics: HVAC Control","lvl2":"Why This Model?"},"content":"At this point, you might wonder: why go through the trouble of building this kind of physics-based model at all? After all, if we can log indoor temperatures, thermostat actions, and weather data, isn’t it easier to just learn a model from data? A neural ODE, for example, would let us define a parameterized function:\\dot{\\mathbf{z}}(t) = f_{\\boldsymbol{\\theta}}(\\mathbf{z}(t), \\mathbf{u}(t), \\mathbf{d}(t)), \\quad \\mathbf{y}(t) = g_{\\boldsymbol{\\theta}}(\\mathbf{z}(t)),\n\nwith both f_{\\boldsymbol{\\theta}} and g_{\\boldsymbol{\\theta}} learned from data. The internal state \\mathbf{z}(t) is not tied to any physical quantity. It just needs to be expressive enough to explain the observations.\n\nThat flexibility can be useful, particularly when a large dataset is already available. But in building control and energy modeling, the constraints are usually different.\n\nOften, the engineer or consultant on site is working under tight time and information budgets. A floor plan might be available, along with some basic specs on insulation or window types, and a few days of logged sensor data. The task might be to simulate load under different weather scenarios, tune a controller, or just help understand why a room is slow to heat up. The model has to be built quickly, adapted easily, and remain understandable to others working on the same system.\n\nIn that context, RC models are often the default choice: not because they are inherently better, but because they fit the workflow.\n\nInterpretability.\nThe parameters correspond to things you can reason about: thermal resistance, capacitance, heat transfer between zones. You can cross-check values against architectural plans, or adjust them manually when something doesn’t line up. You can tell which wall or zone is contributing to slow recovery times.\n\nIdentifiability with limited data.\nRC models can often be calibrated from short data traces, even when not all state variables are directly observable. The structure already imposes constraints: heat flows from hot to cold, dynamics are passive, responses are smooth. Those properties help narrow the space of valid parameter settings. A neural ODE, in contrast, typically needs more data to settle into stable and plausible dynamics—especially if no additional constraints are enforced during training.\n\nSimplicity and reuse.\nOnce the model is built, it’s straightforward to modify. If a window is replaced, or a wall gets insulated, you only need to update a few numbers.  It’s easy to pass along to another engineer or embed in a larger simulation. A model like\\dot{\\mathbf{x}} = \\mathbf{A}\\mathbf{x} + \\mathbf{B}\\mathbf{u} + \\mathbf{E}\\mathbf{d}\n\nis linear and low-dimensional. Simulating it is cheap, even if you do it many times. That may not matter now, but it will matter later, when we want to optimize over trajectories or learn from them.\n\nThis doesn’t mean RC models are always sufficient. They simplify or ignore many effects: solar gains, occupancy, nonlinearities, humidity, equipment switching behavior. If those effects are significant, and you have enough data, a black-box model (neural ODE or otherwise) might achieve lower prediction error. In practice, though, it’s common to combine the two: use the RC structure as a backbone, and learn a residual model to correct for unmodeled dynamics.","type":"content","url":"/dynamics#why-this-model","position":13},{"hierarchy":{"lvl1":"From Deterministic to Stochastic"},"type":"lvl1","url":"/dynamics#from-deterministic-to-stochastic","position":14},{"hierarchy":{"lvl1":"From Deterministic to Stochastic"},"content":"The models we’ve seen so far were deterministic: given an initial state and input sequence, the system evolves in a fixed, predictable way. But real systems rarely behave so neatly. Sensors are noisy. Parameters drift. The world changes in ways we can’t fully model.\n\nTo account for this uncertainty, we move from deterministic dynamics to stochastic models. There are two equivalent but conceptually distinct ways to do this.","type":"content","url":"/dynamics#from-deterministic-to-stochastic","position":15},{"hierarchy":{"lvl1":"From Deterministic to Stochastic","lvl2":"Function plus Noise"},"type":"lvl2","url":"/dynamics#function-plus-noise","position":16},{"hierarchy":{"lvl1":"From Deterministic to Stochastic","lvl2":"Function plus Noise"},"content":"The most direct extension adds a noise term to the dynamics:\\mathbf{x}_{t+1} = f_t(\\mathbf{x}_t, \\mathbf{u}_t, \\mathbf{w}_t), \\quad \\mathbf{w}_t \\sim p_{\\mathbf{w}}.\n\nIf the noise is additive and Gaussian, we recover the standard linear-Gaussian setup used in Kalman filtering:\\mathbf{x}_{t+1} = A\\mathbf{x}_t + B\\mathbf{u}_t + \\mathbf{w}_t, \\quad \\mathbf{w}_t \\sim \\mathcal{N}(0, Q).\n\nBut we’re not restricted to Gaussian or additive noise. For instance, if the noise distribution is non-Gaussian:\\mathbf{x}_{t+1} = f(\\mathbf{x}_t, \\mathbf{u}_t) + \\mathbf{w}_t, \\quad \\mathbf{w}_t \\sim \\text{Laplace}, \\ \\text{or}\\ \\text{Student-t},\n\nthen \\mathbf{x}_{t+1} inherits those properties. This is known as a convolution model: the next-state distribution is a shifted version of the noise distribution, centered around the deterministic prediction. More formally, it’s a special case of a pushforward measure: the randomness from \\mathbf{w}_t is “pushed forward” through the function f to yield a distribution over outcomes.\n\nOr the noise might enter multiplicatively:\\mathbf{x}_{t+1} = f(\\mathbf{x}_t, \\mathbf{u}_t) + \\Gamma(\\mathbf{x}_t, \\mathbf{u}_t) \\mathbf{w}_t,\n\nwhere \\Gamma is a matrix that modulates the effect of the noise, potentially depending on state and control. If \\Gamma is invertible, we can even write down an explicit density via a change-of-variables:p(\\mathbf{x}_{t+1} \\mid \\mathbf{x}_t, \\mathbf{u}_t) = p_{\\mathbf{w}}\\left(\\Gamma^{-1}(\\mathbf{x}_t, \\mathbf{u}_t)\\left[\\mathbf{x}_{t+1} - f(\\mathbf{x}_t, \\mathbf{u}_t)\\right] \\right)\\cdot \\left| \\det \\Gamma^{-1} \\right|.\n\nThis kind of structured noise is common in practice, for example, when disturbances are amplified at certain operating points.\n\nThe function-plus-noise view is natural when we have a physical or simulator-based model and want to account for uncertainty around it. It is constructive: we know how the system evolves and how the randomness enters. This means we can track the source of variability along a trajectory, which is particularly useful for techniques like reparameterization or infinitesimal perturbation analysis (IPA). These methods rely on being able to differentiate through the noise injection mechanism, something that is much easier when the noise is explicit and structured.","type":"content","url":"/dynamics#function-plus-noise","position":17},{"hierarchy":{"lvl1":"From Deterministic to Stochastic","lvl2":"Transition Kernel"},"type":"lvl2","url":"/dynamics#transition-kernel","position":18},{"hierarchy":{"lvl1":"From Deterministic to Stochastic","lvl2":"Transition Kernel"},"content":"The second perspective skips over the internal noise and defines the system directly in terms of the probability distribution over next states:p(\\mathbf{x}_{t+1} \\mid \\mathbf{x}_t, \\mathbf{u}_t).\n\nThis transition kernel encodes all the uncertainty in the system’s evolution, without reference to any underlying noise source or functional form.\n\nThis view is strictly more general: it includes the function-plus-noise case as a special instance. If we do know the function f and the noise distribution p_{\\mathbf{w}} from the generative model, then the transition kernel is obtained by “pushing” the randomness through the function:p(\\mathbf{x}_{t+1} \\mid \\mathbf{x}_t, \\mathbf{u}_t) = \\int \\delta(\\mathbf{x}_{t+1} - f(\\mathbf{x}_t, \\mathbf{u}_t, \\mathbf{w})) \\, p_{\\mathbf{w}}(\\mathbf{w}) \\, d\\mathbf{w}.\n\nThis might look abstract, but it’s just marginalization: for each possible noise value \\mathbf{w}, we compute the resulting next state, and then average over all possible \\mathbf{w}, weighted by how likely each one is.\n\nIf the noise were discrete, this becomes a sum:p(\\mathbf{x}_{t+1} \\mid \\mathbf{x}_t, \\mathbf{u}_t) = \\sum_{i=1}^k \\mathbb{1}\\{f(\\mathbf{x}_t, \\mathbf{u}_t, w_i) = \\mathbf{x}_{t+1}\\} \\cdot p_i\n\nThis abstraction is especially useful when we don’t know (or don’t care about) the underlying function or noise distribution. All we need is the ability to sample transitions or estimate their likelihoods. This is the default formulation in reinforcement learning, econometrics, and other settings focused on behavior rather than mechanism.","type":"content","url":"/dynamics#transition-kernel","position":19},{"hierarchy":{"lvl1":"From Deterministic to Stochastic","lvl2":"Continuous-Time Analogue"},"type":"lvl2","url":"/dynamics#continuous-time-analogue","position":20},{"hierarchy":{"lvl1":"From Deterministic to Stochastic","lvl2":"Continuous-Time Analogue"},"content":"In continuous time, the stochastic dynamics of a system are often described using a stochastic differential equation (SDE):d\\mathbf{X}_t = f(\\mathbf{X}_t, \\mathbf{U}_t)\\,dt + \\sigma(\\mathbf{X}_t, \\mathbf{U}_t)\\,d\\mathbf{W}_t,\n\nwhere \\mathbf{W}_t is Brownian motion. The first term, called the drift, describes the average motion of the system. The second, scaled by \\sigma, models how random fluctuations (diffusion) enter over time. Just like in discrete time, this is a function + noise model: the state evolves through a deterministic path perturbed by stochastic input.\n\nThis generative view again induces a probability distribution over future states. At any future time t + \\Delta t, the system doesn’t land at a single state but is described by a distribution that depends on the initial condition and the noise along the way.\n\nMathematically, this distribution evolves according to what’s called the Fokker–Planck equation—a partial differential equation that governs how probability density “flows” through time. It plays the same role here as the transition kernel did in discrete time: describing how likely the system is to be in any given state, without referring to the noise directly.\n\nWhile the mathematical generalization is clean, working with continuous-time stochastic models can be more challenging. Simulating sample paths is often straightforward (eg. nowadays diffusion models in generative AI), but writing down or computing the exact transition distribution usually isn’t. That’s why many practical methods still rely on discrete-time approximations, even when the underlying system is continuous.","type":"content","url":"/dynamics#continuous-time-analogue","position":21},{"hierarchy":{"lvl1":"From Deterministic to Stochastic","lvl3":"Example: Managing a Québec Hydroelectric Reservoir","lvl2":"Continuous-Time Analogue"},"type":"lvl3","url":"/dynamics#example-managing-a-qu-bec-hydroelectric-reservoir","position":22},{"hierarchy":{"lvl1":"From Deterministic to Stochastic","lvl3":"Example: Managing a Québec Hydroelectric Reservoir","lvl2":"Continuous-Time Analogue"},"content":"On the James Bay plateau, 1 400 km north of Montréal, the Robert-Bourassa reservoir stores roughly 62 km³ of water, more than the volume of Lake Ontario above its minimum operating level. Sixteen giant turbines sit 140 m below the surface, converting that stored head into 5.6 GW of electricity, about a fifth of Hydro-Québec’s total capacity. A steady share of that output feeds Québec’s aluminium smelters, which depend on stable, uninterrupted power.\n\nWater managers face competing objectives:\n\nFlood safety. Sudden snowmelt or storms can overfill the basin, forcing emergency spillways to open. These events are spectacular, but carry real downstream risk and economic cost.\n\nEnergy reliability. If the level falls too low, turbines sit idle and contracts go unmet. Voltage dips at the smelters are measured in lost millions.\n\nA basic deterministic model for the reservoir’s mass balance is just bookkeeping:\\mathbf{x}_{t+1} = \\mathbf{x}_t + \\mathbf{r}_t - \\mathbf{u}_t,\n\nwhere \\mathbf{x}_t is the current reservoir level, \\mathbf{u}_t is the controlled outflow through turbines, and \\mathbf{r}_t is the natural inflow from rainfall and upstream runoff.\n\nBut inflow is variable, and its statistical structure matters. Two hydrological regimes dominate:\n\nIn spring, melting snow over days can produce a long-tailed inflow distribution, often modeled as log-normal or Gamma.\n\nIn summer, convective storms yield a skewed mixture: a point mass at zero (no rain), and a thin but heavy tail capturing sudden bursts.\n\nThis motivates a simple stochastic extension:\\mathbf{x}_{t+1} = \\mathbf{x}_t - \\mathbf{u}_t + \\mathbf{w}_t, \\quad\n\\mathbf{w}_t \\sim\n\\begin{cases}\n0 & \\text{with prob. } p_0, \\\\\\\\\n\\text{LogNormal}(\\mu, \\sigma^2) & \\text{with prob. } 1 - p_0.\n\\end{cases}\n\nHere the physics is fixed, and all uncertainty sits in the inflow term \\mathbf{w}_t. Rather than fitting a full transition model from (\\mathbf{x}_t, \\mathbf{u}_t) to \\mathbf{x}_{t+1}, we can isolate the inflow by rearranging the mass balance:\\hat{\\mathbf{w}}_t = \\mathbf{x}_{t+1} - \\mathbf{x}_t + \\mathbf{u}_t.\n\nThis gives a direct estimate of the realized inflow at each timestep. From there, the problem becomes one of density estimation: fit a probabilistic model to the residuals \\hat{\\mathbf{w}}_t. In spring, this might be a log-normal distribution. In summer, a two-part mixture: a point mass at zero, and an exponential tail. These distributions can be estimated by maximum likelihood, or adjusted using additional features (covariates) such as upstream snowpack or forecasted temperature.\n\nThis setup has practical benefits. Fixing the physical part of the model (how levels respond to inflow and outflow) helps focus the statistical modeling effort. Rather than fitting a full system model, we only need to estimate the variability in inflows. This reduces the number of degrees of freedom and makes the estimation problem easier to interpret. It also avoids conflating uncertainty in inflow with uncertainty in the system’s response.\n\nCompare this to a more generic approach, such as linear regression:\\mathbf{x}_{t+1} = a \\mathbf{x}_t + b \\mathbf{u}_t + \\varepsilon_t.\n\nThis is straightforward to fit, but offers no guarantee that the result behaves sensibly. The model might violate conservation of mass, or compensate for inflow variation by adjusting coefficients a and b. This can lead to misleading conclusions, especially when extrapolating beyond the training data. Hydro‑Québec engineers rely on structured models in practice. Over 150 gauging stations across the La Grande basin report real-time flows, levels, and precipitation to Environment Canada's HYDAT database, which is accessible through a public API. These data feed into Hydro‑Québec's SCADA systems, along with snow-course readings and rainfall estimates. From there, engineers build seasonal inflow models and update them daily.\n\nSynthetic years are then generated by sampling from these models. Each sampled inflow sequence is pushed through the deterministic mass balance, producing a possible reservoir trajectory. These Monte Carlo rollouts are used directly for planning. They help evaluate turbine schedules, size safety margins, and identify periods of elevated risk.\n\nStructured models are not just a matter of physical fidelity. They shape how data is used, how uncertainty is handled, and how downstream decisions are informed. The separation between known dynamics and unknown inputs gives a cleaner interface between estimation and control. ","type":"content","url":"/dynamics#example-managing-a-qu-bec-hydroelectric-reservoir","position":23},{"hierarchy":{"lvl1":"Partial Observability"},"type":"lvl1","url":"/dynamics#partial-observability","position":24},{"hierarchy":{"lvl1":"Partial Observability"},"content":"So far, we’ve assumed that the full system state \\mathbf{x}_t is available. But in most real-world settings, only a partial or noisy observation is accessible. Sensors have limited coverage, measurements come with noise, and some variables aren’t observable at all.\n\nTo model this, we introduce an observation equation alongside the system dynamics:\\begin{aligned}\n\\mathbf{x}_{t+1} &= f_t(\\mathbf{x}_t, \\mathbf{u}_t, \\mathbf{w}_t), \\quad \\mathbf{w}_t \\sim p_{\\mathbf{w}}, \\\\\n\\mathbf{y}_t &= h_t(\\mathbf{x}_t, \\mathbf{v}_t), \\quad \\mathbf{v}_t \\sim p_{\\mathbf{v}}.\n\\end{aligned}\n\nThe state \\mathbf{x}_t evolves under control inputs \\mathbf{u}_t and process noise \\mathbf{w}_t, but we don’t get to see \\mathbf{x}_t directly. Instead, we observe \\mathbf{y}_t, which depends on \\mathbf{x}_t through some possibly nonlinear, noisy function h_t. The noise \\mathbf{v}_t captures measurement uncertainty.\n\nThis setup defines a partially observed system. Even if the underlying dynamics are known, we still face uncertainty due to limited visibility into the true state. The controller or estimator must rely on the observations \\mathbf{y}_{0\\:t} to make sense of the hidden trajectory.\n\nIn the deterministic case, if the output map h_t is full-rank and invertible, we may be able to reconstruct the state directly from the output: no filtering required. But once noise is introduced, that invertibility becomes more subtle: even if h_t is bijective, the presence of \\mathbf{v}_t prevents us from recovering \\mathbf{x}_t exactly. In this case, we must shift from inversion to estimation, often via probabilistic inference.\n\nIn the linear-Gaussian case, the model simplifies to:\\begin{aligned}\n\\mathbf{x}_{t+1} &= A\\mathbf{x}_t + B\\mathbf{u}_t + \\mathbf{w}_t, \\quad \\mathbf{w}_t \\sim \\mathcal{N}(0, Q), \\\\\n\\mathbf{y}_t &= C\\mathbf{x}_t + D\\mathbf{u}_t + \\mathbf{v}_t, \\quad \\mathbf{v}_t \\sim \\mathcal{N}(0, R).\n\\end{aligned}\n\nThis is the classical state-space model used in signal processing and control. It’s fully specified by the system matrices and the covariances Q and R. The state is no longer known, but under these assumptions it can be estimated recursively using the Kalman filter, which maintains a Gaussian belief over \\mathbf{x}_t.\n\nEven when the model is nonlinear or non-Gaussian, the structure remains the same: a dynamic state evolves, and a separate observation process links it to the data we see. Many modern estimation techniques, including extended and unscented Kalman filters, particle filters, and learned neural estimators, build on this core structure.","type":"content","url":"/dynamics#partial-observability","position":25},{"hierarchy":{"lvl1":"Partial Observability","lvl2":"Observation Kernel View"},"type":"lvl2","url":"/dynamics#observation-kernel-view","position":26},{"hierarchy":{"lvl1":"Partial Observability","lvl2":"Observation Kernel View"},"content":"Just as we moved from function-based dynamics to transition kernels, we can abstract away the noise source and define the observation distribution directly:p(\\mathbf{y}_t \\mid \\mathbf{x}_t).\n\nThis kernel summarizes what the sensors tell us about the hidden state. If we know the generative model—say, that \\mathbf{y}_t = h_t(\\mathbf{x}_t) + \\mathbf{v}_t with known p_{\\mathbf{v}}—then this kernel is induced by marginalizing out \\mathbf{v}_t:p(\\mathbf{y}_t \\mid \\mathbf{x}_t) = \\int \\delta\\bigl(\\mathbf{y}_t - h_t(\\mathbf{x}_t, \\mathbf{v})\\bigr)\\, p_{\\mathbf{v}}(\\mathbf{v})\\, d\\mathbf{v}.\n\nBut we don’t have to start from the generative form. In practice, we might define or learn p(\\mathbf{y}_t \\mid \\mathbf{x}_t) directly, especially when dealing with black-box sensors, perception models, or abstract measurement processes.","type":"content","url":"/dynamics#observation-kernel-view","position":27},{"hierarchy":{"lvl1":"Partial Observability","lvl2":"Example – Stabilizing a Telescope’s Vision with Adaptive Optics"},"type":"lvl2","url":"/dynamics#example-stabilizing-a-telescopes-vision-with-adaptive-optics","position":28},{"hierarchy":{"lvl1":"Partial Observability","lvl2":"Example – Stabilizing a Telescope’s Vision with Adaptive Optics"},"content":"On Earth, even the largest telescopes can’t see perfectly. As starlight travels through the atmosphere, tiny air pockets with different temperatures bend the light in slightly different directions. The result is a distorted image: instead of a sharp point, a star looks like a flickering blob. The distortion happens fast, on the order of milliseconds, and changes continuously as wind moves the turbulent layers overhead.\n\nThis is where adaptive optics (AO) comes in. AO systems aim to cancel out these distortions in real time. They do this by measuring how the incoming wavefront of light is distorted and using a flexible mirror to apply a counter-distortion that straightens it back out. But there’s a catch: you can’t observe the wavefront directly. You only get noisy measurements of its slopes (the angles of tilt at various points), and you have to act fast, before the atmosphere changes again.\n\nTo design a controller here, we need a model of how the distortions evolve. And that means building a decision-making model: one that includes uncertainty, partial observability, and fast feedback.\n\nState. The main object we’re trying to track is the distortion of the incoming wavefront. We can’t observe this phase field \\phi(\\mathbf{r}, t) directly, but we can represent it approximately using a finite basis (e.g., Fourier or Zernike). The coefficients of this expansion form our internal state:\\mathbf{x}_t \\in \\mathbb{R}^n \\quad \\text{(wavefront distortion at time } t).\n\nDynamics. The atmosphere evolves in time. A simple but surprisingly effective model assumes the turbulence is “frozen” and just blown across the telescope by the wind. That gives us a discrete-time linear model:\\mathbf{x}_{t+1} = \\mathbf{A} \\mathbf{x}_t + \\mathbf{w}_t,\n\nwhere \\mathbf{A} shifts the distortion pattern in space, and \\mathbf{w}_t is a small random change from evolving turbulence. This noise is not arbitrary: its statistics follow a power law derived from Kolmogorov’s turbulence model. In particular, higher spatial frequencies (small-scale wiggles) have less energy than low ones. That lets us build a prior on how likely different distortions are.\n\nObservations. We can’t see the full wavefront. Instead, we use a wavefront sensor: a camera that captures how the light bends. What it actually measures are local slopes: the gradients of the wavefront, not the wavefront itself. So our observation model is:\\mathbf{y}_t = \\mathbf{C} \\mathbf{x}_t + \\boldsymbol{\\varepsilon}_t,\n\nwhere \\mathbf{C} is a known matrix that maps wavefront distortion to measurable slope angles, and \\boldsymbol{\\varepsilon}_t is measurement noise (e.g., due to photon limits).\n\nControl. Our job is to flatten the wavefront using a deformable mirror. The mirror can apply a small counter-distortion \\mathbf{u}_t that subtracts from the atmospheric one:\\text{Residual state:} \\quad \\mathbf{x}_t^{\\text{res}} = \\mathbf{x}_t - \\mathbf{B} \\mathbf{u}_t.\n\nThe goal is to choose \\mathbf{u}_t to minimize the residual distortion by making the light flat again.\n\nWhy a model matters. Without a model, we’d just react to the current noisy measurements. But with a model, we can predict how the wavefront will evolve, filter out noise, and act preemptively. This is essential in AO, where decisions must be made every millisecond. Kalman filters are often used to track the hidden state \\mathbf{x}_t, combining model predictions with noisy measurements, and linear-quadratic regulators (LQR) or other optimal controllers use those estimates to choose the best correction.\n\nTime structure. This is a rare case where continuous-time modeling also plays a role. The true evolution of the turbulence is continuous, and we can model it using a stochastic differential equation (SDE):d\\mathbf{x}(t) = \\mathbf{F} \\mathbf{x}(t)\\,dt + \\mathbf{G}\\,d\\mathbf{W}(t),\n\nwhere \\mathbf{W}(t) is Brownian motion and the matrix \\mathbf{G} encodes the Kolmogorov spectrum. Discretizing this equation gives us the \\mathbf{A} and \\mathbf{Q} matrices for the discrete-time model above. ## Data-Driven Identification\n\nNot all models come from physics. Sometimes, we fit them directly from data.\n\nEven a basic linear regression of the form:\n\n$$\nx_{t+1} = a x_t + b u_t + c + \\varepsilon_t\n$$\n\nis a dynamical model. But things can get more sophisticated. Subspace identification methods, sparse regressions like SINDy, Koopman embeddings, neural ODEs—all of these let us learn models from observed trajectories. The key question is how much structure we assume. Do we enforce linearity? Time-invariance? Do we try to model the noise?  \n# Comparing Physics-Based RC Models with Black-Box Fits\n\nThe data used in this experiment comes from the *Building Energy Geeks* repository, an open-source collection created to demonstrate statistical learning techniques in building energy performance. The file `statespace.csv` provides a time series of indoor and outdoor temperatures together with heating power and solar irradiance. While not tied to a specific building description in the repository, it is designed to mimic realistic conditions either from actual sensor measurements or detailed simulation outputs. This dataset serves as a concrete foundation to explore the contrast between physics-based modeling and purely data-driven approaches.\n\nAt the core of our study lies the so-called **2R2C model**, a reduced-order representation of building thermal dynamics. The name refers to two resistances and two capacitances arranged in a thermal network that captures how heat flows between the indoor environment, the building envelope, and the outdoors. The indoor air temperature $T_i$ is influenced by the envelope temperature $T_e$, which itself exchanges heat with the external environment at temperature $T_o$. The resistances $R_i$ and $R_o$ describe the ease of heat conduction across these boundaries, while the capacitances $C_i$ and $C_e$ represent the heat storage capacity of the indoor air and of the building mass. By including the effect of heating input $\\Phi_H$ and solar gains $\\Phi_S$, the model balances both controllable and environmental influences.  \n\nMathematically, the dynamics are written as a pair of coupled ordinary differential equations. The first governs the indoor air temperature and is given by\n\n$$\n\\frac{dT_i}{dt} = \\frac{T_e - T_i}{R_i C_i} + \\frac{\\eta_H \\Phi_H}{C_i} + \\frac{A_i \\Phi_S}{C_i},\n$$\n\nwhile the second governs the envelope temperature,\n\n$$\n\\frac{dT_e}{dt} = \\frac{T_i - T_e}{R_i C_e} + \\frac{T_o - T_e}{R_o C_e} + \\frac{A_e \\Phi_S}{C_e}.\n$$\n\nHere $\\eta_H$ represents the efficiency of the heating system and $A_i, A_e$ are effective areas for solar gains. Since the original dataset is indexed in hours rather than seconds, the right-hand side of both equations must be scaled by a factor of 3600 to ensure correct integration over the chosen time unit.\n\nThe task is to identify the parameters of this model from data. To do so, we fit the parameters by minimizing the discrepancy between the simulated indoor temperature $T_i$ and the measured trajectory within a training window of 10 to 40 hours. A robust least-squares method with Huber loss is used so that large deviations, possibly due to noise or outliers, do not dominate the fit. Early time points in the training window are given slightly higher weight to ensure that the transient behavior is captured accurately, which is important when the system is initialized away from equilibrium. Once fitted, the model is simulated forward over the entire 0–100 hour horizon, allowing us to test its predictive power on an unseen window spanning 50 to 90 hours.\n\nTo provide meaningful context, we benchmark this physics-based model against two black-box alternatives. The first is a linear regression model that directly maps the contemporaneous values of outdoor temperature, heating power, and solar irradiance to the indoor temperature. This approach ignores temporal dynamics and treats the problem as a purely static regression. The second is a multilayer perceptron (MLP) that is trained with autoregressive lags. Specifically, the MLP is provided with the recent history of indoor temperatures together with the external inputs to predict the next indoor temperature. During training, a technique known as teacher forcing is employed, meaning the true past values of $T_i$ are always supplied, which allows the network to achieve a very tight fit on the training window. However, when rolled out on the test window without access to the ground-truth future values, small prediction errors accumulate, and the model struggles to generalize.\n\nThe results of this comparison illustrate a fundamental point. Although the MLP is highly flexible and achieves excellent accuracy on the training window, its predictions deteriorate rapidly on unseen data, demonstrating the pitfalls of overfitting and the instability of purely data-driven models in autoregressive settings. The linear regression baseline performs moderately but fails to capture the underlying physics, leading to systematic errors. In contrast, the 2R2C model, despite being governed by only a handful of parameters, extrapolates much more consistently. It responds in the correct direction to changes in heating and solar inputs, maintains stable long-term predictions, and provides parameters that map directly to interpretable physical properties such as insulation and thermal mass.  \n\nThis example therefore highlights the dual advantages of physics-based modeling: the ability to generalize beyond the training window and the guarantee of action-consistency rooted in thermodynamic reasoning. At the same time, it underscores the limitations of purely data-driven black-box models when asked to predict system behavior under conditions not seen during training.\n\n```{code-cell} python\n:tags: [hide-input]\n\n%run _static/rcnetwork.py\n``` ","type":"content","url":"/dynamics#example-stabilizing-a-telescopes-vision-with-adaptive-optics","position":29},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods"},"type":"lvl1","url":"/fqi","position":0},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods"},"content":"The \n\nprevious chapter established the theoretical foundations of simulation-based approximate dynamic programming: Monte Carlo integration for evaluating expectations, Q-functions for efficient action selection, and techniques for mitigating overestimation bias. Those developments assumed we could sample freely from transition distributions and choose optimization parameters without constraint. This chapter develops a unified framework for fitted Q-iteration algorithms that spans both offline and online settings. We begin with batch algorithms that learn from fixed datasets, then show how the same template generates online methods like DQN through systematic variations in data collection, optimization strategy, and function approximation.","type":"content","url":"/fqi","position":1},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl2":"Design Choices in FQI Methods"},"type":"lvl2","url":"/fqi#design-choices-in-fqi-methods","position":2},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl2":"Design Choices in FQI Methods"},"content":"All FQI methods share the same two-level structure built on three core ingredients: a buffer \\mathcal{B}_t of transitions inducing an empirical distribution \\hat{P}_{\\mathcal{B}_t}, a target function g that computes regression targets from the current Q-function, and an optimization procedure to fit the Q-function to the resulting targets. At iteration n, the outer loop constructs targets from individual transitions using the target function: for each transition (s_i, a_i, r_i, s'_i), we compute y_i^{(n)} = g(s_i, a_i, r_i, s'_i; \\boldsymbol{\\theta}_n) where typically g(s,a,r,s'; \\boldsymbol{\\theta}) = r + \\gamma \\max_{a'} q(s',a'; \\boldsymbol{\\theta}). The inner loop solves the regression problem \\min_{\\boldsymbol{\\theta}} \\mathbb{E}_{((s,a),y) \\sim \\hat{P}_n^{\\text{fit}}}[\\ell(q(s, a; \\boldsymbol{\\theta}), y)] to find parameters that match these targets. We can write this abstractly as:\\begin{aligned}\n&\\textbf{repeat } n = 0, 1, 2, \\ldots \\\\\n&\\quad \\text{Sample transitions from } \\hat{P}_{\\mathcal{B}_n} \\text{ and construct targets via } g(\\cdot; \\boldsymbol{\\theta}_n) \\\\\n&\\quad \\boldsymbol{\\theta}^{(n+1)} \\leftarrow \\texttt{fit}(\\hat{P}_n^{\\text{fit}}, \\boldsymbol{\\theta}_{\\text{init}}, K) \\\\\n&\\textbf{until } \\text{convergence}\n\\end{aligned}\n\nThe fit operation minimizes the regression loss using K optimization steps (gradient descent for neural networks, tree construction for ensembles, matrix inversion for linear models) starting from initialization \\boldsymbol{\\theta}_{\\text{init}}. Standard supervised learning uses random initialization (\\boldsymbol{\\theta}_{\\text{init}} = \\boldsymbol{\\theta}_0) and runs to convergence (K = \\infty). Reinforcement learning algorithms vary these choices: warm starting from the previous iteration (\\boldsymbol{\\theta}_{\\text{init}} = \\boldsymbol{\\theta}^{(n)}), partial optimization (K \\in \\{10, \\ldots, 100\\}), or single-step updates (K=1).\n\nThe buffer \\mathcal{B}_t may stay fixed (batch setting) or change (online setting), but the targets always change because they depend on the evolving target function g(\\cdot; \\boldsymbol{\\theta}_n). In practice, we typically have one observed next state per transition, giving g(s_i, a_i, r_i, s'_i; \\boldsymbol{\\theta}) = r_i + \\gamma \\max_{a'} q(s'_i, a'; \\boldsymbol{\\theta}) for transition tuples (s_i, a_i, r_i, s'_i).\n\nNotation: Buffer vs Regression Distribution\n\nWe maintain a careful distinction between two empirical distributions throughout:\n\nBuffer distribution \\hat{P}_{\\mathcal{B}_t} over transitions \\tau = (s, a, r, s'): The empirical distribution induced by the replay buffer \\mathcal{B}_t, which contains raw experience tuples. This is fixed (offline) or evolves via online collection (adding new transitions, dropping old ones).\n\nRegression distribution \\hat{P}_t^{\\text{fit}} over pairs ((s,a), y): The empirical distribution over supervised learning targets. This changes every outer iteration n as we recompute targets using the current target function g(\\cdot; \\boldsymbol{\\theta}_n).\n\nThe relationship: at iteration n, we construct \\hat{P}_n^{\\text{fit}} from \\hat{P}_{\\mathcal{B}_n} by applying the target function:\\hat{P}_n^{\\text{fit}} = (\\mathrm{id}, g(\\cdot; \\boldsymbol{\\theta}_n))_\\# \\hat{P}_{\\mathcal{B}_n}\n\nwhere g(s,a,r,s'; \\boldsymbol{\\theta}_n) = r + \\gamma \\max_{a'} q(s', a'; \\boldsymbol{\\theta}_n) or uses the smooth logsumexp operator.\n\nThis distinction matters pedagogically: the buffer distribution \\hat{P}_{\\mathcal{B}_t} is fixed (offline) or evolves via online collection, while the regression distribution \\hat{P}_t^{\\text{fit}} evolves via target recomputation. Fitted Q-iteration is the outer loop over target recomputation, not the inner loop over gradient steps.\n\nThis template provides a blueprint for instantiating concrete algorithms. Six design axes generate algorithmic diversity: the function approximator (trees, neural networks, linear models), the Bellman operator (hard max vs smooth logsumexp, discussed in the \n\nregularized MDP chapter), the inner optimization strategy (full convergence, K steps, or single step), the initialization scheme (cold vs warm start), the data collection mechanism (offline, online, replay buffer), and bias mitigation approaches (none, double Q-learning, learned correction). While individual algorithms include additional refinements, these axes capture the primary sources of variation. The table below shows how several well-known methods instantiate this template:\n\nAlgorithm\n\nApproximator\n\nBellman\n\nInner Loop\n\nInitialization\n\nData\n\nBias Fix\n\nFQI \n\nErnst et al. (2005)\n\nExtra Trees\n\nHard\n\nFull\n\nCold\n\nOffline\n\nNone\n\nNFQI \n\nRiedmiller (2005)\n\nNeural Net\n\nHard\n\nFull\n\nWarm\n\nOffline\n\nNone\n\nQ-learning \n\nWatkins (1989)\n\nAny\n\nHard\n\nK=1\n\nWarm\n\nOnline\n\nNone\n\nDQN \n\nMnih et al. (2013)\n\nDeep NN\n\nHard\n\nK=1\n\nWarm\n\nReplay\n\nNone\n\nDouble DQN \n\nVan Hasselt et al. (2016)\n\nDeep NN\n\nHard\n\nK=1\n\nWarm\n\nReplay\n\nDouble Q\n\nSoft Q \n\nHaarnoja et al. (2017)\n\nNeural Net\n\nSmooth\n\nK steps\n\nWarm\n\nReplay\n\nNone\n\nThis table omits continuous action methods (NFQCA, DDPG, SAC), which introduce an additional design dimension. We address those in the \n\ncontinuous action chapter. The initialization choice becomes particularly important when moving from batch to online algorithms.","type":"content","url":"/fqi#design-choices-in-fqi-methods","position":3},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl3":"Plug-In Approximation with Empirical Distributions","lvl2":"Design Choices in FQI Methods"},"type":"lvl3","url":"/fqi#plug-in-approximation-with-empirical-distributions","position":4},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl3":"Plug-In Approximation with Empirical Distributions","lvl2":"Design Choices in FQI Methods"},"content":"The exact Bellman operator involves expectations under the true transition law (combined with the behavior policy), which we denote abstractly by P over transitions \\tau = (s,a,r,s'):(\\Bellman q)(s,a) = r(s,a) + \\gamma \\int \\max_{a'} q(s',a')\\, P(ds' \\mid s,a)\n\nIn fitted Q-iteration we never see P directly. Instead, we collect a finite set of transitions in a buffer \\mathcal{B}_t = \\{\\tau_1,\\dots,\\tau_{|\\mathcal{B}_t|}\\} and work with the empirical distribution\\hat{P}_{\\mathcal{B}_t} = \\frac{1}{|\\mathcal{B}_t|} \\sum_{\\tau \\in \\mathcal{B}_t} \\delta_\\tau\n\nwhere \\delta_\\tau denotes a Dirac delta (point mass) centered at transition \\tau. Each \\delta_\\tau is a probability distribution that places mass 1 at the single point \\tau and mass 0 everywhere else. The sum creates a mixture distribution: a uniform distribution over the finite set of observed transitions. Sampling from \\hat{P}_{\\mathcal{B}_t} means picking one transition uniformly at random from the buffer.\n\nFor any integrand g(\\tau) (loss, TD error, gradient term), expectations under P are approximated by expectations under \\hat{P}_{\\mathcal{B}_t} using the sample average estimator:\\mathbb{E}_{\\tau \\sim P}\\big[g(\\tau)\\big] \\;\\approx\\; \\mathbb{E}_{\\tau \\sim \\hat{P}_{\\mathcal{B}_t}}\\big[g(\\tau)\\big] = \\frac{1}{|\\mathcal{B}_t|} \\sum_{\\tau \\in \\mathcal{B}_t} g(\\tau)\n\nThis is exactly the sample average estimator from the Monte Carlo chapter, applied now to transitions. Conceptually, fitted Q-iteration performs plug-in approximate dynamic programming. The plug-in principle is a general approach from statistics: when an algorithm requires an unknown population quantity, substitute its sample-based estimator. Here, we replace the unknown transition law P with the empirical distribution \\hat{P}_{\\mathcal{B}_t} and run value iteration using this empirical Bellman operator:(\\widehat{\\Bellman}_{\\mathcal{B}_t} q)(s,a) \\triangleq r(s,a) + \\gamma\\; \\mathbb{E}_{(r',s') \\mid (s,a)\\sim \\hat{P}_{\\mathcal{B}_t}} \\Big[\\max_{a'} q(s',a')\\Big]\n\nFrom a computational viewpoint, we could describe all of this using sample averages and mini-batch gradients. The empirical distribution notation provides three benefits. First, it unifies offline and online algorithms: both perform value iteration under an empirical law \\hat{P}_{\\mathcal{B}_t}, differing only in whether \\mathcal{B}_t remains fixed or evolves. Second, it shows that methods like DQN perform stochastic optimization of a sample average approximation objective \\mathbb{E}_{\\tau \\sim \\hat{P}_{\\mathcal{B}_t}}[\\ell(\\cdot)], not some ad hoc non-stationary procedure. Third, it cleanly separates two sources of approximation that we will examine shortly: statistical bootstrap (resampling from \\hat{P}_{\\mathcal{B}_t}) versus temporal-difference bootstrap (using estimated values in targets).\n\nMathematical Formulation of Empirical Distributions\n\nThe empirical distribution \\hat{P}_{\\mathcal{B}_t} = \\frac{1}{|\\mathcal{B}_t|}\\sum_{\\tau \\in \\mathcal{B}_t} \\delta_{\\tau} is a discrete probability measure over |\\mathcal{B}_t| points regardless of whether state and action spaces are continuous or discrete. For any measurable set A \\subseteq \\mathcal{S} \\times \\mathcal{A} \\times \\mathbb{R} \\times \\mathcal{S}:\\hat{P}_{\\mathcal{B}_t}(A) = \\frac{1}{|\\mathcal{B}_t|}\\sum_{\\tau \\in \\mathcal{B}_t} \\mathbb{1}[\\tau \\in A] = \\frac{|\\{\\tau \\in \\mathcal{B}_t : \\tau \\in A\\}|}{|\\mathcal{B}_t|}\n\nThe empirical distribution assigns probability 1/|\\mathcal{B}_t| to each observed tuple and zero elsewhere.","type":"content","url":"/fqi#plug-in-approximation-with-empirical-distributions","position":5},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl3":"Data, Buffers, and the Unified Template","lvl2":"Design Choices in FQI Methods"},"type":"lvl3","url":"/fqi#data-buffers-and-the-unified-template","position":6},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl3":"Data, Buffers, and the Unified Template","lvl2":"Design Choices in FQI Methods"},"content":"Fitted Q-iteration is built around three ingredients at any time t:\n\nA replay buffer \\mathcal{B}_t containing transitions, inducing an empirical distribution \\hat{P}_{\\mathcal{B}_t}\n\nA target function g(s,a,r,s'; \\boldsymbol{\\theta}) : \\mathcal{S} \\times \\mathcal{A} \\times \\mathbb{R} \\times \\mathcal{S} \\times \\Theta \\mapsto \\mathbb{R} that computes regression targets for individual transitions. Unlike the Bellman operator \\mathcal{T} which acts on function spaces, g operates on transition tuples with parameters \\boldsymbol{\\theta} (after the semicolon) specifying the current Q-function. Typically g(s,a,r,s'; \\boldsymbol{\\theta}) = r + \\gamma \\max_{a'} q(s',a'; \\boldsymbol{\\theta}) (hard max) or uses the smooth logsumexp operator.\n\nA loss function \\ell and optimization budget (replay ratio, number of gradient steps)\n\nPushing transitions through the target function transforms \\hat{P}_{\\mathcal{B}_t} into a regression distribution \\hat{P}_t^{\\text{fit}} over pairs ((s,a), y), as described in the notation remark above. The inner loop minimizes the empirical risk \\mathbb{E}_{((s,a),y)\\sim \\hat{P}_t^{\\text{fit}}} [\\ell(q(s,a;\\boldsymbol{\\theta}), y)] via stochastic gradient descent on mini-batches.\n\nDifferent algorithms correspond to different ways of evolving the buffer \\mathcal{B}_t and different replay ratios:\n\nOffline FQI. We start from a fixed dataset \\mathcal{D} and never collect new data. The buffer is constant, \\mathcal{B}_t \\equiv \\mathcal{D} and \\hat{P}_{\\mathcal{B}_t} \\equiv \\hat{P}_{\\mathcal{D}}, and only the target function g(\\cdot; \\boldsymbol{\\theta}_t) changes as the Q-function evolves.\n\nReplay (DQN-style). The buffer is a circular buffer of fixed capacity B. At each interaction step we append the new transition and, if the buffer is full, drop the oldest one: \\mathcal{B}_t = \\{\\tau_{t-B+1},\\ldots,\\tau_t\\} and \\hat{P}_{\\mathcal{B}_t} = \\frac{1}{|\\mathcal{B}_t|} \\sum_{\\tau \\in \\mathcal{B}_t} \\delta_{\\tau}. The empirical distribution slides forward through time, but at each update we still sample uniformly from the current buffer.\n\nFully online Q-learning. Tabular Q-learning is the degenerate case with buffer size B=1: we only keep the most recent transition. Then \\hat{P}_{\\mathcal{B}_t} is supported on a single point and each update uses that one sample once.\n\nThe replay ratio (optimization steps per environment transition) quantifies data reuse:\\text{Replay ratio} = \\begin{cases}\nK \\cdot N_{\\text{epochs}} \\cdot N / b & \\text{Offline (batch size } b \\text{ from } N \\text{ transitions)} \\\\\nb & \\text{DQN (one step per transition, batch size } b \\text{)} \\\\\n1 & \\text{Online Q-learning}\n\\end{cases}\n\nwhere K is the number of gradient steps per outer iteration. Large replay ratios reuse the same empirical distribution \\hat{P}_{\\mathcal{B}_t} many times, implementing sample average approximation (offline FQI). Small replay ratios use each sample once, implementing stochastic approximation (online Q-learning). Higher replay ratios reduce variance of estimates under \\hat{P}_{\\mathcal{B}_t} but risk overfitting to the idiosyncrasies of the current empirical law, especially when it reflects outdated policies or narrow coverage.\n\nTwo Notions of Bootstrap\n\nThis perspective separates two different “bootstraps” that appear in FQI:\n\nStatistical bootstrap over data. By sampling with replacement from \\hat{P}_{\\mathcal{B}_t}, we approximate expectations under the (unknown) transition distribution P with expectations under the empirical distribution \\hat{P}_{\\mathcal{B}_t}. This is identical to bootstrap resampling in statistics. Mini-batch training is exactly this: we treat the observed transitions as if they were the entire population and approximate expectations by repeatedly resampling from the empirical law. The replay ratio controls how many such bootstrap samples we take per environment interaction.\n\nTemporal-difference bootstrap over values. When we compute y = r + \\gamma \\max_{a'} q(s',a';\\boldsymbol{\\theta}), we replace the unknown continuation value by our current estimate. This is the TD notion of bootstrapping and the source of maximization bias studied in the previous chapter.\n\nFQI, DQN, and their variants combine both: the empirical distribution \\hat{P}_{\\mathcal{B}_t} encodes how we reuse data (statistical bootstrap), and the target function g encodes how we bootstrap values (TD bootstrap). Most bias-correction techniques (Keane–Wolpin, Double Q-learning, Gumbel losses) modify the second kind of bootstrapping while leaving the statistical bootstrap unchanged.\n\nEvery algorithm in this chapter minimizes an empirical risk of the form \\mathbb{E}_{((s,a),y)\\sim \\hat{P}_t^{\\text{fit}}} [\\ell(q(s,a;\\boldsymbol{\\theta}), y)], where expectations are computed via the sample average estimator over the buffer. Algorithmic diversity arises from choices of buffer evolution, target function, loss, and optimization schedule.","type":"content","url":"/fqi#data-buffers-and-the-unified-template","position":7},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl2":"Batch Algorithms: Ernst’s FQI and NFQI"},"type":"lvl2","url":"/fqi#batch-algorithms-ernsts-fqi-and-nfqi","position":8},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl2":"Batch Algorithms: Ernst’s FQI and NFQI"},"content":"We begin with the simplest case from the buffer perspective. We are given a fixed transition dataset \\mathcal{D} = \\{(s_i,a_i,r_i,s'_i)\\}_{i=1}^N and never collect new data. The replay buffer is frozen:\\mathcal{B}_t \\equiv \\mathcal{D}, \\qquad \\hat{P}_{\\mathcal{B}_t} \\equiv \\hat{P}_{\\mathcal{D}}\n\nso the only thing that changes across iterations is the target function g(\\cdot; \\boldsymbol{\\theta}_n) induced by the current Q-function. Every outer iteration of FQI samples from the same empirical distribution \\hat{P}_{\\mathcal{D}} but pushes it through a new target function, producing a new regression distribution \\hat{P}_n^{\\text{fit}}.\n\nAt each outer iteration n, we construct the input set X^{(n)} = \\{(s_i, a_i)\\}_{i=1}^N from the same transitions (the state-action pairs remain fixed), compute targets y_i^{(n)} = g(s_i, a_i, r_i, s'_i; \\boldsymbol{\\theta}^{(n)}) = r_i + \\gamma \\max_{a'} q(s'_i, a'; \\boldsymbol{\\theta}^{(n)}) using the current Q-function, and solve the regression problem \\boldsymbol{\\theta}^{(n+1)} \\leftarrow \\texttt{fit}(X^{(n)}, y^{(n)}, \\boldsymbol{\\theta}_{\\text{init}}, K). The buffer \\mathcal{B}_t = \\mathcal{D} never changes, but the targets change at every iteration because they depend on the evolving target function g(\\cdot; \\boldsymbol{\\theta}_n). Each transition (s_i, a_i, r_i, s'_i) provides a single Monte Carlo sample s'_i for evaluating the Bellman operator at (s_i, a_i), giving us \\widehat{\\Bellman}_1 q with N=1.\n\nThe following algorithm is simply approximate value iteration where expectations under the transition kernel P are replaced by expectations under the fixed empirical distribution \\hat{P}_{\\mathcal{D}}:\n\nGeneric Fitted Q-Iteration (Batch)\n\nInput: Dataset \\mathcal{D} = \\{(s_i, a_i, r_i, s'_i)\\}_{i=1}^N, function approximator q(s,a; \\boldsymbol{\\theta}), discount factor \\gamma, maximum iterations n_{\\max}\n\nOutput: Learned Q-function parameters \\boldsymbol{\\theta}\n\nInitialize \\boldsymbol{\\theta}_0\n\nn \\leftarrow 0\n\nrepeat  \\quad // Outer loop: Value Iteration\n\n\\quad // Construct regression dataset with Bellman targets\n\n\\quad X^{(n)} \\leftarrow \\{(s_i, a_i) : (s_i, a_i, r_i, s'_i) \\in \\mathcal{D}\\}\n\n\\quad for each (s_i, a_i, r_i, s'_i) \\in \\mathcal{D} do\n\n\\quad\\quad y_i^{(n)} \\leftarrow r_i + \\gamma \\max_{a' \\in \\mathcal{A}} q(s'_i, a'; \\boldsymbol{\\theta}_n)\n\n\\quad end for\n\n\\quad // Inner loop: Fit Q-function to targets (projection step)\n\n\\quad \\boldsymbol{\\theta}_{n+1} \\leftarrow \\texttt{fit}(X^{(n)}, y^{(n)}, \\boldsymbol{\\theta}_{\\text{init}}, K)\n\n\\quad n \\leftarrow n+1\n\nuntil convergence or n \\geq n_{\\max}\n\nreturn \\boldsymbol{\\theta}_n\n\nThe fit operation in line 10 abstracts the inner optimization loop that minimizes \\sum_{i=1}^N \\ell(q(s_i, a_i; \\boldsymbol{\\theta}), y_i^{(n)}). This line encodes the algorithmic choice of which function approximator to use and how to optimize it. The initialization \\boldsymbol{\\theta}_{\\text{init}} and number of optimization steps K control whether we use cold or warm starting and whether we optimize to convergence or perform partial updates.\n\nFitted Q-Iteration (FQI): Ernst et al. \n\nErnst et al. (2005) instantiate this template with extremely randomized trees (extra trees), an ensemble method that partitions the state-action space into regions with piecewise constant Q-values. The fit operation trains the ensemble until completion using the tree construction algorithm. Trees handle high-dimensional inputs naturally and the ensemble reduces overfitting. FQI uses cold start initialization: \\boldsymbol{\\theta}_{\\text{init}} = \\boldsymbol{\\theta}_0 (randomly initialized) at every iteration, since trees don’t naturally support incremental refinement. The loss \\ell is squared error. This method demonstrated that batch reinforcement learning could work with complex function approximators on continuous-state problems.\n\nNeural Fitted Q-Iteration (NFQI): Riedmiller \n\nRiedmiller (2005) replaces the tree ensemble with a neural network q(s,a; \\boldsymbol{\\theta}), providing smooth interpolation across the state-action space. The fit operation runs gradient-based optimization (RProp, chosen for its insensitivity to hyperparameter choices) to convergence: train the network until the loss stops decreasing (multiple epochs through the full dataset \\mathcal{D}), corresponding to K=\\infty in our framework. NFQI uses warm start initialization: \\boldsymbol{\\theta}_{\\text{init}} = \\boldsymbol{\\theta}_n at iteration n, meaning the network continues learning from the previous iteration’s weights rather than resetting. This ensures the network accurately represents the projected Bellman operator before moving to the next outer iteration. For episodic tasks with goal and forbidden regions, Riedmiller uses modified target computations (detailed below).\n\nGoal State Heuristics in NFQI\n\nFor episodic tasks with goal states S^+ and forbidden states S^-, Riedmiller modifies the target structure:y_i^{(n)} = \\begin{cases}\nc(s_i, a_i, s'_i) & \\text{if } s'_i \\in S^+ \\text{ (goal reached)} \\\\\nC^- & \\text{if } s'_i \\in S^- \\text{ (forbidden state, typically } C^- = 1.0\\text{)} \\\\\nc(s_i, a_i, s'_i) + \\gamma \\max_{a'} q(s'_i, a'; \\boldsymbol{\\theta}_n) & \\text{otherwise}\n\\end{cases}\n\nwhere c(s, a, s') is the immediate cost. Goal states have zero future cost (no bootstrapping), forbidden states have high penalty, and regular states use the standard Bellman backup. Additionally, the hint-to-goal heuristic adds synthetic transitions (s, a, s') where s \\in S^+ with target value c(s,a,s') = 0 to explicitly clamp the Q-function to zero in the goal region. This stabilizes learning by encoding the boundary condition without requiring additional prior knowledge.","type":"content","url":"/fqi#batch-algorithms-ernsts-fqi-and-nfqi","position":9},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl2":"From Nested to Flattened Q-Iteration"},"type":"lvl2","url":"/fqi#from-nested-to-flattened-q-iteration","position":10},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl2":"From Nested to Flattened Q-Iteration"},"content":"Fitted Q-iteration has an inherently nested structure: an outer loop performs approximate value iteration by computing Bellman targets, and an inner loop performs regression by fitting the function approximator to those targets. This nested structure shows that FQI is approximate dynamic programming with function approximation, distinct from supervised learning with changing targets.\n\nWhen the inner loop uses gradient descent for K steps, we have:\\texttt{fit}(\\mathcal{D}_n^{\\text{fit}}, \\boldsymbol{\\theta}_n, K) = \\boldsymbol{\\theta}_n - \\alpha \\sum_{k=0}^{K-1} \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}(\\boldsymbol{\\theta}_n^{(k)}; \\mathcal{D}_n^{\\text{fit}})\n\nThis is a sequence of updates \\boldsymbol{\\theta}_n^{(k+1)} = \\boldsymbol{\\theta}_n^{(k)} - \\alpha \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}(\\boldsymbol{\\theta}_n^{(k)}; \\mathcal{D}_n^{\\text{fit}}) for k = 0, \\ldots, K-1 starting from \\boldsymbol{\\theta}_n^{(0)} = \\boldsymbol{\\theta}_n. Since these inner updates are themselves a loop, we can algebraically rewrite the nested loops as a single flattened loop. This flattening is purely representational. The algorithm remains approximate value iteration, but the presentation obscures the conceptual structure.\n\nIn the flattened form, the parameters used for computing targets are called the target network \\boldsymbol{\\theta}_{\\text{target}}, which corresponds to \\boldsymbol{\\theta}_n in the nested form. The target network gets updated every K steps, marking the boundaries between outer iterations.\n\nIn contrast, the parameters being actively trained via gradient descent at each step are called the online network \\boldsymbol{\\theta}_t. In the nested view, these correspond to \\boldsymbol{\\theta}_n^{(k)} within the inner loop. The term “online” here refers to the fact that these parameters are continuously updated at each gradient step, not that data must be collected online. The distinction is between the actively-training parameters (\\boldsymbol{\\theta}_t) and the frozen parameters used for target computation (\\boldsymbol{\\theta}_{\\text{target}}).\n\nThe online/target terminology is standard in the deep RL community. Many modern algorithms, especially those that collect data online like DQN, are presented in flattened form. This can make them appear different from batch methods when they are the same template with different design choices.\n\nTree ensemble methods like random forests or extra trees have no continuous parameter space and no gradient-based optimization. The fit operation builds the entire tree structure in one pass. There’s no sequence of incremental updates to unfold into a single loop. Ernst’s FQI \n\nErnst et al. (2005) retains the explicit nested structure with cold start initialization at each outer iteration, while neural methods can be flattened.","type":"content","url":"/fqi#from-nested-to-flattened-q-iteration","position":11},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl3":"Making the Nested Structure Explicit","lvl2":"From Nested to Flattened Q-Iteration"},"type":"lvl3","url":"/fqi#making-the-nested-structure-explicit","position":12},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl3":"Making the Nested Structure Explicit","lvl2":"From Nested to Flattened Q-Iteration"},"content":"To see how flattening works, we first make the nested structure completely explicit by expanding the fit operation to show the inner gradient descent loop. In terms of the buffer notation, the inner loop approximately minimizes the empirical risk:\\mathbb{E}_{((s,a),y)\\sim \\hat{P}_n^{\\text{fit}}}[\\ell(q(s,a;\\boldsymbol{\\theta}), y)]\n\ninduced by the fixed buffer \\mathcal{B}_n = \\mathcal{D} and target function g(\\cdot; \\boldsymbol{\\theta}_n). Starting from the generic batch FQI template (Algorithm \n\nAlgorithm 1), we replace the abstract fit call with explicit gradient updates:\n\nNeural Fitted Q-Iteration with Explicit Inner Loop\n\nInput: MDP (S, A, P, R, \\gamma), offline transition dataset \\mathcal{D}, neural network q(s,a; \\boldsymbol{\\theta}), learning rate \\alpha, inner optimization steps K, initialization \\boldsymbol{\\theta}_0\n\nOutput: Parameters \\boldsymbol{\\theta} for Q-function approximation\n\nInitialize \\boldsymbol{\\theta}_0 randomly\n\nn \\leftarrow 0  \\quad // Outer loop counter (value iteration)\n\nrepeat\n\n// Compute Bellman targets using current Q-function\n\n\\mathcal{D}_n^{\\text{fit}} \\leftarrow \\emptyset\n\nfor each (s,a,r,s') \\in \\mathcal{D} do\n\ny_{s,a} \\leftarrow r + \\gamma \\max_{a' \\in A} q(s',a'; \\boldsymbol{\\theta}_n)\n\n\\mathcal{D}_n^{\\text{fit}} \\leftarrow \\mathcal{D}_n^{\\text{fit}} \\cup \\{((s,a), y_{s,a})\\}\n\n// Inner optimization loop: fit to targets via gradient descent\n\n\\boldsymbol{\\theta}_n^{(0)} \\leftarrow \\boldsymbol{\\theta}_n \\quad // Warm start from previous outer iteration\n\nk \\leftarrow 0 \\quad // Inner loop counter (regression)\n\nrepeat\n\n\\boldsymbol{\\theta}_n^{(k+1)} \\leftarrow \\boldsymbol{\\theta}_n^{(k)} - \\alpha \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}(\\boldsymbol{\\theta}_n^{(k)}; \\mathcal{D}_n^{\\text{fit}})\n\nk \\leftarrow k + 1\n\nuntil k = K \\quad // Partial optimization: exactly K gradient steps\n\n\\boldsymbol{\\theta}_{n+1} \\leftarrow \\boldsymbol{\\theta}_n^{(K)}\n\nn \\leftarrow n + 1\n\nuntil training complete\n\nreturn \\boldsymbol{\\theta}_n\n\nThis makes the two-level structure completely transparent. The outer loop (indexed by n) computes targets y_{s,a} = r + \\gamma \\max_{a'} q(s',a'; \\boldsymbol{\\theta}_n) using the parameters \\boldsymbol{\\theta}_n from the end of the previous outer iteration. These targets remain fixed throughout the entire inner loop. The inner loop (indexed by k) performs K gradient steps to fit q(s,a; \\boldsymbol{\\theta}) to the regression dataset \\mathcal{D}_n^{\\text{fit}} = \\{((s_i, a_i), y_i)\\}, warm starting from \\boldsymbol{\\theta}_n (the parameters that computed the targets). After K steps, the inner loop produces \\boldsymbol{\\theta}_{n+1} = \\boldsymbol{\\theta}_n^{(K)}, which becomes the starting point for the next outer iteration.\n\nThe notation \\boldsymbol{\\theta}_n^{(k)} indicates that we are at inner step k within outer iteration n. The targets depend only on \\boldsymbol{\\theta}_n = \\boldsymbol{\\theta}_n^{(0)}, not on the intermediate inner iterates \\boldsymbol{\\theta}_n^{(k)} for k > 0.","type":"content","url":"/fqi#making-the-nested-structure-explicit","position":13},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl3":"Flattening into a Single Loop","lvl2":"From Nested to Flattened Q-Iteration"},"type":"lvl3","url":"/fqi#flattening-into-a-single-loop","position":14},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl3":"Flattening into a Single Loop","lvl2":"From Nested to Flattened Q-Iteration"},"content":"We can now flatten the nested structure by treating all gradient steps uniformly and using a global step counter t instead of separate outer/inner counters. We introduce a target network \\boldsymbol{\\theta}_{\\text{target}} that holds the parameters used for computing targets. This target network gets updated every K steps, which marks what would have been the boundary between outer iterations. The transformation works as follows: we replace the outer counter n and inner counter k with a single counter t, where t = nK + k. When n advances from n to n+1, this corresponds to K steps of t: from t = nK to t = (n+1)K. The target network \\boldsymbol{\\theta}_{\\text{target}} equals \\boldsymbol{\\theta}_n throughout outer iteration n and gets updated via \\boldsymbol{\\theta}_{\\text{target}} \\leftarrow \\boldsymbol{\\theta}_t every K steps (when t \\bmod K = 0). Parameters that were \\boldsymbol{\\theta}_n^{(k)} in nested form become \\boldsymbol{\\theta}_t in flattened form.\n\nThis transformation is purely algebraic. No algorithmic behavior changes, only the presentation:\n\nFlattened Neural Fitted Q-Iteration\n\nInput: MDP (S, A, P, R, \\gamma), offline transition dataset \\mathcal{D}, neural network q(s,a; \\boldsymbol{\\theta}), learning rate \\alpha, target update frequency K, initialization \\boldsymbol{\\theta}_0\n\nOutput: Parameters \\boldsymbol{\\theta} for Q-function approximation\n\nInitialize \\boldsymbol{\\theta}_0 randomly\n\n\\boldsymbol{\\theta}_{\\text{target}} \\leftarrow \\boldsymbol{\\theta}_0\n\nt \\leftarrow 0 \\quad // Single flattened loop counter\n\nwhile training do\n\n// Compute targets using fixed target network\n\n\\mathcal{D}_t^{\\text{fit}} \\leftarrow \\emptyset\n\nfor each (s,a,r,s') \\in \\mathcal{D} do\n\ny_{s,a} \\leftarrow r + \\gamma \\max_{a' \\in A} q(s',a'; \\boldsymbol{\\theta}_{\\text{target}})\n\n\\mathcal{D}_t^{\\text{fit}} \\leftarrow \\mathcal{D}_t^{\\text{fit}} \\cup \\{((s,a), y_{s,a})\\}\n\n// Gradient step on online network\n\n\\boldsymbol{\\theta}_{t+1} \\leftarrow \\boldsymbol{\\theta}_t - \\alpha \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}(\\boldsymbol{\\theta}_t; \\mathcal{D}_t^{\\text{fit}})\n\n// Periodic target network update (marks outer iteration boundary)\n\nif t \\bmod K = 0 then\n\n\\boldsymbol{\\theta}_{\\text{target}} \\leftarrow \\boldsymbol{\\theta}_t\n\nt \\leftarrow t + 1\n\nreturn \\boldsymbol{\\theta}_t\n\nAt step t, we have n = \\lfloor t/K \\rfloor (outer iteration) and k = t \\bmod K (position within inner loop). The target network \\boldsymbol{\\theta}_{\\text{target}} equals \\boldsymbol{\\theta}_n throughout the K steps from t = nK to t = (n+1)K - 1, then gets updated to \\boldsymbol{\\theta}_{n+1} at t = (n+1)K. The parameters \\boldsymbol{\\theta}_t correspond to \\boldsymbol{\\theta}_n^{(k)} in the nested form. The flattening reindexes the iteration structure: with K=10, outer iteration n=3, inner step k=7 becomes global step t = 3 \\cdot 10 + 7 = 37.\n\nFlattening replaces the pair (n,k) by a single global step index t, but the underlying empirical distribution \\hat{P}_{\\mathcal{D}} remains the same. We still sample from the fixed offline dataset throughout.\n\nThe target network arises directly from flattening the nested FQI structure. When DQN is presented with a target network that updates every K steps, this is approximate value iteration in flattened form. The algorithm still performs outer loop (value iteration) and inner loop (regression), but the presentation obscures this structure. The periodic target updates mark the boundaries between outer iterations. DQN is batch approximate DP in flattened form, using online data collection with a replay buffer.","type":"content","url":"/fqi#flattening-into-a-single-loop","position":15},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl3":"Smooth Target Updates via Exponential Moving Average","lvl2":"From Nested to Flattened Q-Iteration"},"type":"lvl3","url":"/fqi#smooth-target-updates-via-exponential-moving-average","position":16},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl3":"Smooth Target Updates via Exponential Moving Average","lvl2":"From Nested to Flattened Q-Iteration"},"content":"An alternative to periodic hard updates is exponential moving average (EMA) (also called Polyak averaging), which updates the target network smoothly at every step rather than synchronizing it every K steps:\n\nFlattened Neural Fitted Q-Iteration with EMA Target Updates\n\nInput: MDP (S, A, P, R, \\gamma), offline transition dataset \\mathcal{D}, neural network q(s,a; \\boldsymbol{\\theta}), learning rate \\alpha, EMA rate \\tau \\in (0, 1), initialization \\boldsymbol{\\theta}_0\n\nOutput: Parameters \\boldsymbol{\\theta} for Q-function approximation\n\nInitialize \\boldsymbol{\\theta}_0 randomly\n\n\\boldsymbol{\\theta}_{\\text{target}} \\leftarrow \\boldsymbol{\\theta}_0\n\nt \\leftarrow 0 \\quad // Single flattened loop counter\n\nwhile training do\n\n// Compute targets using fixed target network\n\n\\mathcal{D}_t^{\\text{fit}} \\leftarrow \\emptyset\n\nfor each (s,a,r,s') \\in \\mathcal{D} do\n\ny_{s,a} \\leftarrow r + \\gamma \\max_{a' \\in A} q(s',a'; \\boldsymbol{\\theta}_{\\text{target}})\n\n\\mathcal{D}_t^{\\text{fit}} \\leftarrow \\mathcal{D}_t^{\\text{fit}} \\cup \\{((s,a), y_{s,a})\\}\n\n// Gradient step on online network\n\n\\boldsymbol{\\theta}_{t+1} \\leftarrow \\boldsymbol{\\theta}_t - \\alpha \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}(\\boldsymbol{\\theta}_t; \\mathcal{D}_t^{\\text{fit}})\n\n// ← CHANGED: Smooth EMA update at every step\n\n\\boldsymbol{\\theta}_{\\text{target}} \\leftarrow \\tau\\boldsymbol{\\theta}_{t+1} + (1-\\tau)\\boldsymbol{\\theta}_{\\text{target}}\n\nt \\leftarrow t + 1\n\nreturn \\boldsymbol{\\theta}_t\n\nWith EMA updates, the target network slowly tracks the online network instead of making discrete jumps. For small \\tau (typically \\tau \\in [0.001, 0.01]), the target lags behind the online network by roughly 1/\\tau steps. This provides smoother learning dynamics and avoids the discontinuous changes in targets that occur with periodic hard updates. The EMA approach became popular with DDPG \n\nLillicrap et al. (2015) for continuous control and is now standard in algorithms like TD3 \n\nFujimoto et al. (2018) and SAC \n\nHaarnoja et al. (2018).","type":"content","url":"/fqi#smooth-target-updates-via-exponential-moving-average","position":17},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl2":"Online Algorithms: DQN and Extensions"},"type":"lvl2","url":"/fqi#online-algorithms-dqn-and-extensions","position":18},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl2":"Online Algorithms: DQN and Extensions"},"content":"We now keep the same fitted Q-iteration template but allow the buffer \\mathcal{B}_t and its empirical distribution \\hat{P}_{\\mathcal{B}_t} to evolve while we learn. Instead of repeatedly sampling from a fixed \\hat{P}_{\\mathcal{D}}, we collect new transitions during learning and store them in a circular replay buffer.\n\nNote that “online” here takes on a second meaning: we collect data online (interacting with the environment) while also maintaining the online network (actively-updated parameters \\boldsymbol{\\theta}_t) and target network (frozen parameters \\boldsymbol{\\theta}_{\\text{target}}) distinction from the flattened FQI structure. The online network now plays a dual role: it is both the parameters being trained and the policy used to collect new data for the buffer.\n\nDeep Q-Network (DQN) instantiates the online template with moderate choices along the design axes: buffer capacity B \\approx 10^6, mini-batch size b \\approx 32, and target network update frequency K \\approx 10^4. DQN is not an ad hoc collection of tricks. It is fitted Q-iteration in flattened form (as developed in the nested-to-flattened transformation earlier) with an evolving buffer \\mathcal{B}_t and low replay ratio.","type":"content","url":"/fqi#online-algorithms-dqn-and-extensions","position":19},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl3":"Deep Q-Network (DQN)","lvl2":"Online Algorithms: DQN and Extensions"},"type":"lvl3","url":"/fqi#deep-q-network-dqn","position":20},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl3":"Deep Q-Network (DQN)","lvl2":"Online Algorithms: DQN and Extensions"},"content":"Deep Q-Network (DQN) maintains a circular replay buffer of capacity B (typically B \\approx 10^6). At each environment step, we store the new transition and sample a mini-batch of size b from the buffer for training. This increases the replay ratio, reducing gradient variance at the cost of older, potentially off-policy data.\n\nDQN uses two copies of the Q-network parameters:\n\nThe online network \\boldsymbol{\\theta}_t: actively updated at each gradient step (corresponds to \\boldsymbol{\\theta}_n^{(k)} in the nested view). It serves a dual role: (1) the policy for collecting new data via \\varepsilon-greedy action selection, and (2) the parameters being trained.\n\nThe target network \\boldsymbol{\\theta}_{\\text{target}}: frozen parameters updated every K steps (corresponds to \\boldsymbol{\\theta}_n in the nested view). Used only for computing Bellman targets.\n\nAs shown in the nested-to-flattened section, the target network is not a stabilization trick but the natural consequence of periodic outer-iteration boundaries in flattened FQI. The target network keeps targets fixed for K gradient steps, which corresponds to the inner loop in the nested view:\n\nDeep Q-Network (DQN)\n\nInput: MDP (S, A, P, R, \\gamma), neural network q(s,a; \\boldsymbol{\\theta}), learning rate \\alpha, target update frequency K, replay buffer capacity B, mini-batch size b, exploration rate \\varepsilon\n\nOutput: Parameters \\boldsymbol{\\theta} for Q-function approximation\n\nInitialize \\boldsymbol{\\theta}_0 randomly\n\n\\boldsymbol{\\theta}_{\\text{target}} \\leftarrow \\boldsymbol{\\theta}_0\n\nInitialize replay buffer \\mathcal{B} with capacity B\n\nt \\leftarrow 0\n\nwhile training do\n\nObserve current state s\n\n// Use online network to collect data\n\nSelect action: a \\leftarrow \\begin{cases} \\arg\\max_{a'} q(s,a';\\boldsymbol{\\theta}_t) & \\text{with probability } 1-\\varepsilon \\\\ \\text{random action} & \\text{with probability } \\varepsilon \\end{cases}\n\nExecute a, observe reward r and next state s'\n\nStore (s,a,r,s') in \\mathcal{B}, replacing oldest if full\n\nSample mini-batch of b transitions \\{(s_i,a_i,r_i,s_i')\\}_{i=1}^b from \\mathcal{B}\n\nFor each sampled transition (s_i,a_i,r_i,s_i'):\n\ny_i \\leftarrow r_i + \\gamma \\color{blue}{\\max_{a' \\in A} q(s_i',a'; \\boldsymbol{\\theta}_{\\text{target}})} (both selection AND evaluation with target network)\n\n\\boldsymbol{\\theta}_{t+1} \\leftarrow \\boldsymbol{\\theta}_t - \\alpha \\nabla_{\\boldsymbol{\\theta}} \\frac{1}{b}\\sum_{i=1}^b (q(s_i,a_i;\\boldsymbol{\\theta}_t) - y_i)^2\n\nif t \\bmod K = 0 then\n\n\\boldsymbol{\\theta}_{\\text{target}} \\leftarrow \\boldsymbol{\\theta}_t\n\nt \\leftarrow t + 1\n\nreturn \\boldsymbol{\\theta}_t","type":"content","url":"/fqi#deep-q-network-dqn","position":21},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl3":"Double Deep Q-Network (Double DQN)","lvl2":"Online Algorithms: DQN and Extensions"},"type":"lvl3","url":"/fqi#double-deep-q-network-double-dqn","position":22},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl3":"Double Deep Q-Network (Double DQN)","lvl2":"Online Algorithms: DQN and Extensions"},"content":"Double DQN addresses overestimation bias by decoupling action selection from evaluation. Instead of using the target network for both purposes (as DQN does), Double DQN uses the online network (currently-training parameters \\boldsymbol{\\theta}_t) to select which action looks best, then uses the target network (frozen parameters \\boldsymbol{\\theta}_{\\text{target}}) to evaluate that action:\n\nDouble Deep Q-Network (Double DQN)\n\nInput: MDP (S, A, P, R, \\gamma), neural network q(s,a; \\boldsymbol{\\theta}), learning rate \\alpha, target update frequency K, replay buffer capacity B, mini-batch size b, exploration rate \\varepsilon\n\nOutput: Parameters \\boldsymbol{\\theta} for Q-function approximation\n\nInitialize \\boldsymbol{\\theta}_0 randomly\n\n\\boldsymbol{\\theta}_{\\text{target}} \\leftarrow \\boldsymbol{\\theta}_0\n\nInitialize replay buffer \\mathcal{B} with capacity B\n\nt \\leftarrow 0\n\nwhile training do\n\nObserve current state s\n\n// Use online network to collect data\n\nSelect action: a \\leftarrow \\begin{cases} \\arg\\max_{a'} q(s,a';\\boldsymbol{\\theta}_t) & \\text{with probability } 1-\\varepsilon \\\\ \\text{random action} & \\text{with probability } \\varepsilon \\end{cases}\n\nExecute a, observe reward r and next state s'\n\nStore (s,a,r,s') in \\mathcal{B}, replacing oldest if full\n\nSample mini-batch of b transitions \\{(s_i,a_i,r_i,s_i')\\}_{i=1}^b from \\mathcal{B}\n\nFor each sampled transition (s_i,a_i,r_i,s_i'):\n\na^*_i \\leftarrow \\color{red}{\\arg\\max_{a' \\in A} q(s_i',a'; \\boldsymbol{\\theta}_t)} (action selection with online network)\n\ny_i \\leftarrow r_i + \\gamma \\color{green}{q(s_i',a^*_i; \\boldsymbol{\\theta}_{\\text{target}})} (action evaluation with target network)\n\n\\boldsymbol{\\theta}_{t+1} \\leftarrow \\boldsymbol{\\theta}_t - \\alpha \\nabla_{\\boldsymbol{\\theta}} \\frac{1}{b}\\sum_{i=1}^b (q(s_i,a_i;\\boldsymbol{\\theta}_t) - y_i)^2\n\nif t \\bmod K = 0 then\n\n\\boldsymbol{\\theta}_{\\text{target}} \\leftarrow \\boldsymbol{\\theta}_t\n\nt \\leftarrow t + 1\n\nreturn \\boldsymbol{\\theta}_t\n\nDouble DQN leaves \\mathcal{B}_t and \\hat{P}_{\\mathcal{B}_t} unchanged and modifies only the target function g by decoupling action selection (online network) from evaluation (target network). Compare the two approaches:\n\nDQN (blue): Uses the target network \\boldsymbol{\\theta}_{\\text{target}} for BOTH selecting which action is best AND evaluating it: \\max_{a'} q(s',a'; \\boldsymbol{\\theta}_{\\text{target}}) implicitly finds a^* = \\arg\\max_{a'} q(s',a'; \\boldsymbol{\\theta}_{\\text{target}}) and then evaluates q(s',a^*; \\boldsymbol{\\theta}_{\\text{target}})\n\nDouble DQN (red + green): Uses the online network \\boldsymbol{\\theta}_t to select a^* = \\arg\\max_{a'} q(s',a'; \\boldsymbol{\\theta}_t), then uses the target network \\boldsymbol{\\theta}_{\\text{target}} to evaluate q(s',a^*; \\boldsymbol{\\theta}_{\\text{target}})\n\nThis decoupling prevents overestimation bias because the noise in action selection (from the online network) is independent of the noise in action evaluation (from the target network). Recall that the online network represents the currently-training parameters being updated at each gradient step, while the target network represents the frozen parameters used for computing targets. By using different parameters for selection versus evaluation, we break the correlation that leads to systematic overestimation in the standard max operator.","type":"content","url":"/fqi#double-deep-q-network-double-dqn","position":23},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl3":"Q-Learning: The Limiting Case","lvl2":"Online Algorithms: DQN and Extensions"},"type":"lvl3","url":"/fqi#q-learning-the-limiting-case","position":24},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl3":"Q-Learning: The Limiting Case","lvl2":"Online Algorithms: DQN and Extensions"},"content":"DQN and Double DQN use moderate settings: B \\approx 10^6, K \\approx 10^4, mini-batch size b \\approx 32. We can ask: what happens at the extreme where we minimize buffer capacity, eliminate replay, and update targets every step? This limiting case gives classical Q-learning \n\nWatkins (1989): buffer capacity B=1, target network frequency K=1, and mini-batch size b=1.\n\nIn the buffer perspective, Q-learning has \\mathcal{B}_t = \\{(s_t, a_t, r_t, s'_t)\\} with |\\mathcal{B}_t | = 1. The empirical distribution \\hat{P}_{\\mathcal{B}_t} collapses to a Dirac mass at the current transition. We use each transition exactly once then discard it. There is no separate target network: the parameters used for computing targets are immediately updated after each step (K=1, or equivalently \\tau=1 in EMA). This makes Q-learning a stochastic approximation method with replay ratio 1.\n\nStochastic approximation is a general framework for solving equations of the form \\mathbb{E}[h(X; \\boldsymbol{\\theta})] = 0 using noisy samples, without computing expectations explicitly. The classic example is root-finding: given function f(\\boldsymbol{\\theta}; Z) whose expectation \\mathbb{E}[f(\\boldsymbol{\\theta}; Z)] = F(\\boldsymbol{\\theta}) we want to solve F(\\boldsymbol{\\theta}) = 0, the Robbins-Monro procedure updates:\\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_t - \\alpha_t f(\\boldsymbol{\\theta}_t; Z_t)\n\nusing noisy samples Z_t without ever computing F(\\boldsymbol{\\theta}) or its Jacobian. This is analogous to Newton’s method in the deterministic case, but replaces exact gradients with stochastic estimates and avoids computing or inverting the Jacobian. Under diminishing step sizes (\\alpha_t \\to 0, \\sum_t \\alpha_t = \\infty), the iterates converge to solutions of F(\\boldsymbol{\\theta}) = 0.\n\nQ-learning fits this framework by solving the Bellman residual equation. Recall from the \n\nprojection methods chapter that the Bellman equation q^* = \\Bellman q^* can be written as a residual equation \\Residual(q) \\equiv \\Bellman q - q = 0. For a parameterized Q-function q(s,a; \\boldsymbol{\\theta}), the residual at observed transition (s,a,r,s') is:R(s,a,r,s'; \\boldsymbol{\\theta}) = r + \\gamma \\max_{a'} q(s',a'; \\boldsymbol{\\theta}) - q(s,a; \\boldsymbol{\\theta})\n\nThis is the TD error. Q-learning is a stochastic approximation method for solving \\mathbb{E}_{\\tau \\sim P}[R(\\tau; \\boldsymbol{\\theta})] = 0 where P is the distribution of transitions under the behavior policy. Each observed transition provides a noisy sample of the residual, and the algorithm updates parameters using the gradient of the squared residual without computing expectations explicitly.\n\nThe algorithm works with any function approximator that supports incremental updates. The general form applies a gradient descent step to minimize the squared TD error. The only difference between linear and nonlinear cases is how the gradient \\nabla_{\\boldsymbol{\\theta}} q(s,a; \\boldsymbol{\\theta}) is computed.\n\nQ-Learning\n\nInput: MDP (S, A, P, R, \\gamma), function approximator q(s,a; \\boldsymbol{\\theta}), learning rate \\alpha, exploration rate \\varepsilon\n\nOutput: Parameters \\boldsymbol{\\theta} for Q-function approximation\n\nInitialize \\boldsymbol{\\theta}_0\n\nt \\leftarrow 0\n\nwhile training do\n\nObserve current state s\n\nSelect action: a \\leftarrow \\begin{cases} \\arg\\max_{a' \\in A} q(s,a';\\boldsymbol{\\theta}_t) & \\text{with probability } 1-\\varepsilon \\\\ \\text{random action} & \\text{with probability } \\varepsilon \\end{cases}\n\nExecute a, observe reward r and next state s'\n\nCompute TD target: y \\leftarrow r + \\gamma \\max_{a' \\in A} q(s',a'; \\boldsymbol{\\theta}_t)\n\nUpdate: \\boldsymbol{\\theta}_{t+1} \\leftarrow \\boldsymbol{\\theta}_t - \\alpha \\nabla_{\\boldsymbol{\\theta}} (q(s,a;\\boldsymbol{\\theta}_t) - y)^2\n\nt \\leftarrow t + 1\n\nreturn \\boldsymbol{\\theta}_t\n\nThe gradient in line 5 depends on the choice of function approximator:\n\nTabular: Uses one-hot features \\boldsymbol{\\phi}(s,a) = \\boldsymbol{e}_{(s,a)}, giving table-lookup updates Q(s,a) \\leftarrow Q(s,a) + \\alpha(r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)). Converges under standard stochastic approximation conditions (detailed below).\n\nLinear: q(s,a; \\boldsymbol{\\theta}) = \\boldsymbol{\\theta}^\\top \\boldsymbol{\\phi}(s,a) where \\boldsymbol{\\phi}: \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}^d is a feature map. The gradient is \\nabla_{\\boldsymbol{\\theta}} q(s,a; \\boldsymbol{\\theta}) = \\boldsymbol{\\phi}(s,a), giving the update \\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_t + \\alpha (y - \\boldsymbol{\\theta}_t^\\top \\boldsymbol{\\phi}(s,a)) \\boldsymbol{\\phi}(s,a). Convergence is not guaranteed in general; the max operator combined with function approximation can cause instability.\n\nNonlinear: q(s,a; \\boldsymbol{\\theta}) is a neural network with weights \\boldsymbol{\\theta}. The gradient \\nabla_{\\boldsymbol{\\theta}} q(s,a; \\boldsymbol{\\theta}) is computed via backpropagation. Convergence guarantees do not exist.","type":"content","url":"/fqi#q-learning-the-limiting-case","position":25},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl3":"Convergence Analysis via the ODE Method","lvl2":"Online Algorithms: DQN and Extensions"},"type":"lvl3","url":"/fqi#convergence-analysis-via-the-ode-method","position":26},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl3":"Convergence Analysis via the ODE Method","lvl2":"Online Algorithms: DQN and Extensions"},"content":"Tabular Q-learning converges under diminishing step sizes (\\alpha_t \\to 0, \\sum_t \\alpha_t = \\infty) and sufficient exploration \n\nWatkins & Dayan (1992)\n\nJaakkola et al. (1994)\n\nTsitsiklis (1994). The standard proof technique is the ordinary differential equation (ODE) method \n\nKushner & Yin (2003)\n\nBorkar & Meyn (2000), which analyzes stochastic approximation algorithms by studying an associated deterministic dynamical system.\n\nThe ODE method connects stochastic approximation to deterministic dynamics through a two-step argument. First, we show that as step sizes shrink, the discrete stochastic recursion \\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_t + \\alpha_t h(\\boldsymbol{\\theta}_t, Z_t) tracks the continuous deterministic ODE \\frac{d\\boldsymbol{\\theta}}{dt} = \\bar{h}(\\boldsymbol{\\theta}) where \\bar{h}(\\boldsymbol{\\theta}) = \\mathbb{E}[h(\\boldsymbol{\\theta}, Z)] is the expected update direction. Second, we prove convergence of the ODE using Lyapunov functions or other dynamical systems tools. If the ODE converges to an equilibrium, the stochastic algorithm converges to the same point.\n\nFor tabular Q-learning, the update at state-action pair (s,a) is:Q_{t+1}(s,a) = Q_t(s,a) + \\alpha_t \\big[ r + \\gamma \\max_{a'} Q_t(s',a') - Q_t(s,a) \\big]\n\nwhere (s,a,r,s') is sampled from the transition distribution P induced by the behavior policy. The expected update direction is:\\bar{h}(Q)(s,a) = \\mathbb{E}_{(s,a,r,s') \\sim P} \\big[ r + \\gamma \\max_{a'} Q(s',a') - Q(s,a) \\mid (s,a) \\text{ visited} \\big]\n\nUnder sufficient exploration (all state-action pairs visited infinitely often), this expectation is well-defined. Let \\xi(s,a) denote the stationary distribution over state-action pairs under the behavior policy. The ODE becomes:\\frac{dQ(s,a)}{dt} = \\sum_{s'} p(s'|s,a) \\big[r(s,a,s') + \\gamma \\max_{a'} Q(s',a') \\big] - Q(s,a)\n\nThis is a deterministic dynamical system. The fixed points satisfy Q(s,a) = \\mathbb{E}_{s'}[r(s,a,s') + \\gamma \\max_{a'} Q(s',a')], which is the Bellman optimality equation. Convergence is established by showing this ODE has a global Lyapunov function: the distance \\|Q - Q^*\\| to the optimal Q-function decreases along trajectories. The Bellman operator is a contraction, which ensures the ODE trajectories converge to Q^*.\n\nThe interaction between the stationary distribution \\xi and the empirical distribution \\hat{P}_{\\mathcal{B}_t} is subtle. In pure stochastic approximation (Q-learning), we have \\mathcal{B}_t = \\{(s_t, a_t, r_t, s'_t)\\} with a single transition. Over time, as we explore, the sequence of visited state-action pairs (s_t, a_t) follows the behavior policy, and under ergodicity, the empirical frequency with which we update each (s,a) converges to the stationary distribution \\xi(s,a). The ODE method formalizes this: the expected update \\bar{h}(Q) averages over P, which implicitly weights by how often we visit each (s,a) under the stationary distribution.\n\nFor linear Q-learning with function approximation, the ODE analysis becomes more complex. The \n\nprojection methods chapter shows that non-monotone projections (like linear least squares) can fail to preserve contraction properties. The max operator in Q-learning creates additional complications that prevent general convergence guarantees, even though the algorithm may work in practice for well-chosen features.","type":"content","url":"/fqi#convergence-analysis-via-the-ode-method","position":27},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl2":"Regression Losses and Noise Models"},"type":"lvl2","url":"/fqi#regression-losses-and-noise-models","position":28},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl2":"Regression Losses and Noise Models"},"content":"Fix a particular time t and buffer contents \\mathcal{B}_t. Sampling from \\mathcal{B}_t and pushing transitions through the target function g(\\cdot; \\boldsymbol{\\theta}_t) gives a regression distribution \\hat{P}_t^{\\text{fit}} over pairs ((s,a), y). The fit operation in the inner loop is then a standard statistical estimation problem: given empirical samples from \\hat{P}_t^{\\text{fit}}, choose parameters \\boldsymbol{\\theta} to minimize a loss:\\mathcal{L}(\\boldsymbol{\\theta}; \\mathcal{B}_t) \\approx \\mathbb{E}_{((s,a),y)\\sim \\hat{P}_t^{\\text{fit}}} \\big[\\ell(q(s,a;\\boldsymbol{\\theta}), y)\\big]\n\nThe choice of loss \\ell implicitly specifies a noise model for the targets y under \\hat{P}_t^{\\text{fit}}. Squared error corresponds to Gaussian noise, absolute error to Laplace noise, Gumbel regression to extreme-value noise, and classification losses to non-parametric noise models over value bins. Because Bellman targets are noisy, biased, and bootstrapped, this choice has a direct impact on how the algorithm interprets the empirical distribution \\hat{P}_t^{\\text{fit}}.\n\nThis section examines alternative loss functions for the regression step. The standard approach uses squared error, but the noise in Bellman targets has special structure due to the max operator and bootstrapping. Two strategies have shown empirical success: Gumbel regression, which uses the proper likelihood for extreme-value noise, and classification-based methods, which avoid parametric noise assumptions by working with distributions over value bins.","type":"content","url":"/fqi#regression-losses-and-noise-models","position":29},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl3":"Gumbel Regression","lvl2":"Regression Losses and Noise Models"},"type":"lvl3","url":"/fqi#gumbel-regression","position":30},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl3":"Gumbel Regression","lvl2":"Regression Losses and Noise Models"},"content":"Extreme value theory tells us that the maximum of Gaussian errors has Gumbel-distributed tails. If we take this distribution seriously, maximum likelihood estimation should use a Gumbel likelihood rather than a Gaussian one. Garg, Tang, Kahn, and Levine \n\nGarg et al. (2023) developed this idea in Extreme Q-Learning (XQL). Instead of modeling the Bellman error as additive Gaussian noise:y_i = q^*(s_i, a_i) + \\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\n\nthey model it as Gumbel noise:y_i = q^*(s_i, a_i) + \\varepsilon_i, \\quad \\varepsilon_i \\sim -\\text{Gumbel}(0, \\beta)\n\nThe negative Gumbel distribution arises because we are modeling errors in targets that overestimate the true value. The corresponding maximum likelihood loss is Gumbel regression:\\mathcal{L}_{\\text{Gumbel}}(\\boldsymbol{\\theta}) = \\sum_i \\left[\\frac{q(s_i, a_i; \\boldsymbol{\\theta}) - y_i}{\\beta} + \\exp\\left(\\frac{q(s_i, a_i; \\boldsymbol{\\theta}) - y_i}{\\beta}\\right)\\right]\n\nThe temperature parameter \\beta controls the heaviness of the tail. The score function (gradient with respect to q) is:\\frac{\\partial \\mathcal{L}_{\\text{Gumbel}}}{\\partial q} = \\frac{1}{\\beta}\\left[1 + \\exp\\left(\\frac{q - y}{\\beta}\\right)\\right]\n\nWhen q < y (underestimation), the exponential term is small and the gradient is mild. When q > y (overestimation), the gradient grows exponentially with the error. This asymmetry deliberately penalizes overestimation more heavily than underestimation. When targets are systematically biased upward due to the max operator, this loss geometry pushes the estimates toward conservative Q-values.\n\nThe Gumbel loss can be understood as the natural likelihood for problems involving max operators, just as the Gaussian is the natural likelihood for problems involving averages. The central limit theorem tells us that sums converge to Gaussians; extreme value theory tells us that maxima converge to Gumbel (for light-tailed base distributions). Squared error is optimal for Gaussian noise; Gumbel regression is optimal for Gumbel noise.\n\n#  label: fig-gumbel-loss\n#  caption: Gumbel regression reshapes the loss landscape (left) and gradient geometry (right), penalizing overestimation exponentially while keeping underestimation gentle.\n\n%config InlineBackend.figure_format = 'retina'\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Apply book style\ntry:\n    import scienceplots\n    plt.style.use(['science', 'notebook'])\nexcept (ImportError, OSError):\n    pass  # Use matplotlib defaults\n\n# Set up the figure with two subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n# Define error range (q - y)\nerror = np.linspace(-3, 3, 500)\n\n# Different beta values to show control over asymmetry\nbetas = [0.5, 1.0, 2.0]\ncolors = ['#1f77b4', '#ff7f0e', '#2ca02c']\nlabels = [r'$\\beta = 0.5$ (aggressive)', r'$\\beta = 1.0$ (moderate)', r'$\\beta = 2.0$ (mild)']\n\n# Plot 1: Gumbel loss function\nfor beta, color, label in zip(betas, colors, labels):\n    loss = error / beta + np.exp(error / beta)\n    ax1.plot(error, loss, color=color, linewidth=2, label=label)\n\n# Add reference: squared error for comparison\nsquared_error = error**2\nax1.plot(error, squared_error, 'k--', linewidth=1.5, alpha=0.5, label='Squared error')\n\nax1.axvline(x=0, color='gray', linestyle=':', alpha=0.5)\nax1.axhline(y=0, color='gray', linestyle=':', alpha=0.5)\nax1.set_xlabel(r'Error: $q - y$', fontsize=11)\nax1.set_ylabel('Loss', fontsize=11)\nax1.set_title('Gumbel Loss Function', fontsize=12, fontweight='bold')\nax1.legend(fontsize=9)\nax1.grid(True, alpha=0.3)\nax1.set_ylim([0, 10])\n\n# Annotate regions\nax1.text(-2, 7, 'Underestimation\\n' + r'$(q < y)$', \n         fontsize=9, ha='center', color='darkred', alpha=0.7)\nax1.text(2, 7, 'Overestimation\\n' + r'$(q > y)$', \n         fontsize=9, ha='center', color='darkblue', alpha=0.7)\n\n# Plot 2: Gradient (score function) - showing asymmetry\nfor beta, color, label in zip(betas, colors, labels):\n    gradient = (1/beta) * (1 + np.exp(error / beta))\n    ax2.plot(error, gradient, color=color, linewidth=2, label=label)\n\n# Add reference: gradient of squared error\nsquared_error_grad = 2 * error\nax2.plot(error, squared_error_grad, 'k--', linewidth=1.5, alpha=0.5, label='Squared error')\n\nax2.axvline(x=0, color='gray', linestyle=':', alpha=0.5)\nax2.axhline(y=0, color='gray', linestyle=':', alpha=0.5)\nax2.set_xlabel(r'Error: $q - y$', fontsize=11)\nax2.set_ylabel('Gradient magnitude', fontsize=11)\nax2.set_title('Gumbel Loss Gradient (Score Function)', fontsize=12, fontweight='bold')\nax2.legend(fontsize=9)\nax2.grid(True, alpha=0.3)\nax2.set_ylim([-5, 10])\n\n# Annotate asymmetry\nax2.text(-2, -3, 'Mild gradient\\n(tolerates underestimation)', \n         fontsize=9, ha='center', color='darkred', alpha=0.7)\nax2.text(2, 7, 'Steep gradient\\n(penalizes overestimation)', \n         fontsize=9, ha='center', color='darkblue', alpha=0.7)\n\nplt.tight_layout()\n\nThe left panel shows the Gumbel loss as a function of the error q - y. Notice the asymmetry: the loss grows exponentially for positive errors (overestimation) but increases only linearly for negative errors (underestimation). The parameter \\beta controls the degree of this asymmetry—smaller \\beta values create more aggressive penalization of overestimation.\n\nThe right panel shows the gradient (score function), which determines how strongly the optimizer pushes back against errors. For overestimation (q > y), the gradient grows exponentially, creating strong corrective pressure. For underestimation (q < y), the gradient remains relatively flat (approaching 1/\\beta). This is in stark contrast to squared error (dashed line), which treats over- and underestimation symmetrically. By tuning \\beta, we can calibrate how aggressively the loss combats the overestimation bias induced by the max operator in the Bellman target.\n\nXQL vs Soft Q-Learning: Target Function vs Loss\n\nXQL (Gumbel loss) and Soft Q-learning both involve Gumbel distributions, but they operate at different levels of the FQI template. Recall our unified framework: buffer \\mathcal{B}_t, target function g, loss \\ell, optimization budget K.\n\nSoft Q-learning changes the target function by using the smooth Bellman operator from \n\nregularized MDPs:g^{\\text{soft}}(s,a,r,s'; \\boldsymbol{\\theta}) = r + \\gamma \\frac{1}{\\beta}\\log\\sum_{a'} \\exp(\\beta q(s',a'; \\boldsymbol{\\theta}))\n\nThis replaces the hard max with logsumexp for entropy regularization. The loss remains standard: \\ell(q,y) = (q-y)^2. The Gumbel distribution appears through the Gumbel-max trick (logsumexp = soft max), but this is about making the optimal policy stochastic, not about the noise structure in TD errors.\n\nXQL keeps the hard max target function:g^{\\text{hard}}(s,a,r,s'; \\boldsymbol{\\theta}) = r + \\gamma \\max_{a'} q(s',a'; \\boldsymbol{\\theta})\n\nand instead changes the loss to Gumbel regression: \\ell_{\\text{Gumbel}}(q,y) = \\frac{q-y}{\\beta} + \\exp(\\frac{q-y}{\\beta}). The Gumbel distribution here models the noise in the TD errors themselves, not the policy.\n\nThe asymmetric penalization in XQL (exponential penalty for overestimation) comes from the score function of the Gumbel likelihood, which is designed to handle extreme-value noise. Soft Q-learning uses symmetric L2 loss, so it does not preferentially penalize overestimation. The logsumexp smoothing in Soft Q reduces maximization bias by averaging over actions, but this is a property of the target function, not the loss geometry.\n\nBoth can be combined: Soft Q-learning with Gumbel loss would change both the target function (logsumexp) and the loss (asymmetric penalization).\n\nThe practical advantage of XQL is that the loss function itself handles the asymmetric error structure through its score function. We do not need to estimate variances or compute weighted averages. XQL has shown improvements in both value-based and actor-critic methods, particularly in offline reinforcement learning where the max-operator bias compounds across iterations without corrective exploration.","type":"content","url":"/fqi#gumbel-regression","position":31},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl3":"Classification-Based Q-Learning","lvl2":"Regression Losses and Noise Models"},"type":"lvl3","url":"/fqi#classification-based-q-learning","position":32},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl3":"Classification-Based Q-Learning","lvl2":"Regression Losses and Noise Models"},"content":"From the buffer viewpoint, nothing changes upstream: we still sample transitions from \\hat{P}_{\\mathcal{B}_t} and apply the same target function g(\\cdot; \\boldsymbol{\\theta}_t). Classification-based Q-learning changes only the loss \\ell and target representation. Instead of regressing on a scalar y\\in\\mathbb{R} with L2, we represent values as categorical distributions over bins and use cross-entropy loss.\n\nChoose a finite grid z_1 < z_2 < \\cdots < z_K spanning plausible return values. The network outputs logits \\ell_{\\boldsymbol{\\theta}}(s,a) \\in \\mathbb{R}^K converted to probabilities:p_{\\boldsymbol{\\theta}}(k \\mid s,a) = \\frac{\\exp(\\ell_{\\boldsymbol{\\theta}}(s,a)_k)}{\\sum_{j=1}^K \\exp(\\ell_{\\boldsymbol{\\theta}}(s,a)_j)}, \\quad q(s,a; \\boldsymbol{\\theta}) = \\sum_{k=1}^K z_k \\, p_{\\boldsymbol{\\theta}}(k \\mid s,a)\n\nEach scalar TD target y_i is converted to a target distribution via the two-hot encoding. If y_i falls between bins z_j and z_{j+1}:q_j(y_i) = \\frac{z_{j+1} - y_i}{z_{j+1} - z_j}, \\quad q_{j+1}(y_i) = \\frac{y_i - z_j}{z_{j+1} - z_j}, \\quad q_k(y_i) = 0 \\text{ for } k \\notin \\{j, j+1\\}\n\nThis is barycentric interpolation: \\sum_k z_k q_k(y_i) = y_i recovers the scalar exactly, placing the two-hot encoding within the same framework as linear interpolation in the \n\ndynamic programming chapter (Algorithm \n\nAlgorithm 2).\n\n#  label: fig-two-hot\n#  caption: Two-hot encoding as barycentric interpolation: the left panel shows a single target spread over two bins, while the right panel compares encodings for multiple targets.\n\n%config InlineBackend.figure_format = 'retina'\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Apply book style\ntry:\n    import scienceplots\n    plt.style.use(['science', 'notebook'])\nexcept (ImportError, OSError):\n    pass  # Use matplotlib defaults\n\ndef two_hot_encode(y, z_bins):\n    \"\"\"Compute two-hot encoding weights for target value y on grid z_bins.\"\"\"\n    K = len(z_bins)\n    q = np.zeros(K)\n    \n    # Clip to grid boundaries\n    if y <= z_bins[0]:\n        q[0] = 1.0\n        return q, 0\n    if y >= z_bins[-1]:\n        q[-1] = 1.0\n        return q, K - 1\n    \n    # Find bins j, j+1 such that z_j <= y < z_{j+1}\n    j = np.searchsorted(z_bins, y, side='right') - 1\n    \n    # Barycentric coordinates (linear interpolation)\n    q[j+1] = (y - z_bins[j]) / (z_bins[j+1] - z_bins[j])\n    q[j] = (z_bins[j+1] - y) / (z_bins[j+1] - z_bins[j])\n    \n    return q, j\n\n# Set up the figure with two panels\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n# Define a grid of K=11 bins spanning [-5, 5]\nK = 11\nz_bins = np.linspace(-5, 5, K)\n\n# Panel 1: Two-hot encoding for a specific target\ny_target = 0.7\nq, j = two_hot_encode(y_target, z_bins)\n\n# Plot bins as vertical lines\nfor z in z_bins:\n    ax1.axvline(z, color='lightgray', linestyle='--', linewidth=0.8, alpha=0.5)\n\n# Plot the distribution\ncolors_bar = ['#E63946' if qi > 0 else '#A8DADC' for qi in q]\nax1.bar(z_bins, q, width=(z_bins[1] - z_bins[0]) * 0.8, \n        color=colors_bar, alpha=0.7, edgecolor='black', linewidth=1.5)\n\n# Highlight the target value\nax1.axvline(y_target, color='#2E86AB', linewidth=3, label=f'Target $y = {y_target}$', zorder=10)\nax1.plot(y_target, 0, 'o', color='#2E86AB', markersize=12, zorder=11)\n\n# Annotate the two active bins with barycentric coordinates\nax1.annotate(f'$q_{{{j}}} = {q[j]:.3f}$\\n$= \\\\frac{{z_{{{j+1}}} - y}}{{z_{{{j+1}}} - z_{{{j}}}}}$',\n            xy=(z_bins[j], q[j]), xytext=(z_bins[j] - 1.8, q[j] + 0.25),\n            fontsize=10, color='#E63946', weight='bold',\n            arrowprops=dict(arrowstyle='->', color='#E63946', lw=2))\nax1.annotate(f'$q_{{{j+1}}} = {q[j+1]:.3f}$\\n$= \\\\frac{{y - z_{{{j}}}}}{{z_{{{j+1}}} - z_{{{j}}}}}$',\n            xy=(z_bins[j+1], q[j+1]), xytext=(z_bins[j+1] + 0.5, q[j+1] + 0.25),\n            fontsize=10, color='#E63946', weight='bold',\n            arrowprops=dict(arrowstyle='->', color='#E63946', lw=2))\n\n# Verify barycentric property\nreconstruction = np.sum(z_bins * q)\ntextstr = f'Barycentric property:\\n$\\\\sum_k z_k q_k(y) = {reconstruction:.4f} = y$'\nax1.text(0.02, 0.97, textstr, transform=ax1.transAxes, fontsize=10,\n        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n\nax1.set_xlabel('Bin value $z_k$', fontsize=11)\nax1.set_ylabel('Probability $q_k(y)$', fontsize=11)\nax1.set_title('Two-Hot Encoding = Linear Interpolation', fontsize=12, fontweight='bold')\nax1.set_ylim([0, 1.2])\nax1.legend(loc='upper right', fontsize=10)\nax1.grid(axis='y', alpha=0.3)\n\n# Panel 2: Multiple targets to show the pattern\ntargets = [0.7, -2.3, 3.8]\ncolors = ['#2E86AB', '#A23B72', '#F18F01']\n\nfor idx, (y_target, color) in enumerate(zip(targets, colors)):\n    q, _ = two_hot_encode(y_target, z_bins)\n    offset = idx * 0.2\n    ax2.bar(z_bins + offset, q, width=0.18, color=color, alpha=0.7, \n           label=f'$y = {y_target}$', edgecolor='black', linewidth=0.8)\n\nfor z in z_bins:\n    ax2.axvline(z, color='lightgray', linestyle='--', linewidth=0.5, alpha=0.4)\n\nax2.set_xlabel('Bin value $z_k$', fontsize=11)\nax2.set_ylabel('Probability $q_k(y)$', fontsize=11)\nax2.set_title('Multiple Targets: Each Encoded as Two-Hot', fontsize=12, fontweight='bold')\nax2.legend(loc='upper right', fontsize=10)\nax2.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\n\nThe left panel shows two-hot encoding for y=0.7 on a grid of 11 bins. The target value is distributed over exactly two adjacent bins z_j and z_{j+1} with weights that are barycentric coordinates: the weight assigned to each bin is inversely proportional to the distance from the target to that bin. The inset verifies that \\sum_k z_k q_k(y) exactly recovers the original target—this is the same linear interpolation formula used throughout the book. The right panel shows that different targets produce different two-hot patterns, each concentrating mass on the two bins surrounding the target value.\n\nThe loss minimizes cross-entropy:\\mathcal{L}_{\\text{CE}}(\\boldsymbol{\\theta}) = -\\mathbb{E}_{((s,a),y) \\sim \\hat{P}_t^{\\text{fit}}}\\left[ \\sum_{k=1}^K q_k(y) \\log p_{\\boldsymbol{\\theta}}(k \\mid s, a) \\right]\n\nwhich projects the target distribution onto the predicted distribution in KL geometry on the simplex \\Delta^{K-1} rather than L2 on \\mathbb{R}. The gradient is \\nabla_{\\ell_\\theta} \\mathcal{L}_{\\text{CE}} = p_\\theta - q, bounded in magnitude regardless of target size.\n\nThis provides three sources of implicit robustness. First, gradient influence is bounded: each sample contributes O(1) gradient magnitude per bin, unlike L2 where error magnitude E contributes gradient proportional to E. Second, the finite grid [z_1, z_K] clips extreme targets to boundary bins, preventing outliers from dominating the regression scale. Third, the two-hot encoding spreads mass across neighboring bins, providing label smoothing that averages noisy targets at the same (s,a).\n\nThe two-hot weights q_j(y_i), q_{j+1}(y_i) are barycentric coordinates, identical to linear interpolation in the \n\ndynamic programming chapter (Algorithm \n\nAlgorithm 2). This places the encoding within Gordon’s monotone approximator framework (Definition \n\nDefinition 1): targets are convex combinations preserving order and boundedness. The neural network predicting p_{\\boldsymbol{\\theta}}(\\cdot \\mid s,a) is non-monotone, making classification-based Q-learning a hybrid: monotone target structure paired with flexible function approximation.\n\nEmpirically, cross-entropy loss scales better with network capacity. Farebrother et al. \n\nFarebrother et al. (2024) found that L2-based DQN and CQL degrade when Q-networks scale to large ResNets, while classification loss (specifically HL-Gauss, which uses Gaussian smoothing instead of two-hot) maintains performance. The combination of KL geometry, quantization, and smoothing prevents overfitting to noisy targets that plagues L2 with high-capacity networks.","type":"content","url":"/fqi#classification-based-q-learning","position":33},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl2":"Summary"},"type":"lvl2","url":"/fqi#summary","position":34},{"hierarchy":{"lvl1":"Fitted Q-Iteration Methods","lvl2":"Summary"},"content":"Fitted Q-iteration has a two-level structure: an outer loop applies the Bellman operator to construct targets, an inner loop fits a function approximator to those targets. All algorithms in this chapter instantiate this template with different choices of buffer \\mathcal{B}_t, target function g, loss \\ell, and optimization budget K.\n\nThe empirical distribution \\hat{P}_{\\mathcal{B}_t} unifies offline and online methods through plug-in approximation: replace unknown transition law P with \\hat{P}_{\\mathcal{B}_t} and minimize empirical risk \\mathbb{E}_{((s,a),y)\\sim \\hat{P}_t^{\\text{fit}}} [\\ell(q, y)]. Offline uses fixed \\mathcal{B}_t \\equiv \\mathcal{D} (sample average approximation), online uses circular buffer (Q-learning with B=1 is stochastic approximation, DQN with large B is hybrid).\n\nTarget networks and online networks arise from flattening the nested loops. Merging inner gradient steps with outer value iteration creates a single loop where two sets of parameters coexist: the online network \\boldsymbol{\\theta}_t (actively updated at each gradient step, corresponds to \\boldsymbol{\\theta}_n^{(k)}) and the target network \\boldsymbol{\\theta}_{\\text{target}} (frozen for computing targets, updated every K steps to mark outer-iteration boundaries, corresponds to \\boldsymbol{\\theta}_n). In online algorithms like DQN, the online network additionally serves as the behavior policy for data collection.\n\nThe \n\nnext chapter directly parameterizes and optimizes policies instead of searching over value functions.","type":"content","url":"/fqi#summary","position":35},{"hierarchy":{"lvl1":"Why This Book?"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Why This Book?"},"content":"Reinforcement learning offers a powerful framework for decision-making: systems that learn to act through interaction with their environment. From AlphaGo defeating world champions to Uber’s ride-matching system optimizing across hundreds of cities \n\nUber AI Labs, 2025, from chatbots engaging millions to ANYbotics’ quadruped robots performing complex industrial inspections with policies trained entirely in simulation \n\nANYbotics, 2023, RL has demonstrated remarkable capabilities.\n\nYet compared to supervised learning, which has become routine in industry, reinforcement learning has not achieved the same widespread adoption. Supervised learning benefits from standardized tools and well-defined interfaces: inputs, outputs, and objective metrics. Reinforcement learning, by contrast, requires explicit problem formulation: defining objectives, constraints, and how decisions unfold over time. This additional structure is also what makes RL applicable to a broader class of problems.\n\nAs \n\nIskhakov et al. (2020) notes, a primary challenge is “the difficulty of learning about the objective function and environment facing real-world decision-makers.” We cannot sidestep defining the problem, the objective, and the constraints.\n\nWorking in industry and consulting taught me what my PhD did not: real problems rarely fit neatly into predefined frameworks. Sensors produce noisy data; constraints are non-negotiable; objectives may shift or conflict. Most effort goes into formulating the decision problem, long before selecting an algorithm.\n\nThe chapters that follow address this challenge explicitly. They offer strategies to bridge the gap from theoretical RL formulations to practically useful systems. By carefully structuring decision problems, we can help reinforcement learning achieve broader impact.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Why This Book?","lvl2":"The Decision Problem"},"type":"lvl2","url":"/#the-decision-problem","position":2},{"hierarchy":{"lvl1":"Why This Book?","lvl2":"The Decision Problem"},"content":"The term reinforcement learning gets used in many ways. In the formal sense defined by \n\nSutton & Barto (2018), RL is a problem: learning to act through interaction with an environment. But in common usage, it can mean a family of algorithms, a research community, or a long-term scientific agenda.\n\nThis book takes a practical view: reinforcement learning as a vocabulary for reasoning about decisions under uncertainty. When optimization, feedback, and data intersect, we are in the territory of reinforcement learning, whether we use temporal-difference learning, model-based planning, or simple policy rules. What unifies these approaches is not a specific algorithm, but a shared structure: decision-making through experience.\n\nIn mainstream RL research, the problem is often treated as given. Sutton famously advises: “Approximate the solution, not the problem.” The agent should be shaped by experience, not by handcrafted structure. For Sutton, the problem is the world itself: complex, unknown, and handed to us as-is. We do not design it; we confront it.\n\nThis book takes a different stance. We consider a variety of decision problems, each with its own structure: finite or infinite horizon, discrete or continuous time, deterministic or stochastic dynamics. The problem is not handed to us; it must be defined. What are the goals? What decisions are available? What feedback is observable, and under what constraints?\n\nThis is what Operations Research has long emphasized. Through consulting, I saw it firsthand: production systems running overnight, MIP solvers optimizing decisions with XGBoost predictors as inputs, all live and working. In practice, these systems are doing reinforcement learning, just without calling it that. They define objectives, encode constraints, and optimize over time. The vocabulary differs, but the structure is the same.","type":"content","url":"/#the-decision-problem","position":3},{"hierarchy":{"lvl1":"Why This Book?","lvl2":"What This Book Offers"},"type":"lvl2","url":"/#what-this-book-offers","position":4},{"hierarchy":{"lvl1":"Why This Book?","lvl2":"What This Book Offers"},"content":"Reinforcement learning did not develop in isolation. Its foundations draw from control theory, dynamic programming, operations research, and economics. Many of the same ideas appear under different names in different communities, often with complementary perspectives.\n\nThis book aims to give you a broader view of that landscape. Where do RL algorithms come from? What mathematical structures underlie them? How do they connect to classical methods in optimization and control? Understanding these connections helps you see when a method applies, when it does not, and what alternatives exist.\n\nThe goal is not to survey every technique superficially. It is to go deep on the mathematical foundations that are shared across methods: dynamic programming, function approximation, optimization, and the interplay between them. These structures recur throughout sequential decision-making, whether in reinforcement learning, control theory, or operations research. Master them once, and you can recognize them in different guises.","type":"content","url":"/#what-this-book-offers","position":5},{"hierarchy":{"lvl1":"Why Build a Model? For Whom?"},"type":"lvl1","url":"/modeling","position":0},{"hierarchy":{"lvl1":"Why Build a Model? For Whom?"},"content":"","type":"content","url":"/modeling","position":1},{"hierarchy":{"lvl1":"Why Build a Model? For Whom?"},"type":"lvl1","url":"/modeling#why-build-a-model-for-whom","position":2},{"hierarchy":{"lvl1":"Why Build a Model? For Whom?"},"content":"“The sciences do not try to explain, they hardly even try to interpret, they mainly make models. By a model is meant a mathematical construct which, with the addition of certain verbal interpretations, describes observed phenomena. The justification of such a mathematical construct is solely and precisely that it is expected to work.”\n— John von Neumann\n\nThe word model means different things depending on who you ask.\n\nIn machine learning, it typically refers to a parameterized function (often a neural network) fit to data. When we say “we trained a model,” we usually mean adjusting parameters so it makes good predictions. But that’s a narrow view.\n\nIn control, operations research, or structural economics, a model refers more broadly to a formal specification of a decision problem. It includes how a system evolves over time, what parts of the world we choose to represent, what decisions are available, what can be observed or measured, and how outcomes are evaluated. It also encodes assumptions about time (discrete or continuous, finite or infinite horizon), uncertainty, and information structure.\n\nTo clarify terminology, I’ll use the term decision-making model to refer to this broader object: one that includes not just system dynamics, but also a specification of state, control, observations, objectives, time structure, and information assumptions. In this sense, the model defines the structure of the decision problem. It’s the formal scaffold on which we build optimization or learning procedures.\n\nDepending on the setting, we may ask different things from a decision-making model. Sometimes we want a model that supports counterfactual reasoning or policy evaluation, and are willing to bake in more assumptions to get there. Other times, we just need a model that supports prediction or simulation, even if it remains agnostic about internal mechanisms.\n\nThis mirrors a interesting distinction in econometrics between structural and reduced-form approaches. Structural models aim to capture the underlying process that generates behavior, enabling reasoning about what would happen under alternative policies or conditions. Reduced-form models, by contrast, focus on capturing statistical regularities (often to estimate causal effects) without necessarily modeling the mechanisms that generate them. Both are forms of modeling, just with different goals. The same applies in control and RL: some models are built to support simulation and optimization, while others serve more diagnostic or predictive roles, with fewer assumptions about how the system works internally.\n\nThis chapter steps back from algorithms to focus on the modeling side. What kinds of models do we need to support decision-making from data? What are their assumptions? What do they let us express or ignore? And how do they shape what learning and optimization can even mean?","type":"content","url":"/modeling#why-build-a-model-for-whom","position":3},{"hierarchy":{"lvl1":"Modeling, Realism, and Control"},"type":"lvl1","url":"/modeling#modeling-realism-and-control","position":4},{"hierarchy":{"lvl1":"Modeling, Realism, and Control"},"content":"Realism is only one way to assess a model. When the purpose of modeling is to support control or decision making, accuracy in reproducing every detail of the system is not always necessary. What matters more is whether the model leads to decisions that perform well when applied in practice. A model may simplify the physics, ignore some variables, or group complex interactions into a disturbance term. As long as it retains the core feedback structure relevant to the control task, it can still be effective.\n\nIn some cases, high-fidelity models can be counterproductive. Their complexity makes them harder to understand, slower to simulate, and more difficult to tune. Worse, they may include uncertain parameters that do not affect the control decisions but still influence the outcome of optimization. The resulting decisions can become fragile or overfitted to details that are not stable across different operating conditions.\n\nA useful model for control is one that focuses on the variables, dynamics, and constraints that shape the decisions to be made. It should capture the key trade-offs without trying to account for every effect. In traditional control design, this principle appears through model simplification: engineers reduce the system to a manageable form, then use feedback to absorb remaining uncertainty. Reinforcement learning adopts a similar mindset, though often implicitly. It allows for model error and evaluates success based on the quality of the policy when deployed, rather than on the accuracy of the model itself.","type":"content","url":"/modeling#modeling-realism-and-control","position":5},{"hierarchy":{"lvl1":"Modeling, Realism, and Control","lvl2":"Example: A simple model that supports better decisions"},"type":"lvl2","url":"/modeling#example-a-simple-model-that-supports-better-decisions","position":6},{"hierarchy":{"lvl1":"Modeling, Realism, and Control","lvl2":"Example: A simple model that supports better decisions"},"content":"Researchers at the U.S. National Renewable Energy Laboratory investigated how to reduce cooling costs in a typical home in Austin, Texas \n\nCole et al., 2014. They had access to a detailed EnergyPlus simulation of the building, which included thousands of internal variables: layered wall models, HVAC cycling behavior, occupancy schedules, and detailed weather inputs.\n\nAlthough this simulator could closely reproduce indoor temperatures, it was too slow and too complex to use as a planning tool. Instead, the researchers constructed a much simpler model using just two parameters: an effective thermal resistance and an effective thermal capacitance. This reduced model did not capture short-term temperature fluctuations and could be off by as much as two degrees on hot afternoons.\n\nDespite these inaccuracies, the simplified model proved useful for testing different cooling strategies. One such strategy involved cooling the house early in the morning when electricity prices were low, letting the temperature rise slowly during the expensive late-afternoon period, and reheating only slightly overnight. When this strategy was simulated in the full EnergyPlus model, it reduced peak compressor power by approximately 70 percent and lowered total cooling cost by about 60 percent compared to a standard thermostat schedule.\n\nThe reason this worked is that the simple model captured the most important structural feature of the system: the thermal mass of the building acts as a buffer that allows load shifting over time. That was enough to discover a control strategy that exploited this property. The many other effects present in the full simulation did not change the main conclusions and could be treated as part of the background variability.\n\nThis example shows that a model can be inaccurate in detail but still highly effective in guiding decisions. For control, what matters is not whether the model matches reality in every respect, but whether it helps identify actions that perform well under real-world conditions.","type":"content","url":"/modeling#example-a-simple-model-that-supports-better-decisions","position":7},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming"},"type":"lvl1","url":"/montecarlo","position":0},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming"},"content":"The projection methods from the previous chapter showed how to transform the infinite-dimensional fixed-point problem \\Bellman v = v into a finite-dimensional one by choosing basis functions \\{\\varphi_i\\} and imposing conditions that make the residual R(s) = \\Bellman v(s) - v(s) small. Different projection conditions (Galerkin orthogonality, collocation at points, least squares minimization) yield different finite-dimensional systems to solve.\n\nHowever, we left unresolved the question of how to evaluate the Bellman operator itself. Applying \\Bellman at any state requires computing an expectation:(\\Bellman v)(s) = \\max_{a \\in \\mathcal{A}_s} \\left\\{r(s,a) + \\gamma \\int v(s')p(ds'|s,a)\\right\\}\n\nFor discrete state spaces with a manageable number of states, this expectation is a finite sum we can compute exactly. For continuous or high-dimensional state spaces, we need numerical integration. The projection methods framework is compatible with any quadrature scheme, but leaves the choice of integration method unspecified.\n\nThis chapter addresses the integration subproblem that arises at two levels in approximate dynamic programming. First, we must evaluate the transition expectation \\int v(s')p(ds'|s,a) within the Bellman operator itself. Second, when using projection methods like Galerkin or least squares, we encounter outer integrations for enforcing orthogonality conditions or minimizing residuals over distributions of states. Both require numerical approximation in continuous or high-dimensional spaces.\n\nWe begin by examining deterministic numerical integration methods: quadrature rules that approximate integrals by evaluating integrands at carefully chosen points with associated weights. We discuss how to coordinate the choice of quadrature with the choice of basis functions to balance approximation accuracy and computational cost. Then we turn to Monte Carlo integration, which approximates expectations using random samples rather than deterministic quadrature points. This shift from deterministic to stochastic integration is what brings us into machine learning territory. When we replace exact transition probabilities with samples drawn from simulations or real interactions, projection methods combined with Monte Carlo integration become what the operations research community calls simulation-based approximate dynamic programming and what the machine learning community calls reinforcement learning. By relying on samples rather than explicit probability functions, we move from model-based planning to data-driven learning.","type":"content","url":"/montecarlo","position":1},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl2":"Evaluating the Bellman Operator with Numerical Quadrature"},"type":"lvl2","url":"/montecarlo#evaluating-the-bellman-operator-with-numerical-quadrature","position":2},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl2":"Evaluating the Bellman Operator with Numerical Quadrature"},"content":"Before turning to Monte Carlo methods, we examine the structure of the numerical integration problem. When we apply the Bellman operator to an approximate value function \\hat{v}(s; \\theta) = \\sum_i \\theta_i \\varphi_i(s), we must evaluate integrals of the form:\\int \\hat{v}(s'; \\theta) \\, p(s'|s,a) \\, ds' = \\int \\left(\\sum_i \\theta_i \\varphi_i(s')\\right) p(s'|s,a) \\, ds' = \\sum_i \\theta_i \\int \\varphi_i(s') \\, p(s'|s,a) \\, ds'\n\nThis shows two independent approximations:\n\nValue function approximation: We represent v using basis functions \\{\\varphi_i\\} and coefficients \\theta\n\nQuadrature approximation: We approximate each integral \\int \\varphi_i(s') p(s'|s,a) ds' numerically\n\nThese choices are independent but should be coordinated. To see why, consider what happens in projection methods when we iterate \\hat{v}^{(k+1)} = \\Proj \\Bellman \\hat{v}^{(k)}. In practice, we cannot evaluate \\Bellman exactly due to the integrals, so we compute \\hat{v}^{(k+1)} = \\Proj \\BellmanQuad \\hat{v}^{(k)} instead, where \\BellmanQuad denotes the Bellman operator with numerical quadrature.\n\nThe error in a single iteration can be bounded by the triangle inequality. Let v denote the true fixed point v = \\Bellman v. Then:\\begin{aligned}\n\\|v - \\hat{v}^{(k+1)}\\| &= \\|v - \\Proj \\BellmanQuad \\hat{v}^{(k)}\\| \\\\\n&\\le \\|v - \\Proj \\Bellman v\\| + \\|\\Proj \\Bellman v - \\Proj \\Bellman \\hat{v}^{(k)}\\| + \\|\\Proj \\Bellman \\hat{v}^{(k)} - \\Proj \\BellmanQuad \\hat{v}^{(k)}\\| \\\\\n&= \\underbrace{\\|v - \\Proj \\Bellman v\\|}_{\\text{best approximation error}} + \\underbrace{\\|\\Proj \\Bellman v - \\Proj \\Bellman \\hat{v}^{(k)}\\|}_{\\text{contraction of iterate error}} + \\underbrace{\\|\\Proj \\Bellman \\hat{v}^{(k)} - \\Proj \\BellmanQuad \\hat{v}^{(k)}\\|}_{\\text{quadrature error}}\n\\end{aligned}\n\nThe first term is the best we can do with our basis (how well \\Proj approximates the true solution). The second term decreases with iterations when \\Proj \\Bellman is a contraction. The third term is the error from replacing exact integrals with quadrature, and it does not vanish as iterations proceed.\n\nTo make this concrete, consider evaluating (\\Bellman \\hat{v})(s) for our current approximation \\hat{v}(s; \\theta) = \\sum_i \\theta_i \\varphi_i(s). We want:(\\Bellman \\hat{v})(s) = \\max_{a \\in \\mathcal{A}_s} \\left\\{r(s,a) + \\gamma \\sum_i \\theta_i \\int \\varphi_i(s') p(s'|s,a) ds' \\right\\}\n\nBut we compute instead:(\\BellmanQuad \\hat{v})(s) = \\max_{a \\in \\mathcal{A}_s} \\left\\{r(s,a) + \\gamma \\sum_i \\theta_i \\sum_j w_j \\varphi_i(s'_j) \\right\\}\n\nwhere \\{(s'_j, w_j)\\} are quadrature nodes and weights. If the quadrature error \\|(\\Bellman \\hat{v})(s) - (\\BellmanQuad \\hat{v})(s)\\| is large relative to the basis approximation quality, we cannot exploit the expressive power of the basis. For instance, degree-10 Chebyshev polynomials represent smooth functions to O(10^{-8}) accuracy, but combined with rectangle-rule quadrature (error O(h^2) \\approx 10^{-2}), the quadrature term dominates the error bound. We pay the cost of storing and manipulating 10 coefficients but achieve only O(h^2) convergence in the quadrature mesh size.\n\nThis echoes the coordination principle from continuous optimal control (Chapter on trajectory optimization): when transcribing a continuous-time problem, we use the same quadrature nodes for both the running cost integral and the dynamics integral. There, coordination ensures that “where we pay” aligns with “where we enforce” the dynamics. Here, coordination ensures that integration accuracy matches approximation accuracy. Both are instances of balancing multiple sources of error in numerical methods.\n\nStandard basis-quadrature pairings achieve this balance:\n\nPiecewise constant or linear elements with midpoint or trapezoidal rules\n\nChebyshev polynomials with Gauss-Chebyshev quadrature\n\nLegendre polynomials with Gauss-Legendre quadrature\n\nHermite polynomials with Gauss-Hermite quadrature (for Gaussian shocks)\n\nTo make this concrete, we examine what these pairings look like in practice for collocation and Galerkin projection.","type":"content","url":"/montecarlo#evaluating-the-bellman-operator-with-numerical-quadrature","position":3},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl3":"Orthogonal Collocation with Chebyshev Polynomials","lvl2":"Evaluating the Bellman Operator with Numerical Quadrature"},"type":"lvl3","url":"/montecarlo#orthogonal-collocation-with-chebyshev-polynomials","position":4},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl3":"Orthogonal Collocation with Chebyshev Polynomials","lvl2":"Evaluating the Bellman Operator with Numerical Quadrature"},"content":"Consider approximating the value function using Chebyshev polynomials of degree n-1:\\hat{v}(s; \\theta) = \\sum_{i=0}^{n-1} \\theta_i T_i(s)\n\nFor orthogonal collocation, we place collocation points at the zeros of T_n(s), denoted \\{s_j\\}_{j=1}^n. At each collocation point, we require the Bellman equation to hold exactly:\\hat{v}(s_j; \\theta) = \\max_{a \\in \\mathcal{A}} \\left\\{r(s_j,a) + \\gamma \\int \\hat{v}(s'; \\theta) p(ds'|s_j,a)\\right\\}\n\nThe integral on the right must be approximated using quadrature. With Chebyshev-Gauss quadrature using the same nodes \\{s_k\\}_{k=1}^n and weights \\{w_k\\}_{k=1}^n, this becomes:\\hat{v}(s_j; \\theta) = \\max_{a \\in \\mathcal{A}} \\left\\{r(s_j,a) + \\gamma \\sum_{k=1}^n w_k \\hat{v}(s_k; \\theta) p(s_k|s_j,a)\\right\\}\n\nSubstituting the basis representation \\hat{v}(s_k; \\theta) = \\sum_{i=0}^{n-1} \\theta_i T_i(s_k):\\sum_{i=0}^{n-1} \\theta_i T_i(s_j) = \\max_{a \\in \\mathcal{A}} \\left\\{r(s_j,a) + \\gamma \\sum_{k=1}^n w_k p(s_k|s_j,a) \\sum_{i=0}^{n-1} \\theta_i T_i(s_k)\\right\\}\n\nRearranging:\\sum_{i=0}^{n-1} \\theta_i T_i(s_j) = \\max_{a \\in \\mathcal{A}} \\left\\{r(s_j,a) + \\gamma \\sum_{i=0}^{n-1} \\theta_i \\underbrace{\\sum_{k=1}^n w_k T_i(s_k) p(s_k|s_j,a)}_{B_{ji}^a}\\right\\}\n\nThis yields a system of n nonlinear equations (one per collocation point):\\sum_{i=0}^{n-1} T_i(s_j) \\theta_i = \\max_{a \\in \\mathcal{A}} \\left\\{r(s_j,a) + \\gamma \\sum_{i=0}^{n-1} B_{ji}^a \\theta_i\\right\\}, \\quad j=1,\\ldots,n\n\nThe matrix elements B_{ji}^a can be precomputed once the quadrature nodes, weights, and transition probabilities are known. Solving this system gives the coefficient vector \\theta.","type":"content","url":"/montecarlo#orthogonal-collocation-with-chebyshev-polynomials","position":5},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl3":"Galerkin Projection with Hermite Polynomials","lvl2":"Evaluating the Bellman Operator with Numerical Quadrature"},"type":"lvl3","url":"/montecarlo#galerkin-projection-with-hermite-polynomials","position":6},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl3":"Galerkin Projection with Hermite Polynomials","lvl2":"Evaluating the Bellman Operator with Numerical Quadrature"},"content":"For a problem with Gaussian shocks, we might use Hermite polynomials \\{H_i(s)\\}_{i=0}^{n-1} weighted by the Gaussian density \\phi(s). The Galerkin condition requires:\\int \\left(\\Bellman \\hat{v}(s; \\theta) - \\hat{v}(s; \\theta)\\right) H_j(s) \\phi(s) ds = 0, \\quad j=0,\\ldots,n-1\n\nExpanding the Bellman operator:\\int \\left[\\max_{a \\in \\mathcal{A}} \\left\\{r(s,a) + \\gamma \\int \\hat{v}(s'; \\theta) p(ds'|s,a)\\right\\} - \\hat{v}(s; \\theta)\\right] H_j(s) \\phi(s) ds = 0\n\nWe approximate this outer integral using Gauss-Hermite quadrature with nodes \\{s_\\ell\\}_{\\ell=1}^m and weights \\{w_\\ell\\}_{\\ell=1}^m:\\sum_{\\ell=1}^m w_\\ell \\left[\\max_{a \\in \\mathcal{A}} \\left\\{r(s_\\ell,a) + \\gamma \\int \\hat{v}(s'; \\theta) p(ds'|s_\\ell,a)\\right\\} - \\hat{v}(s_\\ell; \\theta)\\right] H_j(s_\\ell) = 0\n\nThe inner integral (transition expectation) is also approximated using Gauss-Hermite quadrature:\\int \\hat{v}(s'; \\theta) p(ds'|s_\\ell,a) \\approx \\sum_{k=1}^m w_k \\hat{v}(s_k; \\theta) p(s_k|s_\\ell,a)\n\nSubstituting the basis representation and collecting terms:\\sum_{\\ell=1}^m w_\\ell H_j(s_\\ell) \\max_{a \\in \\mathcal{A}} \\left\\{r(s_\\ell,a) + \\gamma \\sum_{i=0}^{n-1} \\theta_i \\sum_{k=1}^m w_k H_i(s_k) p(s_k|s_\\ell,a)\\right\\} = \\sum_{\\ell=1}^m w_\\ell H_j(s_\\ell) \\sum_{i=0}^{n-1} \\theta_i H_i(s_\\ell)\n\nThis gives n nonlinear equations in n unknowns. The right-hand side simplifies using orthogonality: when using the same quadrature nodes for projection and integration, \\sum_{\\ell=1}^m w_\\ell H_j(s_\\ell) H_i(s_\\ell) = \\delta_{ij} \\|H_j\\|^2.\n\nIn both cases, the basis-quadrature pairing ensures that:\n\nThe quadrature nodes appear in both the transition expectation and the outer projection\n\nThe quadrature accuracy matches the polynomial approximation order\n\nPrecomputed matrices capture the dynamics, making iterations efficient","type":"content","url":"/montecarlo#galerkin-projection-with-hermite-polynomials","position":7},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl2":"Monte Carlo Integration"},"type":"lvl2","url":"/montecarlo#monte-carlo-integration","position":8},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl2":"Monte Carlo Integration"},"content":"Deterministic quadrature rules work well when the state space has low dimension, the transition density is smooth, and we can evaluate it cheaply at arbitrary points. In many stochastic control problems none of these conditions truly hold. The state may be high dimensional, the dynamics may be given by a simulator rather than an explicit density, and the cost of each call to the model may be large. In that regime, deterministic quadrature becomes brittle. Monte Carlo methods offer a different way to approximate expectations, one that relies only on the ability to sample from the relevant distributions.","type":"content","url":"/montecarlo#monte-carlo-integration","position":9},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl3":"Monte Carlo as randomized quadrature","lvl2":"Monte Carlo Integration"},"type":"lvl3","url":"/montecarlo#monte-carlo-as-randomized-quadrature","position":10},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl3":"Monte Carlo as randomized quadrature","lvl2":"Monte Carlo Integration"},"content":"Consider a single expectation of the formJ = \\int f(x)\\,p(dx) = \\mathbb{E}[f(X)],\n\nwhere X \\sim p and f is some integrable function. Monte Carlo integration approximates J by drawing independent samples X^{(1)},\\ldots,X^{(N)} \\sim p and forming the sample average\\hat{J}_N \\equiv \\frac{1}{N}\\sum_{n=1}^N f\\bigl(X^{(n)}\\bigr).\n\nThis estimator has two basic properties that will matter throughout this chapter.\n\nFirst, it is unbiased:\\mathbb{E}[\\hat{J}_N]\n= \\mathbb{E}\\left[\\frac{1}{N}\\sum_{n=1}^N f(X^{(n)})\\right]\n= \\frac{1}{N}\\sum_{n=1}^N \\mathbb{E}[f(X^{(n)})]\n= \\mathbb{E}[f(X)]\n= J.\n\nSecond, its variance scales as 1/N. If we write\\sigma^2 \\equiv \\mathrm{Var}[f(X)],\n\nthen independence of the samples gives\\mathrm{Var}(\\hat{J}_N)\n= \\frac{1}{N^2}\\sum_{n=1}^N \\mathrm{Var}\\bigl(f(X^{(n)})\\bigr)\n= \\frac{\\sigma^2}{N}.\n\nThe central limit theorem then says that for large N,\\sqrt{N}\\,(\\hat{J}_N - J) \\Longrightarrow \\mathcal{N}(0,\\sigma^2),\n\nso the integration error decays at rate O(N^{-1/2}). This rate is slow compared to high-order quadrature in low dimension, but it has one crucial advantage: it does not explicitly depend on the dimension of x. Monte Carlo integration pays in variance, not in an exponential growth in the number of nodes.\n\nIt is often helpful to view Monte Carlo as randomized quadrature. A deterministic quadrature rule selects nodes x_j and weights w_j in advance and computes\\sum_j w_j f(x_j).\n\nMonte Carlo can be written in the same form: if we draw X^{(n)} from density p, the sample average\\hat{J}_N = \\frac{1}{N}\\sum_{n=1}^N f(X^{(n)})\n\nis just a quadrature rule with random nodes and equal weights. More advanced Monte Carlo schemes, such as importance sampling, change both the sampling distribution and the weights, but the basic idea remains the same.","type":"content","url":"/montecarlo#monte-carlo-as-randomized-quadrature","position":11},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl3":"Monte Carlo evaluation of the Bellman operator","lvl2":"Monte Carlo Integration"},"type":"lvl3","url":"/montecarlo#monte-carlo-evaluation-of-the-bellman-operator","position":12},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl3":"Monte Carlo evaluation of the Bellman operator","lvl2":"Monte Carlo Integration"},"content":"We now apply this to the Bellman operator. For a fixed value function v and a given state-action pair (s,a), the transition part of the Bellman operator is\\int v(s')\\,p(ds' \\mid s,a)\n= \\mathbb{E}\\left[v(S') \\mid S = s, A = a\\right].\n\nIf we can simulate next states S'^{(1)},\\ldots,S'^{(N)} from the transition kernel p(\\cdot \\mid s,a), either by calling a simulator or by interacting with the environment, we can approximate this expectation by\\widehat{\\mathbb{E}}_N\\bigl[v(S') \\mid s,a\\bigr]\n\\equiv\n\\frac{1}{N}\\sum_{n=1}^N v\\bigl(S'^{(n)}\\bigr).\n\nIf the immediate reward is also random, sayr = r(S,A,S') \\quad \\text{with} \\quad (S', r) \\sim p(\\cdot \\mid s,a),\n\nwe can approximate the full one-step return\\mathbb{E}\\bigl[r + \\gamma v(S') \\mid S = s, A = a\\bigr]\n\nby\\widehat{G}_N(s,a)\n\\equiv\n\\frac{1}{N}\\sum_{n=1}^N \\bigl[r^{(n)} + \\gamma v(S'^{(n)})\\bigr],\n\nwhere (r^{(n)}, S'^{(n)}) are independent samples given (s,a). Again, this is an unbiased estimator of the Bellman expectation for fixed v.\n\nPlugging this into the Bellman operator gives a Monte Carlo Bellman operator:(\\widehat{\\Bellman}_N v)(s)\n\\equiv\n\\max_{a \\in \\mathcal{A}_s}\n\\left\\{\n\\widehat{G}_N(s,a)\n\\right\\}.\n\nThe expectation inside the braces is now replaced by a random sample average. In model-based settings, we implement this by simulating many next states for each candidate action a at state s. In model-free settings, we obtain samples from real interactions and re-use them to estimate the expectation.\n\nAt this stage nothing about approximation or projection has entered yet. For a fixed value function v, Monte Carlo provides unbiased, noisy evaluations of (\\Bellman v)(s). The approximation question arises once we couple this stochastic evaluation with basis functions and projections.","type":"content","url":"/montecarlo#monte-carlo-evaluation-of-the-bellman-operator","position":13},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl3":"Sampling the outer expectations","lvl2":"Monte Carlo Integration"},"type":"lvl3","url":"/montecarlo#sampling-the-outer-expectations","position":14},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl3":"Sampling the outer expectations","lvl2":"Monte Carlo Integration"},"content":"Projection methods introduce a second layer of integration. In Galerkin and least squares schemes, we choose a distribution \\mu over states (and sometimes actions) and enforce conditions of the form\\int R(s; \\theta)\\,p_i(s)\\, \\mu(ds) = 0\n\\quad \\text{or} \\quad\n\\int R(s; \\theta)^2\\,\\mu(ds) \\text{ is minimized}.\n\nHere R(s; \\theta) is the residual function, such asR(s; \\theta) = (\\Bellman \\hat{v})(s;\\theta) - \\hat{v}(s;\\theta),\n\nand p_i are test functions or derivatives of the residual with respect to parameters.\n\nNote a subtle but important shift in perspective from the previous chapter. There, the weight function w(s) in the inner product \\langle f, g \\rangle_w = \\int f(s) g(s) w(s) ds could be any positive weight function (not necessarily normalized). For Monte Carlo integration, however, we need a probability distribution we can sample from. We write \\mu for this sampling distribution and express projection conditions as integrals with respect to \\mu: \\int R(s; \\theta) p_i(s) \\mu(ds). If a problem was originally formulated with an unnormalized weight w(s), we must either (i) normalize it to define \\mu, or (ii) use importance sampling with a different \\mu and reweight samples by w(s)/\\mu(s). In reinforcement learning, \\mu is typically the empirical state visitation distribution from collected trajectories.\n\nThese outer integrals over s are generally not easier to compute than the inner transition expectations. Monte Carlo gives a way to approximate them as well. If we can draw states S^{(1)},\\ldots,S^{(M)} \\sim \\mu, we can approximate, for example, the Galerkin orthogonality conditions by\\int R(s; \\theta)\\,p_i(s)\\,\\mu(ds)\n\\approx\n\\frac{1}{M}\\sum_{m=1}^M R\\bigl(S^{(m)}; \\theta\\bigr)\\,p_i\\bigl(S^{(m)}\\bigr).\n\nSimilarly, a least squares objective\\int R(s; \\theta)^2\\,\\mu(ds)\n\nis approximated by the empirical risk\\frac{1}{M}\\sum_{m=1}^M R\\bigl(S^{(m)}; \\theta\\bigr)^2.\n\nIf we now substitute Monte Carlo estimates for both the inner transition expectations and the outer projection conditions, we obtain fully simulation-based schemes. We no longer need explicit access to the transition kernel p(\\cdot \\mid s,a) or the state distribution \\mu. It is enough to be able to sample from them, either through a simulator or by interacting with the real system.","type":"content","url":"/montecarlo#sampling-the-outer-expectations","position":15},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl3":"Simulation-Based Projection Methods","lvl2":"Monte Carlo Integration"},"type":"lvl3","url":"/montecarlo#simulation-based-projection-methods","position":16},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl3":"Simulation-Based Projection Methods","lvl2":"Monte Carlo Integration"},"content":"Monte Carlo integration replaces both levels of integration in approximate dynamic programming. The transition expectation in the Bellman operator becomes \\frac{1}{N}\\sum_{n=1}^N v(S'^{(n)}), and the outer integrals for projection become empirical averages over sampled states. Writing \\Proj_M for projection using M state samples and \\widehat{\\Bellman}_N for the Monte Carlo Bellman operator with N transition samples, the iteration becomes\\hat{v}^{(k+1)} = \\Proj_M\\,\\widehat{\\Bellman}_N \\hat{v}^{(k)}.\n\nIn the typical reinforcement learning setting, N=1: each observed transition (s_i, a_i, r_i, s'_i) provides exactly one sample of the next state. This means we work with high-variance single-sample estimates r_i + \\gamma v(s'_i) rather than averaged returns. Variance reduction comes from aggregating information across different transitions through the function approximator and from collecting long trajectories with many transitions. We examine this single-sample constraint in detail after introducing Q-functions below.\n\nUnlike deterministic quadrature, which introduces a fixed bias at each iteration, Monte Carlo introduces random error with zero mean but nonzero variance. However, combining Monte Carlo with the maximization in the Bellman operator creates a systematic problem: while the estimate of the expected return for any individual action is unbiased, taking the maximum over these noisy estimates introduces upward bias. This overestimation compounds through value iteration and degrades the resulting policies. We address this challenge in the \n\nnext chapter on fitted Q-iteration.","type":"content","url":"/montecarlo#simulation-based-projection-methods","position":17},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl3":"Amortizing Action Selection via Q-Functions","lvl2":"Monte Carlo Integration"},"type":"lvl3","url":"/montecarlo#amortizing-action-selection-via-q-functions","position":18},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl3":"Amortizing Action Selection via Q-Functions","lvl2":"Monte Carlo Integration"},"content":"Monte Carlo integration enables model-free approximate dynamic programming: we no longer need explicit transition probabilities p(s'|s,a), only the ability to sample next states. However, one computational challenge remains. The standard formulation of an optimal decision rule is\\pi(s) = \\arg\\max_{a \\in \\mathcal{A}} \\left\\{r(s,a) + \\gamma \\int v(s')p(ds'|s,a)\\right\\}.\n\nEven with an optimal value function v^* in hand, extracting an action at state s requires evaluating the transition expectation for each candidate action. In the model-free setting, this means we must draw Monte Carlo samples from each action’s transition distribution every time we select an action. This repeated sampling “at inference time” wastes computation, especially when the same state is visited multiple times.\n\nWe can amortize this computation by working at a different level of representation. Define the state-action value function (or Q-function)q(s,a) = r(s,a) + \\gamma \\int v(s')p(ds'|s,a).\n\nThe Q-function caches the result of evaluating each action at each state. Once we have q, action selection reduces to a finite maximization:\\pi(s) = \\arg\\max_{a \\in \\mathcal{A}(s)} q(s,a).\n\nNo integration appears in this expression. The transition expectation has been precomputed and stored in q itself.\n\nThe optimal Q-function q^* satisfies its own Bellman equation. Substituting the definition of v^*(s) = \\max_a q^*(s,a) into the expression for q:q^*(s,a) = r(s,a) + \\gamma \\int p(ds'|s,a) \\max_{a' \\in \\mathcal{A}(s')} q^*(s', a').\n\nThis defines a Bellman operator on Q-functions:(\\Bellman q)(s,a) = r(s,a) + \\gamma \\int p(ds'|s,a)\\max_{a' \\in \\mathcal{A}(s')} q(s', a').\n\nLike the Bellman operator on value functions, \\Bellman is a \\gamma-contraction in the sup-norm, guaranteeing a unique fixed point q^*. We can thus apply the same projection and Monte Carlo techniques developed for value functions to Q-functions. The computational cost shifts from action selection (repeated sampling at decision time) to training (evaluating expectations during the iterations of approximate value iteration). Once q is learned, acting is cheap.\n\nParametric Q-Value Iteration\n\nInput Given an MDP (S, A, P, R, \\gamma), base points \\mathcal{B} \\subset S, function approximator class q(s,a; \\boldsymbol{\\theta}), maximum iterations N, tolerance \\varepsilon > 0, initial parameters \\boldsymbol{\\theta}_0\n\nOutput Parameters \\boldsymbol{\\theta} for Q-function approximation\n\nInitialize \\boldsymbol{\\theta}_0 (e.g., random initialization)\n\nn \\leftarrow 0\n\nrepeat\n\n\\mathcal{D} \\leftarrow \\emptyset\n\nFor each (s,a) \\in \\mathcal{B} \\times A:\n\ny_{s,a} \\leftarrow r(s,a) + \\gamma \\int p(ds'|s,a)\\max_{a' \\in A} q(s',a'; \\boldsymbol{\\theta}_n)\n\n\\mathcal{D} \\leftarrow \\mathcal{D} \\cup \\{((s,a), y_{s,a})\\}\n\n\\boldsymbol{\\theta}_{n+1} \\leftarrow \\texttt{fit}(\\mathcal{D}, \\boldsymbol{\\theta}_{\\text{init}}=\\boldsymbol{\\theta}_0, K=\\infty)\n\n\\delta \\leftarrow \\frac{1}{|\\mathcal{D}||A|}\\sum_{(s,a) \\in \\mathcal{D} \\times A} (q(s,a; \\boldsymbol{\\theta}_{n+1}) - q(s,a; \\boldsymbol{\\theta}_n))^2\n\nn \\leftarrow n + 1\n\nuntil (\\delta < \\varepsilon or n \\geq N)\n\nreturn \\boldsymbol{\\theta}_n\n\nThe algorithm above evaluates the expectation \\int p(ds'|s,a)\\max_{a'} q(s',a'; \\boldsymbol{\\theta}_n) at each state-action pair. In principle, we could approximate this integral using many Monte Carlo samples: draw N next states from p(\\cdot|s,a) and average \\frac{1}{N}\\sum_{i=1}^N \\max_{a'} q(s'_i, a') to reduce variance. This is standard Monte Carlo integration applied to the Bellman operator.","type":"content","url":"/montecarlo#amortizing-action-selection-via-q-functions","position":19},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl3":"The Single-Sample Paradigm","lvl2":"Monte Carlo Integration"},"type":"lvl3","url":"/montecarlo#the-single-sample-paradigm","position":20},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl3":"The Single-Sample Paradigm","lvl2":"Monte Carlo Integration"},"content":"Most reinforcement learning algorithms do not follow this pattern. Instead, they work with datasets of transition tuples \\mathcal{D} = \\{(s_i, a_i, r_i, s'_i)\\}_{i=1}^M, where each tuple records a single observed next state s'_i from executing action a_i in state s_i. Each transition provides exactly one sample of the dynamics: the target becomes y_i = r_i + \\gamma \\max_{a'} q(s'_i, a') using only the single observed s'_i.\n\nThis design choice has two origins. First, it reflects genuine constraints in some problem settings. A robot executing action a in state s observes one outcome s' and cannot rewind to the exact same state to sample alternative continuations. Real physical systems, biological experiments, and costly interactions (clinical trials, manufacturing processes) provide one sample per executed action. However, many domains where RL is applied do not face this constraint. Game emulators support state saving and restoration, physics simulators can reset to arbitrary configurations, and software environments can be cloned cheaply. The single-sample pattern persists in these settings not from necessity but from algorithmic convention.\n\nSecond, and perhaps more fundamentally, the single-sample paradigm emerged from reinforcement learning’s roots in computational neuroscience and models of biological intelligence. Early work viewed RL as a model of animal learning, where organisms experience one consequence per action and must learn from streaming sensory data. This perspective shaped the field’s algorithmic agenda: develop methods that learn from single sequential experiences, as biological agents do. Temporal difference learning, Q-learning, and actor-critic methods all follow this template.\n\nThese origins matter because they established design choices that persist even when the original constraints do not apply. In many practical applications, we have fast simulators that support arbitrary state initialization (e.g., physics engines for robotics, game emulators for Atari). We could collect multiple next-state samples per (s,a) pair to reduce variance in target computation. However, most algorithms do not exploit this capability. The transition tuple (s_i, a_i, r_i, s'_i) with N=1 remains the standard data structure, and methods compensate for high variance through other means: larger replay buffers, function approximation that pools information across samples, multi-step returns, and careful optimization strategies.\n\nThis creates an asymmetry in how algorithms use Monte Carlo integration. For the outer expectation (sampling which states to evaluate at), algorithms freely adjust sample size: offline methods reuse entire datasets, replay buffers store thousands of transitions, online methods process one sample per step. For the inner expectation (sampling next states to evaluate the Bellman operator at a given (s,a)), the overwhelming majority of practical methods use N=1 regardless of whether the environment permits larger samples.\n\nThe single-sample paradigm shapes three aspects of algorithm design:\n\nData structure: Algorithms store and manipulate tuples \\{(s_i, a_i, r_i, s'_i)\\}, not state-action pairs with multiple next-state samples\n\nVariance reduction: Comes from aggregation across different samples through function approximation and from trajectory length, not from repeated sampling at fixed (s,a)\n\nTarget computation: Each transition yields exactly one target y_i = r_i + \\gamma \\max_{a'} q(s'_i, a') evaluated at the single observed s'_i\n\nThe algorithms that follow in the next chapters all adopt this single-sample structure. Understanding its origins clarifies when the design choice might be revisited: in domains with cheap, resettable simulators and high target variance, averaging multiple next-state samples per (s,a) pair offers a direct variance reduction mechanism that is rarely exploited in current practice.","type":"content","url":"/montecarlo#the-single-sample-paradigm","position":21},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl2":"Overestimation Bias and Mitigation Strategies"},"type":"lvl2","url":"/montecarlo#overestimation-bias-and-mitigation-strategies","position":22},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl2":"Overestimation Bias and Mitigation Strategies"},"content":"Monte Carlo integration provides unbiased estimates of individual expectations, but when we apply the Bellman operator’s maximization to these noisy estimates, a systematic problem arises: taking the maximum over noisy values creates upward bias. This overestimation compounds through iterative algorithms and can severely degrade the quality of learned policies. This section examines the sources of this bias and presents two approaches for mitigating it.","type":"content","url":"/montecarlo#overestimation-bias-and-mitigation-strategies","position":23},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl3":"Sources of Overestimation Bias","lvl2":"Overestimation Bias and Mitigation Strategies"},"type":"lvl3","url":"/montecarlo#sources-of-overestimation-bias","position":24},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl3":"Sources of Overestimation Bias","lvl2":"Overestimation Bias and Mitigation Strategies"},"content":"When we apply Monte Carlo integration to the Bellman operator, each individual expectation is unbiased. The problem arises when we maximize over these noisy estimates. To make this precise, consider a given state-action pair (s,a) and value function v. Define the true continuation value\\mu(s,a) \\equiv \\int v(s')\\,p(ds' \\mid s,a),\n\nand its Monte Carlo approximation with N samples:\\hat{\\mu}_N(s,a) \\equiv \\frac{1}{N}\\sum_{i=1}^N v(s'_i), \\quad s'_i \\sim p(\\cdot|s,a).\n\nEach estimator is unbiased: \\mathbb{E}[\\hat{\\mu}_N(s,a)] = \\mu(s,a). The Monte Carlo Bellman operator is(\\widehat{\\Bellman}_N v)(s) = \\max_{a \\in \\mathcal{A}_s} \\left\\{ r(s,a) + \\gamma \\hat{\\mu}_N(s,a)\\right\\}.\n\nWhile each \\hat{\\mu}_N(s,a) is unbiased, the maximization introduces systematic overestimation. To see why, let a^* be the truly optimal action. The maximum of any collection is at least as large as any particular element:\\mathbb{E}\\big[\\max_a \\{r(s,a) + \\gamma \\hat{\\mu}_N(s,a)\\}\\big] \\ge \\mathbb{E}\\big[r(s,a^*) + \\gamma \\hat{\\mu}_N(s,a^*)\\big] = r(s,a^*) + \\gamma \\mu(s,a^*) = (\\Bellman v)(s).\n\nThe inequality is strict whenever multiple actions have nonzero variance. The maximization selects whichever action happens to receive a positive noise realization, and that inflated estimate contributes to the target value. This is Jensen’s inequality for the max operator: \\mathbb{E}[\\max_a Y_a] \\ge \\max_a \\mathbb{E}[Y_a] for random variables \\{Y_a\\}, since the max is convex. Even though we start with unbiased estimators, taking the maximum breaks unbiasedness.\n\nOperationally, this means that repeatedly sampling fresh states and taking the max will not, on average, converge to the true value but systematically land above it. Unlike noise that averages out, this bias persists no matter how many samples we draw. Worse, it compounds across iterations as overestimates feed into future target computations.\n\nConnection to deterministic policies\n\nThis inequality also underlies why deterministic policies are optimal in MDPs (Theorem \n\nTheorem 3): \\max_a w(a) \\ge \\sum_a q(a) w(a) shows randomization cannot improve expected returns. The same mathematical principle appears in two contexts: (1) deterministic policies suffice, (2) maximization over noisy estimates creates upward bias.\n\nMonte Carlo value iteration applies v_{k+1} = \\widehat{\\Bellman}_N v_k repeatedly. The overestimation bias does not stay confined to a single iteration: it accumulates through the recursion. At iteration 2, we compute Monte Carlo estimates \\hat{\\mu}_N(s,a) = \\frac{1}{N}\\sum_{i=1}^N v_1(s'_i), but v_1 is already biased upward from iteration 1. Averaging an overestimated function and then maximizing over noisy estimates compounds the bias: \\mathbb{E}[v_k] \\ge \\mathbb{E}[v_{k-1}] \\ge \\cdots \\ge v^*. Without correction, this feedback loop can produce severely inflated value estimates that yield poor policies.","type":"content","url":"/montecarlo#sources-of-overestimation-bias","position":25},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl3":"Learning the Bias Correction","lvl2":"Overestimation Bias and Mitigation Strategies"},"type":"lvl3","url":"/montecarlo#learning-the-bias-correction","position":26},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl3":"Learning the Bias Correction","lvl2":"Overestimation Bias and Mitigation Strategies"},"content":"One approach, developed by Keane and Wolpin \n\nKeane & Wolpin (1994) in the context of dynamic discrete choice models, treats the bias itself as a quantity to be learned and subtracted. For a given value function v and Monte Carlo sample size N, define the bias at state s as\\delta(s) = \\mathbb{E}\\big[(\\widehat{\\Bellman}_N v)(s)\\big] - (\\Bellman v)(s) \\ge 0.\n\nWhile we cannot compute \\delta(s) directly (we lack both the expectation and the exact Bellman application), the bias has structure. It depends on observable quantities: the number of actions |\\mathcal{A}_s|, the sample size N, and the spread of action values. When one action dominates, \\delta(s) is small. When several actions have similar values, noise is more likely to flip the maximizer, increasing \\delta(s).\n\nRather than deriving \\delta(s) analytically, Keane and Wolpin proposed learning it empirically. The strategy follows a “simulate on a subset, interpolate everywhere” template common in econometric dynamic programming. At a carefully chosen set of states, we run both high-fidelity simulation (many samples or exact integration) and the low-fidelity N-sample estimate used in value iteration. The gap between these estimates provides training data for the bias. We then fit a regression model g_\\eta that predicts \\delta(s) from features of the state and action-value distribution. During value iteration, we subtract the predicted bias from the raw Monte Carlo maximum.\n\nUseful features for predicting the bias include the spread of action values (from a separate high-fidelity simulation), the number of actions, and gaps to the best action. These are cheap to compute and track regimes where maximization bias is large. The procedure can be summarized as:\n\nKeane-Wolpin Bias-Corrected Value Iteration\n\nInput: MDP, current value v_k, sample size N, learned bias correction g_\\eta\n\nOutput: Bias-corrected next value v_{k+1}\n\nFor each state s do:\n\nFor each action a do:\n\nDraw N samples: s'_1, \\ldots, s'_N \\sim p(\\cdot|s,a)\n\nCompute \\hat{\\mu}_N(s,a) = \\frac{1}{N}\\sum_{i=1}^N v_k(s'_i)\n\nCompute raw max: M(s) = \\max_a \\{r(s,a) + \\gamma \\hat{\\mu}_N(s,a)\\}\n\nConstruct features \\phi(s) from action-value spread (using separate high-fidelity estimate)\n\nBias-corrected value: v_{k+1}(s) = M(s) - g_\\eta(\\phi(s))\n\nThe regression model g_\\eta is trained offline by comparing high and low-fidelity simulations at a grid of states, then applied during each value iteration step. This approach has been influential in econometrics for structural estimation problems. However, it has seen limited adoption in reinforcement learning. The computational overhead is substantial: it requires high-fidelity simulation at training states, careful feature engineering, and maintaining the regression model throughout learning. More critically, the circular dependency between the bias estimate and the value function can amplify errors if g_\\eta is misspecified.","type":"content","url":"/montecarlo#learning-the-bias-correction","position":27},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl3":"Analytical Bias Correction","lvl2":"Overestimation Bias and Mitigation Strategies"},"type":"lvl3","url":"/montecarlo#analytical-bias-correction","position":28},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl3":"Analytical Bias Correction","lvl2":"Overestimation Bias and Mitigation Strategies"},"content":"Lee et al. (2013)\n\nLee & Powell (2019) developed an alternative that derives the bias analytically using extreme value theory rather than learning it empirically. Their analysis considers the distributional structure of the noise in Monte Carlo estimates. When rewards are Gaussian with variance \\sigma^2, the bias in \\max_a \\widehat{R}(s,a) can be computed from extreme value theory.\n\nFor a single-state MDP where all randomness comes from stochastic rewards, suppose that for each action a, the observed reward is \\widehat{R}(s,a) = \\mu_a + \\varepsilon_a where \\varepsilon_a \\sim \\mathcal{N}(0, \\sigma^2). When we form the target \\max_a \\widehat{R}(s,a), we are taking the maximum of M = |\\mathcal{A}(s)| independent Gaussians. After appropriate centering and scaling, this maximum converges in distribution to a Gumbel random variable. The expected bias can be computed as:\\mathbb{E}\\left[\\max_{a} \\varepsilon_a\\right] \\approx \\sigma \\left(\\frac{\\xi}{b_M} + b_M\\right)\n\nwhere b_M = \\sqrt{2\\log M - \\log\\log M - \\log 4\\pi} and \\xi \\approx 0.5772 is the Euler-Mascheroni constant. This quantity is positive and grows with the number of actions M. The bias scales with the noise level \\sigma and with the action space size.\n\nThe corrected target becomes:\\tilde{y}_i = r_i + \\gamma \\max_{a'} q(s'_i, a'; \\boldsymbol{\\theta}^{(n)}) - B(s_i, a_i)\n\nwhere the bias correction term is:B(s,a) = \\left(\\frac{\\xi}{b_{|A(s)|}} + b_{|A(s)|}\\right) \\hat{\\sigma}(s,a)\n\nHere \\hat{\\sigma}(s,a) = \\sqrt{\\widehat{\\text{Var}}[R(s,a)]} is the estimated standard deviation of the reward at state-action pair (s,a). This can be computed from multiple observed reward samples at the same (s,a) pair if available.\n\nThis approach is exact for single-state MDPs where Q-values equal expected rewards and all bias comes from reward noise. For general multi-state MDPs, the situation is more complex. The bias arises from the max operator over Q-values at the next state s', not from reward stochasticity at the current state. Lee and Powell’s multistate extension addresses this by introducing a separate correction term B^T for transition stochasticity, estimated empirically by averaging \\max_{a'} q(s', a') over multiple sampled next states from the same (s,a) pair, rather than using an analytical formula. The single-state analytical correction remains valuable for bandit problems and as a first-order approximation when reward variance dominates, but does not directly address max-operator bias from function approximation error in the Q-values themselves. The correction is optimal specifically for Gaussian errors. If the true errors have heavier tails, the correction may be insufficient.\n\nLee-Powell’s analytical formula can be viewed as deriving what Keane-Wolpin learned empirically, replacing the regression model with a closed-form expression from extreme value theory. The trade-off is generality versus computational cost: Keane-Wolpin adapts to any noise structure but requires expensive high-fidelity simulations at training states; Lee-Powell assumes Gaussian noise but evaluates instantly once \\hat{\\sigma} is estimated. Both subtract a correction term from targets while preserving squared error loss.\n\nAn alternative to explicit bias correction is to replace the hard max operator with a soft maximum. The \n\nregularized MDP chapter discusses smooth Bellman operators that use logsumexp or Gaussian uncertainty-weighted aggregation to avoid the discontinuity and overestimation inherent in taking a hard maximum. These approaches modify the Bellman operator itself rather than correcting its bias after the fact, preserving squared error loss while constructing smoother targets. Another approach, examined in the \n\nnext chapter, changes the loss function itself to match the noise structure in TD targets.","type":"content","url":"/montecarlo#analytical-bias-correction","position":29},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl3":"Decoupling Selection and Evaluation","lvl2":"Overestimation Bias and Mitigation Strategies"},"type":"lvl3","url":"/montecarlo#decoupling-selection-and-evaluation","position":30},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl3":"Decoupling Selection and Evaluation","lvl2":"Overestimation Bias and Mitigation Strategies"},"content":"An alternative approach modifies the estimator itself to break the coupling that creates bias. In the standard Monte Carlo update \\max_a \\{r(s,a) + \\gamma \\hat{\\mu}_N(s,a)\\}, the same noisy estimate both selects which action looks best and provides the value assigned to that action. To see the problem, decompose the estimator into its mean and noise:\\hat{\\mu}_N(s,a) = \\mu(s,a) + \\varepsilon_a,\n\nwhere \\varepsilon_a is zero-mean noise. Whichever action happens to have the largest \\varepsilon_a gets selected, and that same positive noise inflates the target value:Y = r(s,a^\\star) + \\gamma \\mu(s,a^\\star) + \\gamma \\varepsilon_{a^\\star},\n\nwhere a^\\star = \\arg\\max_a \\{r(s,a) + \\gamma \\mu(s,a) + \\gamma \\varepsilon_a\\}. This coupling (using the same random variable \\varepsilon_{a^\\star} for both selection and evaluation) produces \\mathbb{E}[Y] \\ge \\max_a \\{r(s,a) + \\gamma \\mu(s,a)\\}.\n\nDouble Q-learning \n\nVan Hasselt et al. (2016) breaks this coupling by maintaining two independent Monte Carlo estimates. Conceptually, for each action a at state s, we would draw two separate sets of samples from p(\\cdot|s,a):\\hat{\\mu}^{(1)}_N(s,a) = \\mu(s,a) + \\varepsilon^{(1)}_a, \\quad \\hat{\\mu}^{(2)}_N(s,a) = \\mu(s,a) + \\varepsilon^{(2)}_a,\n\nwhere \\varepsilon^{(1)}_a and \\varepsilon^{(2)}_a are independent zero-mean noise terms. We use the first estimate to select the action but the second to evaluate it:a^\\star = \\arg\\max_{a} \\left\\{r(s,a) + \\gamma \\hat{\\mu}^{(1)}_N(s,a)\\right\\}, \\quad Y = r(s,a^\\star) + \\gamma \\hat{\\mu}^{(2)}_N(s,a^\\star).\n\nNote that a^\\star is a random variable. It is the result of maximizing over noisy estimates \\hat{\\mu}^{(1)}_N(s,a), and therefore depends on the noise \\varepsilon^{(1)}. The target value Y uses the second estimator \\hat{\\mu}^{(2)}_N(s,a^\\star), evaluating it at the randomly selected action a^\\star. Expanding the target value to make the dependencies explicit:Y = r(s,a^\\star) + \\gamma \\hat{\\mu}^{(2)}_N(s,a^\\star) = r(s,a^\\star) + \\gamma \\mu(s,a^\\star) + \\gamma \\varepsilon^{(2)}_{a^\\star},\n\nwhere a^\\star itself depends on \\varepsilon^{(1)}. To see why this eliminates evaluation bias, write the expectation as a nested expectation:\\mathbb{E}[Y] = \\mathbb{E}_{\\varepsilon^{(1)}}\\Big[\\mathbb{E}_{\\varepsilon^{(2)}}\\big[r(s,a^\\star) + \\gamma \\mu(s,a^\\star) + \\gamma \\varepsilon^{(2)}_{a^\\star} \\mid a^\\star\\big]\\Big].\n\nThe inner expectation, conditioned on the selected action a^\\star, equals:\\mathbb{E}_{\\varepsilon^{(2)}}\\big[r(s,a^\\star) + \\gamma \\mu(s,a^\\star) + \\gamma \\varepsilon^{(2)}_{a^\\star} \\mid a^\\star\\big] = r(s,a^\\star) + \\gamma \\mu(s,a^\\star),\n\nbecause \\mathbb{E}[\\varepsilon^{(2)}_{a^\\star} \\mid a^\\star] = 0. Why does this conditional expectation vanish? Because a^\\star is a random variable determined entirely by \\varepsilon^{(1)} (through the argmax operation), while \\varepsilon^{(2)} is independent of \\varepsilon^{(1)}. Therefore, \\varepsilon^{(2)} is independent of a^\\star. By the fundamental property of independence, for any random variables X and Y where X \\perp Y, we have \\mathbb{E}[X \\mid Y] = \\mathbb{E}[X]. Applying this:\\mathbb{E}[\\varepsilon^{(2)}_{a^\\star} \\mid a^\\star] = \\mathbb{E}[\\varepsilon^{(2)}_{a^\\star}] = 0,\n\nwhere the final equality holds because each \\varepsilon^{(2)}_a has zero mean. Taking the outer expectation:\\mathbb{E}[Y] = \\mathbb{E}_{\\varepsilon^{(1)}}\\big[r(s,a^\\star) + \\gamma \\mu(s,a^\\star)\\big].\n\nThe evaluation noise contributes zero on average because it is independent of the selection. However, double Q-learning does not eliminate all bias. Two distinct sources of bias arise when maximizing over noisy estimates:\n\nSelection bias: Using \\arg\\max_a \\{r(s,a) + \\gamma \\mu(s,a) + \\gamma \\varepsilon^{(1)}_a\\} tends to select actions that received positive noise. The noise \\varepsilon^{(1)} causes us to favor actions that got lucky, so a^\\star is not uniformly distributed but skewed toward actions with positive \\varepsilon^{(1)}_a. This means \\mathbb{E}_{\\varepsilon^{(1)}}[\\mu(s,a^\\star)] \\ge \\max_a \\mu(s,a).\n\nEvaluation bias: In the standard estimator, we use the same noisy estimate \\hat{\\mu}_N(s,a^\\star) that selected a^\\star to also evaluate it. If a^\\star was selected because \\varepsilon_{a^\\star} happened to be large and positive, that same inflated estimate provides the target value. This creates additional bias beyond the selection effect.\n\nDouble Q-learning eliminates evaluation bias by using independent noise for evaluation: even though a^\\star is biased toward actions with positive \\varepsilon^{(1)}, the evaluation uses \\varepsilon^{(2)} which is independent of the selection, so \\mathbb{E}[\\varepsilon^{(2)}_{a^\\star} \\mid a^\\star] = 0. Selection bias remains, but removing evaluation bias substantially reduces total overestimation.\n\nImplementation via different Q-functions\n\nThe conceptual framework above assumes we can draw multiple independent samples from p(\\cdot|s,a) for each state-action pair. In practice, this would require resetting a simulator to the same state and sampling multiple times, which is often infeasible. The practical implementation achieves the same effect differently: maintain two Q-functions that are trained on different data or updated at different times (e.g., one is a slowly-updating target network). Since the two Q-functions experience different noise realizations during training, their errors remain less correlated than if we used a single Q-function for both selection and evaluation. This is how Double DQN works, as we’ll see in the \n\nnext chapter on online learning.\n\nThe following algorithm implements this principle with Monte Carlo integration. We maintain two Q-functions and alternate which one selects actions and which one evaluates them. The bias reduction mechanism applies whether we store Q-values in a table or use function approximation.\n\nNote that this algorithm is written in the theoretical form that assumes we can draw N samples from each (s,a) pair. In practice with single-trajectory data (N=1, one quadruple (s,a,r,s') per transition), the double Q-learning principle still applies but we work with the single observed next state s' and maintain two Q-functions that are trained on different subsets of data or updated at different frequencies to achieve the independence needed for bias reduction.\n\nDouble Q Value Iteration with Monte Carlo\n\nInput: MDP, state sample \\mathcal{S}, Monte Carlo sample size N, maximum iterations K\n\nOutput: Two Q-functions q^{(1)}, q^{(2)}\n\nInitialize q^{(1)}_0(s,a) and q^{(2)}_0(s,a) for all s,a\n\nk \\leftarrow 0\n\nrepeat\n\n\\quad for each s \\in \\mathcal{S} do\n\n\\quad\\quad for each a \\in \\mathcal{A}(s) do\n\n\\quad\\quad\\quad Draw N next states: s'_1, \\ldots, s'_N \\sim p(\\cdot \\mid s,a)\n\n\\quad\\quad\\quad // Compute targets using decoupled selection-evaluation\n\n\\quad\\quad\\quad for i = 1, \\ldots, N do\n\n\\quad\\quad\\quad\\quad a^{(1)}_i \\leftarrow \\arg\\max_{a'} q^{(1)}_k(s'_i, a')\n\n\\quad\\quad\\quad\\quad a^{(2)}_i \\leftarrow \\arg\\max_{a'} q^{(2)}_k(s'_i, a')\n\n\\quad\\quad\\quad end for\n\n\\quad\\quad\\quad q^{(1)}_{k+1}(s,a) \\leftarrow r(s,a) + \\frac{\\gamma}{N}\\sum_{i=1}^N q^{(2)}_k(s'_i, a^{(1)}_i)\n\n\\quad\\quad\\quad q^{(2)}_{k+1}(s,a) \\leftarrow r(s,a) + \\frac{\\gamma}{N}\\sum_{i=1}^N q^{(1)}_k(s'_i, a^{(2)}_i)\n\n\\quad\\quad end for\n\n\\quad end for\n\n\\quad k \\leftarrow k+1\n\nuntil convergence or k \\geq K\n\nreturn q^{(1)}_k, q^{(2)}_k\n\nAt lines 9 and 12, q^{(1)} selects the action but q^{(2)} evaluates it. The noise in q^{(1)} influences which action is chosen, but the independent noise in q^{(2)} does not inflate the evaluation. Lines 10 and 13 apply the same principle symmetrically for updating q^{(2)}.","type":"content","url":"/montecarlo#decoupling-selection-and-evaluation","position":31},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl3":"Weighted Estimators: A Third Approach","lvl2":"Overestimation Bias and Mitigation Strategies"},"type":"lvl3","url":"/montecarlo#weighted-estimators-a-third-approach","position":32},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl3":"Weighted Estimators: A Third Approach","lvl2":"Overestimation Bias and Mitigation Strategies"},"content":"The Keane-Wolpin and double Q-learning approaches represent two strategies for addressing overestimation bias: explicit correction and decoupled estimation. A third approach, developed by D’Eramo et al. \n\nD'Eramo et al. (2016), replaces the hard maximum with a probability-weighted average that accounts for uncertainty in value estimates.\n\nThe maximum estimator (ME) used in standard Q-learning takes the maximum of sample mean Q-values:\\hat{\\mu}^{ME}_* = \\max_{a \\in \\mathcal{A}} \\hat{\\mu}_a\n\nwhere \\hat{\\mu}_a is the sample mean Q-value for action a. While each individual \\hat{\\mu}_a may be unbiased, the maximum is systematically upward biased. The bias can be bounded by:\\text{Bias}(\\hat{\\mu}^{ME}_*) \\leq \\sqrt{\\frac{M-1}{M}\\sum_{a=1}^M \\frac{\\sigma_a^2}{n_a}}\n\nwhere M = |\\mathcal{A}| is the number of actions, \\sigma_a^2 is the variance of Q-values for action a, and n_a is the number of samples used to estimate it.\n\nThe double estimator (DE) from double Q-learning reduces this bias by decoupling selection and evaluation, but introduces negative bias:\\mathbb{E}[\\hat{\\mu}^{DE}_*] \\leq \\max_a \\mu_a\n\nThis pessimism can be problematic when one action is clearly superior to others, as the estimator may systematically undervalue the optimal action.\n\nThe weighted estimator (WE) provides a middle ground. Instead of taking a hard maximum, compute a weighted average where weights reflect the probability that each action is optimal. Under a Gaussian approximation for the sampling distribution of Q-value estimates, the weight for action a is:w_a = \\mathbb{P}\\left(a \\in \\arg\\max_{a' \\in \\mathcal{A}} \\hat{\\mu}_{a'}\\right) = \\int_{-\\infty}^{+\\infty} \\phi\\left(\\frac{x - \\hat{\\mu}_a}{\\hat{\\sigma}_a/\\sqrt{n_a}}\\right) \\prod_{b \\neq a} \\Phi\\left(\\frac{x - \\hat{\\mu}_b}{\\hat{\\sigma}_b/\\sqrt{n_b}}\\right) dx\n\nwhere \\phi and \\Phi are the standard normal PDF and CDF. This integral computes the probability that action a achieves the maximum by integrating over all possible true values x. For each x, the \\phi term gives the probability density that action a has true value x, while the product of \\Phi terms gives the probability that all other actions have true values below x.\n\nThe weighted estimator then becomes:\\hat{\\mu}^{WE}_* = \\sum_{a \\in \\mathcal{A}} w_a \\hat{\\mu}_a\n\nD’Eramo et al. established that this estimator satisfies \\text{Bias}(\\hat{\\mu}^{WE}_*) \\leq \\text{Bias}(\\hat{\\mu}^{ME}_*) and \\text{Bias}(\\hat{\\mu}^{WE}_*) \\geq \\text{Bias}(\\hat{\\mu}^{DE}_*). The weighted estimator thus provides a middle ground between the optimism of ME and the pessimism of DE.\n\nThe relative performance depends on the problem structure. When one action is clearly superior (\\mu_1 - \\mu_2 \\gg \\sigma), the signal dominates the noise and ME performs well with minimal overestimation. When multiple actions have similar values (\\mu_1 \\approx \\mu_2), eliminating overestimation becomes critical and DE excels despite its pessimism. In intermediate cases, which represent most practical scenarios, WE adapts to the actual uncertainty and avoids both extremes.\n\nThe weighted estimator can be incorporated into Q-learning by maintaining variance estimates alongside Q-values and computing the weighted target at each step. The weights w_a themselves can serve as an exploration policy that naturally adapts to estimation uncertainty. This approach adapts automatically to heterogeneous uncertainty across actions and provides balanced bias, but requires maintaining variance estimates (adding memory overhead) and computing integrals (expensive for large action spaces). The method assumes Gaussian sampling distributions, though it proves robust in practice even when this assumption is violated.\n\nThe weighted estimator relates to the smooth Bellman operators discussed in the \n\nregularized MDP chapter. Both replace the discontinuous hard maximum with smooth aggregation, but weighted estimation adapts to state-action-specific uncertainty while logsumexp applies uniform smoothing via temperature. The choice among ME, DE, and WE depends on computational constraints and problem characteristics. When variance estimates are available and action spaces are small, WE offers a principled approach that balances the extremes of overestimation and underestimation. When maintaining variance is expensive or action spaces are large, double Q-learning provides a simpler alternative that eliminates evaluation bias without explicit probability weighting.","type":"content","url":"/montecarlo#weighted-estimators-a-third-approach","position":33},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl2":"Summary and Forward Look"},"type":"lvl2","url":"/montecarlo#summary-and-forward-look","position":34},{"hierarchy":{"lvl1":"Monte Carlo Integration in Approximate Dynamic Programming","lvl2":"Summary and Forward Look"},"content":"This chapter developed the simulation-based approach to approximate dynamic programming by replacing analytical integration with Monte Carlo sampling. We showed how deterministic quadrature rules give way to randomized estimation when transition densities are expensive to evaluate or the state space is high-dimensional. We need only the ability to sample from transition distributions, not to compute their densities explicitly.\n\nThree foundational components emerged:\n\nMonte Carlo Bellman operator: Replace exact expectations \\int v(s')p(ds'|s,a) with sample averages \\frac{1}{N}\\sum v(s'_i), typically with N=1 in practice\n\nQ-functions: Cache state-action values to amortize the cost of action selection, eliminating the need for repeated sampling at decision time\n\nBias mitigation: Address the systematic overestimation that arises from maximizing over noisy estimates through four approaches: learning empirical corrections (Keane-Wolpin), analytical adjustment from extreme value theory (Lee-Powell), decoupling selection from evaluation (double Q-learning), and probability-weighted aggregation (weighted estimators)\n\nThe algorithms presented (Parametric Q-Value Iteration, Keane-Wolpin, Double Q) remain in the theoretical setting where we can draw multiple samples from each state-action pair and choose optimization parameters freely. The \n\nnext chapter addresses the practical constraints of reinforcement learning: working with fixed offline datasets of single-sample transitions, choosing function approximators and optimization strategies, and understanding the algorithmic design space that yields methods like FQI, NFQI, and DQN.","type":"content","url":"/montecarlo#summary-and-forward-look","position":35},{"hierarchy":{"lvl1":"Model Predictive Control"},"type":"lvl1","url":"/mpc","position":0},{"hierarchy":{"lvl1":"Model Predictive Control"},"content":"The trajectory optimization methods presented so far compute a complete control trajectory from an initial state to a final time or state. Once computed, this trajectory is executed without modification, making these methods fundamentally open-loop. The control function, \\mathbf{u}[k] in discrete time or \\mathbf{u}(t) in continuous time, depends only on the clock, reading off precomputed values from memory or interpolating between them. This approach assumes perfect models and no disturbances. Under these idealized conditions, repeating the same control sequence from the same initial state would always produce identical results.\n\nReal systems face modeling errors, external disturbances, and measurement noise that accumulate over time. A precomputed trajectory becomes increasingly irrelevant as these perturbations push the actual system state away from the predicted path. The solution is to incorporate feedback, making control decisions that respond to the current state rather than blindly following a predetermined schedule. While dynamic programming provides the theoretical framework for deriving feedback policies through value functions and Bellman equations, there exists a more direct approach that leverages the trajectory optimization methods already developed.","type":"content","url":"/mpc","position":1},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Closing the Loop by Replanning"},"type":"lvl3","url":"/mpc#closing-the-loop-by-replanning","position":2},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Closing the Loop by Replanning"},"content":"Model Predictive Control creates a feedback controller by repeatedly solving trajectory optimization problems. Rather than computing a single trajectory for the entire task duration, MPC solves a finite-horizon problem at each time step, starting from the current measured state. The controller then applies only the first control action from this solution before repeating the entire process. This strategy transforms any trajectory optimization method into a feedback controller.","type":"content","url":"/mpc#closing-the-loop-by-replanning","position":3},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"The Receding Horizon Principle","lvl3":"Closing the Loop by Replanning"},"type":"lvl4","url":"/mpc#the-receding-horizon-principle","position":4},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"The Receding Horizon Principle","lvl3":"Closing the Loop by Replanning"},"content":"The defining characteristic of MPC is its receding horizon strategy. At each time step, the controller solves an optimization problem looking a fixed duration into the future, but this prediction window constantly moves forward in time. The horizon “recedes” because it always starts from the current time and extends forward by the same amount.\n\nConsider the discrete-time optimal control problem in Bolza form:\\begin{aligned}\n\\text{minimize} \\quad & c_T(\\mathbf{x}_N) + \\sum_{k=0}^{N-1} c(\\mathbf{x}_k, \\mathbf{u}_k) \\\\\n\\text{subject to} \\quad & \\mathbf{x}_{k+1} = \\mathbf{f}(\\mathbf{x}_k, \\mathbf{u}_k) \\\\\n& \\mathbf{g}(\\mathbf{x}_k, \\mathbf{u}_k) \\leq \\mathbf{0} \\\\\n& \\mathbf{u}_{\\text{min}} \\leq \\mathbf{u}_k \\leq \\mathbf{u}_{\\text{max}} \\\\\n\\text{given} \\quad & \\mathbf{x}_0 = \\mathbf{x}_{\\text{current}}\n\\end{aligned}\n\nAt time step t, this problem optimizes over the interval [t, t+N]. At the next time step t+1, the horizon shifts to [t+1, t+N+1]. What makes this work is that only the first control \\mathbf{u}_0^* from each optimization is applied. The remaining controls \\mathbf{u}_1^*, \\ldots, \\mathbf{u}_{N-1}^* are discarded, though they may initialize the next optimization through warm-starting.\n\nThis receding horizon principle enables feedback without computing an explicit policy. By constantly updating predictions based on current measurements, MPC naturally corrects for disturbances and model errors. The apparent waste of computing but not using most of the trajectory is actually the mechanism that provides robustness.","type":"content","url":"/mpc#the-receding-horizon-principle","position":5},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"Horizon Selection and Problem Formulation","lvl3":"Closing the Loop by Replanning"},"type":"lvl4","url":"/mpc#horizon-selection-and-problem-formulation","position":6},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"Horizon Selection and Problem Formulation","lvl3":"Closing the Loop by Replanning"},"content":"The choice of prediction horizon depends on the control objective. We distinguish between three cases, each requiring different mathematical formulations.","type":"content","url":"/mpc#horizon-selection-and-problem-formulation","position":7},{"hierarchy":{"lvl1":"Model Predictive Control","lvl5":"Infinite-Horizon Regulation","lvl4":"Horizon Selection and Problem Formulation","lvl3":"Closing the Loop by Replanning"},"type":"lvl5","url":"/mpc#infinite-horizon-regulation","position":8},{"hierarchy":{"lvl1":"Model Predictive Control","lvl5":"Infinite-Horizon Regulation","lvl4":"Horizon Selection and Problem Formulation","lvl3":"Closing the Loop by Replanning"},"content":"For stabilization problems where the system must operate indefinitely around an equilibrium, the true objective is:J_\\infty = \\sum_{k=0}^{\\infty} c(\\mathbf{x}_k, \\mathbf{u}_k)\n\nSince this cannot be solved directly, MPC approximates it with:\\begin{aligned}\n\\text{minimize} \\quad & V_f(\\mathbf{x}_N) + \\sum_{k=0}^{N-1} c(\\mathbf{x}_k, \\mathbf{u}_k) \\\\\n\\text{subject to} \\quad & \\mathbf{x}_{k+1} = \\mathbf{f}(\\mathbf{x}_k, \\mathbf{u}_k) \\\\\n& \\mathbf{x}_N \\in \\mathcal{X}_f \\\\\n& \\text{other constraints}\n\\end{aligned}\n\nThe terminal cost V_f(\\mathbf{x}_N) approximates \\sum_{k=N}^{\\infty} c(\\mathbf{x}_k, \\mathbf{u}_k), the cost-to-go beyond the horizon. The terminal constraint \\mathbf{x}_N \\in \\mathcal{X}_f ensures the state reaches a region where a known stabilizing controller exists. Without these terminal ingredients, the finite-horizon approximation may produce unstable behavior, as the controller ignores consequences beyond the horizon.","type":"content","url":"/mpc#infinite-horizon-regulation","position":9},{"hierarchy":{"lvl1":"Model Predictive Control","lvl5":"Finite-Duration Tasks","lvl4":"Horizon Selection and Problem Formulation","lvl3":"Closing the Loop by Replanning"},"type":"lvl5","url":"/mpc#finite-duration-tasks","position":10},{"hierarchy":{"lvl1":"Model Predictive Control","lvl5":"Finite-Duration Tasks","lvl4":"Horizon Selection and Problem Formulation","lvl3":"Closing the Loop by Replanning"},"content":"For tasks ending at time t_f, the true objective spans from current time t to t_f:J_{[t, t_f]} = c_f(\\mathbf{x}(t_f)) + \\sum_{k=t}^{t_f-1} c(\\mathbf{x}_k, \\mathbf{u}_k)\n\nThe MPC formulation must adapt as time progresses:\\begin{aligned}\n\\text{minimize} \\quad & c_{T,k}(\\mathbf{x}_{N_k}) + \\sum_{j=0}^{N_k-1} c(\\mathbf{x}_j, \\mathbf{u}_j) \\\\\n\\text{where} \\quad & N_k = \\min(N, t_f - t_k) \\\\\n& c_{T,k} = \\begin{cases}\nc_f & \\text{if } t_k + N_k = t_f \\\\\nc_T & \\text{otherwise}\n\\end{cases}\n\\end{aligned}\n\nAs the task approaches completion, the horizon shrinks and the terminal cost switches from the approximation c_T to the true final cost c_f. This prevents the controller from optimizing beyond task completion, which would produce meaningless or aggressive control actions.","type":"content","url":"/mpc#finite-duration-tasks","position":11},{"hierarchy":{"lvl1":"Model Predictive Control","lvl5":"Periodic Tasks","lvl4":"Horizon Selection and Problem Formulation","lvl3":"Closing the Loop by Replanning"},"type":"lvl5","url":"/mpc#periodic-tasks","position":12},{"hierarchy":{"lvl1":"Model Predictive Control","lvl5":"Periodic Tasks","lvl4":"Horizon Selection and Problem Formulation","lvl3":"Closing the Loop by Replanning"},"content":"Some systems operate on repeating cycles where the optimal behavior depends on the time of day, week, or season. Consider a commercial building where heating costs are higher at night, electricity prices vary hourly, and occupancy patterns repeat daily. The MPC controller must account for these periodic patterns while planning over a finite horizon.\n\nFor tasks with period T_p, such as daily building operations, the formulation accounts for transitions across period boundaries:\\begin{aligned}\n\\text{minimize} \\quad & \\sum_{k=0}^{N-1} c_k(\\mathbf{x}_k, \\mathbf{u}_k, \\phi_k) \\\\\n\\text{where} \\quad & \\phi_k = (t + k) \\mod T_p \\\\\n& c_k(\\cdot, \\cdot, \\phi) = \\begin{cases}\nc_{\\text{day}}(\\cdot, \\cdot) & \\text{if } \\phi \\in [6\\text{am}, 6\\text{pm}] \\\\\nc_{\\text{night}}(\\cdot, \\cdot) & \\text{otherwise}\n\\end{cases}\n\\end{aligned}\n\nThe cost function changes based on the phase \\phi within the period. Constraints may similarly depend on the phase, reflecting different operational requirements at different times.","type":"content","url":"/mpc#periodic-tasks","position":13},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"The MPC Algorithm","lvl3":"Closing the Loop by Replanning"},"type":"lvl4","url":"/mpc#the-mpc-algorithm","position":14},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"The MPC Algorithm","lvl3":"Closing the Loop by Replanning"},"content":"The complete MPC procedure implements the receding horizon principle through repeated optimization:\n\nModel Predictive Control with Horizon Management\n\nInput:\n\nNominal prediction horizon N\n\nSampling period \\Delta t\n\nTask type: {infinite, finite with duration t_f, periodic with period T_p}\n\nCost functions and dynamics\n\nConstraints\n\nProcedure:\n\nInitialize time t \\leftarrow 0\n\nMeasure initial state \\mathbf{x}_{\\text{current}} \\leftarrow \\mathbf{x}(t)\n\nWhile task continues:\n\nDetermine effective horizon and costs:\n\nIf infinite task:\n\nN_{\\text{eff}} \\leftarrow N\n\nUse terminal cost V_f and constraint \\mathcal{X}_f\n\nIf finite task:\n\nN_{\\text{eff}} \\leftarrow \\min(N, \\lfloor(t_f - t)/\\Delta t\\rfloor)\n\nIf t + N_{\\text{eff}}\\Delta t = t_f: use final cost c_f\n\nOtherwise: use approximation c_T\n\nIf periodic task:\n\nN_{\\text{eff}} \\leftarrow N\n\nAdjust costs/constraints based on phase\n\nSolve optimization:\nMinimize over \\mathbf{u}_{0:N_{\\text{eff}}-1} subject to dynamics, constraints, and \\mathbf{x}_0 = \\mathbf{x}_{\\text{current}}\n\nApply receding horizon control:\n\nExtract \\mathbf{u}^*_0 from solution\n\nApply to system for duration \\Delta t\n\nMeasure new state\n\nAdvance time: t \\leftarrow t + \\Delta t\n\nEnd While \n### Computational Considerations\n\nThe receding horizon principle requires solving optimization problems in real-time, placing stringent demands on the solver. Each problem must be solved within the sampling period $\\Delta t$. If the solver requires more time, the system operates without new control updates, potentially degrading performance or stability.\n\nFortunately, successive MPC problems differ only in their initial conditions and possibly their horizons. This similarity enables warm-starting strategies where the previous solution initializes the current optimization. The standard approach shifts the previous trajectory forward by one time step and appends a nominal control at the end. This initialization typically lies close to the new optimum, dramatically reducing iteration counts.\n\nThe computational burden also depends on the horizon length $N$. Longer horizons provide better approximations to infinite-horizon problems and enable more sophisticated maneuvers, but increase problem size. The choice of $N$ balances solution quality against computational resources. For linear systems with quadratic costs, horizons of 10-50 steps often suffice. Nonlinear systems may require longer horizons to capture essential dynamics, though move-blocking and other parameterization techniques can reduce the effective number of decision variables.  ### Connection to Dynamic Programming\n\nThe receding horizon principle connects MPC to the dynamic programming framework covered in the next chapter. Each MPC optimization implicitly computes the optimal cost-to-go $V_N(\\mathbf{x})$ from the current state over the horizon. This finite-horizon value function approximates the true infinite-horizon value function that dynamic programming seeks globally.\n\nThe connection becomes clearer when we consider what MPC actually does: it solves a finite-horizon optimization problem and extracts only the first control action. The remaining $N-1$ steps of the optimal trajectory are discarded, but the terminal cost $V_f$ approximates the value function at the horizon boundary. This suggests hybrid approaches where approximate value functions from dynamic programming provide terminal costs for MPC, combining global optimality properties with local constraint handling capabilities.\n\nThis idea is what we would refer to as **bootstrapping** when working with temporal difference learning methods in reinforcement learning. In temporal difference methods like Q-learning or SARSA, bootstrapping occurs when we use our current estimate of the value function to update itself—essentially \"pulling ourselves up by our bootstraps.\" Similarly, MPC bootstraps by using its finite-horizon value function approximation (computed through optimization) to make decisions, even though this approximation may not be perfect. The terminal cost $V_f$ acts as a bootstrap target, providing a value estimate for states beyond the horizon that guides the optimization process. \n ","type":"content","url":"/mpc#the-mpc-algorithm","position":15},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"Successive Linearization and Quadratic Approximations","lvl3":"Closing the Loop by Replanning"},"type":"lvl4","url":"/mpc#successive-linearization-and-quadratic-approximations","position":16},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"Successive Linearization and Quadratic Approximations","lvl3":"Closing the Loop by Replanning"},"content":"For many regulation and tracking problems, the nonlinear dynamics and costs we encounter can be approximated locally by linear and quadratic functions. The basic idea is to linearize the system around the current operating point and approximate the cost with a quadratic form. This reduces each MPC subproblem to a quadratic program (QP), which can be solved reliably and very quickly using standard solvers.\n\nSuppose the true dynamics are nonlinear,\\mathbf{x}_{k+1} = f(\\mathbf{x}_k,\\mathbf{u}_k).\n\nAround a nominal trajectory (\\bar{\\mathbf{x}}_k,\\bar{\\mathbf{u}}_k), we take a first-order expansion:\\mathbf{x}_{k+1} \\approx f(\\bar{\\mathbf{x}}_k,\\bar{\\mathbf{u}}_k) \n+ \\mathbf{A}_k(\\mathbf{x}_k - \\bar{\\mathbf{x}}_k) \n+ \\mathbf{B}_k(\\mathbf{u}_k - \\bar{\\mathbf{u}}_k),\n\nwith Jacobians\\mathbf{A}_k = \\frac{\\partial f}{\\partial \\mathbf{x}}(\\bar{\\mathbf{x}}_k,\\bar{\\mathbf{u}}_k), \n\\qquad\n\\mathbf{B}_k = \\frac{\\partial f}{\\partial \\mathbf{u}}(\\bar{\\mathbf{x}}_k,\\bar{\\mathbf{u}}_k).\n\nSimilarly, if the stage cost is nonlinear,c(\\mathbf{x}_k,\\mathbf{u}_k),\n\nwe approximate it quadratically near the nominal point:c(\\mathbf{x}_k,\\mathbf{u}_k) \\;\\approx\\; \n\\|\\mathbf{x}_k - \\mathbf{x}_k^{\\text{ref}}\\|_{\\mathbf{Q}_k}^2 \n+ \\|\\mathbf{u}_k - \\mathbf{u}_k^{\\text{ref}}\\|_{\\mathbf{R}_k}^2,\n\nwith positive semidefinite weighting matrices \\mathbf{Q}_k and \\mathbf{R}_k.\n\nThe resulting MPC subproblem has the form\\begin{aligned}\n\\min_{\\mathbf{x}_{0:N},\\mathbf{u}_{0:N-1}} \\quad &\n\\|\\mathbf{x}_N - \\mathbf{x}_N^{\\text{ref}}\\|_{\\mathbf{P}}^2\n+ \\sum_{k=0}^{N-1} \n\\left(\n\\|\\mathbf{x}_k - \\mathbf{x}_k^{\\text{ref}}\\|_{\\mathbf{Q}_k}^2\n+ \\|\\mathbf{u}_k - \\mathbf{u}_k^{\\text{ref}}\\|_{\\mathbf{R}_k}^2\n\\right) \\\\\n\\text{s.t.} \\quad &\n\\mathbf{x}_{k+1} = \\mathbf{A}_k \\mathbf{x}_k + \\mathbf{B}_k \\mathbf{u}_k + \\mathbf{d}_k, \\\\\n& \\mathbf{u}_{\\min} \\leq \\mathbf{u}_k \\leq \\mathbf{u}_{\\max}, \\\\\n& \\mathbf{x}_{\\min} \\leq \\mathbf{x}_k \\leq \\mathbf{x}_{\\max}, \\\\\n& \\mathbf{x}_0 = \\mathbf{x}_{\\text{current}} ,\n\\end{aligned}\n\nwhere \\mathbf{d}_k = f(\\bar{\\mathbf{x}}_k,\\bar{\\mathbf{u}}_k) - \\mathbf{A}_k \\bar{\\mathbf{x}}_k - \\mathbf{B}_k \\bar{\\mathbf{u}}_k captures the local affine offset.\n\nBecause the dynamics are now linear and the cost quadratic, this optimization problem is a convex quadratic program. Quadratic programs are attractive in practice: they can be solved at kilohertz rates with mature numerical methods, making them the backbone of many real-time MPC implementations.\n\nAt each MPC step, the controller updates its linearization around the new operating point, constructs the local QP, and solves it. The process repeats, with the linear model and quadratic cost refreshed at every reoptimization. Despite the approximation, this yields a closed-loop controller that inherits the fast computation of QPs while retaining the ability to track trajectories of the underlying nonlinear system.","type":"content","url":"/mpc#successive-linearization-and-quadratic-approximations","position":17},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Theoretical Guarantees"},"type":"lvl3","url":"/mpc#theoretical-guarantees","position":18},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Theoretical Guarantees"},"content":"The finite-horizon approximation in MPC brings a new challenge: the controller cannot see consequences beyond the horizon. Without proper design, this myopia can destabilize even simple systems. The solution is to carefully encode information about the infinite-horizon problem into the finite-horizon optimization through its terminal conditions.\n\nBefore diving into the mathematics, we should first establish what “stability” means and which tasks these theoretical guarantees address, as the notion of stability varies significantly across different control objectives.","type":"content","url":"/mpc#theoretical-guarantees","position":19},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"Stability Notions Across Control Tasks","lvl3":"Theoretical Guarantees"},"type":"lvl4","url":"/mpc#stability-notions-across-control-tasks","position":20},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"Stability Notions Across Control Tasks","lvl3":"Theoretical Guarantees"},"content":"The terminal conditions provide different types of guarantees depending on the control objective. For regulation problems, where the task is to drive the state to a fixed equilibrium (\\mathbf{x}_\\mathrm{eq}, \\mathbf{u}_\\mathrm{eq}) (often shifted to the origin), the stability guarantee is asymptotic stability: starting sufficiently close to the equilibrium, we have \\mathbf{x}_k \\to \\mathbf{x}_\\mathrm{eq} while constraints remain satisfied throughout the trajectory (recursive feasibility). This requires the stage cost \\ell(\\mathbf{x},\\mathbf{u}) to be positive definite in the deviation from equilibrium.\n\nWhen tracking a constant setpoint, the task becomes following a constant reference (\\mathbf{x}_\\mathrm{ref},\\mathbf{u}_\\mathrm{ref}) that solves the steady-state equations. This problem is handled by working in error coordinates \\tilde{\\mathbf{x}}=\\mathbf{x}-\\mathbf{x}_\\mathrm{ref} and \\tilde{\\mathbf{u}}=\\mathbf{u}-\\mathbf{u}_\\mathrm{ref}, transforming the tracking problem into a regulation problem for the error system. The stability guarantee becomes asymptotic tracking, meaning \\tilde{\\mathbf{x}}_k \\to 0, again with recursive feasibility.\n\nThe terminal conditions we discuss below primarily address regulation and constant reference tracking. Time-varying tracking and economic MPC require additional techniques such as tube MPC and dissipativity theory.","type":"content","url":"/mpc#stability-notions-across-control-tasks","position":21},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"MPC with Stability Guarantees","lvl3":"Theoretical Guarantees"},"type":"lvl4","url":"/mpc#mpc-with-stability-guarantees","position":22},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"MPC with Stability Guarantees","lvl3":"Theoretical Guarantees"},"content":"To provide theoretical guarantees, the finite-horizon MPC problem is augmented with three interconnected components. The terminal cost V_f(\\mathbf{x}) approximates the cost-to-go beyond the horizon, providing a surrogate for the infinite-horizon tail that cannot be explicitly optimized. The terminal constraint set \\mathcal{X}_f defines a region where we have local knowledge of how to stabilize the system. Finally, the terminal controller \\kappa_f(\\mathbf{x}) provides a local stabilizing control law that remains valid within \\mathcal{X}_f.\n\nThese components must satisfy specific compatibility conditions to provide theoretical guarantees:\n\nRecursive Feasibility and Asymptotic Stability\n\nConsider the MPC problem with terminal cost V_f, terminal set \\mathcal{X}_f, and local controller \\kappa_f. If the following conditions hold:\n\nControl invariance: For all \\mathbf{x} \\in \\mathcal{X}_f, we have \\mathbf{f}(\\mathbf{x}, \\kappa_f(\\mathbf{x})) \\in \\mathcal{X}_f (the set is invariant) and \\mathbf{g}(\\mathbf{x}, \\kappa_f(\\mathbf{x})) \\leq \\mathbf{0} (constraints remain satisfied).\n\nLyapunov decrease: For all \\mathbf{x} \\in \\mathcal{X}_f:V_f(\\mathbf{f}(\\mathbf{x}, \\kappa_f(\\mathbf{x}))) - V_f(\\mathbf{x}) \\leq -\\ell(\\mathbf{x}, \\kappa_f(\\mathbf{x}))\n\nwhere \\ell is the stage cost.\n\nThen the MPC controller achieves recursive feasibility (if the problem is feasible at time k, it remains feasible at time k+1), asymptotic stability to the target equilibrium for regulation problems, and monotonic cost decrease along trajectories until the target is reached. \n### Why Terminal Conditions Work\n\nUnderstanding why the terminal conditions guarantee recursive feasibility and asymptotic stability requires examining what the controller actually does from one step to the next. Suppose at time $k$ the MPC optimizer finds an optimal sequence of controls $(\\mathbf{u}_0^*, \\ldots, \\mathbf{u}_{N-1}^*)$ and states $(\\mathbf{x}_0^*, \\ldots, \\mathbf{x}_N^*)$, where $\\mathbf{x}_N^* \\in \\mathcal{X}_f$. The first control $\\mathbf{u}_0^*$ is applied to the system, and the remaining plan is discarded according to the receding horizon principle.\n\nAt time $k+1$, we need a new plan starting from the updated state $\\mathbf{x}_{\\text{new}} = \\mathbf{x}_1^*$. A natural fallback strategy is to **shift** the previous plan forward by one step and **append the terminal controller** $\\boldsymbol{\\kappa}_f$ at the end, yielding controls $(\\mathbf{u}_1^*, \\ldots, \\mathbf{u}_{N-1}^*, \\boldsymbol{\\kappa}_f(\\mathbf{x}_N^*))$ with states recomputed from the dynamics starting from $\\mathbf{x}_1^*$.\n\nThis shifted plan may no longer be optimal, but the **Lyapunov decrease condition** ensures it remains feasible and leads to progress. The condition\n\n$$\nV_f(\\mathbf{f}(\\mathbf{x}, \\kappa_f(\\mathbf{x}))) - V_f(\\mathbf{x}) \\leq -\\ell(\\mathbf{x}, \\kappa_f(\\mathbf{x}))\n\\quad \\forall\\, \\mathbf{x} \\in \\mathcal{X}_f\n$$\n\nrequires that the terminal cost $V_f$ decrease faster than the stage cost accumulates when following $\\boldsymbol{\\kappa}_f$ inside $\\mathcal{X}_f$. This means the controller makes progress in both state evolution and predicted cost-to-go.\n\nThis \"one-step contractiveness\" property ensures that applying $\\kappa_f$ within the terminal set leads to value decrease. When used at the horizon's tail, this guarantees that even a non-optimal shifted trajectory results in lower overall cost, making $V_N$ behave like a Lyapunov function for regulation and tracking tasks.  ### Computing Terminal Conditions in Practice\n\nFor linear systems with quadratic costs, the terminal conditions follow naturally from LQR theory. The process begins by solving the infinite-horizon LQR problem:\n\n$$\\mathbf{P} = \\mathbf{Q} + \\mathbf{A}^T \\mathbf{P} \\mathbf{A} - \\mathbf{A}^T \\mathbf{P} \\mathbf{B}(\\mathbf{R} + \\mathbf{B}^T \\mathbf{P} \\mathbf{B})^{-1} \\mathbf{B}^T \\mathbf{P} \\mathbf{A}$$\n$$\\mathbf{K} = -(\\mathbf{R} + \\mathbf{B}^T \\mathbf{P} \\mathbf{B})^{-1} \\mathbf{B}^T \\mathbf{P} \\mathbf{A}$$\n\nThe terminal cost and controller then follow directly: $V_f(\\mathbf{x}) = \\mathbf{x}^T \\mathbf{P} \\mathbf{x}$ and $\\kappa_f(\\mathbf{x}) = \\mathbf{K}\\mathbf{x}$. \n\nConstructing the terminal set $\\mathcal{X}_f$ presents more options with varying computational complexity. The most powerful but computationally intensive approach computes the **maximal control-invariant set**: the largest set where $\\mathbf{u} = \\mathbf{K}\\mathbf{x}$ keeps the state feasible indefinitely. This involves fixed-point iterations on polytopes. A more tractable alternative uses **ellipsoidal approximations**, finding the largest $\\alpha$ such that $\\mathcal{X}_f = \\{\\mathbf{x} : \\mathbf{x}^T \\mathbf{P} \\mathbf{x} \\leq \\alpha\\}$ satisfies all constraints under $\\mathbf{u} = \\mathbf{K}\\mathbf{x}$. The most conservative but always feasible approach starts with a small safe set where constraints are satisfied and grows it until hitting constraint boundaries.\n\nFor nonlinear systems, we linearize around the equilibrium to compute $\\mathbf{P}$ and $\\mathbf{K}$, then verify the decrease condition holds locally. The terminal set becomes a neighborhood where the linear approximation remains valid.  \n### Performance Implications\n\nThe terminal conditions create inherent tradeoffs between conservatism and computational burden. Larger terminal sets $\\mathcal{X}_f$ provide greater regions of attraction and impose fewer restrictions on trajectories, but require more intensive computation. Smaller terminal sets may necessitate longer horizons to reach from typical initial conditions. Similarly, more accurate terminal costs $V_f$ provide tighter approximations of infinite-horizon costs, enabling effective control with shorter horizons.\n\nThe importance of terminal conditions diminishes as the horizon length increases. With $N \\to \\infty$, any stabilizing terminal controller suffices for stability. In practice, short horizons (N = 10-20) make terminal conditions crucial for stability, medium horizons (N = 20-50) benefit from but don't critically depend on them, while long horizons (N > 50) often omit them entirely, relying solely on horizon length for stability. ","type":"content","url":"/mpc#mpc-with-stability-guarantees","position":23},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"Suboptimality Bounds","lvl3":"Theoretical Guarantees"},"type":"lvl4","url":"/mpc#suboptimality-bounds","position":24},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"Suboptimality Bounds","lvl3":"Theoretical Guarantees"},"content":"The finite-horizon MPC value V_N(\\mathbf{x}) provides an upper bound approximation of the true infinite-horizon value V_\\infty(\\mathbf{x}). Understanding how close this approximation can be tells us about the effectiveness of short-horizon MPC.\n\nThe upper bound V_N(\\mathbf{x}) \\geq V_\\infty(\\mathbf{x}) follows immediately from the fact that MPC considers fewer control choices. The infinite-horizon controller can choose any sequence (\\mathbf{u}_0, \\mathbf{u}_1, \\mathbf{u}_2, \\ldots), while the N-horizon controller is restricted to sequences of the form (\\mathbf{u}_0, \\ldots, \\mathbf{u}_{N-1}, \\kappa_f(\\mathbf{x}_N), \\kappa_f(\\mathbf{x}_{N+1}), \\ldots) where the tail follows the fixed terminal controller. Since the infinite-horizon problem optimizes over a larger feasible set, its optimal value cannot exceed that of the finite-horizon problem.","type":"content","url":"/mpc#suboptimality-bounds","position":25},{"hierarchy":{"lvl1":"Model Predictive Control","lvl5":"Deriving the Approximation Error","lvl4":"Suboptimality Bounds","lvl3":"Theoretical Guarantees"},"type":"lvl5","url":"/mpc#deriving-the-approximation-error","position":26},{"hierarchy":{"lvl1":"Model Predictive Control","lvl5":"Deriving the Approximation Error","lvl4":"Suboptimality Bounds","lvl3":"Theoretical Guarantees"},"content":"The interesting question is bounding the approximation error \\varepsilon_N = V_N(\\mathbf{x}) - V_\\infty(\\mathbf{x}). This error represents the cost of being forced to use \\kappa_f beyond the horizon rather than continuing to optimize.\n\nLet (\\mathbf{u}_0^*, \\mathbf{u}_1^*, \\ldots) denote the infinite-horizon optimal control sequence with corresponding state trajectory (\\mathbf{x}_0^*, \\mathbf{x}_1^*, \\ldots) where \\mathbf{x}_0^* = \\mathbf{x}. The infinite-horizon cost is:V_\\infty(\\mathbf{x}) = \\sum_{k=0}^{\\infty} \\ell(\\mathbf{x}_k^*, \\mathbf{u}_k^*)\n\nNow consider what happens when we truncate this optimal sequence at horizon N and continue with the terminal controller. The cost becomes:\\tilde{V}_N(\\mathbf{x}) = \\sum_{k=0}^{N-1} \\ell(\\mathbf{x}_k^*, \\mathbf{u}_k^*) + V_f(\\mathbf{x}_N^*)\n\nwhere V_f(\\mathbf{x}_N^*) approximates the tail cost \\sum_{k=N}^{\\infty} \\ell(\\mathbf{x}_k^*, \\mathbf{u}_k^*).\n\nSince V_N(\\mathbf{x}) is the optimal N-horizon cost (which may do better than this particular truncated sequence), we have V_N(\\mathbf{x}) \\leq \\tilde{V}_N(\\mathbf{x}). The approximation error therefore satisfies:\\varepsilon_N \\leq \\tilde{V}_N(\\mathbf{x}) - V_\\infty(\\mathbf{x}) = V_f(\\mathbf{x}_N^*) - \\sum_{k=N}^{\\infty} \\ell(\\mathbf{x}_k^*, \\mathbf{u}_k^*)\n\nThis bound shows that the approximation error depends on how well the terminal cost V_f approximates the true tail cost along the infinite-horizon optimal trajectory. \n#### Exponential Convergence in the Linear-Quadratic Case\n\nFor linear systems $\\mathbf{x}_{k+1} = \\mathbf{A}\\mathbf{x}_k + \\mathbf{B}\\mathbf{u}_k$ with quadratic costs $\\ell(\\mathbf{x}, \\mathbf{u}) = \\mathbf{x}^T\\mathbf{Q}\\mathbf{x} + \\mathbf{u}^T\\mathbf{R}\\mathbf{u}$, we can compute this error exactly. When the terminal cost is the LQR cost-to-go $V_f(\\mathbf{x}) = \\mathbf{x}^T\\mathbf{P}\\mathbf{x}$, the infinite-horizon optimal trajectory satisfies $\\mathbf{x}_k^* = \\mathbf{A}_{cl}^k \\mathbf{x}$ where $\\mathbf{A}_{cl} = \\mathbf{A} + \\mathbf{B}\\mathbf{K}$ is the closed-loop matrix.\n\nThe tail cost from time $N$ onward becomes:\n$\\sum_{k=N}^{\\infty} \\ell(\\mathbf{x}_k^*, \\mathbf{u}_k^*) = \\sum_{k=N}^{\\infty} (\\mathbf{x}_k^*)^T \\mathbf{Q}_{cl} \\mathbf{x}_k^* = (\\mathbf{x}_N^*)^T \\left(\\sum_{k=0}^{\\infty} (\\mathbf{A}_{cl}^T)^k \\mathbf{Q}_{cl} \\mathbf{A}_{cl}^k\\right) \\mathbf{x}_N^*$\n\nwhere $\\mathbf{Q}_{cl} = \\mathbf{Q} + \\mathbf{K}^T\\mathbf{R}\\mathbf{K}$ captures the quadratic cost under the optimal controller $\\mathbf{u} = \\mathbf{K}\\mathbf{x}$.\n\nThe infinite sum equals $\\mathbf{P}$ by definition of the LQR solution, so the terminal cost $V_f(\\mathbf{x}_N^*) = (\\mathbf{x}_N^*)^T \\mathbf{P} \\mathbf{x}_N^*$ exactly matches the true tail cost when computed along the infinite-horizon optimal trajectory. This gives $\\varepsilon_N = 0$ when following the infinite-horizon optimal path exactly!\n\nHowever, the finite-horizon optimizer typically finds a different trajectory for the first $N$ steps, leading to a different $\\mathbf{x}_N$. The approximation error then depends on how much the finite-horizon trajectory deviates from the infinite-horizon one. Since both are optimal for their respective problems and the terminal cost provides the correct tail approximation, this deviation shrinks exponentially with horizon length at a rate determined by the eigenvalues of $\\mathbf{A}_{cl}$.\n\nSpecifically, if $\\rho(\\mathbf{A}_{cl})$ denotes the spectral radius of the closed-loop matrix, then:\n\n$\\varepsilon_N = O(\\rho(\\mathbf{A}_{cl})^N)$\n\nFor stable systems, $\\rho(\\mathbf{A}_{cl}) < 1$, yielding exponential convergence. This explains why short horizons (N = 10-30) often achieve near-optimal performance: the approximation error decreases exponentially fast, making even modest horizons highly effective for regulation and constant reference tracking tasks.\n\n#### Implications for Horizon Selection\n\nThese bounds provide practical guidance for choosing prediction horizons. The exponential convergence means that beyond a certain horizon length, further increases yield diminishing returns. The optimal horizon balances approximation accuracy against computational cost, with the break-even point typically occurring when $\\rho(\\mathbf{A}_{cl})^N$ drops below the desired tolerance level.\n\nFor systems with slow dynamics (eigenvalues close to one), longer horizons may be necessary, while systems with fast dynamics achieve good approximations with surprisingly short horizons. This analysis also explains why terminal conditions become less critical as horizons increase: the exponential decay ensures that the tail beyond any reasonable horizon contributes negligibly to the total cost.  \n### When Terminal Constraints Cause Infeasibility\n\nThe terminal constraint $\\mathbf{x}_N \\in \\mathcal{X}_f$ can make the optimization infeasible, especially for:\n- Large disturbances pushing the state far from equilibrium\n- Short horizons that cannot reach $\\mathcal{X}_f$ in time\n- Conservative terminal sets that are unnecessarily small\n\nCommon remedies:\n\n1. **Soft terminal constraints**: Replace hard constraint with penalty\n   $$\\text{minimize} \\quad V_f(\\mathbf{x}_N) + \\rho \\cdot d(\\mathbf{x}_N, \\mathcal{X}_f) + \\ldots$$\n   where $d(\\cdot, \\mathcal{X}_f)$ measures distance to the set\n\n2. **Adaptive horizons**: Extend horizon when far from $\\mathcal{X}_f$\n\n3. **Backup strategy**: If infeasible, switch to unconstrained MPC or a fallback controller, then re-enable terminal constraints once feasible\n\nThe choice depends on whether theoretical guarantees or practical performance takes priority. Many industrial implementations omit terminal constraints entirely, relying on well-tuned horizons and costs to ensure stability.\n  \n# The Landscape of MPC Variants\n\nOnce the basic idea of receding horizon control is clear, it is helpful to see how the same backbone accommodates many variations. In every case, we transcribe the continuous problem to a nonlinear program of the form\n\n$$\n\\begin{aligned}\n\\text{minimize}\\quad & c_T(\\mathbf{x}_N)+\\sum_{k=0}^{N-1} w_k\\,c(\\mathbf{x}_k,\\mathbf{u}_k) \\\\\n\\text{subject to}\\quad & \\mathbf{x}_{k+1}=\\mathbf{F}_k(\\mathbf{x}_k,\\mathbf{u}_k) \\\\\n& \\mathbf{g}(\\mathbf{x}_k,\\mathbf{u}_k)\\leq \\mathbf{0}, \\\\\n& \\mathbf{x}_{\\min}\\leq \\mathbf{x}_k\\leq \\mathbf{x}_{\\max},\\quad \\mathbf{u}_{\\min}\\leq \\mathbf{u}_k\\leq \\mathbf{u}_{\\max}, \\\\\n& \\mathbf{x}_0=\\mathbf{x}_{\\text{current}} ,\n\\end{aligned}\n$$\n\nwith \\$\\mathbf{F}\\_k\\$ the chosen discretization of the dynamics and \\$w\\_k\\$ the quadrature weights. From this skeleton, we can construct several families of MPC.\n\nIn **tracking MPC**, the stage and terminal costs are quadratic penalties on deviation from a reference trajectory,\n\n$$\nc(\\mathbf{x}_k,\\mathbf{u}_k)=\\|\\mathbf{x}_k-\\mathbf{x}_k^{\\text{ref}}\\|_{\\mathbf{Q}}^2+\\|\\mathbf{u}_k-\\mathbf{u}_k^{\\text{ref}}\\|_{\\mathbf{R}}^2 , \\qquad c_T(\\mathbf{x}_N)=\\|\\mathbf{x}_N-\\mathbf{x}_N^{\\text{ref}}\\|_{\\mathbf{P}}^2 .\n$$\n\nThis is the industrial workhorse, ensuring that the system follows a desired profile within limits.\n\nIn **regulatory MPC**, the reference is fixed at an equilibrium \\$(\\mathbf{x}^e,\\mathbf{u}^e)\\$, and the quadratic penalty encourages return to this point. Terminal constraints are often added so that stability can be guaranteed.\n\nIn **economic MPC**, the quadratic structure disappears altogether. Instead, the cost encodes economic performance,\n\n$$\nc(\\mathbf{x}_k,\\mathbf{u}_k)=c_{\\text{econ}}(\\mathbf{x}_k,\\mathbf{u}_k),\n$$\n\nfor instance energy cost, profit, or resource efficiency. The optimization then steers the system not toward a setpoint but toward economically optimal regimes.\n\nWhen uncertainty is represented by bounded sets, one arrives at **robust MPC**, which seeks controls that satisfy the constraints for all admissible disturbances. The resulting NLP has a min–max structure. A tractable alternative is tube MPC, where the nominal optimization is carried out with tightened constraints to guarantee feasibility of the true system under a disturbance feedback law.\n\nIf uncertainty is stochastic, the formulation turns into **stochastic MPC**, where the objective is the expected cost and the constraints are imposed with high probability,\n\n$$\n\\mathbb{P}\\!\\left[\\mathbf{g}(\\mathbf{x}_k,\\mathbf{u}_k)\\leq \\mathbf{0}\\right]\\geq 1-\\varepsilon .\n$$\n\nScenario-based versions replace expectations by a sampled, deterministic problem.\n\nSome systems require discrete choices, such as switching devices on or off. **Hybrid MPC** introduces integer variables into the transcription, producing a mixed-integer NLP that can handle such logic.\n\nFor large networks of subsystems, **distributed MPC** coordinates several local predictive controllers that optimize their own subsystems while communicating through coupling constraints.\n\nFinally, in settings where models are uncertain or slowly drifting, one finds **adaptive or learning-based MPC**, which uses parameter estimation or machine learning to update the model \\$\\mathbf{F}\\_k(\\cdot;\\theta\\_t)\\$ and possibly the cost function. The optimization step remains the same, but the model evolves as more data are collected.\n\nThese formulations illustrate that MPC is less a single algorithm than a recipe. The scaffolding is always the same: finite horizon prediction, state and input constraints, and receding horizon application of the control. What changes from one variant to another is the cost function, the way uncertainty is treated, the presence of discrete decisions, or the architecture across multiple agents. ","type":"content","url":"/mpc#deriving-the-approximation-error","position":27},{"hierarchy":{"lvl1":"Model Predictive Control","lvl2":"The Landscape of MPC Variants"},"type":"lvl2","url":"/mpc#the-landscape-of-mpc-variants","position":28},{"hierarchy":{"lvl1":"Model Predictive Control","lvl2":"The Landscape of MPC Variants"},"content":"Once the basic idea of receding-horizon control is clear, it is helpful to see how the same backbone accommodates many variations. In every case, we transcribe the continuous-time optimal control problem into a nonlinear program of the form\\begin{aligned}\n    \\text{minimize} \\quad & c(\\mathbf{x}_N) + \\sum_{k=0}^{N-1} w_k\\,c(\\mathbf{x}_k, \\mathbf{u}_k) \\\\\n    \\text{subject to} \\quad & \\mathbf{x}_{k+1} = \\mathbf{F}_k(\\mathbf{x}_k, \\mathbf{u}_k) \\\\\n                            & \\mathbf{g}(\\mathbf{x}_k, \\mathbf{u}_k) \\leq \\mathbf{0} \\\\\n                            & \\mathbf{x}_{\\min} \\leq \\mathbf{x}_k \\leq \\mathbf{x}_{\\max} \\\\\n                            & \\mathbf{u}_{\\min} \\leq \\mathbf{u}_k \\leq \\mathbf{u}_{\\max} \\\\\n    \\text{given} \\quad & \\mathbf{x}_0 = \\hat{\\mathbf{x}}(t) \\enspace .\n\\end{aligned}\n\nThe components in this NLP come from discretizing the continuous-time problem with a fixed horizon [t, t+T] and step size \\Delta t. The stage weights w_k and discrete dynamics \\mathbf{F}_k are determined by the choice of quadrature and integration scheme. With this blueprint in place, the rest is a matter of interpretation: how we define the cost, how we handle uncertainty, how we treat constraints, and what structure we exploit.","type":"content","url":"/mpc#the-landscape-of-mpc-variants","position":29},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Tracking MPC","lvl2":"The Landscape of MPC Variants"},"type":"lvl3","url":"/mpc#tracking-mpc","position":30},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Tracking MPC","lvl2":"The Landscape of MPC Variants"},"content":"The most common setup is reference tracking. Here, we are given time-varying target trajectories (\\mathbf{x}_k^{\\text{ref}}, \\mathbf{u}_k^{\\text{ref}}), and the controller’s job is to keep the system close to these. The cost is typically quadratic:\\begin{aligned}\n    c(\\mathbf{x}_k, \\mathbf{u}_k) &= \\| \\mathbf{x}_k - \\mathbf{x}_k^{\\text{ref}} \\|_{\\mathbf{Q}}^2 + \\| \\mathbf{u}_k - \\mathbf{u}_k^{\\text{ref}} \\|_{\\mathbf{R}}^2 \\\\\n    c(\\mathbf{x}_N) &= \\| \\mathbf{x}_N - \\mathbf{x}_N^{\\text{ref}} \\|_{\\mathbf{P}}^2 \\enspace .\n\\end{aligned}\n\nWhen dynamics are linear and constraints are polyhedral, this yields a convex quadratic program at each time step.","type":"content","url":"/mpc#tracking-mpc","position":31},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Regulatory MPC","lvl2":"The Landscape of MPC Variants"},"type":"lvl3","url":"/mpc#regulatory-mpc","position":32},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Regulatory MPC","lvl2":"The Landscape of MPC Variants"},"content":"In regulation tasks, we aim to bring the system back to an equilibrium point (\\mathbf{x}^e, \\mathbf{u}^e), typically in the presence of disturbances. This is simply tracking MPC with constant references:\\begin{aligned}\n    c(\\mathbf{x}_k, \\mathbf{u}_k) &= \\| \\mathbf{x}_k - \\mathbf{x}^e \\|_{\\mathbf{Q}}^2 + \\| \\mathbf{u}_k - \\mathbf{u}^e \\|_{\\mathbf{R}}^2 \\\\\n    c(\\mathbf{x}_N) &= \\| \\mathbf{x}_N - \\mathbf{x}^e \\|_{\\mathbf{P}}^2 \\enspace .\n\\end{aligned}\n\nTo guarantee stability, it is common to include a terminal constraint \\mathbf{x}_N \\in \\mathcal{X}_f, where \\mathcal{X}_f is a control-invariant set under a known feedback law.","type":"content","url":"/mpc#regulatory-mpc","position":33},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Economic MPC","lvl2":"The Landscape of MPC Variants"},"type":"lvl3","url":"/mpc#economic-mpc","position":34},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Economic MPC","lvl2":"The Landscape of MPC Variants"},"content":"Not all systems operate around a reference. Sometimes the goal is to optimize a true economic objective (eg. energy cost, revenue, efficiency) directly. This gives rise to economic MPC, where the cost functions reflect real operational performance:c(\\mathbf{x}_k, \\mathbf{u}_k) = c_{\\text{op}}(\\mathbf{x}_k, \\mathbf{u}_k), \\qquad\nc(\\mathbf{x}_N) = c_{\\text{op},T}(\\mathbf{x}_N) \\enspace .\n\nThere is no reference trajectory here. The cost function determines the optimal behavior. In this setting, standard stability arguments no longer apply automatically, and one must be careful to add terminal penalties or constraints that ensure the closed-loop system remains well-behaved.","type":"content","url":"/mpc#economic-mpc","position":35},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Robust MPC","lvl2":"The Landscape of MPC Variants"},"type":"lvl3","url":"/mpc#robust-mpc","position":36},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Robust MPC","lvl2":"The Landscape of MPC Variants"},"content":"Some systems are exposed to external disturbances or small errors in the model. In those cases, we want the controller to make decisions that will still work no matter what happens, as long as the disturbances stay within some known bounds. This is the idea behind robust MPC.\n\nInstead of planning a single trajectory, the controller plans a “nominal” path (what would happen in the absence of any disturbance) and then adds a feedback correction to react to whatever disturbances actually occur. This looks like:\\mathbf{u}_k = \\bar{\\mathbf{u}}_k + \\mathbf{K} (\\mathbf{x}_k - \\bar{\\mathbf{x}}_k) \\enspace ,\n\nwhere \\bar{\\mathbf{u}}_k is the planned input and \\mathbf{K} is a feedback gain that pulls the system back toward the nominal path if it deviates.\n\nBecause we know the worst-case size of the disturbance, we can estimate how far the real state might drift from the plan, and “shrink” the constraints accordingly. The result is that the nominal plan is kept safely away from constraint boundaries, so even if the system gets pushed around, it stays inside limits. This is often called tube MPC because the true trajectory stays inside a tube around the nominal one.\n\nThe main benefit is that we can handle uncertainty without solving a complicated worst-case optimization at every time step. All the uncertainty is accounted for in the design of the feedback \\mathbf{K} and the tightened constraints.","type":"content","url":"/mpc#robust-mpc","position":37},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Stochastic MPC","lvl2":"The Landscape of MPC Variants"},"type":"lvl3","url":"/mpc#stochastic-mpc","position":38},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Stochastic MPC","lvl2":"The Landscape of MPC Variants"},"content":"If disturbances are random rather than adversarial, a natural goal is to optimize expected cost while enforcing constraints probabilistically. This gives rise to stochastic MPC, in which:\n\nThe cost becomes an expectation:\\mathbb{E} \\left[ c(\\mathbf{x}_N) + \\sum_{k=0}^{N-1} w_k\\, c(\\mathbf{x}_k, \\mathbf{u}_k) \\right]\n\nConstraints are allowed to be violated with small probability:\\mathbb{P}[\\mathbf{g}(\\mathbf{x}_k, \\mathbf{u}_k) \\leq \\mathbf{0}] \\geq 1 - \\varepsilon\n\nIn practice, expectations are approximated using a finite set of disturbance scenarios drawn ahead of time. For each scenario, the system dynamics are simulated forward using the same control inputs \\mathbf{u}_k, which are shared across all scenarios to respect non-anticipativity. The result is a single deterministic optimization problem with multiple parallel copies of the dynamics, one per sampled future. This retains the standard MPC structure, with only moderate growth in problem size.\n\nDespite appearances, this is not dynamic programming. There is no value function or tree of all possible paths. There is only a finite set of futures chosen a priori, and optimized over directly. This scenario-based approach is common in energy systems such as hydro scheduling, where inflows are uncertain but sample trajectories can be generated from forecasts.\n\nRisk constraints are typically enforced across all scenarios or encoded using risk measures like CVaR. For example, one might penalize violations that occur in the worst (1 - \\alpha)\\% of samples, while still optimizing expected performance overall.","type":"content","url":"/mpc#stochastic-mpc","position":39},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Hybrid and Mixed-Integer MPC","lvl2":"The Landscape of MPC Variants"},"type":"lvl3","url":"/mpc#hybrid-and-mixed-integer-mpc","position":40},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Hybrid and Mixed-Integer MPC","lvl2":"The Landscape of MPC Variants"},"content":"When systems involve discrete switches  (eg. on/off valves, mode selection, or combinatorial logic) the MPC problem must include integer or binary variables. These show up in constraints like\\boldsymbol{\\delta}_k \\in \\{0,1\\}^m, \\qquad \\mathbf{u}_k \\in \\mathcal{U}(\\boldsymbol{\\delta}_k)\n\nalong with mode-dependent dynamics and costs. The resulting formulation is a mixed-integer nonlinear program (MINLP). The receding-horizon idea is the same, but each solve is more expensive due to the combinatorial nature of the decision space.","type":"content","url":"/mpc#hybrid-and-mixed-integer-mpc","position":41},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Distributed and Decentralized MPC","lvl2":"The Landscape of MPC Variants"},"type":"lvl3","url":"/mpc#distributed-and-decentralized-mpc","position":42},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Distributed and Decentralized MPC","lvl2":"The Landscape of MPC Variants"},"content":"Large-scale systems often consist of interacting subsystems. Distributed MPC decomposes the global NLP into smaller ones that run in parallel, with coordination constraints enforcing consistency across shared variables:\\sum_{i} \\mathbf{H}^i \\mathbf{z}^i_k = \\mathbf{0} \\qquad \\text{(coupling constraint)}\n\nEach subsystem solves a local problem over its own state and input variables, then exchanges information with neighbors. Coordination can be done via primal–dual methods, ADMM, or consensus schemes, but each local block looks like a standard MPC problem.","type":"content","url":"/mpc#distributed-and-decentralized-mpc","position":43},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Adaptive and Learning-Based MPC","lvl2":"The Landscape of MPC Variants"},"type":"lvl3","url":"/mpc#adaptive-and-learning-based-mpc","position":44},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Adaptive and Learning-Based MPC","lvl2":"The Landscape of MPC Variants"},"content":"In practice, we may not know the true model \\mathbf{F}_k or cost function c precisely. In adaptive MPC, these are updated online from data:\\mathbf{x}_{k+1} = \\mathbf{F}_k(\\mathbf{x}_k, \\mathbf{u}_k; \\boldsymbol{\\theta}_t), \\qquad\nc(\\mathbf{x}_k, \\mathbf{u}_k) = c(\\mathbf{x}_k, \\mathbf{u}_k; \\boldsymbol{\\phi}_t)\n\nThe parameters \\boldsymbol{\\theta}_t and \\boldsymbol{\\phi}_t are learned in real time. When combined with policy distillation, value approximation, or trajectory imitation, this leads to overlaps with reinforcement learning where the MPC solutions act as supervision for a reactive policy.","type":"content","url":"/mpc#adaptive-and-learning-based-mpc","position":45},{"hierarchy":{"lvl1":"Model Predictive Control","lvl2":"Robustness in Real-Time MPC"},"type":"lvl2","url":"/mpc#robustness-in-real-time-mpc","position":46},{"hierarchy":{"lvl1":"Model Predictive Control","lvl2":"Robustness in Real-Time MPC"},"content":"The trajectory optimization methods we have studied assume perfect models and deterministic dynamics. In practice, however, MPC controllers must operate in environments where models are approximate, disturbances are unpredictable, and computational resources are limited. The mathematical elegance of optimal control must always yield to the engineering reality of robust operation as perfect optimality is less important than reliable operation. This philosophy permeates industrial MPC applications. A controller that achieves 95% performance 100% of the time is superior to one that achieves 100% performance 95% of the time and fails catastrophically the remaining 5%. Airlines accept suboptimal fuel consumption over missed approaches, power grids tolerate efficiency losses to prevent blackouts, and chemical plants sacrifice yield for safety. By designing for failure, we want to to create MPC systems that degrade gracefully rather than fail catastrophically, maintaining safety and stability even when the impossible is asked of them.","type":"content","url":"/mpc#robustness-in-real-time-mpc","position":47},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Example: Wind Farm Yield Optimization","lvl2":"Robustness in Real-Time MPC"},"type":"lvl3","url":"/mpc#example-wind-farm-yield-optimization","position":48},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Example: Wind Farm Yield Optimization","lvl2":"Robustness in Real-Time MPC"},"content":"Consider a wind farm where MPC controllers coordinate individual turbines to maximize overall power production while minimizing wake interference. Each turbine can adjust both its thrust coefficient (through blade pitch) and yaw angle to redirect its wake away from downstream turbines. At time t_k, the MPC controller solves the optimization problem:\\begin{aligned}\n\\min_{\\mathbf{u}_{0:N-1}} \\quad & \\sum_{i=0}^{N-1} \\|\\mathbf{x}_i - \\mathbf{x}_i^{\\text{ref}}\\|_{\\mathbf{Q}}^2 + \\|\\mathbf{u}_i\\|_{\\mathbf{R}}^2 \\\\\n\\text{s.t.} \\quad & \\mathbf{x}_{i+1} = \\mathbf{f}(\\mathbf{x}_i, \\mathbf{u}_i) \\\\\n& \\mathbf{x}_i \\in \\mathcal{X}_{\\text{safe}} \\\\\n& \\|\\mathbf{u}_i\\|_\\infty \\leq u_{\\max} \\\\\n& \\mathbf{x}_0 = \\mathbf{x}_{\\text{current}}\n\\end{aligned}\n\nNow suppose an unexpected wind direction change occurs, shifting the incoming wind vector by 30 degrees. The current state \\mathbf{x}_{\\text{current}} reflects wake patterns that no longer align with the new wind direction, and the optimizer discovers that no feasible trajectory exists that can redirect all wakes appropriately within the physical limits of yaw rate and thrust adjustment. The solver reports infeasibility.\n\nThis scenario illustrates the fundamental challenge of real-time MPC: constraint incompatibility. When disturbances push the system into states from which recovery appears impossible, or when reference trajectories demand physically impossible maneuvers, the intersection of all constraint sets becomes empty. Model mismatch compounds this problem as prediction errors accumulate over the horizon.\n\nEven when feasible solutions exist, computational constraints can prevent their discovery. A control loop running at 100 Hz allows only 10 milliseconds per iteration. If the solver requires 15 milliseconds to converge, we face an impossible choice: delay the control action and risk destabilizing the system, or apply an unconverged iterate that may violate critical constraints.\n\nA third failure mode involves numerical instabilities: ill-conditioned matrices, rank deficiency, or division by zero in the linear algebra routines. These failures are particularly problematic because they occur sporadically, triggered by specific state configurations that create near-singular conditions in the optimization problem.","type":"content","url":"/mpc#example-wind-farm-yield-optimization","position":49},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Softening Constraints Through Slack Variables","lvl2":"Robustness in Real-Time MPC"},"type":"lvl3","url":"/mpc#softening-constraints-through-slack-variables","position":50},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Softening Constraints Through Slack Variables","lvl2":"Robustness in Real-Time MPC"},"content":"The first approach to handling infeasibility recognizes that not all constraints carry equal importance. A chemical reactor’s temperature must never exceed the runaway threshold: this is a hard constraint that cannot be violated. However, maintaining temperature within an optimal efficiency band is merely desirable. This can be treated as a soft constraint that we prefer to satisfy but can relax when necessary.\n\nThis hierarchy motivates reformulating the optimization problem using slack variables:\\begin{aligned}\n\\min_{\\mathbf{u}, \\boldsymbol{\\epsilon}} \\quad & \\sum_{i=0}^{N-1} \\|\\mathbf{x}_i - \\mathbf{x}_i^{\\text{ref}}\\|_{\\mathbf{Q}}^2 + \\|\\mathbf{u}_i\\|_{\\mathbf{R}}^2 + \\boldsymbol{\\rho}^T \\boldsymbol{\\epsilon}_i \\\\\n\\text{s.t.} \\quad & \\mathbf{x}_{i+1} = \\mathbf{f}(\\mathbf{x}_i, \\mathbf{u}_i) \\\\\n& \\mathbf{g}_{\\text{hard}}(\\mathbf{x}_i, \\mathbf{u}_i) \\leq \\mathbf{0} \\\\\n& \\mathbf{g}_{\\text{soft}}(\\mathbf{x}_i, \\mathbf{u}_i) \\leq \\boldsymbol{\\epsilon}_i \\\\\n& \\boldsymbol{\\epsilon}_i \\geq \\mathbf{0}\n\\end{aligned}\n\nThe penalty weights \\boldsymbol{\\rho} encode our priorities. Safety constraints might use \\rho_j = 10^6, while comfort constraints use \\rho_j = 1. This reformulated problem is always feasible as long as the hard constraints alone admit a solution. That is: we can always make the slack variables \\boldsymbol{\\epsilon} sufficiently large to satisfy the soft constraints.\n\nRather than treating constraints as binary hard/soft categories, we can establish a constraint hierarchy that enables graceful degradation:\\begin{aligned}\n\\text{Safety:} \\quad & T_{\\text{reactor}} \\leq T_{\\text{runaway}} - 10 \\quad & \\rho = \\infty \\text{ (hard)} \\\\\n\\text{Equipment:} \\quad & 0 \\leq u_{\\text{valve}} \\leq 100 \\quad & \\rho = 10^4 \\\\\n\\text{Efficiency:} \\quad & T_{\\text{optimal}} - 5 \\leq T \\leq T_{\\text{optimal}} + 5 \\quad & \\rho = 10^2 \\\\\n\\text{Comfort:} \\quad & |T - T_{\\text{setpoint}}| \\leq 1 \\quad & \\rho = 1\n\\end{aligned}\n\nAs conditions deteriorate, the controller abandons objectives in reverse priority order, maintaining safety even when optimality becomes impossible.","type":"content","url":"/mpc#softening-constraints-through-slack-variables","position":51},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Feasibility Restoration","lvl2":"Robustness in Real-Time MPC"},"type":"lvl3","url":"/mpc#feasibility-restoration","position":52},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Feasibility Restoration","lvl2":"Robustness in Real-Time MPC"},"content":"When even soft constraints prove insufficient (perhaps due to catastrophic solver failure or corrupted problem structure) we need feasibility restoration that finds any feasible point regardless of optimality:\\begin{aligned}\n\\min_{\\mathbf{u}, \\mathbf{s}} \\quad & \\|\\mathbf{s}\\|_1 \\\\\n\\text{s.t.} \\quad & \\mathbf{x}_{i+1} = \\mathbf{f}(\\mathbf{x}_i, \\mathbf{u}_i) + \\mathbf{s}_i \\\\\n& \\mathbf{x}_{\\min} - \\mathbf{s}_{x,i} \\leq \\mathbf{x}_i \\leq \\mathbf{x}_{\\max} + \\mathbf{s}_{x,i} \\\\\n& \\mathbf{u}_{\\min} \\leq \\mathbf{u}_i \\leq \\mathbf{u}_{\\max} \\\\\n& \\mathbf{s} \\geq \\mathbf{0}\n\\end{aligned}\n\nThis formulation temporarily relaxes even the dynamics constraints, finding the “least infeasible” solution. It answers the question: if we must violate something, what is the minimal violation required? Once feasibility is restored, we can warm-start the original problem from this point.","type":"content","url":"/mpc#feasibility-restoration","position":53},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Reference Governors","lvl2":"Robustness in Real-Time MPC"},"type":"lvl3","url":"/mpc#reference-governors","position":54},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Reference Governors","lvl2":"Robustness in Real-Time MPC"},"content":"Rather than reacting to infeasibility after it occurs, we can prevent it by filtering references through a reference governor. Consider an aircraft following waypoints. Instead of passing waypoints directly to the MPC, the governor asks: what is the closest approachable reference from our current state?\\mathbf{r}_{\\text{filtered}} = \\arg\\max_{\\kappa \\in [0,1]} \\kappa \\quad \\text{s.t. MPC}(\\mathbf{x}_{\\text{current}}, \\kappa \\mathbf{r}_{\\text{desired}} + (1-\\kappa)\\mathbf{x}_{\\text{current}}) \\text{ is feasible}\n\nThe governor performs a line search between the current state (always feasible since staying put requires no action) and the desired reference (potentially infeasible). This guarantees the MPC always receives feasible problems while making maximum progress toward the goal.\n\nFor computational efficiency, we can pre-compute the maximal output admissible set:\\mathcal{O}_\\infty = \\{\\mathbf{r} : \\exists \\text{ feasible trajectory from } \\mathbf{x} \\text{ to } \\mathbf{r} \\text{ respecting all constraints}\\}\n\nOnline, the governor simply projects the desired reference onto \\mathcal{O}_\\infty.","type":"content","url":"/mpc#reference-governors","position":55},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Backup Controllers","lvl2":"Robustness in Real-Time MPC"},"type":"lvl3","url":"/mpc#backup-controllers","position":56},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Backup Controllers","lvl2":"Robustness in Real-Time MPC"},"content":"When MPC fails entirely (due to solver crashes, timeouts, or numerical failures) we need backup controllers that require minimal computation while guaranteeing stability and keeping the system away from dangerous regions.\n\nThe standard approach uses a pre-computed local LQR controller around the equilibrium:\\mathbf{K}_{\\text{LQR}}, \\mathbf{P} = \\text{LQR}(\\mathbf{A}, \\mathbf{B}, \\mathbf{Q}, \\mathbf{R})\n\nwhere (\\mathbf{A}, \\mathbf{B}) are the linearized dynamics at equilibrium. When MPC fails:\\mathbf{u}_{\\text{backup}} = \\begin{cases}\n\\mathbf{K}_{\\text{LQR}}(\\mathbf{x} - \\mathbf{x}_{\\text{eq}}) & \\text{if } \\mathbf{x} \\in \\mathcal{X}_{\\text{LQR}} \\\\\n\\mathbf{u}_{\\text{safe}} & \\text{otherwise}\n\\end{cases}\n\nThe region \\mathcal{X}_{\\text{LQR}} = \\{\\mathbf{x} : (\\mathbf{x} - \\mathbf{x}_{\\text{eq}})^T \\mathbf{P} (\\mathbf{x} - \\mathbf{x}_{\\text{eq}}) \\leq \\alpha\\} represents the largest invariant set where LQR is guaranteed to work.","type":"content","url":"/mpc#backup-controllers","position":57},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Cascade Architectures","lvl2":"Robustness in Real-Time MPC"},"type":"lvl3","url":"/mpc#cascade-architectures","position":58},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Cascade Architectures","lvl2":"Robustness in Real-Time MPC"},"content":"Production MPC systems rarely rely on a single solver. Instead, they implement a cascade of increasingly conservative controllers that trade optimality for reliability:def get_control(self, x, time_budget):\n    \"\"\"\n    Multi-level cascade for robust real-time control\n    \"\"\"\n    time_remaining = time_budget\n    \n    # Level 1: Full nonlinear MPC\n    if time_remaining > 5e-3:  # 5ms minimum\n        try:\n            u, solve_time = self.solve_nmpc(x, time_remaining)\n            if converged:\n                return u\n        except:\n            pass\n        time_remaining -= solve_time\n    \n    # Level 2: Simplified linear MPC\n    if time_remaining > 1e-3:  # 1ms minimum\n        try:\n            # Linearize around current state\n            A, B = self.linearize_dynamics(x)\n            u, solve_time = self.solve_lmpc(x, A, B, time_remaining)\n            return u\n        except:\n            pass\n        time_remaining -= solve_time\n    \n    # Level 3: Explicit MPC lookup\n    if time_remaining > 1e-4:  # 0.1ms minimum\n        region = self.find_critical_region(x)\n        if region is not None:\n            return self.explicit_control_law[region](x)\n    \n    # Level 4: LQR backup\n    if self.in_lqr_region(x):\n        return self.K_lqr @ (x - self.x_eq)\n    \n    # Level 5: Emergency safe mode\n    return self.emergency_stop(x)\n\nEach level trades optimality for reliability: Level 1 provides optimal but computationally expensive control, Level 2 offers suboptimal but faster solutions, Level 3 provides pre-computed instant evaluation, Level 4 ensures stabilizing control without tracking, and Level 5 implements safe shutdown.\n\nEven when using backup controllers, we can maintain solution continuity through persistent warm-starting:\\begin{aligned}\n\\mathbf{z}_{\\text{warm}}^{(k+1)} = \\begin{cases}\n\\text{shift}(\\mathbf{z}^{(k)}) & \\text{if MPC succeeded at time } k \\\\\n\\text{lift}(\\mathbf{u}_{\\text{backup}}^{(k)}) & \\text{if backup controller used} \\\\\n\\text{propagate}(\\mathbf{z}_{\\text{warm}}^{(k)}) & \\text{if maintaining virtual solution}\n\\end{cases}\n\\end{aligned}\n\nThe shift operation takes a successful MPC solution and moves it forward by one time step, appending a terminal action: [\\mathbf{u}_1^{(k)}, \\mathbf{u}_2^{(k)}, \\ldots, \\mathbf{u}_{N-1}^{(k)}, \\kappa_f(\\mathbf{x}_N^{(k)})]. This shifted sequence provides natural temporal continuity for the next optimization.\n\nWhen MPC fails and backup control is applied, the lift operation extends the single backup action \\mathbf{u}_{\\text{backup}}^{(k)} into a full horizon-length sequence, either by repetition or by simulating the backup controller forward. This creates a reasonable warm-start guess from limited information.\n\nThe propagate operation maintains a “virtual” trajectory by continuing to evolve the previous solution as if it were still being executed, even when the actual system follows backup control. This forward simulation keeps the warm-start temporally aligned and relevant for when MPC recovers.","type":"content","url":"/mpc#cascade-architectures","position":59},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Example: Chemical Reactor Control Under Failure","lvl2":"Robustness in Real-Time MPC"},"type":"lvl3","url":"/mpc#example-chemical-reactor-control-under-failure","position":60},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Example: Chemical Reactor Control Under Failure","lvl2":"Robustness in Real-Time MPC"},"content":"Consider a continuous stirred tank reactor (CSTR) where an exothermic reaction must be controlled:\\begin{aligned}\n\\dot{C}_A &= \\frac{q}{V}(C_{A,in} - C_A) - k_0 e^{-E/RT} C_A \\\\\n\\dot{T} &= \\frac{q}{V}(T_{in} - T) + \\frac{\\Delta H}{\\rho c_p} k_0 e^{-E/RT} C_A - \\frac{UA}{\\rho c_p V}(T - T_c)\n\\end{aligned}\n\nThe MPC must maintain temperature below the runaway threshold T_{\\text{runaway}} while maximizing conversion. Under normal operation, it solves:\\begin{aligned}\n\\min \\quad & -C_A(t_f) + \\int_0^{t_f} \\|T - T_{\\text{optimal}}\\|^2 dt \\\\\n\\text{s.t.} \\quad & T \\leq T_{\\text{runaway}} - \\Delta T_{\\text{safety}} \\\\\n& q_{\\min} \\leq q \\leq q_{\\max}\n\\end{aligned}\n\nWhen the cooling system partially fails, T_c suddenly increases. The MPC cannot maintain T_{\\text{optimal}} within safety limits. The cascade activates: soft constraints allow T to exceed T_{\\text{optimal}} with penalty, the reference governor reduces the production target C_{A,\\text{target}}, and if still infeasible, the backup controller switches to maximum cooling q = q_{\\max}. If temperature approaches runaway, emergency shutdown stops the feed with q = 0.","type":"content","url":"/mpc#example-chemical-reactor-control-under-failure","position":61},{"hierarchy":{"lvl1":"Model Predictive Control","lvl2":"Computational Efficiency via Parametric Programming"},"type":"lvl2","url":"/mpc#computational-efficiency-via-parametric-programming","position":62},{"hierarchy":{"lvl1":"Model Predictive Control","lvl2":"Computational Efficiency via Parametric Programming"},"content":"Real-time model predictive control places strict limits on computation. In applications such as adaptive optics, the controller must run at kilohertz rates. A sampling frequency of 1000 Hz allows only one millisecond per step to compute and apply a control input. This makes efficiency a first-class concern.\n\nThe structure of MPC lends itself naturally to optimization reuse. Each time step requires solving a problem with the same dynamics and constraints. Only the initial state, forecasts, or reference signals change. Instead of treating each instance as a new problem, we can frame MPC as a parametric optimization problem and focus on how the solution evolves with the parameter.","type":"content","url":"/mpc#computational-efficiency-via-parametric-programming","position":63},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"General Framework: Parametric Optimization","lvl2":"Computational Efficiency via Parametric Programming"},"type":"lvl3","url":"/mpc#general-framework-parametric-optimization","position":64},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"General Framework: Parametric Optimization","lvl2":"Computational Efficiency via Parametric Programming"},"content":"We begin with a general optimization problem indexed by a parameter \\boldsymbol{\\theta} \\in \\Theta \\subset \\mathbb{R}^p:\\begin{aligned}\n\\min_{\\mathbf{x} \\in \\mathbb{R}^n} \\quad & f(\\mathbf{x}; \\boldsymbol{\\theta}) \\\\\n\\text{s.t.} \\quad & \\mathbf{g}(\\mathbf{x}; \\boldsymbol{\\theta}) \\le \\mathbf{0}, \\\\\n& \\mathbf{h}(\\mathbf{x}; \\boldsymbol{\\theta}) = \\mathbf{0}.\n\\end{aligned}\n\nFor each value of \\boldsymbol{\\theta}, we obtain a concrete optimization problem. The goal is to understand how the optimizer \\mathbf{x}^\\star(\\boldsymbol{\\theta}) and value functionv(\\boldsymbol{\\theta}) := \\inf\\{\\, f(\\mathbf{x}; \\boldsymbol{\\theta}) : \\mathbf{x} \\text{ feasible at } \\boldsymbol{\\theta}\\,\\}\n\ndepend on \\boldsymbol{\\theta}.\n\nWhen the problem is smooth and regular, the Karush–Kuhn–Tucker (KKT) conditions characterize optimality:\\begin{aligned}\n\\nabla_{\\mathbf{x}} f(\\mathbf{x}; \\boldsymbol{\\theta})\n+ \\nabla_{\\mathbf{x}} \\mathbf{g}(\\mathbf{x}; \\boldsymbol{\\theta})^\\top \\boldsymbol{\\lambda}\n+ \\nabla_{\\mathbf{x}} \\mathbf{h}(\\mathbf{x}; \\boldsymbol{\\theta})^\\top \\boldsymbol{\\nu} &= 0, \\\\\n\\mathbf{g}(\\mathbf{x}; \\boldsymbol{\\theta}) \\le 0, \\quad\n\\boldsymbol{\\lambda} \\ge 0, \\quad\n\\lambda_i g_i(\\mathbf{x}; \\boldsymbol{\\theta}) &= 0, \\\\\n\\mathbf{h}(\\mathbf{x}; \\boldsymbol{\\theta}) &= 0.\n\\end{aligned}\n\nIf the active set remains fixed over changes in \\boldsymbol{\\theta}, the implicit function theorem ensures that the mappings\\boldsymbol{\\theta} \\mapsto \\mathbf{x}^\\star(\\boldsymbol{\\theta}), \\quad\n\\boldsymbol{\\theta} \\mapsto \\boldsymbol{\\lambda}^\\star(\\boldsymbol{\\theta}), \\quad\n\\boldsymbol{\\theta} \\mapsto \\boldsymbol{\\nu}^\\star(\\boldsymbol{\\theta})\n\nare differentiable.\n\nIn linear and quadratic programming, this structure becomes even more tractable. Consider a linear program with affine dependence on \\boldsymbol{\\theta}:\\min_{\\mathbf{x}} \\ \\mathbf{c}(\\boldsymbol{\\theta})^\\top \\mathbf{x}\n\\quad \\text{s.t.} \\quad \\mathbf{A}(\\boldsymbol{\\theta})\\mathbf{x} \\le \\mathbf{b}(\\boldsymbol{\\theta}).\n\nEach active set determines a basis and thus a region in \\Theta where the solution is affine in \\boldsymbol{\\theta}. The feasible parameter space is partitioned into polyhedral regions, each with its own affine law.\n\nSimilarly, in strictly convex quadratic programs\\min_{\\mathbf{x}} \\ \\tfrac{1}{2} \\mathbf{x}^\\top \\mathbf{H} \\mathbf{x} + \\mathbf{q}(\\boldsymbol{\\theta})^\\top \\mathbf{x}\n\\quad \\text{s.t.} \\quad \\mathbf{A}\\mathbf{x} \\le \\mathbf{b}(\\boldsymbol{\\theta}), \\qquad \\mathbf{H} \\succ 0,\n\neach active set again leads to an affine optimizer, with piecewise-affine global structure and a piecewise-quadratic value function.\n\nParametric programming focuses on the structure of the map \\boldsymbol{\\theta} \\mapsto \\mathbf{x}^\\star(\\boldsymbol{\\theta}), and the regions over which this map takes a simple form.","type":"content","url":"/mpc#general-framework-parametric-optimization","position":65},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"Solution Sensitivity via the Implicit Function Theorem","lvl3":"General Framework: Parametric Optimization","lvl2":"Computational Efficiency via Parametric Programming"},"type":"lvl4","url":"/mpc#solution-sensitivity-via-the-implicit-function-theorem","position":66},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"Solution Sensitivity via the Implicit Function Theorem","lvl3":"General Framework: Parametric Optimization","lvl2":"Computational Efficiency via Parametric Programming"},"content":"We often meet equations of the formF(y,\\boldsymbol{\\theta})=0,\n\nwhere y\\in\\mathbb{R}^m are unknowns and \\boldsymbol{\\theta}\\in\\mathbb{R}^p are parameters. The implicit function theorem says that, if F is smooth and the Jacobian with respect to y,\\frac{\\partial F}{\\partial y}(y^\\star,\\boldsymbol{\\theta}^\\star),\n\nis invertible at a solution (y^\\star,\\boldsymbol{\\theta}^\\star), then in a neighborhood of \\boldsymbol{\\theta}^\\star there exists a unique smooth mapping y(\\boldsymbol{\\theta}) with F(y(\\boldsymbol{\\theta}),\\boldsymbol{\\theta})=0 and y(\\boldsymbol{\\theta}^\\star)=y^\\star. Moreover, its derivative is\\frac{d y}{d\\boldsymbol{\\theta}}(\\boldsymbol{\\theta}^\\star)\n\\;=\\;\n-\\Big(\\tfrac{\\partial F}{\\partial y}(y^\\star,\\boldsymbol{\\theta}^\\star)\\Big)^{-1}\n\\;\\tfrac{\\partial F}{\\partial \\boldsymbol{\\theta}}(y^\\star,\\boldsymbol{\\theta}^\\star).\n\nIn words: if the square Jacobian in y is nonsingular, the solution varies smoothly with the parameter, and we can differentiate it by solving one linear system.\n\nReturn to (P_{\\theta}) and its KKT system. Collect the primal and dual variables intoy \\;:=\\; (\\mathbf{x},\\,\\boldsymbol{\\lambda},\\,\\boldsymbol{\\nu}),\n\nand write the KKT equations as a single residualF(y,\\boldsymbol{\\theta}) \\;=\\; \n\\begin{bmatrix}\n\\nabla_{\\mathbf{x}} f(\\mathbf{x};\\boldsymbol{\\theta})\n+ \\nabla_{\\mathbf{x}} \\mathbf{g}(\\mathbf{x};\\boldsymbol{\\theta})^\\top \\boldsymbol{\\lambda}\n+ \\nabla_{\\mathbf{x}} \\mathbf{h}(\\mathbf{x};\\boldsymbol{\\theta})^\\top \\boldsymbol{\\nu} \\\\\n\\mathbf{h}(\\mathbf{x};\\boldsymbol{\\theta}) \\\\\n\\mathbf{g}_\\mathcal{A}(\\mathbf{x};\\boldsymbol{\\theta})\n\\end{bmatrix}\n\\;=\\; \\mathbf{0}.\n\nHere \\mathcal{A} denotes the set of inequality constraints active at the solution (the complementarity part is encoded by keeping \\mathcal{A} fixed; see below).\n\nTo invoke IFT, we need the Jacobian \\partial F/\\partial y to be invertible at (y^\\star,\\boldsymbol{\\theta}^\\star). Standard regularity conditions that ensure this are:\n\nLICQ (Linear Independence Constraint Qualification) at (\\mathbf{x}^\\star,\\boldsymbol{\\theta}^\\star): the gradients of all active constraints are linearly independent.\n\nSecond-order sufficiency on the critical cone (the Lagrangian Hessian is positive definite on feasible directions).\n\nStrict complementarity (optional but convenient): each active inequality has strictly positive multiplier.\n\nUnder these, the KKT matrix,K \\;=\\;\n\\frac{\\partial F}{\\partial y}(y^\\star,\\boldsymbol{\\theta}^\\star)\n\\;=\\;\n\\begin{bmatrix}\n\\nabla^2_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}(\\mathbf{x}^\\star,\\boldsymbol{\\lambda}^\\star,\\boldsymbol{\\nu}^\\star;\\boldsymbol{\\theta}^\\star)\n& \\nabla_{\\mathbf{x}} \\mathbf{g}_\\mathcal{A}(\\mathbf{x}^\\star;\\boldsymbol{\\theta}^\\star)^\\top\n& \\nabla_{\\mathbf{x}} \\mathbf{h}(\\mathbf{x}^\\star;\\boldsymbol{\\theta}^\\star)^\\top \\\\\n\\nabla_{\\mathbf{x}} \\mathbf{g}_\\mathcal{A}(\\mathbf{x}^\\star;\\boldsymbol{\\theta}^\\star) & 0 & 0 \\\\\n\\nabla_{\\mathbf{x}} \\mathbf{h}(\\mathbf{x}^\\star;\\boldsymbol{\\theta}^\\star) & 0 & 0\n\\end{bmatrix},\n\nis nonsingular. Here \\mathcal{L}=f+\\boldsymbol{\\lambda}^\\top \\mathbf{g}+\\boldsymbol{\\nu}^\\top \\mathbf{h}.\n\nThe right-hand side sensitivity to parameters isG \\;=\\; \\frac{\\partial F}{\\partial \\boldsymbol{\\theta}}(y^\\star,\\boldsymbol{\\theta}^\\star)\n\\;=\\;\n\\begin{bmatrix}\n\\nabla_{\\boldsymbol{\\theta}}\\nabla_{\\mathbf{x}} f\n+ \\sum_{i\\in\\mathcal{A}} \\lambda_i^\\star \\nabla_{\\boldsymbol{\\theta}}\\nabla_{\\mathbf{x}} g_i\n+ \\sum_j \\nu_j^\\star \\nabla_{\\boldsymbol{\\theta}}\\nabla_{\\mathbf{x}} h_j \\\\\n\\nabla_{\\boldsymbol{\\theta}} \\mathbf{h} \\\\\n\\nabla_{\\boldsymbol{\\theta}} \\mathbf{g}_\\mathcal{A}\n\\end{bmatrix}_{(\\mathbf{x}^\\star,\\boldsymbol{\\theta}^\\star)} .\n\nIFT then gives local differentiability of the optimizer and multipliers:\\frac{d y^\\star}{d\\boldsymbol{\\theta}}(\\boldsymbol{\\theta}^\\star)\n\\;=\\; -\\,K^{-1} G.\n\nThe formula above is valid as long as the active set \\mathcal{A} does not change. If a constraint switches between active/inactive, the mapping remains piecewise smooth, but the derivative may jump. In MPC, this is exactly why warm-starts are very effective most of the time and occasionally require a refactorization when the active set flips.\n\nIn parametric MPC, \\boldsymbol{\\theta} gathers the current state, references, and forecasts. The IFT tells us that, under regularity and a stable active set, the optimal trajectory and first input vary smoothly with \\boldsymbol{\\theta}. The linear map -K^{-1}G is exactly the object used in sensitivity-based warm starts and real-time iterations: small changes in \\boldsymbol{\\theta} can be propagated through a single KKT solve to update the primal–dual guess before taking one or two Newton/SQP steps.","type":"content","url":"/mpc#solution-sensitivity-via-the-implicit-function-theorem","position":67},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"Predictor-Corrector MPC","lvl3":"General Framework: Parametric Optimization","lvl2":"Computational Efficiency via Parametric Programming"},"type":"lvl4","url":"/mpc#predictor-corrector-mpc","position":68},{"hierarchy":{"lvl1":"Model Predictive Control","lvl4":"Predictor-Corrector MPC","lvl3":"General Framework: Parametric Optimization","lvl2":"Computational Efficiency via Parametric Programming"},"content":"We start with a smooth root-finding problemF(y)=0,\\qquad F:\\mathbb{R}^m\\to\\mathbb{R}^m.\n\nNewton’s method iteratesy^{(t+1)} \\;=\\; y^{(t)} - \\big[\\nabla F(y^{(t)})\\big]^{-1} F\\big(y^{(t)}\\big),\n\nor equivalently solves the linearized system\\nabla F(y^{(t)})\\,\\Delta y^{(t)} = -F\\big(y^{(t)}\\big),\\qquad y^{(t+1)}=y^{(t)}+\\Delta y^{(t)}.\n\nConvergence is local and fast when the Jacobian is nonsingular and the initial guess is close.\n\nNow suppose the root depends on a parameter:F\\big(y,\\theta\\big)=0,\\qquad \\theta\\in\\mathbb{R}.\n\nWe want the solution path \\theta\\mapsto y^\\star(\\theta). Numerical continuation advances \\theta in small steps and uses the previous solution as a warm start for the next Newton solve. This is the simplest and most effective way to “track” solutions of parametric systems.\n\nAt a known solution (y^\\star,\\theta^\\star), differentiate F(y^\\star(\\theta),\\theta)=0 with respect to \\theta:\\nabla_y F(y^\\star,\\theta^\\star)\\,\\frac{dy^\\star}{d\\theta}(\\theta^\\star) \\;+\\; \\nabla_\\theta F(y^\\star,\\theta^\\star) \\;=\\; 0.\n\nIf \\nabla_y F is invertible (IFT conditions), the tangent is\\frac{dy^\\star}{d\\theta}(\\theta^\\star) \\;=\\; -\\big[\\nabla_y F(y^\\star,\\theta^\\star)\\big]^{-1}\\,\\nabla_\\theta F(y^\\star,\\theta^\\star).\n\nThis is exactly the implicit differentiation formula. Continuation uses it as a predictor:y_{\\text{pred}} \\;=\\; y^\\star(\\theta^\\star) \\;+\\; \\Delta\\theta\\;\\frac{dy^\\star}{d\\theta}(\\theta^\\star).\n\nThen a few corrector steps apply Newton to F(\\,\\cdot\\,,\\theta^\\star+\\Delta\\theta)=0 starting from y_{\\text{pred}}. If Newton converges quickly, the step \\Delta\\theta was appropriate; otherwise reduce \\Delta\\theta and retry.\n\nFor parametric KKT systems, set y=(\\mathbf{x},\\boldsymbol{\\lambda},\\boldsymbol{\\nu}) where \\mathbf{x} stacks the primal decision variables (states and inputs), and F(y,\\theta)=0 the KKT residual with \\theta collecting state, references, forecasts. The KKT matrix K=\\partial F/\\partial y and parameter sensitivity G=\\partial F/\\partial \\theta give the tangent\\frac{dy^\\star}{d\\theta} \\;=\\; -\\,K^{-1}G.\n\nContinuation then becomes:\n\nPredictor: y_{\\text{pred}} = y^\\star + (\\Delta\\theta)\\,(-K^{-1}G).\n\nCorrector: a few Newton/SQP steps on the KKT equations at the new \\theta.\n\nIn MPC, this yields efficient warm starts across time: as the parameter \\theta_t (current state, references) changes slightly, we predict the new primal–dual point and correct with 1–2 iterations—often enough to hit tolerance in real time. \n## Application to MPC\n\nWe now specialize this idea to the structure of finite-horizon MPC. Fix a prediction horizon $N$. At each time step, we solve a problem with fixed structure and varying data. Define\n\n$$\n\\boldsymbol{\\theta} := (\\mathbf{x}_0,\\, \\mathbf{r},\\, \\mathbf{w}),\n$$\n\nwhich includes the current state $\\mathbf{x}_0$, reference signals $\\mathbf{r}$, and exogenous forecasts $\\mathbf{w}$.\n\nThe finite-horizon problem becomes\n\n$$\n\\begin{aligned}\n\\min_{z} \\quad & J(z;\\boldsymbol{\\theta}) \\\\\n\\text{s.t.} \\quad & c(z;\\boldsymbol{\\theta}) = 0 \\\\\n& d(z;\\boldsymbol{\\theta}) \\leq 0,\n\\end{aligned}\n$$\n\nwith decision variable $z = (\\mathbf{x}_{0:N}, \\mathbf{u}_{0:N-1})$. The equality constraints enforce dynamics and terminal conditions. The inequalities encode input and state bounds.\n\nSolving $(P_\\theta)$ produces an optimal trajectory $z^\\star(\\boldsymbol{\\theta})$. The control law is the first input:\n\n$$\n\\pi(\\boldsymbol{\\theta}) := \\mathbf{u}_0^\\star(\\boldsymbol{\\theta}).\n$$\n\nThis mapping from parameter to input defines the MPC policy. Parametric programming helps us understand and exploit its structure to speed up evaluation.\n\n## Two Approaches to Efficient MPC\n\nParametric structure can be used in two main ways: either to construct an explicit control law offline, or to warm-start the optimizer online using sensitivity information.\n\n### Explicit MPC\n\nWhen the problem is a linear or quadratic program with affine dependence on $\\boldsymbol{\\theta}$, we can work out the solution symbolically. The parameter space is partitioned into regions $\\mathcal{R}_1, \\dots, \\mathcal{R}_M$, each associated with a fixed active set. On each region:\n\n$$\nz^\\star(\\boldsymbol{\\theta}) = A_r\\,\\boldsymbol{\\theta} + b_r,\n\\qquad\n\\pi(\\boldsymbol{\\theta}) = K_r\\,\\boldsymbol{\\theta} + k_r.\n$$\n\nAt runtime, we identify which region contains $\\boldsymbol{\\theta}$, then apply the corresponding affine formula. This approach avoids optimization entirely during deployment.\n\nIt requires storing the region definitions and control laws. The number of regions grows with horizon length and constraint count, which limits this approach to systems with low state dimension and short horizons.\n\n### Sensitivity-Based MPC\n\nWhen symbolic enumeration is intractable, we can still track how the solution varies locally. Suppose we have a solution $\\bar{y} = (\\bar{z}, \\bar{\\lambda}, \\bar{\\nu})$ at parameter $\\bar{\\boldsymbol{\\theta}}$. The KKT system reads:\n\n$$\nF(y; \\boldsymbol{\\theta}) = 0.\n$$\n\nDifferentiating with respect to $\\boldsymbol{\\theta}$,\n\n$$\n\\frac{\\partial F}{\\partial y}(\\bar{y}; \\bar{\\boldsymbol{\\theta}})\\, \\mathrm{d}y\n= - \\frac{\\partial F}{\\partial \\boldsymbol{\\theta}}(\\bar{y}; \\bar{\\boldsymbol{\\theta}})\\, \\mathrm{d}\\boldsymbol{\\theta}.\n$$\n\nLet $K$ be the KKT matrix and $G$ the sensitivity of the residual. Then the sensitivity operator $T$ satisfies\n\n$$\nK T = -G \\quad \\Rightarrow \\quad \\mathrm{d}y = T\\, \\mathrm{d}\\boldsymbol{\\theta}.\n$$\n\nIf $\\boldsymbol{\\theta}$ changes slightly, we update the primal-dual pair:\n\n$$\ny^{(0)} \\leftarrow \\bar{y} + T\\,\\Delta\\boldsymbol{\\theta},\n$$\n\nand use it as the starting point for Newton or SQP.\n\nThis is the basis of real-time iteration schemes. When the active set is stable, the warm start is accurate to first order. When it changes, we refactorize and repeat, still with far less effort than solving from scratch. ","type":"content","url":"/mpc#predictor-corrector-mpc","position":69},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Amortized Optimization and Neural Approximation of Controllers","lvl2":"Computational Efficiency via Parametric Programming"},"type":"lvl3","url":"/mpc#amortized-optimization-and-neural-approximation-of-controllers","position":70},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Amortized Optimization and Neural Approximation of Controllers","lvl2":"Computational Efficiency via Parametric Programming"},"content":"The idea of reusing structure across similar optimization problems is not exclusive to parametric programming. In machine learning, a related concept known as amortized optimization aims to reduce the cost of repeated inference by replacing explicit optimization with a function that has been learned to approximate the solution map. This approach shifts the computational burden from online solving to offline training.\n\nThe goal is to construct a function \\hat{\\pi}_{\\phi}(\\boldsymbol{\\theta}), typically parameterized by a neural network, that maps the input \\boldsymbol{\\theta} to an approximate solution \\hat{z}^\\star(\\boldsymbol{\\theta}) or control action \\hat{\\mathbf{u}}_0^\\star(\\boldsymbol{\\theta}). Once trained, this map can be evaluated quickly at runtime, with no need to solve an optimization problem explicitly.\n\nAmortized optimization has emerged in several contexts:\n\nIn probabilistic inference, where variational autoencoders (VAEs) amortize the computation of posterior distributions across a dataset.\n\nIn meta-learning, where the objective is to learn a model that generalizes across tasks by internalizing how to adapt.\n\nIn hyperparameter optimization, where learning a surrogate model can guide the search over configuration space efficiently.\n\nThis perspective has also begun to influence control. Recent work investigates how to amortize nonlinear MPC (NMPC) policies into neural networks. The training data come from solving many instances of the underlying optimal control problem offline. The resulting neural policy \\hat{\\pi}_\\phi acts as a differentiable, low-latency controller that can generalize to new situations within the training distribution.\n\nCompared to explicit MPC, which partitions the parameter space and stores exact solutions region by region, amortized control smooths over the domain by learning an approximate policy globally. It is less precise, but scalable to high-dimensional problems where enumeration of regions is impossible.\n\nNeural network amortization is advantageous due to the expressivity of these models. However, the challenge is ensuring constraint satisfaction and safety, which are hard to guarantee with unconstrained neural approximators. Hybrid approaches attempt to address this by combining a neural warm-start policy with a final projection step, or by embedding the network within a constrained optimization layer. Other strategies include learning structured architectures that respect known physics or control symmetries.","type":"content","url":"/mpc#amortized-optimization-and-neural-approximation-of-controllers","position":71},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Imitation Learning Framework","lvl2":"Computational Efficiency via Parametric Programming"},"type":"lvl3","url":"/mpc#imitation-learning-framework","position":72},{"hierarchy":{"lvl1":"Model Predictive Control","lvl3":"Imitation Learning Framework","lvl2":"Computational Efficiency via Parametric Programming"},"content":"Consider a fixed horizon N and parameter vector \\boldsymbol{\\theta} encoding the current state, references, and forecasts. The oracle MPC controller solves\\begin{aligned}\nz^\\star(\\boldsymbol{\\theta}) \\in \\arg\\min_{z=(\\mathbf{x}_{0:N},\\mathbf{u}_{0:N-1})}\n&\\; J(z;\\boldsymbol{\\theta})\\\\\n\\text{s.t. }& \\mathbf{x}_{k+1}=f(\\mathbf{x}_k,\\mathbf{u}_k;\\boldsymbol{\\theta}),\\quad k=0..N-1,\\\\\n& g(\\mathbf{x}_k,\\mathbf{u}_k;\\boldsymbol{\\theta})\\le 0,\\; h(\\mathbf{x}_N;\\boldsymbol{\\theta})=0.\n\\end{aligned}\n\nThe applied action is \\pi^\\star(\\boldsymbol{\\theta}) := \\mathbf{u}_0^\\star(\\boldsymbol{\\theta}). Our goal is to learn a fast surrogate mapping \\hat{\\pi}_\\phi:\\boldsymbol{\\theta}\\mapsto \\hat{\\mathbf{u}}_0 \\approx \\pi^\\star(\\boldsymbol{\\theta}) that can be evaluated in microseconds, optionally followed by a safety projection layer.\n\nSupervised learning from oracle solutions.\nOne first samples parameters \\boldsymbol{\\theta}^{(i)} from the operational domain and solves the corresponding NMPC problems offline. The resulting dataset\\mathcal{D} = \\{ (\\boldsymbol{\\theta}^{(i)},\\, \\mathbf{u}_0^\\star(\\boldsymbol{\\theta}^{(i)})) \\}_{i=1}^M\n\nis then used to train a neural network \\hat{\\pi}_\\phi by minimizing\\min_\\phi \\; \\frac{1}{M}\\sum_{i=1}^M \\big\\|\\hat{\\pi}_\\phi(\\boldsymbol{\\theta}^{(i)}) - \\mathbf{u}_0^\\star(\\boldsymbol{\\theta}^{(i)})\\big\\|^2 .\n\nOnce trained, the network acts as a surrogate for the optimizer, providing instantaneous evaluations that approximate the MPC law.","type":"content","url":"/mpc#imitation-learning-framework","position":73},{"hierarchy":{"lvl1":"Model Predictive Control","lvl2":"Example: Propofol Infusion Control"},"type":"lvl2","url":"/mpc#example-propofol-infusion-control","position":74},{"hierarchy":{"lvl1":"Model Predictive Control","lvl2":"Example: Propofol Infusion Control"},"content":"This problem explores the control of propofol infusion in total intravenous anesthesia (TIVA). Our presentation follows the problem formulation developped by \n\nSawaguchi et al. (2008). The primary objective is to maintain the desired level of unconsciousness while minimizing adverse reactions and ensuring quick recovery after surgery.\n\nThe level of unconsciousness is measured by the Bispectral Index (BIS), which is obtained using an electroencephalography (EEG) device. The BIS ranges from 0 (complete suppression of brain activity) to 100 (fully awake), with the target range for general anesthesia typically between 40 and 60.\n\nThe goal is to design a control system that regulates the infusion rate of propofol to maintain the BIS within the target range. This can be formulated as an optimal control problem:\\begin{align*}\n\\min_{u(t)} & \\int_{0}^{T} \\left( BIS(t) - BIS_{\\text{target}} \\right)^2 + \\lambda\\, u(t)^2 \\, dt \\\\\n\\text{subject to:} \\\\\n\\dot{x}_1 &= -(k_{10} + k_{12} + k_{13})x_1 + k_{21}x_2 + k_{31}x_3 + \\frac{u(t)}{V_1} \\\\\n\\dot{x}_2 &= k_{12}x_1 - k_{21}x_2 \\\\\n\\dot{x}_3 &= k_{13}x_1 - k_{31}x_3 \\\\\n\\dot{x}_e &= k_{e0}(x_1 - x_e) \\\\\nBIS(t) &= E_0 - E_{\\text{max}}\\frac{x_e^\\gamma}{x_e^\\gamma + EC_{50}^\\gamma}\n\\end{align*}\n\nWhere:\n\nu(t) is the propofol infusion rate (mg/kg/h)\n\nx_1, x_2, and x_3 are the drug concentrations in different body compartments\n\nx_e is the effect-site concentration\n\nk_{ij} are rate constants for drug transfer between compartments\n\nBIS(t) is the Bispectral Index\n\n\\lambda is a regularization parameter penalizing excessive drug use\n\nE_0, E_{\\text{max}}, EC_{50}, and \\gamma are parameters of the pharmacodynamic model\n\nThe specific dynamics model used in this problem is so-called “Pharmacokinetic-Pharmacodynamic Model” and consists of three main components:\n\nPharmacokinetic Model, which describes how the drug distributes through the body over time. It’s based on a three-compartment model:\n\nCentral compartment (blood and well-perfused organs)\n\nShallow peripheral compartment (muscle and other tissues)\n\nDeep peripheral compartment (fat)\n\nEffect Site Model, which represents the delay between drug concentration in the blood and its effect on the brain.\n\nPharmacodynamic Model that relates the effect-site concentration to the observed BIS.\n\nThe propofol infusion control problem presents several interesting challenges from a research perspective.\nFirst, there is a delay in how fast the drug can reach a different compartments in addition to the BIS measurements which can lag. This could lead to instability if not properly addressed in the control design.\n\nFurthermore, every patient is different from another. Hence, we cannot simply learn a single controller offline and hope that it will generalize to an entire patient population. We will account for this variability through Model Predictive Control (MPC) and dynamically adapt to the model mismatch through replanning. How a patient will react to a given dose of drug also varies and must be carefully controlled to avoid overdoses. This adds an additional layer of complexity since we have to incorporate safety constraints. Finally, the patient might suddenly change state, for example due to surgical stimuli, and the controller must be able to adapt quickly to compensate for the disturbance to the system.\n\n#  label: fig-mpc-propofol\n#  caption: Closed-loop MPC for propofol infusion keeps the Bispectral Index near the target (top), regulates infusion rates (middle), and tracks the effect-site concentration (bottom).\n\n%config InlineBackend.figure_format = 'retina'\nimport numpy as np\nfrom scipy.optimize import minimize\nimport matplotlib.pyplot as plt\n\n# Apply book style\ntry:\n    import scienceplots\n    plt.style.use(['science', 'notebook'])\nexcept (ImportError, OSError):\n    pass  # Use matplotlib defaults\n\nclass Patient:\n    def __init__(self, age, weight):\n        self.age = age\n        self.weight = weight\n        self.set_pk_params()\n        self.set_pd_params()\n\n    def set_pk_params(self):\n        self.v1 = 4.27 * (self.weight / 70) ** 0.71 * (self.age / 30) ** (-0.39)\n        self.v2 = 18.9 * (self.weight / 70) ** 0.64 * (self.age / 30) ** (-0.62)\n        self.v3 = 238 * (self.weight / 70) ** 0.95\n        self.cl1 = 1.89 * (self.weight / 70) ** 0.75 * (self.age / 30) ** (-0.25)\n        self.cl2 = 1.29 * (self.weight / 70) ** 0.62\n        self.cl3 = 0.836 * (self.weight / 70) ** 0.77\n        self.k10 = self.cl1 / self.v1\n        self.k12 = self.cl2 / self.v1\n        self.k13 = self.cl3 / self.v1\n        self.k21 = self.cl2 / self.v2\n        self.k31 = self.cl3 / self.v3\n        self.ke0 = 0.456\n\n    def set_pd_params(self):\n        self.E0 = 100\n        self.Emax = 100\n        self.EC50 = 3.4\n        self.gamma = 3\n\ndef pk_model(x, u, patient):\n    x1, x2, x3, xe = x\n    dx1 = -(patient.k10 + patient.k12 + patient.k13) * x1 + patient.k21 * x2 + patient.k31 * x3 + u / patient.v1\n    dx2 = patient.k12 * x1 - patient.k21 * x2\n    dx3 = patient.k13 * x1 - patient.k31 * x3\n    dxe = patient.ke0 * (x1 - xe)\n    return np.array([dx1, dx2, dx3, dxe])\n\ndef pd_model(ce, patient):\n    return patient.E0 - patient.Emax * (ce ** patient.gamma) / (ce ** patient.gamma + patient.EC50 ** patient.gamma)\n\ndef simulate_step(x, u, patient, dt):\n    x_next = x + dt * pk_model(x, u, patient)\n    bis = pd_model(x_next[3], patient)\n    return x_next, bis\n\ndef objective(u, x0, patient, dt, N, target_bis):\n    x = x0.copy()\n    total_cost = 0\n    for i in range(N):\n        x, bis = simulate_step(x, u[i], patient, dt)\n        total_cost += (bis - target_bis)**2 + 0.1 * u[i]**2\n    return total_cost\n\ndef mpc_step(x0, patient, dt, N, target_bis):\n    u0 = 10 * np.ones(N)  # Initial guess\n    bounds = [(0, 20)] * N  # Infusion rate between 0 and 20 mg/kg/h\n    \n    result = minimize(objective, u0, args=(x0, patient, dt, N, target_bis),\n                      method='SLSQP', bounds=bounds)\n    \n    return result.x[0]  # Return only the first control input\n\ndef run_mpc_simulation(patient, T, dt, N, target_bis):\n    steps = int(T / dt)\n    x = np.zeros((steps+1, 4))\n    bis = np.zeros(steps+1)\n    u = np.zeros(steps)\n    \n    for i in range(steps):\n        # Add noise to the current state to simulate real-world uncertainty\n        x_noisy = x[i] + np.random.normal(0, 0.01, size=4)\n        \n        # Use noisy state for MPC planning\n        u[i] = mpc_step(x_noisy, patient, dt, N, target_bis)\n        \n        # Evolve the true state using the deterministic model\n        x[i+1], bis[i] = simulate_step(x[i], u[i], patient, dt)\n    \n    bis[-1] = pd_model(x[-1, 3], patient)\n    return x, bis, u\n\n# Set up the problem\npatient = Patient(age=40, weight=70)\nT = 120  # Total time in minutes\ndt = 0.5  # Time step in minutes\nN = 20  # Prediction horizon\ntarget_bis = 50  # Target BIS value\n\n# Run MPC simulation\nx, bis, u = run_mpc_simulation(patient, T, dt, N, target_bis)\n\n# Plot results\nt = np.arange(0, T+dt, dt)\nfig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 12), sharex=True)\n\nax1.plot(t, bis)\nax1.set_ylabel('BIS')\nax1.set_ylim(0, 100)\nax1.axhline(y=target_bis, color='r', linestyle='--')\n\nax2.plot(t[:-1], u)\nax2.set_ylabel('Infusion Rate (mg/kg/h)')\n\nax3.plot(t, x[:, 3])\nax3.set_ylabel('Effect-site Concentration (µg/mL)')\nax3.set_xlabel('Time (min)')\n\nplt.tight_layout()\n\nprint(f\"Initial BIS: {bis[0]:.2f}\")\nprint(f\"Final BIS: {bis[-1]:.2f}\")\nprint(f\"Mean infusion rate: {np.mean(u):.2f} mg/kg/h\")\nprint(f\"Final effect-site concentration: {x[-1, 3]:.2f} µg/mL\")\n\n ### Deployment Patterns\n\nThere are several ways to use an amortized controller once it has been trained. The simplest option is **direct amortization**, where the control input is taken to be $u = \\hat{\\pi}_\\phi(\\boldsymbol{\\theta})$. In this case, the neural network provides the control action directly, with no optimization performed during deployment.\n\nA second option is **amortization with projection**, where the network output $\\tilde u = \\hat{\\pi}_\\phi(\\boldsymbol{\\theta})$ is passed through a small optimization step, such as a quadratic program or barrier-function filter, in order to enforce constraints. This adds a negligible computational overhead but restores guarantees of feasibility and safety.\n\nWe could for example integrate a convex approximation of the MPC subproblem directly as a differentiable layer inside the network. The network proposes a candidate action $\\tilde u$, which is then corrected through a small quadratic program:\n\n$$\nu = \\arg\\min_v \\tfrac12\\|v-\\tilde u\\|^2 \\quad \\text{s.t. } g(x,v)\\le 0.\n$$\n\nGradients are propagated through this correction using implicit differentiation, allowing the network to be trained end-to-end while retaining constraint satisfaction. This hybrid keeps the fast evaluation of a learned map while preserving the structure of MPC.\n\nA third option is **amortized warm-starting**, where the neural network provides an initialization for one or two Newton or SQP iterations of the underlying NMPC problem. In this setting, the learned map delivers an excellent starting point, so the optimizer converges quickly and the cost of re-solving at each time step is greatly reduced.  ## Demo: Batch Bioreactor MPC with do-mpc\n\nWe illustrate nonlinear MPC on a fed-batch bioreactor. The process has four states: biomass concentration \\(X_s\\), substrate \\(S_s\\), product \\(P_s\\), and liquid volume \\(V_s\\). The manipulated feed flow \\(u_{\\text{inp}}\\) augments volume and changes concentrations. The dynamics are\n\n$$\n\\begin{aligned}\n\\dot X_s &= \\mu(S_s)X_s - \\tfrac{u_{\\text{inp}}}{V_s} X_s, \\\\\n\\dot S_s &= -\\tfrac{\\mu(S_s)X_s}{Y_x} - \\tfrac{v X_s}{Y_p} + \\tfrac{u_{\\text{inp}}}{V_s}(S_{\\text{in}} - S_s), \\\\\n\\dot P_s &= v X_s - \\tfrac{u_{\\text{inp}}}{V_s} P_s, \\\\\n\\dot V_s &= u_{\\text{inp}},\n\\end{aligned}\n$$\n\nwith inhibited Monod kinetics\n\n$$\n\\mu(S_s) = \\frac{\\mu_m S_s}{K_m + S_s + S_s^2/K_i}.\n$$\n\nWe impose bounds on states and input, e.g. \\(0 \\le X_s \\le 3.7\\), \\(0 \\le P_s \\le 3.0\\), and \\(0 \\le u_{\\text{inp}} \\le 0.2\\). Two parameters are uncertain: yield \\(Y_x\\) and inlet concentration \\(S_{\\text{in}}\\). We treat them via a small scenario set with non-anticipativity (a single input sequence is shared across scenarios).\n\nAt each MPC step, we solve a finite-horizon problem that encourages product formation while regularizing effort:\n\n$$\n\\min_{x_{0:N},u_{0:N-1}} \\; -\\,P_s(N) + \\sum_{k=0}^{N-1} \\big( -\\,P_s(k) + \\rho\\, u_k^2 \\big)\n$$\n\nsubject to the discretized dynamics and box constraints for all uncertainty scenarios, sharing the inputs across scenarios. The continuous-time ODEs are discretized by orthogonal collocation on finite elements, producing an NLP [orthogonal collocation](https://www.do-mpc.com/en/latest/theory_orthogonal_collocation.html). The resulting NMPC is re-solved in a receding-horizon loop [MPC basics](https://www.do-mpc.com/en/latest/theory_mpc.html).\n\nThe cell below runs the closed-loop simulation and plots the states and input. The script is adapted from the do-mpc Batch Bioreactor example.\n\n```{code-cell} python\n:tags: [hide-input]\n\n%config InlineBackend.figure_format = 'retina'\nimport numpy as np\nimport do_mpc\nfrom casadi import *  # noqa: F401 - do-mpc constructs CasADi symbols under the hood\nimport matplotlib.pyplot as plt\n\n# Apply book style\ntry:\n    import scienceplots\n    plt.style.use(['science', 'notebook'])\nexcept (ImportError, OSError):\n    pass  # Use matplotlib defaults\n\n\ndef build_model():\n    model_type = 'continuous'\n    model = do_mpc.model.Model(model_type)\n\n    # States\n    X_s = model.set_variable('_x', 'X_s')  # biomass\n    S_s = model.set_variable('_x', 'S_s')  # substrate\n    P_s = model.set_variable('_x', 'P_s')  # product\n    V_s = model.set_variable('_x', 'V_s')  # volume\n\n    # Control input (feed flow)\n    inp = model.set_variable('_u', 'inp')\n\n    # Certain parameters\n    mu_m = 0.02\n    K_m = 0.05\n    K_i = 5.0\n    v_par = 0.004\n    Y_p = 1.2\n\n    # Uncertain parameters\n    Y_x = model.set_variable('_p', 'Y_x')\n    S_in = model.set_variable('_p', 'S_in')\n\n    # Specific growth rate\n    mu_S = mu_m * S_s / (K_m + S_s + (S_s**2 / K_i))\n\n    # Dynamics\n    model.set_rhs('X_s', mu_S * X_s - inp / V_s * X_s)\n    model.set_rhs('S_s', -mu_S * X_s / Y_x - v_par * X_s / Y_p + inp / V_s * (S_in - S_s))\n    model.set_rhs('P_s', v_par * X_s - inp / V_s * P_s)\n    model.set_rhs('V_s', inp)\n\n    model.setup()\n    return model\n\n\ndef build_mpc(model):\n    mpc = do_mpc.controller.MPC(model)\n    setup_mpc = {\n        'n_horizon': 30,\n        't_step': 1.0,\n        'n_robust': 1,\n        'store_full_solution': True,\n    }\n    mpc.set_param(**setup_mpc)\n\n    # Objective: encourage product formation and small inputs (economic-like MPC)\n    X_s = model.x['X_s']\n    S_s = model.x['S_s']\n    P_s = model.x['P_s']\n    V_s = model.x['V_s']\n    inp = model.u['inp']\n\n    mterm = -P_s  # maximize product at horizon end\n    lterm = -P_s + 1e-4 * inp**2  # small input penalty\n    mpc.set_objective(mterm=mterm, lterm=lterm)\n\n    # Box constraints\n    mpc.bounds['lower', '_x', 'X_s'] = 0.0\n    mpc.bounds['lower', '_x', 'S_s'] = -0.01\n    mpc.bounds['lower', '_x', 'P_s'] = 0.0\n    mpc.bounds['lower', '_x', 'V_s'] = 0.0\n    mpc.bounds['upper', '_x', 'X_s'] = 3.7\n    mpc.bounds['upper', '_x', 'P_s'] = 3.0\n    mpc.bounds['lower', '_u', 'inp'] = 0.0\n    mpc.bounds['upper', '_u', 'inp'] = 0.2\n\n    # Uncertainty scenarios (shared control across scenarios)\n    Y_x_values = np.array([0.5, 0.4, 0.3])\n    S_in_values = np.array([200.0, 220.0, 180.0])\n    mpc.set_uncertainty_values(Y_x=Y_x_values, S_in=S_in_values)\n\n    mpc.setup()\n    return mpc\n\n\ndef build_estimator(model):\n    return do_mpc.estimator.StateFeedback(model)\n\n\ndef build_simulator(model):\n    simulator = do_mpc.simulator.Simulator(model)\n    params_sim = {\n        'integration_tool': 'cvodes',\n        'abstol': 1e-10,\n        'reltol': 1e-10,\n        't_step': 1.0,\n    }\n    simulator.set_param(**params_sim)\n\n    # Realizations of uncertain parameters used by the simulator\n    p_num = simulator.get_p_template()\n    p_num['Y_x'] = 0.4\n    p_num['S_in'] = 200.0\n\n    def p_fun(t_now):\n        return p_num\n\n    simulator.set_p_fun(p_fun)\n    simulator.setup()\n    return simulator\n\n\ndef run_closed_loop():\n    model = build_model()\n    mpc = build_mpc(model)\n    estimator = build_estimator(model)\n    simulator = build_simulator(model)\n\n    # Initial state\n    x0 = np.array([1.0, 0.5, 0.0, 120.0])\n    mpc.x0 = x0\n    estimator.x0 = x0\n    simulator.x0 = x0\n    mpc.set_initial_guess()\n\n    # Closed-loop simulation\n    n_steps = 60\n    for _ in range(n_steps):\n        u0 = mpc.make_step(x0)\n        y_next = simulator.make_step(u0)\n        x0 = estimator.make_step(y_next)\n\n    # Visualization\n    import matplotlib.pyplot as plt\n\n# Apply book style\ntry:\n    import scienceplots\n    plt.style.use(['science', 'notebook'])\nexcept (ImportError, OSError):\n    pass  # Use matplotlib defaults\n\n    mpc_graphics = do_mpc.graphics.Graphics(mpc.data)\n    sim_graphics = do_mpc.graphics.Graphics(simulator.data)\n\n    fig, ax = plt.subplots(5, sharex=True, figsize=(12, 8))\n    fig.align_ylabels()\n\n    for g in [sim_graphics, mpc_graphics]:\n        g.add_line(var_type='_x', var_name='X_s', axis=ax[0], color='#1f77b4')\n        g.add_line(var_type='_x', var_name='S_s', axis=ax[1], color='#1f77b4')\n        g.add_line(var_type='_x', var_name='P_s', axis=ax[2], color='#1f77b4')\n        g.add_line(var_type='_x', var_name='V_s', axis=ax[3], color='#1f77b4')\n        g.add_line(var_type='_u', var_name='inp', axis=ax[4], color='#1f77b4')\n\n    ax[0].set_ylabel('X_s [mol/l]')\n    ax[1].set_ylabel('S_s [mol/l]')\n    ax[2].set_ylabel('P_s [mol/l]')\n    ax[3].set_ylabel('V_s [m^3]')\n    ax[4].set_ylabel('u_inp [m^3/min]')\n    ax[4].set_xlabel('t [min]')\n\n    # Plot full horizon results\n    sim_graphics.plot_results()\n    mpc_graphics.plot_predictions()\n    mpc_graphics.reset_axes()\n    plt.tight_layout()\n    plt.show()\n\n\n# Run the closed-loop simulation\nrun_closed_loop()\n```\n\n### Interactive Animation\n\nThe following cell creates an interactive animation of the batch bioreactor control process, showing the MPC predictions and the evolution of the system states in real-time. The visualization includes a tank representation with liquid level and biomass particles, along with time-series plots of all states and control inputs.\n\n```{code-cell} python\n:tags: [hide-input]\n\n%config InlineBackend.figure_format = 'retina'\nimport numpy as np\nimport do_mpc\nfrom casadi import *  # noqa: F401 - do-mpc constructs CasADi symbols under the hood\nimport matplotlib.pyplot as plt\n\n# Apply book style\ntry:\n    import scienceplots\n    plt.style.use(['science', 'notebook'])\nexcept (ImportError, OSError):\n    pass  # Use matplotlib defaults\nfrom matplotlib.animation import FuncAnimation\nfrom IPython.display import HTML, display\nimport matplotlib.patches as mpatches\nfrom contextlib import redirect_stdout, redirect_stderr\nimport io\n\n\ndef build_model():\n    model_type = 'continuous'\n    model = do_mpc.model.Model(model_type)\n\n    # States\n    X_s = model.set_variable('_x', 'X_s')  # biomass\n    S_s = model.set_variable('_x', 'S_s')  # substrate\n    P_s = model.set_variable('_x', 'P_s')  # product\n    V_s = model.set_variable('_x', 'V_s')  # volume\n\n    # Control input (feed flow)\n    inp = model.set_variable('_u', 'inp')\n\n    # Certain parameters\n    mu_m = 0.02\n    K_m = 0.05\n    K_i = 5.0\n    v_par = 0.004\n    Y_p = 1.2\n\n    # Uncertain parameters\n    Y_x = model.set_variable('_p', 'Y_x')\n    S_in = model.set_variable('_p', 'S_in')\n\n    # Auxiliary term for specific growth rate\n    mu = mu_m * S_s / (K_m + S_s + S_s**2 / K_i)\n    model.set_expression('mu', mu)\n\n    # Differential equations\n    model.set_rhs('X_s', mu * X_s - inp / V_s * X_s)\n    model.set_rhs('S_s', -mu * X_s / Y_x - v_par * X_s / Y_p + inp / V_s * (S_in - S_s))\n    model.set_rhs('P_s', v_par * X_s - inp / V_s * P_s)\n    model.set_rhs('V_s', inp)\n\n    model.setup()\n    return model\n\n\ndef setup_mpc(model):\n    mpc = do_mpc.controller.MPC(model)\n\n    setup_mpc = {\n        'n_horizon': 20,\n        't_step': 1.0,\n        'n_robust': 1,\n        'store_full_solution': True,\n        # Silence IPOPT/CasADi prints\n        'nlpsol_opts': {\n            'ipopt.print_level': 0,\n            'ipopt.sb': 'yes',\n            'print_time': 0,\n        },\n    }\n    mpc.set_param(**setup_mpc)\n\n    # Objective\n    mterm = model.aux['mu']  # terminal cost on growth\n    lterm = model.aux['mu']  # stage cost on growth\n    mpc.set_objective(mterm=mterm, lterm=lterm)\n\n    # Constraints\n    mpc.bounds['lower', '_u', 'inp'] = 0.0\n    mpc.bounds['upper', '_u', 'inp'] = 0.2\n    \n    mpc.bounds['lower', '_x', 'X_s'] = 0.0\n    mpc.bounds['lower', '_x', 'S_s'] = 0.0\n    mpc.bounds['lower', '_x', 'P_s'] = 0.0\n    mpc.bounds['lower', '_x', 'V_s'] = 0.0\n\n    # Uncertain parameters\n    Y_x_values = np.array([0.5])  # single scenario to avoid prediction dim issues\n    S_in_values = np.array([200.0])\n    mpc.set_uncertainty_values(Y_x=Y_x_values, S_in=S_in_values)\n\n    mpc.setup()\n    return mpc\n\n\ndef setup_simulator(model):\n    simulator = do_mpc.simulator.Simulator(model)\n    simulator.set_param(t_step=1.0)\n\n    # Set uncertain parameters for simulation\n    p_template = simulator.get_p_template()\n    p_template['Y_x'] = 0.5\n    p_template['S_in'] = 200.0\n    simulator.set_p_fun(lambda t_now: p_template)\n\n    simulator.setup()\n    return simulator\n\n\ndef setup_estimator(model):\n    estimator = do_mpc.estimator.StateFeedback(model)\n    return estimator\n\n\ndef run_closed_loop_simulation():\n    \"\"\"Run the closed-loop simulation and return results (silencing solver output).\"\"\"\n    # Build system\n    model = build_model()\n    mpc = setup_mpc(model)\n    simulator = setup_simulator(model)\n    estimator = setup_estimator(model)\n\n    # Initial state\n    x0 = np.array([1.0, 150.0, 0.0, 120.0]).reshape(-1, 1)\n    mpc.x0 = x0\n    simulator.x0 = x0\n    estimator.x0 = x0\n    mpc.set_initial_guess()\n\n    # Storage for results\n    results = {\n        't': [],\n        'x': [],\n        'u': [],\n    }\n\n    # Silence IPOPT/CasADi output during the loop\n    fnull = io.StringIO()\n    with redirect_stdout(fnull), redirect_stderr(fnull):\n        for k in range(60):\n            u0 = mpc.make_step(x0)\n            y_next = simulator.make_step(u0)\n            x0 = estimator.make_step(y_next)\n            \n            # Store results\n            results['t'].append(k)\n            results['x'].append(x0.flatten())\n            results['u'].append(u0.flatten())\n\n    # Convert to arrays\n    results['x'] = np.array(results['x'])\n    results['u'] = np.array(results['u'])\n    \n    return results\n\n\ndef create_animation(results):\n    \"\"\"Create an animated visualization of the batch bioreactor process.\"\"\"\n    \n    # Create figure with subplots\n    fig = plt.figure(figsize=(14, 10))\n    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n    \n    # Tank visualization (left side)\n    ax_tank = fig.add_subplot(gs[:, 0])\n    \n    # State plots (middle and right)\n    ax_biomass = fig.add_subplot(gs[0, 1])\n    ax_substrate = fig.add_subplot(gs[0, 2])\n    ax_product = fig.add_subplot(gs[1, 1])\n    ax_volume = fig.add_subplot(gs[1, 2])\n    ax_control = fig.add_subplot(gs[2, 1:])\n    \n    # Setup axes\n    n_steps = len(results['t'])\n    \n    # State axes setup\n    for ax, label, ylim in [\n        (ax_biomass, 'Biomass X_s [g/L]', (0, 6)),\n        (ax_substrate, 'Substrate S_s [g/L]', (0, 250)),\n        (ax_product, 'Product P_s [g/L]', (0, 50)),\n        (ax_volume, 'Volume V_s [L]', (100, 200))\n    ]:\n        ax.set_xlim(0, n_steps)\n        ax.set_ylim(ylim)\n        ax.set_xlabel('Time [h]')\n        ax.set_ylabel(label)\n        ax.grid(True, alpha=0.3)\n    \n    ax_control.set_xlim(0, n_steps)\n    ax_control.set_ylim(-0.01, 0.21)\n    ax_control.set_xlabel('Time [h]')\n    ax_control.set_ylabel('Feed Flow u [L/h]')\n    ax_control.grid(True, alpha=0.3)\n    \n    # Tank setup\n    ax_tank.set_xlim(-1.5, 1.5)\n    ax_tank.set_ylim(0, 3)\n    ax_tank.set_aspect('equal')\n    ax_tank.axis('off')\n    ax_tank.set_title('Batch Bioreactor', fontsize=14, fontweight='bold')\n    \n    # Initialize plot elements\n    lines = {\n        'biomass': ax_biomass.plot([], [], 'g-', lw=2, label='Biomass')[0],\n        'substrate': ax_substrate.plot([], [], 'b-', lw=2, label='Substrate')[0],\n        'product': ax_product.plot([], [], 'r-', lw=2, label='Product')[0],\n        'volume': ax_volume.plot([], [], 'm-', lw=2, label='Volume')[0],\n        'control': ax_control.plot([], [], 'k-', lw=2, label='Control')[0],\n    }\n    \n    # Current point markers\n    markers = {\n        'biomass': ax_biomass.plot([], [], 'go', markersize=8)[0],\n        'substrate': ax_substrate.plot([], [], 'bo', markersize=8)[0],\n        'product': ax_product.plot([], [], 'ro', markersize=8)[0],\n        'volume': ax_volume.plot([], [], 'mo', markersize=8)[0],\n        'control': ax_control.plot([], [], 'ko', markersize=8)[0],\n    }\n    \n    # Tank components\n    tank_outline = mpatches.FancyBboxPatch(\n        (-1, 0.2), 2, 2, boxstyle=\"round,pad=0.02\",\n        linewidth=3, edgecolor='black', facecolor='none'\n    )\n    ax_tank.add_patch(tank_outline)\n    \n    # Liquid in tank (will be updated)\n    liquid = mpatches.Rectangle((-0.95, 0.25), 1.9, 1.0, \n                                facecolor='lightblue', alpha=0.6)\n    ax_tank.add_patch(liquid)\n    \n    # Biomass particles (circles)\n    biomass_particles = []\n    for _ in range(10):\n        particle = plt.Circle((0, 1), 0.05, color='green', alpha=0.7)\n        ax_tank.add_patch(particle)\n        biomass_particles.append(particle)\n    \n    # Feed pipe and valve\n    feed_pipe = mpatches.Rectangle((0.8, 2.2), 0.1, 0.5, \n                                   facecolor='gray', edgecolor='black')\n    ax_tank.add_patch(feed_pipe)\n    \n    valve = mpatches.FancyBboxPatch(\n        (0.75, 2.15), 0.2, 0.1, boxstyle=\"round,pad=0.01\",\n        linewidth=2, edgecolor='black', facecolor='red', alpha=0.5\n    )\n    ax_tank.add_patch(valve)\n    \n    # Text displays\n    time_text = fig.text(0.02, 0.98, '', fontsize=12, fontweight='bold',\n                         transform=fig.transFigure)\n    \n    tank_text = ax_tank.text(0, 2.7, '', ha='center', fontsize=10)\n    \n    # Add legend to one subplot\n    ax_biomass.legend(loc='upper right')\n    \n    def init():\n        \"\"\"Initialize animation.\"\"\"\n        for line in lines.values():\n            line.set_data([], [])\n        for marker in markers.values():\n            marker.set_data([], [])\n        return list(lines.values()) + list(markers.values())\n    \n    def animate(frame):\n        \"\"\"Animation function.\"\"\"\n        # Update time text\n        time_text.set_text(f'Time: {frame:.0f} h')\n        \n        # Update history lines\n        t_data = results['t'][:frame+1]\n        x_data = results['x'][:frame+1]\n        u_data = results['u'][:frame+1]\n        \n        if frame > 0:\n            lines['biomass'].set_data(t_data, x_data[:, 0])\n            lines['substrate'].set_data(t_data, x_data[:, 1])\n            lines['product'].set_data(t_data, x_data[:, 2])\n            lines['volume'].set_data(t_data, x_data[:, 3])\n            lines['control'].set_data(t_data, u_data[:, 0])\n            \n            # Update current point markers\n            markers['biomass'].set_data([frame], [x_data[frame, 0]])\n            markers['substrate'].set_data([frame], [x_data[frame, 1]])\n            markers['product'].set_data([frame], [x_data[frame, 2]])\n            markers['volume'].set_data([frame], [x_data[frame, 3]])\n            markers['control'].set_data([frame], [u_data[frame, 0]])\n        \n        # Update tank visualization\n        if frame < len(x_data):\n            # Update liquid level based on volume\n            volume = x_data[frame, 3]\n            liquid_height = 1.5 * (volume / 200.0)  # Normalize to tank height\n            liquid.set_height(liquid_height)\n            \n            # Update biomass particles\n            biomass_conc = x_data[frame, 0]\n            n_visible = int(10 * min(biomass_conc / 5.0, 1.0))  # Scale particles\n            \n            for i, particle in enumerate(biomass_particles):\n                if i < n_visible:\n                    # Random position in liquid\n                    x = np.random.uniform(-0.8, 0.8)\n                    y = np.random.uniform(0.3, 0.25 + liquid_height * 0.9)\n                    particle.set_center((x, y))\n                    particle.set_alpha(0.7)\n                else:\n                    particle.set_alpha(0)\n            \n            # Update valve color based on control input\n            u_val = u_data[frame, 0] if frame < len(u_data) else 0\n            valve_color = plt.cm.RdYlGn_r(u_val / 0.2)  # Red=high flow, Green=low\n            valve.set_facecolor(valve_color)\n            valve.set_alpha(0.8 if u_val > 0.01 else 0.3)\n            \n            # Update tank text\n            tank_text.set_text(\n                f'V={volume:.1f}L, X={biomass_conc:.2f}g/L\\n'\n                f'S={x_data[frame, 1]:.1f}g/L, P={x_data[frame, 2]:.1f}g/L'\n            )\n        \n        return (list(lines.values()) + list(markers.values()) + \n                biomass_particles + [liquid, valve, tank_text, time_text])\n    \n    # Create animation\n    anim = FuncAnimation(fig, animate, init_func=init, \n                        frames=n_steps, interval=100, blit=False)\n    \n    plt.suptitle('Batch Bioreactor Control with do-mpc', fontsize=16, fontweight='bold')\n    \n    return fig, anim\n\n\n# Run simulation and create animation (no prints)\nresults = run_closed_loop_simulation()\nfig, anim = create_animation(results)\n\n# Render like the pendulum example: JS HTML animation and no extra prints\njs_anim = anim.to_jshtml()\nplt.close(fig)\ndisplay(HTML(js_anim))\n``` ","type":"content","url":"/mpc#example-propofol-infusion-control","position":75},{"hierarchy":{"lvl1":"Policy Gradient Methods"},"type":"lvl1","url":"/pg","position":0},{"hierarchy":{"lvl1":"Policy Gradient Methods"},"content":"The \n\nprevious chapter showed how to handle continuous action spaces in fitted Q-iteration by amortizing action selection with policy networks. Methods like NFQCA, DDPG, TD3, and SAC all learn both a Q-function and a policy, using the Q-function to guide policy improvement. This chapter explores a different approach: optimizing policies directly without maintaining explicit value functions.\n\nDirect policy optimization offers several advantages. First, it naturally handles stochastic policies, which can be essential for partially observable environments or problems requiring explicit exploration. Second, it avoids the detour through value function approximation, which may introduce errors that compound during policy extraction. Third, for problems with simple policy classes but complex value landscapes, directly searching in policy space can be more efficient than searching in value space.\n\nThe foundation of policy gradient methods rests on computing gradients of expected returns with respect to policy parameters. This chapter develops the mathematical machinery needed for this computation, starting with general derivative estimation techniques from stochastic optimization, then specializing to reinforcement learning settings, and finally examining variance reduction methods that make these estimators practical.","type":"content","url":"/pg","position":1},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl2":"Derivative Estimation for Stochastic Optimization"},"type":"lvl2","url":"/pg#derivative-estimation-for-stochastic-optimization","position":2},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl2":"Derivative Estimation for Stochastic Optimization"},"content":"Consider optimizing an objective that involves an expectation:J(\\theta) = \\mathbb{E}_{x \\sim p(x;\\theta)}[f(x,\\theta)]\n\nFor concreteness, consider a simple example where x \\sim \\mathcal{N}(\\theta,1) and f(x,\\theta) = x^2\\theta. The derivative we seek is:\\frac{d}{d\\theta}J(\\theta) = \\frac{d}{d\\theta}\\int x^2\\theta p(x;\\theta)dx\n\nWhile we can compute this exactly for the Gaussian example, this is often impossible for more general problems. We might then be tempted to approximate our objective using samples:J(\\theta) \\approx \\frac{1}{N}\\sum_{i=1}^N f(x_i,\\theta), \\quad x_i \\sim p(x;\\theta)\n\nThen differentiate this approximation:\\frac{d}{d\\theta}J(\\theta) \\approx \\frac{1}{N}\\sum_{i=1}^N \\frac{\\partial}{\\partial \\theta}f(x_i,\\theta)\n\nHowever, this naive approach ignores that the samples themselves depend on \\theta. The correct derivative requires the product rule:\\frac{d}{d\\theta}J(\\theta) = \\int \\frac{\\partial}{\\partial \\theta}[f(x,\\theta)p(x;\\theta)]dx = \\int \\left[\\frac{\\partial f}{\\partial \\theta}p(x;\\theta) + f(x,\\theta)\\frac{\\partial p(x;\\theta)}{\\partial \\theta}\\right]dx\n\nWhile the first term could be numerically integrated using Monte Carlo, the second one cannot as it is not in the form of an expectation.\n\nTo transform our objective so that the Monte Carlo estimator for the objective could be differentiated directly while ensuring that the resulting derivative is unbiased, there are two main solutions: a change of measure, or a change of variables.","type":"content","url":"/pg#derivative-estimation-for-stochastic-optimization","position":3},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl3":"The Likelihood Ratio Method","lvl2":"Derivative Estimation for Stochastic Optimization"},"type":"lvl3","url":"/pg#the-likelihood-ratio-method","position":4},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl3":"The Likelihood Ratio Method","lvl2":"Derivative Estimation for Stochastic Optimization"},"content":"One solution comes from rewriting our objective using a proposal distribution q(x) that does not depend on \\theta:J(\\theta) = \\int f(x,\\theta)\\frac{p(x;\\theta)}{q(x)}q(x)dx = \\mathbb{E}_{x \\sim q(x)}\\left[f(x,\\theta)\\frac{p(x;\\theta)}{q(x)}\\right]\n\nDefine the likelihood ratio \\rho(x, q, \\theta) \\equiv \\frac{p(x;\\theta)}{q(x)}, where we treat q as a separate argument. The objective becomes:J(\\theta) = \\mathbb{E}_{x \\sim q(x)}[f(x,\\theta)\\rho(x, q, \\theta)]\n\nWhen we differentiate J, we take the partial derivative with respect to \\theta while holding q fixed (since q does not depend on \\theta):\\frac{d}{d\\theta}J(\\theta) = \\mathbb{E}_{x \\sim q(x)}\\left[f(x,\\theta)\\frac{\\partial \\rho}{\\partial \\theta}(x, q, \\theta) + \\rho(x, q, \\theta)\\frac{\\partial f}{\\partial \\theta}(x,\\theta)\\right]\n\nThe partial derivative of \\rho with respect to \\theta (treating q as fixed) is:\\frac{\\partial \\rho}{\\partial \\theta}(x, q, \\theta) = \\frac{1}{q(x)}\\frac{\\partial p(x;\\theta)}{\\partial \\theta} = \\rho(x, q, \\theta)\\frac{\\partial \\log p(x;\\theta)}{\\partial \\theta}\n\nNow fix any reference parameter \\theta_0 and choose the proposal distribution q(x) = p(x;\\theta_0). This is a fixed distribution that does not change as \\theta varies. We simply evaluate the family p(x;\\cdot) at the specific point \\theta_0. With this choice, evaluating the gradient at \\theta = \\theta_0 gives \\rho(x, q, \\theta_0) = p(x;\\theta_0)/p(x;\\theta_0) = 1. The gradient formula becomes:\\frac{d}{d\\theta}J(\\theta)\\Big|_{\\theta=\\theta_0} = \\mathbb{E}_{x \\sim p(x;\\theta_0)}\\left[f(x,\\theta_0)\\frac{\\partial \\log p(x;\\theta)}{\\partial \\theta}\\Big|_{\\theta_0} + \\frac{\\partial f(x,\\theta)}{\\partial \\theta}\\Big|_{\\theta_0}\\right]\n\nSince \\theta_0 is arbitrary, we can drop the subscript and write the score function estimator as:\\frac{d}{d\\theta}J(\\theta) = \\mathbb{E}_{x \\sim p(x;\\theta)}\\left[f(x,\\theta)\\frac{\\partial \\log p(x;\\theta)}{\\partial \\theta} + \\frac{\\partial f(x,\\theta)}{\\partial \\theta}\\right]","type":"content","url":"/pg#the-likelihood-ratio-method","position":5},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl3":"The Reparameterization Trick","lvl2":"Derivative Estimation for Stochastic Optimization"},"type":"lvl3","url":"/pg#the-reparameterization-trick","position":6},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl3":"The Reparameterization Trick","lvl2":"Derivative Estimation for Stochastic Optimization"},"content":"An alternative approach eliminates the \\theta-dependence in the sampling distribution by expressing x through a deterministic transformation of the noise:x = g(\\epsilon,\\theta), \\quad \\epsilon \\sim q(\\epsilon)\n\nTherefore if we want to sample from some target distribution p(x;\\theta), we can do so by first sampling from a simple base distribution q(\\epsilon) (like a standard normal) and then transforming those samples through a carefully chosen function g. If g(\\cdot,\\theta) is invertible, the change of variables formula tells us how these distributions relate:p(x;\\theta) = q(g^{-1}(x,\\theta))\\left|\\det\\frac{\\partial g^{-1}(x,\\theta)}{\\partial x}\\right| = q(\\epsilon)\\left|\\det\\frac{\\partial g(\\epsilon,\\theta)}{\\partial \\epsilon}\\right|^{-1}\n\nFor example, if we want to sample from any multivariate Gaussian distributions with covariance matrix \\Sigma and mean \\mu, it suffices to be able to sample from a standard normal noise and compute the linear transformation:x = \\mu + \\Sigma^{1/2}\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,I)\n\nwhere \\Sigma^{1/2} is the matrix square root obtained via Cholesky decomposition. In the univariate case, this transformation is simply:x = \\mu + \\sigma \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,1)\n\nwhere \\sigma = \\sqrt{\\sigma^2} is the standard deviation (square root of the variance).","type":"content","url":"/pg#the-reparameterization-trick","position":7},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl4":"Common Examples of Reparameterization","lvl3":"The Reparameterization Trick","lvl2":"Derivative Estimation for Stochastic Optimization"},"type":"lvl4","url":"/pg#common-examples-of-reparameterization","position":8},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl4":"Common Examples of Reparameterization","lvl3":"The Reparameterization Trick","lvl2":"Derivative Estimation for Stochastic Optimization"},"content":"","type":"content","url":"/pg#common-examples-of-reparameterization","position":9},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl5":"The Truncated Normal Distribution","lvl4":"Common Examples of Reparameterization","lvl3":"The Reparameterization Trick","lvl2":"Derivative Estimation for Stochastic Optimization"},"type":"lvl5","url":"/pg#the-truncated-normal-distribution","position":10},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl5":"The Truncated Normal Distribution","lvl4":"Common Examples of Reparameterization","lvl3":"The Reparameterization Trick","lvl2":"Derivative Estimation for Stochastic Optimization"},"content":"When we need samples constrained to an interval [a,b], we can use the truncated normal distribution. To sample from it, we transform uniform noise through the inverse cumulative distribution function (CDF) of the standard normal:x = \\Phi^{-1}(u\\Phi(b) + (1-u)\\Phi(a)), \\quad u \\sim \\text{Uniform}(0,1)\n\nHere:\n\n\\Phi(z) = \\frac{1}{2}\\left[1 + \\text{erf}\\left(\\frac{z}{\\sqrt{2}}\\right)\\right] is the CDF of the standard normal distribution\n\n\\Phi^{-1} is its inverse (the quantile function)\n\n\\text{erf}(z) = \\frac{2}{\\sqrt{\\pi}}\\int_0^z e^{-t^2}dt is the error function\n\nThe resulting samples follow a normal distribution restricted to [a,b], with the density properly normalized over this interval.","type":"content","url":"/pg#the-truncated-normal-distribution","position":11},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl5":"The Kumaraswamy Distribution","lvl4":"Common Examples of Reparameterization","lvl3":"The Reparameterization Trick","lvl2":"Derivative Estimation for Stochastic Optimization"},"type":"lvl5","url":"/pg#the-kumaraswamy-distribution","position":12},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl5":"The Kumaraswamy Distribution","lvl4":"Common Examples of Reparameterization","lvl3":"The Reparameterization Trick","lvl2":"Derivative Estimation for Stochastic Optimization"},"content":"When we need samples in the unit interval [0,1], a natural choice might be the Beta distribution. However, its inverse CDF doesn’t have a closed form. Instead, we can use the Kumaraswamy distribution as a convenient approximation, which allows for a simple reparameterization:x = (1-(1-u^{\\alpha})^{1/\\beta}), \\quad u \\sim \\text{Uniform}(0,1)\n\nwhere:\n\n\\alpha, \\beta > 0 are shape parameters that control the distribution\n\n\\alpha determines the concentration around 0\n\n\\beta determines the concentration around 1\n\nThe distribution is similar to Beta(α,β) but with analytically tractable CDF and inverse CDF\n\nThe Kumaraswamy distribution has density:f(x; \\alpha, \\beta) = \\alpha\\beta x^{\\alpha-1}(1-x^{\\alpha})^{\\beta-1}, \\quad x \\in [0,1]","type":"content","url":"/pg#the-kumaraswamy-distribution","position":13},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl5":"The Gumbel-Softmax Distribution","lvl4":"Common Examples of Reparameterization","lvl3":"The Reparameterization Trick","lvl2":"Derivative Estimation for Stochastic Optimization"},"type":"lvl5","url":"/pg#the-gumbel-softmax-distribution","position":14},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl5":"The Gumbel-Softmax Distribution","lvl4":"Common Examples of Reparameterization","lvl3":"The Reparameterization Trick","lvl2":"Derivative Estimation for Stochastic Optimization"},"content":"When sampling from a categorical distribution with probabilities \\{\\pi_i\\}, one approach uses \\text{Gumbel}(0,1) noise combined with the argmax of log-perturbed probabilities:\\text{argmax}_i(\\log \\pi_i + g_i), \\quad g_i \\sim \\text{Gumbel}(0,1)\n\nThis approach, known in machine learning as the Gumbel-Max trick, relies on sampling Gumbel noise from uniform random variables through the transformation g_i = -\\log(-\\log(u_i)) where u_i \\sim \\text{Uniform}(0,1). To see why this gives us samples from the categorical distribution, consider the probability of selecting category i:\\begin{align*}\nP(\\text{argmax}_j(\\log \\pi_j + g_j) = i) &= P(\\log \\pi_i + g_i > \\log \\pi_j + g_j \\text{ for all } j \\neq i) \\\\\n&= P(g_i - g_j > \\log \\pi_j - \\log \\pi_i \\text{ for all } j \\neq i)\n\\end{align*}\n\nSince the difference of two Gumbel random variables follows a logistic distribution, g_i - g_j \\sim \\text{Logistic}(0,1), and these differences are independent for different j (due to the independence of the original Gumbel variables), we can write:\\begin{align*}\nP(\\text{argmax}_j(\\log \\pi_j + g_j) = i) &= \\prod_{j \\neq i} P(g_i - g_j > \\log \\pi_j - \\log \\pi_i) \\\\\n&= \\prod_{j \\neq i} \\frac{\\pi_i}{\\pi_i + \\pi_j} = \\pi_i\n\\end{align*}\n\nThe last equality requires some additional algebra to show, but follows from the fact that these probabilities must sum to 1 over all i.\n\nWhile we have shown that the Gumbel-Max trick gives us exact samples from a categorical distribution, the argmax operation isn’t differentiable. For stochastic optimization problems of the form:\\mathbb{E}_{x \\sim p(x;\\theta)}[f(x)] = \\mathbb{E}_{\\epsilon \\sim \\text{Gumbel}(0,1)}[f(g(\\epsilon,\\theta))]\n\nwe need g to be differentiable with respect to \\theta. This leads us to consider a continuous relaxation where we replace the hard argmax with a temperature-controlled softmax:z_i = \\frac{\\exp((\\log \\pi_i + g_i)/\\tau)}{\\sum_j \\exp((\\log \\pi_j + g_j)/\\tau)}\n\nAs \\tau \\to 0, this approximation approaches the argmax:\\lim_{\\tau \\to 0} \\frac{\\exp(x_i/\\tau)}{\\sum_j \\exp(x_j/\\tau)} = \\begin{cases} 1 & \\text{if } x_i = \\max_j x_j \\\\ 0 & \\text{otherwise} \\end{cases}\n\nThe resulting distribution over the probability simplex is called the Gumbel-Softmax (or Concrete) distribution. The temperature parameter \\tau controls the discreteness of our samples: smaller values give samples closer to one-hot vectors but with less stable gradients, while larger values give smoother gradients but more diffuse samples.","type":"content","url":"/pg#the-gumbel-softmax-distribution","position":15},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl3":"Numerical Analysis of Gradient Estimators","lvl2":"Derivative Estimation for Stochastic Optimization"},"type":"lvl3","url":"/pg#numerical-analysis-of-gradient-estimators","position":16},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl3":"Numerical Analysis of Gradient Estimators","lvl2":"Derivative Estimation for Stochastic Optimization"},"content":"Let us examine the behavior of our three gradient estimators for the stochastic optimization objective:J(\\theta) = \\mathbb{E}_{x \\sim \\mathcal{N}(\\theta,1)}[x^2\\theta]\n\nTo get an analytical expression for the derivative, first note that we can factor out \\theta to obtain J(\\theta) = \\theta\\mathbb{E}[x^2] where x \\sim \\mathcal{N}(\\theta,1). By definition of the variance, we know that \\text{Var}(x) = \\mathbb{E}[x^2] - (\\mathbb{E}[x])^2, which we can rearrange to \\mathbb{E}[x^2] = \\text{Var}(x) + (\\mathbb{E}[x])^2. Since x \\sim \\mathcal{N}(\\theta,1), we have \\text{Var}(x) = 1 and \\mathbb{E}[x] = \\theta, therefore \\mathbb{E}[x^2] = 1 + \\theta^2. This gives us:J(\\theta) = \\theta(1 + \\theta^2)\n\nNow differentiating with respect to \\theta using the product rule yields:\\frac{d}{d\\theta}J(\\theta) = 1 + 3\\theta^2\n\nFor concreteness, we fix \\theta = 1.0 and analyze samples drawn using Monte Carlo estimation with batch size 1000 and 1000 independent trials. Evaluating at \\theta = 1 gives us \\frac{d}{d\\theta}J(\\theta)\\big|_{\\theta=1} = 1 + 3(1)^2 = 4, which serves as our ground truth against which we compare our estimators:\n\nFirst, we consider the naive estimator that incorrectly differentiates the Monte Carlo approximation:\\hat{g}_{\\text{naive}}(\\theta) = \\frac{1}{N}\\sum_{i=1}^N x_i^2\n\nFor x \\sim \\mathcal{N}(1,1), we have \\mathbb{E}[x^2] = \\theta^2 + 1 = 2.0 and \\mathbb{E}[\\hat{g}_{\\text{naive}}] = 2.0. We should therefore expect a bias of about -2 in our experiment.\n\nThen we compute the score function estimator:\\hat{g}_{\\text{SF}}(\\theta) = \\frac{1}{N}\\sum_{i=1}^N \\left[x_i^2\\theta(x_i - \\theta) + x_i^2\\right]\n\nThis estimator is unbiased with \\mathbb{E}[\\hat{g}_{\\text{SF}}] = 4\n\nFinally, through the reparameterization x = \\theta + \\epsilon where \\epsilon \\sim \\mathcal{N}(0,1), we obtain:\\hat{g}_{\\text{RT}}(\\theta) = \\frac{1}{N}\\sum_{i=1}^N \\left[2\\theta(\\theta + \\epsilon_i) + (\\theta + \\epsilon_i)^2\\right]\n\nThis estimator is also unbiased with \\mathbb{E}[\\hat{g}_{\\text{RT}}] = 4.\n\n%config InlineBackend.figure_format = 'retina'\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\n# Apply book style\ntry:\n    import scienceplots\n    plt.style.use(['science', 'notebook'])\nexcept (ImportError, OSError):\n    pass  # Use matplotlib defaults\n\nkey = jax.random.PRNGKey(0)\n\n# Define the objective function f(x,θ) = x²θ where x ~ N(θ, 1)\ndef objective(x, theta):\n    return x**2 * theta\n\n# Naive Monte Carlo gradient estimation\n@jax.jit\ndef naive_gradient_batch(key, theta):\n    samples = jax.random.normal(key, (1000,)) + theta\n    # Use jax.grad on the objective with respect to theta\n    grad_fn = jax.grad(lambda t: jnp.mean(objective(samples, t)))\n    return grad_fn(theta)\n\n# Score function estimator (REINFORCE)\n@jax.jit\ndef score_function_batch(key, theta):\n    samples = jax.random.normal(key, (1000,)) + theta\n    # f(x,θ) * ∂logp(x|θ)/∂θ + ∂f(x,θ)/∂θ\n    # score function for N(θ,1) is (x-θ)\n    score = samples - theta\n    return jnp.mean(objective(samples, theta) * score + samples**2)\n\n# Reparameterization gradient\n@jax.jit\ndef reparam_gradient_batch(key, theta):\n    eps = jax.random.normal(key, (1000,))\n    # Use reparameterization x = θ + ε, ε ~ N(0,1)\n    grad_fn = jax.grad(lambda t: jnp.mean(objective(t + eps, t)))\n    return grad_fn(theta)\n\n# Run trials\nn_trials = 1000\ntheta = 1.0\ntrue_grad = 1 + 3 * theta**2\n\nkeys = jax.random.split(key, n_trials)\nnaive_estimates = jnp.array([naive_gradient_batch(k, theta) for k in keys])\nscore_estimates = jnp.array([score_function_batch(k, theta) for k in keys])\nreparam_estimates = jnp.array([reparam_gradient_batch(k, theta) for k in keys])\n\n# Create violin plots with individual points\nplt.figure(figsize=(12, 6))\ndata = [naive_estimates, score_estimates, reparam_estimates]\ncolors = ['#ff9999', '#66b3ff', '#99ff99']\n\nparts = plt.violinplot(data, showextrema=False)\nfor i, pc in enumerate(parts['bodies']):\n    pc.set_facecolor(colors[i])\n    pc.set_alpha(0.7)\n\n# Add box plots\nplt.boxplot(data, notch=True, showfliers=False)\n\n# Add true gradient line\nplt.axhline(y=true_grad, color='r', linestyle='--', label='True Gradient')\n\nplt.xticks([1, 2, 3], ['Naive', 'Score Function', 'Reparam'])\nplt.ylabel('Gradient Estimate')\nplt.title(f'Gradient Estimators (θ={theta}, true grad={true_grad:.2f})')\nplt.grid(True, alpha=0.3)\nplt.legend()\n\n# Print statistics\nmethods = {\n    'Naive': naive_estimates,\n    'Score Function': score_estimates, \n    'Reparameterization': reparam_estimates\n}\n\nfor name, estimates in methods.items():\n    bias = jnp.mean(estimates) - true_grad\n    variance = jnp.var(estimates)\n    print(f\"\\n{name}:\")\n    print(f\"Mean: {jnp.mean(estimates):.6f}\")\n    print(f\"Bias: {bias:.6f}\")\n    print(f\"Variance: {variance:.6f}\")\n    print(f\"MSE: {bias**2 + variance:.6f}\")\n\nThe numerical experiments corroborate our theory. The naive estimator consistently underestimates the true gradient by 2.0, though it maintains a relatively small variance. This systematic bias would make it unsuitable for optimization despite its low variance. The score function estimator corrects this bias but introduces substantial variance. While unbiased, this estimator would require many samples to achieve reliable gradient estimates. Finally, the reparameterization trick achieves a much lower variance while remaining unbiased. While this experiment is for didactic purposes only, it reproduces what is commonly found in practice: that when applicable, the reparameterization estimator tends to perform better than the score function counterpart.","type":"content","url":"/pg#numerical-analysis-of-gradient-estimators","position":17},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl2":"Score Function Methods in Reinforcement Learning"},"type":"lvl2","url":"/pg#score-function-methods-in-reinforcement-learning","position":18},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl2":"Score Function Methods in Reinforcement Learning"},"content":"The score function estimator from the previous section applies directly to reinforcement learning. Since it requires only the ability to evaluate and differentiate \\log \\pi_{\\boldsymbol{w}}(a|s), it works with any differentiable policy, including discrete action spaces where reparameterization is unavailable. It requires no model of the environment dynamics.\n\nLet G(\\tau) \\equiv \\sum_{t=0}^T r(s_t, a_t) be the sum of undiscounted rewards in a trajectory \\tau. The stochastic optimization problem we face is to maximize:J(\\boldsymbol{w}) = \\mathbb{E}_{\\tau \\sim p(\\tau;\\boldsymbol{w})}[G(\\tau)]\n\nwhere \\tau = (s_0,a_0,s_1,a_1,...) is a trajectory and G(\\tau) is the total return.\nApplying the score function estimator, we get:\\begin{align*}\n\\nabla_{\\boldsymbol{w}}J(\\boldsymbol{w}) &= \\nabla_{\\boldsymbol{w}}\\mathbb{E}_{\\tau}[G(\\tau)] \\\\\n&= \\mathbb{E}_{\\tau}\\left[G(\\tau)\\nabla_{\\boldsymbol{w}}\\log p(\\tau;\\boldsymbol{w})\\right] \\\\\n&= \\mathbb{E}_{\\tau}\\left[G(\\tau)\\nabla_{\\boldsymbol{w}}\\sum_{t=0}^T\\log \\pi_{\\boldsymbol{w}}(a_t|s_t)\\right] \\\\\n&= \\mathbb{E}_{\\tau}\\left[G(\\tau)\\sum_{t=0}^T\\nabla_{\\boldsymbol{w}}\\log \\pi_{\\boldsymbol{w}}(a_t|s_t)\\right]\n\\end{align*}\n\nWe have eliminated the need to know the transition probabilities in this estimator since the probability of a trajectory factorizes as:p(\\tau;\\boldsymbol{w}) = p(s_0)\\prod_{t=0}^T \\pi_{\\boldsymbol{w}}(a_t|s_t)p(s_{t+1}|s_t,a_t)\n\nTherefore, only the policy depends on \\boldsymbol{w}. When taking the logarithm of this product, we get a sum where all the \\boldsymbol{w}-independent terms vanish. The final estimator samples trajectories under the distribution p(\\tau; \\boldsymbol{w}) and computes:\\nabla_{\\boldsymbol{w}}J(\\boldsymbol{w}) \\approx \\frac{1}{N}\\sum_{i=1}^N\\left[G(\\tau^{(i)})\\sum_{t=0}^T\\nabla_{\\boldsymbol{w}}\\log \\pi_{\\boldsymbol{w}}(a_t^{(i)}|s_t^{(i)})\\right]\n\nThis is a direct application of the score function estimator. However, we rarely use this form in practice and instead make several improvements to further reduce the variance.","type":"content","url":"/pg#score-function-methods-in-reinforcement-learning","position":19},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl3":"Leveraging Conditional Independence","lvl2":"Score Function Methods in Reinforcement Learning"},"type":"lvl3","url":"/pg#leveraging-conditional-independence","position":20},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl3":"Leveraging Conditional Independence","lvl2":"Score Function Methods in Reinforcement Learning"},"content":"Given the Markov property of the MDP, rewards r_k for k < t are conditionally independent of action a_t given the history h_t = (s_0,a_0,...,s_{t-1},a_{t-1},s_t). This allows us to only need to consider future rewards when computing policy gradients.\\begin{align*}\n\\nabla_{\\boldsymbol{w}}J(\\boldsymbol{w}) &= \\mathbb{E}_{\\tau}\\left[\\sum_{t=0}^T\\nabla_{\\boldsymbol{w}}\\log \\pi_{\\boldsymbol{w}}(a_t|s_t)\\sum_{k=0}^T r_k\\right] \\\\\n&= \\mathbb{E}_{\\tau}\\left[\\sum_{t=0}^T\\nabla_{\\boldsymbol{w}}\\log \\pi_{\\boldsymbol{w}}(a_t|s_t)\\left(\\sum_{k=0}^{t-1} r_k + \\sum_{k=t}^T r_k\\right)\\right] \\\\\n&= \\mathbb{E}_{\\tau}\\left[\\sum_{t=0}^T\\nabla_{\\boldsymbol{w}}\\log \\pi_{\\boldsymbol{w}}(a_t|s_t)\\sum_{k=t}^T r_k\\right]\n\\end{align*}\n\nThe conditional independence assumption means that the term \\mathbb{E}_{\\tau}\\left[\\sum_{t=0}^T\\nabla_{\\boldsymbol{w}}\\log \\pi_{\\boldsymbol{w}}(a_t|s_t)\\sum_{k=0}^{t-1} r_k \\right] vanishes. To see this, factor the trajectory distribution as:p(\\tau) = p(s_0,...,s_t,a_0,...,a_{t-1}) \\pi_{\\boldsymbol{w}}(a_t|s_t) p(s_{t+1},...,s_T,a_{t+1},...,a_T|s_t,a_t)\n\nWe can now re-write a single term of this summation as:\\mathbb{E}_{\\tau}\\left[\\nabla_{\\boldsymbol{w}}\\log \\pi_{\\boldsymbol{w}}(a_t|s_t)\\sum_{k=0}^{t-1} r_k\\right] = \\mathbb{E}_{s_{0:t},a_{0:t-1}}\\left[\\sum_{k=0}^{t-1} r_k \\, \\mathbb{E}_{a_t}\\left[\\nabla_{\\boldsymbol{w}}\\log \\pi_{\\boldsymbol{w}}(a_t|s_t)\\right]\\right]\n\nThe inner expectation is zero because\\begin{align*}\n\\mathbb{E}_{a_t}\\left[\\nabla_{\\boldsymbol{w}}\\log \\pi_{\\boldsymbol{w}}(a_t|s_t)\\right] &= \\int \\nabla_{\\boldsymbol{w}}\\log \\pi_{\\boldsymbol{w}}(a_t|s_t)\\pi_{\\boldsymbol{w}}(a_t|s_t)da_t \\\\\n&= \\int \\frac{\\nabla_{\\boldsymbol{w}}\\pi_{\\boldsymbol{w}}(a_t|s_t)}{\\pi_{\\boldsymbol{w}}(a_t|s_t)}\\pi_{\\boldsymbol{w}}(a_t|s_t)da_t \\\\\n&= \\int \\nabla_{\\boldsymbol{w}}\\pi_{\\boldsymbol{w}}(a_t|s_t)da_t \\\\\n&= \\nabla_{\\boldsymbol{w}}\\int \\pi_{\\boldsymbol{w}}(a_t|s_t)da_t \\\\\n&= \\nabla_{\\boldsymbol{w}}1 = 0\n\\end{align*}\n\nThe Monte Carlo estimator becomes:\\nabla_{\\boldsymbol{w}}J(\\boldsymbol{w}) \\approx \\frac{1}{N}\\sum_{i=1}^N\\left[\\sum_{t=0}^T\\nabla_{\\boldsymbol{w}}\\log \\pi_{\\boldsymbol{w}}(a_t^{(i)}|s_t^{(i)})\\sum_{k=t}^T r_k^{(i)}\\right]\n\nThis gives us the REINFORCE algorithm:\n\nREINFORCE\n\nInput: Policy parameterization \\pi_{\\boldsymbol{w}}(a|s)Output: Updated policy parameters \\boldsymbol{w}Hyperparameters: Learning rate \\alpha, number of episodes N, episode length T\n\nInitialize parameters \\boldsymbol{w}\n\nFor episode = 1, ..., N do:\n\nCollect trajectory \\tau = (s_0, a_0, r_0, ..., s_T, a_T, r_T) using policy \\pi_{\\boldsymbol{w}}(a|s)\n\nCompute returns: G_t = \\sum_{k=t}^T r_k for t = 0, ..., T\n\nCompute gradient estimate: \\hat{g} = \\sum_{t=0}^T \\nabla_{\\boldsymbol{w}}\\log \\pi_{\\boldsymbol{w}}(a_t|s_t) G_t\n\nUpdate policy: \\boldsymbol{w} \\leftarrow \\boldsymbol{w} + \\alpha \\hat{g}\n\nReturn \\boldsymbol{w}\n\nThe benefit of this estimator compared to the naive one (which would weight each score function by the full trajectory return G(\\tau)) is that it generally has less variance. This variance reduction arises from the conditional independence structure we exploited: past rewards do not depend on future actions. More formally, this estimator is an instance of a variance reduction technique known as the Extended Conditional Monte Carlo Method.","type":"content","url":"/pg#leveraging-conditional-independence","position":21},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl4":"The Surrogate Loss Perspective","lvl3":"Leveraging Conditional Independence","lvl2":"Score Function Methods in Reinforcement Learning"},"type":"lvl4","url":"/pg#the-surrogate-loss-perspective","position":22},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl4":"The Surrogate Loss Perspective","lvl3":"Leveraging Conditional Independence","lvl2":"Score Function Methods in Reinforcement Learning"},"content":"The algorithm above computes a gradient estimate \\hat{g} explicitly. In practice, implementations using automatic differentiation frameworks take a different approach: they define a surrogate loss whose gradient matches the REINFORCE estimator. For a single trajectory, consider:L_{\\text{surrogate}}(\\boldsymbol{w}) = -\\sum_{t=0}^T \\log \\pi_{\\boldsymbol{w}}(a_t|s_t) \\, G_t\n\nwhere the returns G_t and actions a_t are treated as fixed constants (detached from the computation graph). Taking the gradient with respect to \\boldsymbol{w}:\\nabla_{\\boldsymbol{w}} L_{\\text{surrogate}} = -\\sum_{t=0}^T \\nabla_{\\boldsymbol{w}}\\log \\pi_{\\boldsymbol{w}}(a_t|s_t) \\, G_t\n\nMinimizing this surrogate loss via gradient descent yields the same update as maximizing expected return via REINFORCE. The negative sign converts our maximization problem into a minimization suitable for standard optimizers.\n\nThis surrogate loss is not the expected return J(\\boldsymbol{w}) we are trying to maximize. It is a computational device that produces the correct gradient at the current parameter values. Several properties distinguish it from a true loss function:\n\nIt changes each iteration. The returns G_t come from trajectories sampled under the current policy. After updating \\boldsymbol{w}, we must collect new trajectories and construct a new surrogate loss.\n\nIts value is not meaningful. Unlike supervised learning where the loss measures prediction error, the numerical value of L_{\\text{surrogate}} has no direct interpretation. Only its gradient matters.\n\nIt is valid only locally. The surrogate loss provides the correct gradient only at the parameters used to collect the data. Moving far from those parameters invalidates the gradient estimate.\n\nThis perspective explains why policy gradient code often looks different from the pseudocode above. Instead of computing \\hat{g} explicitly, implementations define the surrogate loss and call loss.backward():# Surrogate loss implementation (single trajectory)\nlog_probs = [policy.log_prob(a_t, s_t) for s_t, a_t in trajectory]\nreturns = compute_returns(rewards)\nsurrogate_loss = -sum(lp * G for lp, G in zip(log_probs, returns))\nsurrogate_loss.backward()  # computes REINFORCE gradient\noptimizer.step()","type":"content","url":"/pg#the-surrogate-loss-perspective","position":23},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl3":"Variance Reduction via Control Variates","lvl2":"Score Function Methods in Reinforcement Learning"},"type":"lvl3","url":"/pg#variance-reduction-via-control-variates","position":24},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl3":"Variance Reduction via Control Variates","lvl2":"Score Function Methods in Reinforcement Learning"},"content":"Recall that the REINFORCE gradient estimator, after leveraging conditional independence, takes the form:\\nabla_{\\boldsymbol{w}}J(\\boldsymbol{w}) \\approx \\frac{1}{N}\\sum_{i=1}^N\\left[\\sum_{t=0}^T\\nabla_{\\boldsymbol{w}}\\log \\pi_{\\boldsymbol{w}}(a_t^{(i)}|s_t^{(i)})\\sum_{k=t}^T r_k^{(i)}\\right]\n\nThis is a sum over trajectories and timesteps. The gradient contribution at timestep t of trajectory i is:\\nabla_{\\boldsymbol{w}}\\log \\pi_{\\boldsymbol{w}}(a_t^{(i)}|s_t^{(i)})\\sum_{k=t}^T r_k^{(i)}\n\nWhile unbiased, this estimator suffers from high variance because the return \\sum_{k=t}^T r_k can vary significantly across trajectories even for the same state-action pair. The control variate method provides a principled way to reduce this variance.","type":"content","url":"/pg#variance-reduction-via-control-variates","position":25},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl4":"General Control Variate Theory","lvl3":"Variance Reduction via Control Variates","lvl2":"Score Function Methods in Reinforcement Learning"},"type":"lvl4","url":"/pg#general-control-variate-theory","position":26},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl4":"General Control Variate Theory","lvl3":"Variance Reduction via Control Variates","lvl2":"Score Function Methods in Reinforcement Learning"},"content":"For a general estimator Z of some quantity \\mu = \\mathbb{E}[Z], and a control variate C with known expectation \\mathbb{E}[C]=0, we can construct:Z_{\\text{cv}} = Z - \\alpha C\n\nThis remains unbiased since \\mathbb{E}[Z_{\\text{cv}}] = \\mathbb{E}[Z] - \\alpha\\mathbb{E}[C] = \\mathbb{E}[Z]. The variance is:\\text{Var}(Z_{\\text{cv}}) = \\text{Var}(Z) + \\alpha^2\\text{Var}(C) - 2\\alpha\\text{Cov}(Z,C)\n\nThe -2\\alpha\\text{Cov}(Z,C) term is what enables variance reduction. If Z and C are positively correlated, we can choose \\alpha > 0 to make this term negative and large in magnitude, reducing the overall variance. However, the \\alpha^2\\text{Var}(C) term grows quadratically with \\alpha, so if we make \\alpha too large, this quadratic term will eventually dominate and the variance will increase rather than decrease. The variance as a function of \\alpha is a parabola opening upward, with a unique minimum. Setting \\frac{d}{d\\alpha}\\text{Var}(Z_{\\text{cv}}) = 0 gives:\\alpha^* = \\frac{\\text{Cov}(Z,C)}{\\text{Var}(C)}\n\nThis is the coefficient from ordinary least squares regression: we predict the estimator Z using the control variate C as the predictor. Since \\mathbb{E}[C] = 0, the linear model is Z \\approx \\mathbb{E}[Z] + \\alpha^* C, where \\alpha^* is the OLS slope coefficient. The control variate estimator Z_{\\text{cv}} = Z - \\alpha^* C computes the residual: the part of Z that cannot be explained by C.\n\nSubstituting \\alpha^* into the variance formula yields:\\text{Var}(Z_{\\text{cv}}) = \\text{Var}(Z) - \\frac{[\\text{Cov}(Z,C)]^2}{\\text{Var}(C)} = (1 - R^2) \\text{Var}(Z)\n\nwhere R^2 = \\frac{[\\text{Cov}(Z,C)]^2}{\\text{Var}(Z)\\text{Var}(C)} is the coefficient of determination from regressing Z on C. The variance reduction is R^2 \\text{Var}(Z): the better C predicts Z, the more variance we eliminate.","type":"content","url":"/pg#general-control-variate-theory","position":27},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl4":"Application to REINFORCE","lvl3":"Variance Reduction via Control Variates","lvl2":"Score Function Methods in Reinforcement Learning"},"type":"lvl4","url":"/pg#application-to-reinforce","position":28},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl4":"Application to REINFORCE","lvl3":"Variance Reduction via Control Variates","lvl2":"Score Function Methods in Reinforcement Learning"},"content":"In the reinforcement learning setting, our REINFORCE gradient estimator is a sum over timesteps: \\sum_{t=0}^T Z_t where each Z_t represents the gradient contribution at timestep t. We apply control variates separately to each term. Since \\text{Var}(\\sum_t Z_t) = \\sum_t \\text{Var}(Z_t) + \\sum_{t \\neq s} \\text{Cov}(Z_t, Z_s), reducing the variance of each Z_t reduces the total variance, though we do not explicitly address the cross-timestep covariance terms.\n\nFor a given trajectory at state s_t, the gradient contribution at time t is:Z_t = \\nabla_{\\boldsymbol{w}}\\log \\pi_{\\boldsymbol{w}}(a_t|s_t)\\sum_{k=t}^T r_k\n\nThis is the product of the score function \\nabla_{\\boldsymbol{w}}\\log \\pi_{\\boldsymbol{w}}(a_t|s_t) and the return-to-go \\sum_{k=t}^T r_k. We can subtract any state-dependent function b(s_t) from the return without introducing bias, as long as b(s_t) does not depend on a_t. This is because:\\mathbb{E}_{a_t \\sim \\pi_{\\boldsymbol{w}}(\\cdot|s_t)}\\left[\\nabla_{\\boldsymbol{w}}\\log \\pi_{\\boldsymbol{w}}(a_t|s_t)b(s_t)\\right] = b(s_t)\\mathbb{E}_{a_t}\\left[\\nabla_{\\boldsymbol{w}}\\log \\pi_{\\boldsymbol{w}}(a_t|s_t)\\right] = 0\n\nwhere the last equality follows from the score function identity \n\n(38).\n\nWe can now define our control variate as:C_t = \\nabla_{\\boldsymbol{w}}\\log \\pi_{\\boldsymbol{w}}(a_t|s_t) \\cdot b(s_t)\n\nwhere b(s_t) is a baseline function that depends only on the state. This satisfies \\mathbb{E}[C_t|s_t] = 0. Our control variate estimator becomes:Z_{t,\\text{cv}} = Z_t - C_t = \\nabla_{\\boldsymbol{w}}\\log \\pi_{\\boldsymbol{w}}(a_t|s_t)\\left(\\sum_{k=t}^T r_k - b(s_t)\\right)\n\nThe optimal baseline b^*(s_t) minimizes the variance. To find it, consider the scalar parameter case for simplicity. Write g(a_t) \\equiv \\nabla_w \\log \\pi_w(a_t|s_t) and G_t = \\sum_{k=t}^T r_k. We want to minimize:b^*(s_t) = \\arg\\min_{b} \\text{Var}_{a_t \\sim \\pi_{\\boldsymbol{w}}(\\cdot|s_t)}\\left[g(a_t)(G_t - b)\\right]\n\nSince the mean does not depend on b, minimizing the variance is equivalent to minimizing the second moment \\mathbb{E}[g(a_t)^2(G_t - b)^2|s_t]. Expanding and taking the derivative with respect to b gives:b^*(s_t) = \\frac{\\mathbb{E}_{a_t|s_t}\\left[g(a_t)^2 G_t\\right]}{\\mathbb{E}_{a_t|s_t}\\left[g(a_t)^2\\right]}\n\nFor vector-valued parameters \\boldsymbol{w}, we minimize a scalar proxy such as the trace of the covariance matrix, which yields the same formula with \\|\\nabla_{\\boldsymbol{w}}\\log \\pi_{\\boldsymbol{w}}(a_t|s_t)\\|^2 in place of g(a_t)^2:b^*(s_t) = \\frac{\\mathbb{E}_{a_t|s_t}\\left[\\|\\nabla_{\\boldsymbol{w}}\\log \\pi_{\\boldsymbol{w}}(a_t|s_t)\\|^2 G_t\\right]}{\\mathbb{E}_{a_t|s_t}\\left[\\|\\nabla_{\\boldsymbol{w}}\\log \\pi_{\\boldsymbol{w}}(a_t|s_t)\\|^2\\right]}\n\nThis is the exact optimal baseline: a weighted average of returns where the weights are the squared norms of the score function. In practice, we treat the squared norm as roughly constant across actions at a given state, which leads to the simpler and widely used choice:b(s_t) \\approx \\mathbb{E}[G_t|s_t] = v^{\\pi_{\\boldsymbol{w}}}(s_t)\n\nWith this approximation, the variance-reduced gradient contribution at timestep t becomes:Z_{\\text{cv},t} = \\nabla_{\\boldsymbol{w}}\\log \\pi_{\\boldsymbol{w}}(a_t|s_t)\\left(\\sum_{k=t}^T r_k - v^{\\pi_{\\boldsymbol{w}}}(s_t)\\right)\n\nThe term in parentheses is exactly the advantage function: A^{\\pi_{\\boldsymbol{w}}}(s_t, a_t) = q^{\\pi_{\\boldsymbol{w}}}(s_t, a_t) - v^{\\pi_{\\boldsymbol{w}}}(s_t), where the Q-function is approximated by the Monte Carlo return \\sum_{k=t}^T r_k. The full gradient estimate for a trajectory is then the sum over all timesteps:\\hat{g} = \\sum_{t=0}^T Z_{\\text{cv},t} = \\sum_{t=0}^T \\nabla_{\\boldsymbol{w}}\\log \\pi_{\\boldsymbol{w}}(a_t|s_t)\\left(G_t - v^{\\pi_{\\boldsymbol{w}}}(s_t)\\right)\n\nIn practice, we do not have access to the true value function and must learn it. Unlike the methods in the \n\namortization chapter, where we learned value functions to approximate the optimal Q-function, here our goal is policy evaluation: estimating the value of the current policy \\pi_{\\boldsymbol{w}}. The same function approximation techniques apply, but we target v^{\\pi_{\\boldsymbol{w}}} rather than v^*. The simplest approach is to regress from states to Monte Carlo returns, learning what \n\nWilliams (1992) called a “baseline”:\n\nPolicy Gradient with Simple Baseline\n\nInput: Policy parameterization \\pi_{\\boldsymbol{w}}(a|s), baseline function b(s;\\boldsymbol{\\theta})Output: Updated policy parameters \\boldsymbol{w}Hyperparameters: Learning rates \\alpha_w, \\alpha_\\theta, number of episodes N, episode length T\n\nInitialize parameters \\boldsymbol{w}, \\boldsymbol{\\theta}\n\nFor episode = 1, ..., N do:\n\nCollect trajectory \\tau = (s_0, a_0, r_0, ..., s_T, a_T, r_T) using policy \\pi_{\\boldsymbol{w}}(a|s)\n\nCompute returns: G_t = \\sum_{k=t}^T r_k for t = 0, ..., T\n\nUpdate baseline: \\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\alpha_\\theta \\nabla_{\\boldsymbol{\\theta}}\\sum_{t=0}^T (G_t - b(s_t;\\boldsymbol{\\theta}))^2\n\nCompute gradient estimate: \\hat{g} = \\sum_{t=0}^T \\nabla_{\\boldsymbol{w}}\\log \\pi_{\\boldsymbol{w}}(a_t|s_t)(G_t - b(s_t;\\boldsymbol{\\theta}))\n\nUpdate policy: \\boldsymbol{w} \\leftarrow \\boldsymbol{w} + \\alpha_w \\hat{g}\n\nReturn \\boldsymbol{w}\n\nWhen implementing this algorithm nowadays, we always use mini-batching to make full use of our GPUs. Therefore, a more representative variant for this algorithm would be:\n\nPolicy Gradient with Optimal Control Variate and Mini-batches\n\nInput: Policy parameterization \\pi_{\\boldsymbol{w}}(a|s), value function v(s;\\boldsymbol{\\theta})Output: Updated policy parameters \\boldsymbol{w}Hyperparameters: Learning rates \\alpha_w, \\alpha_\\theta, number of iterations N, episode length T, batch size B, mini-batch size M\n\nInitialize parameters \\boldsymbol{w}, \\boldsymbol{\\theta}\n\nFor iteration = 1, ..., N:\n\nInitialize empty buffer \\mathcal{D}\n\nFor b = 1, ..., B:\n\nCollect trajectory \\tau_b = (s_0, a_0, r_0, ..., s_T, a_T, r_T) using policy \\pi_{\\boldsymbol{w}}(a|s)\n\nCompute returns: G_t = \\sum_{k=t}^T r_k for all t\n\nStore tuple (s_t, a_t, G_t)_{t=0}^T in \\mathcal{D}\n\nFor value_epoch = 1, ..., K:\n\nSample mini-batch \\mathcal{B}_v of size M from \\mathcal{D}\n\nCompute value loss: L_v = \\frac{1}{M}\\sum_{(s,a,G) \\in \\mathcal{B}_v} (v(s;\\boldsymbol{\\theta}) - G)^2\n\nUpdate value function: \\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\alpha_\\theta \\nabla_{\\boldsymbol{\\theta}}L_v\n\nCompute advantages: A(s,a) = G - v(s;\\boldsymbol{\\theta}) for all (s,a,G) \\in \\mathcal{D}\n\nNormalize advantages: A \\leftarrow \\frac{A - \\mu_A}{\\sigma_A}\n\nFor policy_epoch = 1, ..., J:\n\nSample mini-batch \\mathcal{B}_\\pi of size M from \\mathcal{D}\n\nCompute policy loss: L_\\pi = -\\frac{1}{M}\\sum_{(s,a,A) \\in \\mathcal{B}_\\pi} \\log \\pi_{\\boldsymbol{w}}(a|s)A\n\nUpdate policy: \\boldsymbol{w} \\leftarrow \\boldsymbol{w} - \\alpha_w \\nabla_{\\boldsymbol{w}}L_\\pi\n\nReturn \\boldsymbol{w}\n\nThe value function is trained by regressing states directly to their sampled Monte Carlo returns G. Advantage normalization (step 2.5) is not part of the optimal baseline derivation but improves optimization in practice and is standard in modern implementations.","type":"content","url":"/pg#application-to-reinforce","position":29},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl3":"Generalized Advantage Estimation","lvl2":"Score Function Methods in Reinforcement Learning"},"type":"lvl3","url":"/pg#generalized-advantage-estimation","position":30},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl3":"Generalized Advantage Estimation","lvl2":"Score Function Methods in Reinforcement Learning"},"content":"The baseline construction gave us a gradient estimator of the form:\\nabla_{\\boldsymbol{w}}J(\\boldsymbol{w}) \\approx \\frac{1}{N}\\sum_{i=1}^N \\sum_{t=0}^T \\nabla_{\\boldsymbol{w}}\\log \\pi_{\\boldsymbol{w}}(a_t^{(i)}|s_t^{(i)}) \\left(G_t^{(i)} - v(s_t^{(i)})\\right)\n\nwhere G_t = \\sum_{k=t}^T r_k is the Monte Carlo return from time t. For each visited state-action pair (s_t, a_t), the term in parentheses\\widehat{A}_t^{\\text{MC}} = G_t - v(s_t)\n\nis a Monte Carlo estimate of the advantage A^{\\pi}(s_t, a_t) = q^{\\pi}(s_t, a_t) - v^{\\pi}(s_t). If the baseline equals the true value function, v = v^{\\pi}, then \\mathbb{E}[\\widehat{A}_t^{\\text{MC}} | s_t, a_t] = A^{\\pi}(s_t, a_t), so this estimator is unbiased.\n\nHowever, as an estimator it has two limitations. First, it has high variance because G_t depends on all future rewards. Second, it uses the value function only as a baseline, not as a predictor of long-term returns. We essentially discard the information in v(s_{t+1}), v(s_{t+2}), \\ldots\n\nGAE addresses these issues by constructing a family of estimators that interpolate between pure Monte Carlo and pure bootstrapping. A parameter \\lambda controls the bias-variance tradeoff.","type":"content","url":"/pg#generalized-advantage-estimation","position":31},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl4":"Decomposing the Monte Carlo Advantage","lvl3":"Generalized Advantage Estimation","lvl2":"Score Function Methods in Reinforcement Learning"},"type":"lvl4","url":"/pg#decomposing-the-monte-carlo-advantage","position":32},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl4":"Decomposing the Monte Carlo Advantage","lvl3":"Generalized Advantage Estimation","lvl2":"Score Function Methods in Reinforcement Learning"},"content":"Fix a value function v(s) (not necessarily equal to v^{\\pi}) and define the one-step residual:\\delta_t = r_t + \\gamma v(s_{t+1}) - v(s_t)\n\nStart from the Monte Carlo advantage and add and subtract \\gamma v(s_{t+1}):\\begin{align*}\nG_t - v(s_t) &= r_t + \\gamma G_{t+1} - v(s_t) \\\\\n&= r_t + \\gamma v(s_{t+1}) - v(s_t) + \\gamma(G_{t+1} - v(s_{t+1})) \\\\\n&= \\delta_t + \\gamma(G_{t+1} - v(s_{t+1}))\n\\end{align*}\n\nApplying this decomposition recursively yields:G_t - v(s_t) = \\sum_{l=0}^{T-t} \\gamma^l \\delta_{t+l}\n\nThe Monte Carlo advantage is exactly the discounted sum of future residuals. This is an algebraic identity, not an approximation.\n\nThe sequence \\{\\delta_{t+l}\\}_{l \\geq 0} provides incremental corrections to the value function as we move forward in time. The term \\delta_t depends only on (s_t, a_t, s_{t+1}); \\delta_{t+1} depends on (s_{t+1}, a_{t+1}, s_{t+2}), and so on. As l increases, the corrections become more noisy (they depend on more random outcomes) and more sensitive to errors in the value function at later states. Although the full sum is unbiased when v = v^{\\pi}, it can have high variance and can be badly affected by approximation error in v.","type":"content","url":"/pg#decomposing-the-monte-carlo-advantage","position":33},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl4":"GAE as a Shrinkage Estimator","lvl3":"Generalized Advantage Estimation","lvl2":"Score Function Methods in Reinforcement Learning"},"type":"lvl4","url":"/pg#gae-as-a-shrinkage-estimator","position":34},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl4":"GAE as a Shrinkage Estimator","lvl3":"Generalized Advantage Estimation","lvl2":"Score Function Methods in Reinforcement Learning"},"content":"The decomposition above suggests a family of estimators that downweight residuals farther in the future. Let \\lambda \\in [0,1] and define:A_t^{\\lambda} = \\sum_{l=0}^{T-t} (\\gamma\\lambda)^l \\delta_{t+l}\n\nThis is the generalized advantage estimator A_t^{\\text{GAE}(\\gamma,\\lambda)}.\n\nTwo special cases illustrate the extremes. When \\lambda = 1, we recover the Monte Carlo advantage:A_t^{\\lambda=1} = \\sum_{l=0}^{T-t} \\gamma^l \\delta_{t+l} = G_t - v(s_t)\n\nWhen \\lambda = 0, we keep only the immediate residual:A_t^{\\lambda=0} = \\delta_t = r_t + \\gamma v(s_{t+1}) - v(s_t)\n\nIntermediate values 0 < \\lambda < 1 interpolate between these extremes. The influence of \\delta_{t+l} decays geometrically as (\\gamma\\lambda)^l. The parameter \\lambda acts as a shrinkage parameter: small \\lambda shrinks the estimator toward the one-step residual; large \\lambda allows the estimator to behave more like the Monte Carlo advantage.\n\nIf v = v^{\\pi} is the true value function, then \\mathbb{E}[\\delta_t | s_t, a_t] = A^{\\pi}(s_t, a_t) and \\mathbb{E}[\\delta_{t+l} | s_t, a_t] = 0 for l \\geq 1. In this case:\\mathbb{E}[A_t^{\\lambda} | s_t, a_t] = \\sum_{l=0}^{T-t} (\\gamma\\lambda)^l \\mathbb{E}[\\delta_{t+l} | s_t, a_t] = A^{\\pi}(s_t, a_t)\n\nfor all \\lambda \\in [0,1]. When the value function is exact, GAE is unbiased regardless of \\lambda; changing \\lambda only affects variance.\n\nIn practice, we approximate v^{\\pi} with a function approximator, and the residuals \\delta_{t+l} inherit approximation error. Distant residuals involve multiple applications of the approximate value function and are more contaminated by modeling error. Downweighting them (choosing \\lambda < 1) introduces bias but can reduce variance and limit the impact of those errors.","type":"content","url":"/pg#gae-as-a-shrinkage-estimator","position":35},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl4":"Mixture of Multi-Step Estimators","lvl3":"Generalized Advantage Estimation","lvl2":"Score Function Methods in Reinforcement Learning"},"type":"lvl4","url":"/pg#mixture-of-multi-step-estimators","position":36},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl4":"Mixture of Multi-Step Estimators","lvl3":"Generalized Advantage Estimation","lvl2":"Score Function Methods in Reinforcement Learning"},"content":"Another perspective on GAE comes from multi-step returns. Define the k-step return from time t:G_t^{(k)} = \\sum_{l=0}^{k-1} \\gamma^l r_{t+l} + \\gamma^k v(s_{t+k})\n\nand the corresponding k-step advantage estimator A_t^{(k)} = G_t^{(k)} - v(s_t). Each A_t^{(k)} uses k rewards before bootstrapping; larger k means more variance but less bootstrapping error.\n\nThe GAE estimator can be written as a geometric mixture:A_t^{\\lambda} = (1-\\lambda) \\sum_{k=1}^{T-t} \\lambda^{k-1} A_t^{(k)}\n\nGAE is a weighted average of the k-step advantage estimators, with shorter horizons weighted more heavily when \\lambda is small.","type":"content","url":"/pg#mixture-of-multi-step-estimators","position":37},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl4":"Using GAE in the Policy Gradient","lvl3":"Generalized Advantage Estimation","lvl2":"Score Function Methods in Reinforcement Learning"},"type":"lvl4","url":"/pg#using-gae-in-the-policy-gradient","position":38},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl4":"Using GAE in the Policy Gradient","lvl3":"Generalized Advantage Estimation","lvl2":"Score Function Methods in Reinforcement Learning"},"content":"Once we choose \\lambda, we plug A_t^{\\lambda} in place of G_t - v(s_t) in the policy gradient estimator:\\nabla_{\\boldsymbol{w}}J(\\boldsymbol{w}) \\approx \\frac{1}{N}\\sum_{i=1}^N \\sum_{t=0}^T \\nabla_{\\boldsymbol{w}}\\log \\pi_{\\boldsymbol{w}}(a_t^{(i)}|s_t^{(i)}) A_t^{\\lambda,(i)}\n\nWe still use a control variate to reduce variance (the baseline v), but now we construct the advantage target by smoothing the sequence of residuals \\{\\delta_t\\} with a geometrically decaying kernel.\n\nFor the value function, it is convenient to define the \\lambda-return:G_t^{\\lambda} = A_t^{\\lambda} + v(s_t)\n\nWhen \\lambda = 1, G_t^{\\lambda} reduces to the Monte Carlo return; when \\lambda = 0, it becomes the one-step bootstrapped target r_t + \\gamma v(s_{t+1}).\n\nPolicy Gradient with GAE and Mini-batches\n\nInput: Policy parameterization \\pi_{\\boldsymbol{w}}(a|s), value function v(s;\\boldsymbol{\\theta})Output: Updated policy parameters \\boldsymbol{w}Hyperparameters: Learning rates \\alpha_w, \\alpha_\\theta, number of iterations N, episode length T, batch size B, mini-batch size M, discount \\gamma, GAE parameter \\lambda\n\nInitialize parameters \\boldsymbol{w}, \\boldsymbol{\\theta}\n\nFor iteration = 1, ..., N:\n\nInitialize empty buffer \\mathcal{D}\n\nFor b = 1, ..., B:\n\nCollect trajectory \\tau_b = (s_0, a_0, r_0, ..., s_T, a_T, r_T) using policy \\pi_{\\boldsymbol{w}}(a|s)\n\nCompute value predictions v_t = v(s_t;\\boldsymbol{\\theta}) for t = 0, \\ldots, T and set v_{T+1} = 0\n\nCompute residuals: \\delta_t = r_t + \\gamma v_{t+1} - v_t for t = 0, \\ldots, T\n\nCompute GAE advantages backwards:\n\nSet A_T = \\delta_T\n\nFor t = T-1, ..., 0: A_t = \\delta_t + (\\gamma\\lambda) A_{t+1}\n\nCompute \\lambda-returns: G_t^{\\lambda} = A_t + v_t for t = 0, \\ldots, T\n\nStore tuples (s_t, a_t, A_t, G_t^{\\lambda})_{t=0}^T in \\mathcal{D}\n\nFor value_epoch = 1, ..., K:\n\nSample mini-batch \\mathcal{B}_v of size M from \\mathcal{D}\n\nCompute value loss: L_v = \\frac{1}{M}\\sum_{(s,\\cdot,\\cdot,G^{\\lambda}) \\in \\mathcal{B}_v} (v(s;\\boldsymbol{\\theta}) - G^{\\lambda})^2\n\nUpdate value function: \\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\alpha_\\theta \\nabla_{\\boldsymbol{\\theta}}L_v\n\nNormalize advantages: A \\leftarrow \\frac{A - \\mu_A}{\\sigma_A}\n\nFor policy_epoch = 1, ..., J:\n\nSample mini-batch \\mathcal{B}_\\pi of size M from \\mathcal{D}\n\nCompute policy loss: L_\\pi = -\\frac{1}{M}\\sum_{(s,a,A,\\cdot) \\in \\mathcal{B}_\\pi} \\log \\pi_{\\boldsymbol{w}}(a|s) A\n\nUpdate policy: \\boldsymbol{w} \\leftarrow \\boldsymbol{w} - \\alpha_w \\nabla_{\\boldsymbol{w}}L_\\pi\n\nReturn \\boldsymbol{w}\n\nWhen \\lambda = 1, this reduces (up to advantage normalization) to the Monte Carlo baseline algorithm earlier in the chapter. When \\lambda = 0, advantages become the one-step residuals \\delta_t, and the \\lambda-returns reduce to standard one-step bootstrapped targets.","type":"content","url":"/pg#using-gae-in-the-policy-gradient","position":39},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl4":"Actor-Critic as the \\lambda = 0 Limit","lvl3":"Generalized Advantage Estimation","lvl2":"Score Function Methods in Reinforcement Learning"},"type":"lvl4","url":"/pg#actor-critic-as-the-lambda-0-limit","position":40},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl4":"Actor-Critic as the \\lambda = 0 Limit","lvl3":"Generalized Advantage Estimation","lvl2":"Score Function Methods in Reinforcement Learning"},"content":"The case \\lambda = 0 is particularly simple. The advantage becomes:A_t^{\\lambda=0} = \\delta_t = r_t + \\gamma v(s_{t+1}) - v(s_t)\n\nand the policy update reduces to:\\boldsymbol{w} \\leftarrow \\boldsymbol{w} + \\alpha_w \\nabla_{\\boldsymbol{w}}\\log \\pi_{\\boldsymbol{w}}(a_t|s_t) \\delta_t\n\nwhile the value update becomes a standard one-step regression toward r_t + \\gamma v(s_{t+1}). This gives the online actor-critic algorithm:\n\nActor-Critic with One-Step Residuals\n\nInput: Policy parameterization \\pi_{\\boldsymbol{w}}(a|s), value function v(s;\\boldsymbol{\\theta})Output: Updated policy parameters \\boldsymbol{w}Hyperparameters: Learning rates \\alpha_w, \\alpha_\\theta, number of episodes N, episode length T, discount \\gamma\n\nInitialize parameters \\boldsymbol{w}, \\boldsymbol{\\theta}\n\nFor episode = 1, ..., N do:\n\nInitialize state s_0\n\nFor t = 0, ..., T do:\n\nSample action: a_t \\sim \\pi_{\\boldsymbol{w}}(\\cdot|s_t)\n\nExecute a_t, observe r_t, s_{t+1}\n\nCompute residual: \\delta_t = r_t + \\gamma v(s_{t+1};\\boldsymbol{\\theta}) - v(s_t;\\boldsymbol{\\theta})\n\nUpdate value function: \\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} + \\alpha_\\theta \\delta_t \\nabla_{\\boldsymbol{\\theta}}v(s_t;\\boldsymbol{\\theta})\n\nUpdate policy: \\boldsymbol{w} \\leftarrow \\boldsymbol{w} + \\alpha_w \\nabla_{\\boldsymbol{w}}\\log \\pi_{\\boldsymbol{w}}(a_t|s_t) \\delta_t\n\nReturn \\boldsymbol{w}\n\nThis algorithm was derived by Sutton in his 1984 thesis as an “adaptive heuristic” for temporal credit assignment. In the language of this chapter, it is the \\lambda = 0 member of the GAE family: it uses the most local residual \\delta_t as both the target for the value function and the advantage estimate for the policy gradient.","type":"content","url":"/pg#actor-critic-as-the-lambda-0-limit","position":41},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl2":"Likelihood Ratio Methods in Reinforcement Learning"},"type":"lvl2","url":"/pg#likelihood-ratio-methods-in-reinforcement-learning","position":42},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl2":"Likelihood Ratio Methods in Reinforcement Learning"},"content":"The score function estimator from the previous section is a special case of the likelihood ratio method where the proposal distribution equals the target distribution. We now consider the general case where they differ.\n\nRecall the likelihood ratio gradient estimator from the beginning of this chapter. For objective J(\\theta) = \\mathbb{E}_{x \\sim p(x;\\theta)}[f(x)] and any proposal distribution q(x):\\nabla_\\theta J(\\theta) = \\mathbb{E}_{x \\sim q(x)}\\left[f(x) \\rho(x; q, \\theta) \\nabla_\\theta \\log p(x;\\theta)\\right]\n\nwhere \\rho(x; q, \\theta) = \\frac{p(x;\\theta)}{q(x)} is the likelihood ratio. The partial derivative \\frac{\\partial \\rho}{\\partial \\theta} = \\rho \\nabla_\\theta \\log p holds because x is treated as fixed, having been sampled from q, which does not depend on \\theta.\n\nIn reinforcement learning, let x = \\tau be a trajectory, f(\\tau) = G(\\tau) the return, p(\\tau;\\boldsymbol{w}) the trajectory distribution under policy \\pi_{\\boldsymbol{w}}, and q(\\tau) the trajectory distribution under some other policy \\pi_q. The gradient becomes:\\nabla_{\\boldsymbol{w}} J(\\boldsymbol{w}) = \\mathbb{E}_{\\tau \\sim \\pi_q}\\left[G(\\tau) \\rho(\\tau) \\sum_{t=0}^T \\nabla_{\\boldsymbol{w}} \\log \\pi_{\\boldsymbol{w}}(a_t|s_t)\\right]\n\nwhere the trajectory likelihood ratio simplifies because transition probabilities cancel:\\rho(\\tau) = \\frac{p(\\tau;\\boldsymbol{w})}{q(\\tau)} = \\prod_{t=0}^T \\frac{\\pi_{\\boldsymbol{w}}(a_t|s_t)}{\\pi_q(a_t|s_t)} = \\prod_{t=0}^T \\rho_t\n\nThis product of T+1 ratios can become extremely large or small as T grows, leading to high variance. The temporal structure provides some relief: since \\mathbb{E}_{a \\sim \\pi_q}[\\rho] = 1, future ratios \\rho_{k} for k > t that do not affect the reward r_t can be marginalized out. However, past ratios \\rho_{0:t-1} are still needed to correctly weight the probability of reaching state s_t.\n\nIn practice, algorithms like PPO and TRPO make an additional approximation: they use only the per-step ratio \\rho_t rather than the cumulative product \\rho_{0:t}. This ignores the mismatch between the state distributions induced by the two policies. Combined with a baseline b(s_t), the approximate estimator is:\\nabla_{\\boldsymbol{w}} J(\\boldsymbol{w}) \\approx \\mathbb{E}_{\\tau \\sim \\pi_q}\\left[\\sum_{t=0}^T \\rho_t \\nabla_{\\boldsymbol{w}} \\log \\pi_{\\boldsymbol{w}}(a_t|s_t) (G_t - b(s_t))\\right]\n\nThis approximation corresponds to maximizing the importance-weighted surrogate objective:L^{\\text{IS}}(\\boldsymbol{w}) = \\mathbb{E}_{\\tau \\sim \\pi_q}\\left[\\sum_{t=0}^T \\rho_t A_t\\right]\n\nwhere A_t = G_t - b(s_t). Taking the gradient with respect to \\boldsymbol{w}, only \\rho_t depends on \\boldsymbol{w} (since trajectories are sampled from \\pi_q):\\nabla_{\\boldsymbol{w}} L^{\\text{IS}}(\\boldsymbol{w}) = \\mathbb{E}_{\\tau \\sim \\pi_q}\\left[\\sum_{t=0}^T A_t \\nabla_{\\boldsymbol{w}} \\rho_t\\right]\n\nThe gradient of the ratio is:\\nabla_{\\boldsymbol{w}} \\rho_t = \\nabla_{\\boldsymbol{w}} \\frac{\\pi_{\\boldsymbol{w}}(a_t|s_t)}{\\pi_q(a_t|s_t)} = \\frac{\\nabla_{\\boldsymbol{w}} \\pi_{\\boldsymbol{w}}(a_t|s_t)}{\\pi_q(a_t|s_t)} = \\rho_t \\nabla_{\\boldsymbol{w}} \\log \\pi_{\\boldsymbol{w}}(a_t|s_t)\n\nSubstituting back:\\nabla_{\\boldsymbol{w}} L^{\\text{IS}}(\\boldsymbol{w}) = \\mathbb{E}_{\\tau \\sim \\pi_q}\\left[\\sum_{t=0}^T \\rho_t \\nabla_{\\boldsymbol{w}} \\log \\pi_{\\boldsymbol{w}}(a_t|s_t) A_t\\right]\n\nThis matches equation \n\n(76). When \\pi_q = \\pi_{\\boldsymbol{w}}, the ratios \\rho_t = 1 and we recover the score function estimator. The approximation error grows as the policies diverge, which motivates the trust region and clipping mechanisms discussed below.","type":"content","url":"/pg#likelihood-ratio-methods-in-reinforcement-learning","position":43},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl3":"Variance and the Dominance Condition","lvl2":"Likelihood Ratio Methods in Reinforcement Learning"},"type":"lvl3","url":"/pg#variance-and-the-dominance-condition","position":44},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl3":"Variance and the Dominance Condition","lvl2":"Likelihood Ratio Methods in Reinforcement Learning"},"content":"The ratio \\rho_t = \\pi_{\\boldsymbol{w}}(a_t|s_t)/\\pi_q(a_t|s_t) is well-behaved only when the two policies are similar. If \\pi_{\\boldsymbol{w}} assigns high probability to an action where \\pi_q assigns low probability, the ratio explodes. For example, if \\pi_q(a|s) = 0.01 and \\pi_{\\boldsymbol{w}}(a|s) = 0.5, then \\rho = 50, amplifying any noise in the advantage estimate.\n\nImportance sampling also requires the dominance condition: the support of \\pi_{\\boldsymbol{w}} must be contained in the support of \\pi_q. If \\pi_{\\boldsymbol{w}}(a|s) > 0 but \\pi_q(a|s) = 0, the ratio is undefined. Stochastic policies typically have full support, but the ratio can still become arbitrarily large as \\pi_q(a|s) \\to 0.\n\nA common use case is to set \\pi_q = \\pi_{\\boldsymbol{w}_{\\text{old}}}, a previous version of the policy. This allows reusing data across multiple gradient steps: collect trajectories once, then update \\boldsymbol{w} several times. But each update moves \\boldsymbol{w} further from \\boldsymbol{w}_{\\text{old}}, making the ratios more extreme. Eventually, the gradient signal is dominated by a few samples with large weights.","type":"content","url":"/pg#variance-and-the-dominance-condition","position":45},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl3":"Proximal Policy Optimization","lvl2":"Likelihood Ratio Methods in Reinforcement Learning"},"type":"lvl3","url":"/pg#proximal-policy-optimization","position":46},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl3":"Proximal Policy Optimization","lvl2":"Likelihood Ratio Methods in Reinforcement Learning"},"content":"The variance issues suggest a natural solution: keep the ratio \\rho_t close to 1 by ensuring the new policy stays close to the behavior policy. This keeps the importance-weighted surrogate L^{\\text{IS}}(\\boldsymbol{w}) from \n\n(77) well-behaved.\n\nTrust Region Policy Optimization (TRPO) formalizes this by adding a constraint on the KL divergence between the old and new policies:\\max_{\\boldsymbol{w}} L^{\\text{IS}}(\\boldsymbol{w}) \\quad \\text{subject to} \\quad \\mathbb{E}_s\\left[D_{\\text{KL}}(\\pi_{\\boldsymbol{w}_{\\text{old}}}(\\cdot|s) \\| \\pi_{\\boldsymbol{w}}(\\cdot|s))\\right] \\leq \\delta\n\nThe KL constraint ensures that the two distributions remain similar, which bounds how extreme the importance weights can become. This is a constrained optimization problem, and one could in principle apply standard methods such as projected gradient descent or augmented Lagrangian approaches (as discussed in the \n\ntrajectory optimization chapter). TRPO takes a different approach: it uses a second-order Taylor approximation of the KL constraint around the current parameters and solves the resulting trust region subproblem using conjugate gradient methods. This involves computing the Fisher information matrix (the Hessian of the KL divergence), which adds computational overhead.\n\nProximal Policy Optimization (PPO) achieves similar behavior through a simpler mechanism: rather than constraining the distributions to be similar, it directly clips the ratio \\rho_t to prevent it from moving too far from 1. This is a construction-level guarantee rather than an optimization-level constraint.","type":"content","url":"/pg#proximal-policy-optimization","position":47},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl4":"From Trajectory Expectations to State-Action Averages","lvl3":"Proximal Policy Optimization","lvl2":"Likelihood Ratio Methods in Reinforcement Learning"},"type":"lvl4","url":"/pg#from-trajectory-expectations-to-state-action-averages","position":48},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl4":"From Trajectory Expectations to State-Action Averages","lvl3":"Proximal Policy Optimization","lvl2":"Likelihood Ratio Methods in Reinforcement Learning"},"content":"Before defining the PPO objective, we need to clarify the relationship between the trajectory-level surrogate \n\n(77) and the state-action level objective that PPO actually optimizes. The importance-weighted surrogate is defined as an expectation over trajectories:L^{\\text{IS}}(\\boldsymbol{w}) = \\mathbb{E}_{\\tau \\sim \\pi_q}\\left[\\sum_{t=0}^T \\rho_t A_t\\right]\n\nWe can rewrite this as an expectation over state-action pairs by introducing a sampling distribution. For a finite horizon T, define the averaged time-marginal distribution:\\xi_{\\pi_q}(s, a) = \\frac{1}{T+1} \\sum_{t=0}^{T} d_t^{\\pi_q}(s) \\pi_q(a|s)\n\nwhere d_t^{\\pi_q}(s) is the probability of being in state s at time t when following policy \\pi_q from the initial distribution. This is the uniform mixture over the time-indexed state-action distributions: we pick a timestep t uniformly at random from \\{0, 1, \\ldots, T\\}, then sample (s, a) from the joint distribution at that timestep.\n\nWith this definition, the trajectory expectation becomes:\\mathbb{E}_{\\tau \\sim \\pi_q}\\left[\\sum_{t=0}^T \\rho_t A_t\\right] = (T+1) \\cdot \\mathbb{E}_{(s,a) \\sim \\xi_{\\pi_q}}\\left[\\rho(s, a) A(s, a)\\right]\n\nThe factor (T+1) is just a constant that does not affect the optimization. This reformulation shows that the importance-weighted surrogate is equivalent to an expectation over state-action pairs drawn from the averaged time-marginal distribution. This is not a stationary distribution or a discounted visitation distribution, but the empirical mixture induced by the finite-horizon rollout procedure.","type":"content","url":"/pg#from-trajectory-expectations-to-state-action-averages","position":49},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl4":"The Clipped Surrogate Objective","lvl3":"Proximal Policy Optimization","lvl2":"Likelihood Ratio Methods in Reinforcement Learning"},"type":"lvl4","url":"/pg#the-clipped-surrogate-objective","position":50},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl4":"The Clipped Surrogate Objective","lvl3":"Proximal Policy Optimization","lvl2":"Likelihood Ratio Methods in Reinforcement Learning"},"content":"PPO replaces the linear importance-weighted term \\rho A with a clipped version. For a state-action pair (s, a) with advantage A and importance ratio \\rho(\\boldsymbol{w}) = \\pi_{\\boldsymbol{w}}(a|s) / \\pi_{\\boldsymbol{w}_{\\text{old}}}(a|s), define the per-sample clipped objective:\\ell^{\\text{CLIP}}(\\boldsymbol{w}; s, a, A) = \\min\\left(\\rho(\\boldsymbol{w}) A, \\, \\text{clip}(\\rho(\\boldsymbol{w}), 1-\\epsilon, 1+\\epsilon) A\\right)\n\nwhere \\epsilon is a hyperparameter (typically 0.1 or 0.2) and \\text{clip}(x, a, b) = \\max(a, \\min(x, b)) restricts x to the interval [a, b].\n\nThe population-level PPO objective is then:L^{\\text{CLIP}}(\\boldsymbol{w}) = \\mathbb{E}_{(s,a,A) \\sim \\xi_{\\pi_{\\boldsymbol{w}_{\\text{old}}}}}\\left[\\ell^{\\text{CLIP}}(\\boldsymbol{w}; s, a, A)\\right]\n\nwhere the expectation is taken over the averaged time-marginal distribution \n\n(83) induced by \\pi_{\\boldsymbol{w}_{\\text{old}}}.\n\nIn practice, we never compute this expectation exactly. Instead, we collect a batch of transitions \\mathcal{D} = \\{(s_t^{(i)}, a_t^{(i)}, A_t^{(i)})\\} by running \\pi_{\\boldsymbol{w}_{\\text{old}}} and approximate the expectation with an empirical average:\\hat{L}^{\\text{CLIP}}(\\boldsymbol{w}; \\mathcal{D}) = \\frac{1}{|\\mathcal{D}|} \\sum_{(s,a,A) \\in \\mathcal{D}} \\ell^{\\text{CLIP}}(\\boldsymbol{w}; s, a, A)\n\nThis is the same plug-in approximation used in \n\nfitted Q-iteration: replace the unknown population distribution with the empirical distribution \\hat{P}_{\\mathcal{D}} induced by the collected batch, then compute the sample average. The empirical surrogate \\hat{L}^{\\text{CLIP}} is simply an expectation under \\hat{P}_{\\mathcal{D}}. No assumptions about stationarity or discounted visitation are needed. We just average over the transitions we collected.","type":"content","url":"/pg#the-clipped-surrogate-objective","position":51},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl4":"Intuition for the Clipping Mechanism","lvl3":"Proximal Policy Optimization","lvl2":"Likelihood Ratio Methods in Reinforcement Learning"},"type":"lvl4","url":"/pg#intuition-for-the-clipping-mechanism","position":52},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl4":"Intuition for the Clipping Mechanism","lvl3":"Proximal Policy Optimization","lvl2":"Likelihood Ratio Methods in Reinforcement Learning"},"content":"The \\min operator in \n\n(85) selects the more pessimistic estimate. Consider the two cases:\n\nPositive advantage (A > 0): The action is better than average, so we want to increase \\pi_{\\boldsymbol{w}}(a|s). The unclipped term \\rho A increases with \\rho. The clipped term stops increasing once \\rho > 1 + \\epsilon. Taking the minimum means we get the benefit of increasing \\rho only up to 1 + \\epsilon.\n\nNegative advantage (A < 0): The action is worse than average, so we want to decrease \\pi_{\\boldsymbol{w}}(a|s). The unclipped term \\rho A becomes less negative (improves) as \\rho decreases. The clipped term stops improving once \\rho < 1 - \\epsilon. Taking the minimum means we get the benefit of decreasing \\rho only down to 1 - \\epsilon.\n\nIn both cases, the clipping removes the incentive to move the probability ratio beyond the interval [1-\\epsilon, 1+\\epsilon]. This keeps the new policy close to the old policy without explicitly computing or constraining the KL divergence.\n\nProximal Policy Optimization (PPO-Clip)\n\nInput: Policy \\pi_{\\boldsymbol{w}}(a|s), value function v(s;\\boldsymbol{\\theta})Output: Updated parameters \\boldsymbol{w}Hyperparameters: Clip parameter \\epsilon, learning rates \\alpha_w, \\alpha_\\theta, number of iterations N, batch size B, mini-batch size M, epochs per iteration K, GAE parameters \\gamma, \\lambda\n\nInitialize parameters \\boldsymbol{w}, \\boldsymbol{\\theta}\n\nFor iteration = 1, ..., N:\n\nCollect batch \\mathcal{D} of B trajectories using policy \\pi_{\\boldsymbol{w}}\n\nCompute GAE advantages A_t and \\lambda-returns G_t^{\\lambda} for all timesteps\n\nStore old log-probabilities: \\log \\pi_{\\text{old}}(a_t|s_t) = \\log \\pi_{\\boldsymbol{w}}(a_t|s_t) for all (s_t, a_t)\n\nNormalize advantages: A \\leftarrow \\frac{A - \\mu_A}{\\sigma_A}\n\nFor epoch = 1, ..., K:\n\nShuffle \\mathcal{D} and partition into mini-batches of size M\n\nFor each mini-batch \\mathcal{B}:\n\nCompute ratios: \\rho = \\exp(\\log \\pi_{\\boldsymbol{w}}(a|s) - \\log \\pi_{\\text{old}}(a|s)) for all (s, a) \\in \\mathcal{B}\n\nCompute per-sample objectives: \\ell^{\\text{CLIP}} = \\min(\\rho A, \\text{clip}(\\rho, 1-\\epsilon, 1+\\epsilon) A)\n\nCompute empirical surrogate: \\hat{L}^{\\text{CLIP}} = \\frac{1}{|\\mathcal{B}|}\\sum_{(s,a,A) \\in \\mathcal{B}} \\ell^{\\text{CLIP}}\n\nCompute value loss: L_v = \\frac{1}{|\\mathcal{B}|}\\sum_{(s,G^{\\lambda}) \\in \\mathcal{B}} (v(s;\\boldsymbol{\\theta}) - G^{\\lambda})^2\n\nUpdate policy: \\boldsymbol{w} \\leftarrow \\boldsymbol{w} + \\alpha_w \\nabla_{\\boldsymbol{w}} \\hat{L}^{\\text{CLIP}}\n\nUpdate value function: \\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\alpha_\\theta \\nabla_{\\boldsymbol{\\theta}} L_v\n\nReturn \\boldsymbol{w}\n\nThe algorithm collects a batch of trajectories, then performs K epochs of mini-batch updates on the same data. The empirical surrogate \\hat{L}^{\\text{CLIP}} approximates the population objective \n\n(86) using samples from the averaged time-marginal distribution. The clipped objective ensures that even after multiple updates, the policy does not move too far from the policy that collected the data. The ratio \\rho is computed in log-space for numerical stability.\n\nPPO has become one of the most widely used policy gradient algorithms due to its simplicity and robustness. Compared to TRPO, it avoids the computational overhead of constrained optimization while achieving similar sample efficiency. The clip parameter \\epsilon is the main hyperparameter controlling the trust region size: smaller values keep the policy closer to the behavior policy but may slow learning, while larger values allow faster updates but risk instability.","type":"content","url":"/pg#intuition-for-the-clipping-mechanism","position":53},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl2":"The Policy Gradient Theorem"},"type":"lvl2","url":"/pg#the-policy-gradient-theorem","position":54},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl2":"The Policy Gradient Theorem"},"content":"The algorithms developed so far (REINFORCE, actor-critic, GAE, and PPO) all estimate policy gradients from sampled trajectories. We now establish the theoretical foundation for these estimators by deriving the policy gradient theorem in the discounted infinite-horizon setting.\n\nSutton et al. (1999) provided the original derivation. Here we present an alternative approach using the Implicit Function Theorem, which frames policy optimization as a bilevel problem:\\max_{\\mathbf{w}} \\alpha^\\top \\mathbf{v}_\\gamma^{\\pi_{\\boldsymbol{w}}}\n\nsubject to:(\\mathbf{I} - \\gamma \\mathbf{P}_{\\pi_{\\boldsymbol{w}}}) \\mathbf{v}_\\gamma^{\\pi_{\\boldsymbol{w}}} = \\mathbf{r}_{\\pi_{\\boldsymbol{w}}}\n\nThe Implicit Function Theorem states that if there is a solution to the problem F(\\mathbf{v}, \\mathbf{w}) = 0, then we can “reparameterize” our problem as F(\\mathbf{v}(\\mathbf{w}), \\mathbf{w}) where \\mathbf{v}(\\mathbf{w}) is an implicit function of \\mathbf{w}. If the Jacobian \\frac{\\partial F}{\\partial \\mathbf{v}} is invertible, then:\\frac{d\\mathbf{v}(\\mathbf{w})}{d\\mathbf{w}} = -\\left(\\frac{\\partial F(\\mathbf{v}(\\mathbf{w}), \\mathbf{w})}{\\partial \\mathbf{v}}\\right)^{-1}\\frac{\\partial F(\\mathbf{v}(\\mathbf{w}), \\mathbf{w})}{\\partial \\mathbf{w}}\n\nHere we made it clear in our notation that the derivative must be evaluated at root (\\mathbf{v}(\\mathbf{w}), \\mathbf{w}) of F. For the remaining of this derivation, we will drop this dependence to make notation more compact.\n\nApplying this to our case with F(\\mathbf{v}, \\mathbf{w}) = (\\mathbf{I} - \\gamma \\mathbf{P}_{\\pi_{\\boldsymbol{w}}})\\mathbf{v} - \\mathbf{r}_{\\pi_{\\boldsymbol{w}}}:\\frac{\\partial \\mathbf{v}_\\gamma^{\\pi_{\\boldsymbol{w}}}}{\\partial \\mathbf{w}} = (\\mathbf{I} - \\gamma \\mathbf{P}_{\\pi_{\\boldsymbol{w}}})^{-1}\\left(\\frac{\\partial \\mathbf{r}_{\\pi_{\\boldsymbol{w}}}}{\\partial \\mathbf{w}} + \\gamma \\frac{\\partial \\mathbf{P}_{\\pi_{\\boldsymbol{w}}}}{\\partial \\mathbf{w}}\\mathbf{v}_\\gamma^{\\pi_{\\boldsymbol{w}}}\\right)\n\nThen:\\begin{align*}\n\\nabla_{\\mathbf{w}}J(\\mathbf{w}) &= \\alpha^\\top \\frac{\\partial \\mathbf{v}_\\gamma^{\\pi_{\\boldsymbol{w}}}}{\\partial \\mathbf{w}} \\\\\n&= \\mathbf{x}_\\alpha^\\top\\left(\\frac{\\partial \\mathbf{r}_{\\pi_{\\boldsymbol{w}}}}{\\partial \\mathbf{w}} + \\gamma \\frac{\\partial \\mathbf{P}_{\\pi_{\\boldsymbol{w}}}}{\\partial \\mathbf{w}}\\mathbf{v}_\\gamma^{\\pi_{\\boldsymbol{w}}}\\right)\n\\end{align*}\n\nwhere we have defined the discounted state visitation distribution:\\mathbf{x}_\\alpha^\\top \\equiv \\alpha^\\top(\\mathbf{I} - \\gamma \\mathbf{P}_{\\pi_{\\boldsymbol{w}}})^{-1}.\n\nRecall the vector notation for MDPs from the \n\ndynamic programming chapter:\\begin{align*}\n\\mathbf{r}_\\pi(s) &\\equiv \\sum_{a \\in \\mathcal{A}_s} \\pi(a \\mid s) \\, r(s, a), \\\\\n[\\mathbf{P}_\\pi]_{s,s'} &\\equiv \\sum_{a \\in \\mathcal{A}_s} \\pi(a \\mid s) \\, p(s' \\mid s, a).\n\\end{align*}\n\nTaking derivatives with respect to \\mathbf{w} gives:\\begin{align*}\n\\left[\\frac{\\partial \\mathbf{r}_{\\pi_{\\boldsymbol{w}}}}{\\partial \\mathbf{w}}\\right]_s &= \\sum_{a \\in \\mathcal{A}_s} \\nabla_{\\mathbf{w}}\\pi_{\\boldsymbol{w}}(a \\mid s) \\, r(s,a), \\\\\n\\left[\\frac{\\partial \\mathbf{P}_{\\pi_{\\boldsymbol{w}}}}{\\partial \\mathbf{w}}\\mathbf{v}_\\gamma^{\\pi_{\\boldsymbol{w}}}\\right]_s &= \\sum_{a \\in \\mathcal{A}_s} \\nabla_{\\mathbf{w}}\\pi_{\\boldsymbol{w}}(a \\mid s)\\sum_{s'} p(s' \\mid s,a) \\, v_\\gamma^{\\pi_{\\boldsymbol{w}}}(s').\n\\end{align*}\n\nSubstituting back:\\begin{align*}\n\\nabla_{\\mathbf{w}}J(\\mathbf{w}) &= \\sum_s x_\\alpha(s)\\left(\\sum_a \\nabla_{\\mathbf{w}}\\pi_{\\boldsymbol{w}}(a \\mid s) \\, r(s,a) + \\gamma\\sum_a \\nabla_{\\mathbf{w}}\\pi_{\\boldsymbol{w}}(a \\mid s)\\sum_{s'} p(s' \\mid s,a) \\, v_\\gamma^{\\pi_{\\boldsymbol{w}}}(s')\\right) \\\\\n&= \\sum_s x_\\alpha(s)\\sum_a \\nabla_{\\mathbf{w}}\\pi_{\\boldsymbol{w}}(a \\mid s)\\left(r(s,a) + \\gamma \\sum_{s'} p(s' \\mid s,a) \\, v_\\gamma^{\\pi_{\\boldsymbol{w}}}(s')\\right)\n\\end{align*}\n\nThis is the policy gradient theorem, where x_\\alpha(s) is the discounted state visitation distribution and the term in parentheses is the state-action value function q^{\\pi_{\\boldsymbol{w}}}(s,a).","type":"content","url":"/pg#the-policy-gradient-theorem","position":55},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl3":"Normalized Discounted State Visitation Distribution","lvl2":"The Policy Gradient Theorem"},"type":"lvl3","url":"/pg#normalized-discounted-state-visitation-distribution","position":56},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl3":"Normalized Discounted State Visitation Distribution","lvl2":"The Policy Gradient Theorem"},"content":"The discounted state visitation x_\\alpha(s) is not normalized. Therefore the expression we obtained above is not an expectation. However, we can transform it into one by normalizing by 1 - \\gamma. Note that for any initial distribution \\alpha:\\sum_s x_\\alpha(s) = \\alpha^\\top(\\mathbf{I} - \\gamma \\mathbf{P}_{\\pi_{\\boldsymbol{w}}})^{-1}\\mathbf{1} = \\frac{\\alpha^\\top\\mathbf{1}}{1-\\gamma} = \\frac{1}{1-\\gamma}\n\nTherefore, defining the normalized state distribution \\xi_\\alpha(s) = (1-\\gamma)x_\\alpha(s), we can write:\\begin{align*}\n\\nabla_{\\mathbf{w}}J(\\mathbf{w}) &= \\frac{1}{1-\\gamma}\\sum_s \\xi_\\alpha(s)\\sum_a \\nabla_{\\mathbf{w}}\\pi_{\\boldsymbol{w}}(a \\mid s)\\left(r(s,a) + \\gamma \\sum_{s'} p(s' \\mid s,a) \\, v_\\gamma^{\\pi_{\\boldsymbol{w}}}(s')\\right) \\\\\n&= \\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim\\xi_\\alpha}\\left[\\sum_a \\nabla_{\\mathbf{w}}\\pi_{\\boldsymbol{w}}(a \\mid s) \\, q^{\\pi_{\\boldsymbol{w}}}(s,a)\\right]\n\\end{align*}\n\nNow we have expressed the policy gradient theorem in terms of expectations under the normalized discounted state visitation distribution. But what does sampling from \\xi_\\alpha mean? Recall that \\mathbf{x}_\\alpha^\\top = \\alpha^\\top(\\mathbf{I} - \\gamma \\mathbf{P}_{\\pi_{\\boldsymbol{w}}})^{-1}. Using the Neumann series expansion (valid when \\|\\gamma \\mathbf{P}_{\\pi_{\\boldsymbol{w}}}\\| < 1, which holds for \\gamma < 1 since \\mathbf{P}_{\\pi_{\\boldsymbol{w}}} is a stochastic matrix) we have:\\boldsymbol{\\xi}_\\alpha^\\top = (1-\\gamma)\\alpha^\\top\\sum_{k=0}^{\\infty} (\\gamma \\mathbf{P}_{\\pi_{\\boldsymbol{w}}})^k\n\nWe can then factor out the first term from this summation to obtain:\\begin{align*}\n\\boldsymbol{\\xi}_\\alpha^\\top &= (1-\\gamma)\\alpha^\\top\\sum_{k=0}^{\\infty} (\\gamma \\mathbf{P}_{\\pi_{\\boldsymbol{w}}})^k \\\\\n&= (1-\\gamma)\\alpha^\\top + (1-\\gamma)\\alpha^\\top\\sum_{k=1}^{\\infty} (\\gamma \\mathbf{P}_{\\pi_{\\boldsymbol{w}}})^k \\\\\n&= (1-\\gamma)\\alpha^\\top + (1-\\gamma)\\alpha^\\top\\gamma\\mathbf{P}_{\\pi_{\\boldsymbol{w}}}\\sum_{k=0}^{\\infty} (\\gamma \\mathbf{P}_{\\pi_{\\boldsymbol{w}}})^k \\\\\n&= (1-\\gamma)\\alpha^\\top + \\gamma\\boldsymbol{\\xi}_\\alpha^\\top \\mathbf{P}_{\\pi_{\\boldsymbol{w}}}\n\\end{align*}\n\nThe balance equation:\\boldsymbol{\\xi}_\\alpha^\\top = (1-\\gamma)\\alpha^\\top + \\gamma\\boldsymbol{\\xi}_\\alpha^\\top \\mathbf{P}_{\\pi_{\\boldsymbol{w}}}\n\nshows that \\boldsymbol{\\xi}_\\alpha is a mixture distribution: with probability 1-\\gamma you draw a state from the initial distribution \\alpha (reset), and with probability \\gamma you follow the policy dynamics \\mathbf{P}_{\\pi_{\\boldsymbol{w}}} from the current state (continue). This interpretation directly connects to the geometric process: at each step you either terminate and resample from \\alpha (with probability 1-\\gamma) or continue following the policy (with probability \\gamma).\n\nimport numpy as np\n\ndef sample_from_discounted_visitation(\n    alpha, \n    policy, \n    transition_model, \n    gamma, \n    n_samples=1000\n):\n    \"\"\"Sample states from the discounted visitation distribution.\n    \n    Args:\n        alpha: Initial state distribution (vector of probabilities)\n        policy: Function (state -> action probabilities)\n        transition_model: Function (state, action -> next state probabilities)\n        gamma: Discount factor\n        n_samples: Number of states to sample\n    \n    Returns:\n        Array of sampled states\n    \"\"\"\n    samples = []\n    n_states = len(alpha)\n    \n    # Initialize state from alpha\n    current_state = np.random.choice(n_states, p=alpha)\n    \n    for _ in range(n_samples):\n        samples.append(current_state)\n        \n        # With probability (1-gamma): reset\n        if np.random.random() > gamma:\n            current_state = np.random.choice(n_states, p=alpha)\n        # With probability gamma: continue\n        else:\n            # Sample action from policy\n            action_probs = policy(current_state)\n            action = np.random.choice(len(action_probs), p=action_probs)\n            \n            # Sample next state from transition model\n            next_state_probs = transition_model(current_state, action)\n            current_state = np.random.choice(n_states, p=next_state_probs)\n    \n    return np.array(samples)\n\n# Example usage for a simple 2-state MDP\nalpha = np.array([0.7, 0.3])  # Initial distribution\npolicy = lambda s: np.array([0.8, 0.2])  # Dummy policy\ntransition_model = lambda s, a: np.array([0.9, 0.1])  # Dummy transitions\ngamma = 0.9\n\nsamples = sample_from_discounted_visitation(alpha, policy, transition_model, gamma)\n\n# Check empirical distribution\nprint(\"Empirical state distribution:\")\nprint(np.bincount(samples) / len(samples))\n\nWhile the math shows that sampling from the discounted visitation distribution \\boldsymbol{\\xi}_\\alpha would give us unbiased policy gradient estimates, Thomas (2014) demonstrated that this implementation can be detrimental to performance in practice. The issue arises because terminating trajectories early (with probability 1-\\gamma) reduces the effective amount of data we collect from each trajectory. This early termination weakens the learning signal, as many trajectories don’t reach meaningful terminal states or rewards.\n\nTherefore, in practice, we typically sample complete trajectories from the undiscounted process (running the policy until natural termination or a fixed horizon) while still using \\gamma in the advantage estimation. This approach preserves the full learning signal from each trajectory and has been empirically shown to lead to better performance.\n\nThis is one of several cases in RL where the theoretically optimal procedure differs from the best practical implementation.","type":"content","url":"/pg#normalized-discounted-state-visitation-distribution","position":57},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl3":"The Actor-Critic Architecture","lvl2":"The Policy Gradient Theorem"},"type":"lvl3","url":"/pg#the-actor-critic-architecture","position":58},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl3":"The Actor-Critic Architecture","lvl2":"The Policy Gradient Theorem"},"content":"The policy gradient theorem shows that the gradient depends on the action-value function q^{\\pi_{\\boldsymbol{w}}}(s,a). In practice, we do not have access to the true q-function and must estimate it. This leads to the actor-critic architecture: the actor maintains the policy \\pi_{\\boldsymbol{w}}, while the critic maintains an estimate of the value function.\n\nThis architecture traces back to Sutton’s 1984 thesis, where he proposed the Adaptive Heuristic Critic. The actor uses the critic’s value estimates to compute advantage estimates for the policy gradient, while the critic learns from the same trajectories generated by the actor. The algorithms we developed earlier (REINFORCE with baseline, GAE, and the one-step actor-critic) are all instances of this architecture.\n\nWe are simultaneously learning two functions that depend on each other, which creates a stability challenge. The actor’s gradient uses the critic’s estimates, but the critic is trained on data generated by the actor’s policy. If both change too quickly, the learning process can become unstable.\n\nKonda (2002) analyzed this coupled learning problem and established convergence guarantees under a two-timescale condition: the critic must update faster than the actor. Intuitively, the critic needs to “track” the current policy’s value function before the actor uses those estimates to update. If the actor moves too fast, it uses stale or inaccurate value estimates, leading to poor gradient estimates.\n\nIn practice, this is implemented by using different learning rates: a larger learning rate \\alpha_\\theta for the critic and a smaller learning rate \\alpha_w for the actor, with \\alpha_\\theta > \\alpha_w. Alternatively, one can perform multiple critic updates per actor update. The soft actor-critic algorithm discussed earlier in the \n\namortization chapter follows this same principle, inheriting the actor-critic structure while incorporating entropy regularization and learning Q-functions directly.\n\nThe actor-critic architecture also connects to the bilevel optimization perspective of the policy gradient theorem: the outer problem optimizes the policy, while the inner problem solves for the value function given that policy. The two-timescale condition ensures that the inner problem is approximately solved before taking a step on the outer problem.","type":"content","url":"/pg#the-actor-critic-architecture","position":59},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl2":"Reparameterization Methods in Reinforcement Learning"},"type":"lvl2","url":"/pg#reparameterization-methods-in-reinforcement-learning","position":60},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl2":"Reparameterization Methods in Reinforcement Learning"},"content":"When dynamics are known or can be learned, reparameterization provides an alternative to score function methods. By expressing actions and state transitions as deterministic functions of noise, we can backpropagate through trajectories to compute policy gradients with lower variance than score function estimators.","type":"content","url":"/pg#reparameterization-methods-in-reinforcement-learning","position":61},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl3":"Stochastic Value Gradients","lvl2":"Reparameterization Methods in Reinforcement Learning"},"type":"lvl3","url":"/pg#stochastic-value-gradients","position":62},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl3":"Stochastic Value Gradients","lvl2":"Reparameterization Methods in Reinforcement Learning"},"content":"The reparameterization trick requires that we can express our random variable as a deterministic function of noise. In reinforcement learning, this applies naturally when we have a learned model of the dynamics. Consider a stochastic policy \\pi_{\\boldsymbol{w}}(a|s) that we can reparameterize as a = \\pi_{\\boldsymbol{w}}(s,\\epsilon) where \\epsilon \\sim p(\\epsilon), and a dynamics model s' = f(s,a,\\xi) where \\xi \\sim p(\\xi) represents environment stochasticity. Both transformations are deterministic given the noise variables.\n\nWith these reparameterizations, we can write an n-step return as a differentiable function of the noise:R_n(s_0,\\{\\epsilon_i\\},\\{\\xi_i\\}) = \\sum_{i=0}^{n-1} \\gamma^i r(s_i,a_i)\n\nwhere a_i = \\pi_{\\boldsymbol{w}}(s_i,\\epsilon_i) and s_{i+1} = f(s_i,a_i,\\xi_i) for i=0,...,n-1. The objective becomes:J(\\boldsymbol{w}) = \\mathbb{E}_{\\{\\epsilon_i\\},\\{\\xi_i\\}}[R_n(s_0,\\{\\epsilon_i\\},\\{\\xi_i\\})]\n\nWe can now apply the reparameterization gradient estimator:\\nabla_{\\boldsymbol{w}}J(\\boldsymbol{w}) = \\mathbb{E}_{\\{\\epsilon_i\\},\\{\\xi_i\\}}\\left[\\nabla_{\\boldsymbol{w}}R_n(s_0,\\{\\epsilon_i\\},\\{\\xi_i\\})\\right]\n\nThis gradient can be computed by automatic differentiation through the sequence of policy and model evaluations. The computation requires backpropagating through n steps of model rollouts, which becomes expensive for large n but avoids the high variance of score function estimators.\n\nThe Stochastic Value Gradients (SVG) framework \n\nHeess et al., 2015 uses this approach while introducing a hybrid objective that combines model rollouts with value function bootstrapping:J^{\\text{SVG}(n)}(\\boldsymbol{w}) = \\mathbb{E}_{\\{\\epsilon_i\\},\\{\\xi_i\\}}\\left[\\sum_{i=0}^{n-1} \\gamma^i r(s_i,a_i) + \\gamma^n q(s_n,a_n;\\theta)\\right]\n\nThe terminal value function q(s_n,a_n;\\theta) approximates the value beyond horizon n, allowing shorter rollouts while still capturing long-term value. This creates a spectrum of algorithms parameterized by n.","type":"content","url":"/pg#stochastic-value-gradients","position":63},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl4":"SVG(0): Model-Free Reparameterization","lvl3":"Stochastic Value Gradients","lvl2":"Reparameterization Methods in Reinforcement Learning"},"type":"lvl4","url":"/pg#svg-0-model-free-reparameterization","position":64},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl4":"SVG(0): Model-Free Reparameterization","lvl3":"Stochastic Value Gradients","lvl2":"Reparameterization Methods in Reinforcement Learning"},"content":"When n=0, the objective collapses to:J^{\\text{SVG}(0)}(\\boldsymbol{w}) = \\mathbb{E}_{s \\sim \\rho}\\mathbb{E}_{\\epsilon \\sim p(\\epsilon)}\\left[q(s,\\pi_{\\boldsymbol{w}}(s,\\epsilon);\\theta)\\right]\n\nNo model is required. We simply differentiate the critic with respect to actions sampled from the reparameterized policy. This is the approach used in DDPG \n\nLillicrap et al., 2015 (with a deterministic policy where \\epsilon is absent) and SAC \n\nHaarnoja et al., 2018 (where \\epsilon produces the stochastic component). The gradient is:\\nabla_{\\boldsymbol{w}} J^{\\text{SVG}(0)} = \\mathbb{E}_{s,\\epsilon}\\left[\\nabla_a q(s,a;\\theta)\\big|_{a=\\pi_{\\boldsymbol{w}}(s,\\epsilon)} \\nabla_{\\boldsymbol{w}} \\pi_{\\boldsymbol{w}}(s,\\epsilon)\\right]\n\nThis requires only that the critic q be differentiable with respect to actions, not a learned dynamics model. All bias comes from errors in the value function approximation.","type":"content","url":"/pg#svg-0-model-free-reparameterization","position":65},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl4":"SVG(1) to SVG(n): Model-Based Rollouts","lvl3":"Stochastic Value Gradients","lvl2":"Reparameterization Methods in Reinforcement Learning"},"type":"lvl4","url":"/pg#svg-1-to-svg-n-model-based-rollouts","position":66},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl4":"SVG(1) to SVG(n): Model-Based Rollouts","lvl3":"Stochastic Value Gradients","lvl2":"Reparameterization Methods in Reinforcement Learning"},"content":"For n \\geq 1, we unroll a learned dynamics model for n steps before bootstrapping with the critic. Consider SVG(1):J^{\\text{SVG}(1)}(\\boldsymbol{w}) = \\mathbb{E}_{s,\\epsilon,\\xi}\\left[r(s,\\pi_{\\boldsymbol{w}}(s,\\epsilon)) + \\gamma q(f(s,\\pi_{\\boldsymbol{w}}(s,\\epsilon),\\xi), \\pi_{\\boldsymbol{w}}(s',\\epsilon');\\theta)\\right]\n\nwhere s' = f(s,\\pi_{\\boldsymbol{w}}(s,\\epsilon),\\xi) is the next state predicted by the model. The gradient now flows through both the reward and the model transition. Increasing n propagates reward information more directly through the model rollout, reducing reliance on the critic. However, model errors compound over the horizon. If the model is inaccurate, longer rollouts can degrade performance.","type":"content","url":"/pg#svg-1-to-svg-n-model-based-rollouts","position":67},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl4":"SVG(\\infty): Pure Model-Based Optimization","lvl3":"Stochastic Value Gradients","lvl2":"Reparameterization Methods in Reinforcement Learning"},"type":"lvl4","url":"/pg#svg-infty-pure-model-based-optimization","position":68},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl4":"SVG(\\infty): Pure Model-Based Optimization","lvl3":"Stochastic Value Gradients","lvl2":"Reparameterization Methods in Reinforcement Learning"},"content":"As n \\to \\infty, we eliminate the critic entirely:J^{\\text{SVG}(\\infty)}(\\boldsymbol{w}) = \\mathbb{E}_{\\{\\epsilon_i\\},\\{\\xi_i\\}}\\left[\\sum_{i=0}^{T-1} \\gamma^i r(s_i,\\pi_{\\boldsymbol{w}}(s_i,\\epsilon_i))\\right]\n\nThis is pure model-based policy optimization, differentiating through the entire trajectory. Approaches like PILCO \n\nDeisenroth & Rasmussen, 2011 and Dreamer \n\nHafner et al., 2019 operate in this regime. With an accurate model, this provides the most direct gradient signal. The tradeoff is computational: backpropagating through hundreds of time steps is expensive, and gradient magnitudes can explode or vanish over long horizons.\n\nThe choice of n reflects a fundamental bias-variance tradeoff. Small n relies on the critic for long-term value estimation, inheriting its approximation errors. Large n relies on the model, accumulating its prediction errors. In practice, intermediate values like n=5 or n=10 often work well when combined with a reasonably accurate learned model.","type":"content","url":"/pg#svg-infty-pure-model-based-optimization","position":69},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl4":"Noise Inference for Off-Policy Learning","lvl3":"Stochastic Value Gradients","lvl2":"Reparameterization Methods in Reinforcement Learning"},"type":"lvl4","url":"/pg#noise-inference-for-off-policy-learning","position":70},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl4":"Noise Inference for Off-Policy Learning","lvl3":"Stochastic Value Gradients","lvl2":"Reparameterization Methods in Reinforcement Learning"},"content":"A subtle issue arises when combining reparameterization with experience replay. SVG naturally supports off-policy learning: states s can be sampled from a replay buffer rather than the current policy. However, reparameterization requires the noise variables \\epsilon that generated each action.\n\nFor on-policy data, we can simply store \\epsilon alongside each transition (s, a, r, s'). For off-policy data collected under a different policy, the noise is unknown. To apply reparameterization gradients to such data, we must infer the noise that would have produced the observed action under the current policy.\n\nFor invertible policies, this is straightforward. If a = \\pi_{\\boldsymbol{w}}(s, \\epsilon) with \\epsilon \\sim \\mathcal{N}(0, I), and the policy takes the form a = \\mu_{\\boldsymbol{w}}(s) + \\sigma_{\\boldsymbol{w}}(s) \\odot \\epsilon (as in a Gaussian policy), we can recover the noise exactly:\\epsilon = \\frac{a - \\mu_{\\boldsymbol{w}}(s)}{\\sigma_{\\boldsymbol{w}}(s)}\n\nThis recovered \\epsilon can then be used for gradient computation. However, this introduces a subtle dependence: the inferred \\epsilon depends on the current policy parameters \\boldsymbol{w}, not just the data. As the policy changes during training, the same action a corresponds to different noise values.\n\nFor dynamics noise \\xi, the situation is more complex. If we have a probabilistic model s' = f(s, a, \\xi) and observe the actual next state s', we could in principle infer \\xi. In practice, environment stochasticity is often treated as irreducible: we cannot replay the exact same noise realization. SVG handles this by either: (1) using deterministic models and ignoring environment stochasticity, (2) re-simulating from the model rather than using observed next states, or (3) using importance weighting to correct for the distribution mismatch.\n\nThe noise inference perspective connects reparameterization gradients to the broader question of credit assignment in RL. By explicitly tracking which noise realizations led to which outcomes, we can more precisely attribute value to policy parameters rather than to lucky or unlucky samples.\n\nWhen dynamics are deterministic or can be accurately reparameterized, SVG-style methods offer an efficient alternative to the score function methods developed in the previous section. However, many reinforcement learning problems involve unknown dynamics or dynamics that resist accurate modeling. In those settings, score function methods remain the primary tool since they require only the ability to sample trajectories under the policy.","type":"content","url":"/pg#noise-inference-for-off-policy-learning","position":71},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl2":"Summary"},"type":"lvl2","url":"/pg#summary","position":72},{"hierarchy":{"lvl1":"Policy Gradient Methods","lvl2":"Summary"},"content":"This chapter developed the mathematical foundations for policy gradient methods. Starting from general derivative estimation techniques in stochastic optimization, we saw two main approaches: the likelihood ratio (score function) method and the reparameterization trick. While the reparameterization trick typically offers lower variance, it requires that the sampling distribution be reparameterizable, making it inapplicable to discrete actions or environments with complex dynamics.\n\nFor reinforcement learning, the score function estimator provides a model-free gradient that depends only on the policy parametrization, not the transition dynamics. Through variance reduction techniques (leveraging conditional independence, using control variates, and the Generalized Advantage Estimator), we can make these gradients practical for learning. The likelihood ratio perspective then led to importance-weighted surrogates and PPO’s clipped objective for stable off-policy updates.\n\nWe also established the policy gradient theorem, which provides the theoretical foundation for these estimators in the discounted infinite-horizon setting. The actor-critic architecture emerges from approximating the value function that appears in this theorem, with the two-timescale condition ensuring stable learning.\n\nWhen dynamics models are available, reparameterization through Stochastic Value Gradients offers lower-variance alternatives. SVG(0) recovers actor-critic methods like DDPG and SAC, while SVG(\\infty) represents pure model-based optimization through differentiable simulation.","type":"content","url":"/pg#summary","position":73},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations"},"type":"lvl1","url":"/projection","position":0},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations"},"content":"The Bellman optimality equation \\Bellman v = v is a functional equation: an equation where the unknown is an entire function rather than a finite-dimensional vector. When the state space is continuous or very large, we cannot represent the value function exactly on a computer. We must instead work with finite-dimensional approximations. This motivates weighted residual methods (also called minimum residual methods), a general framework for transforming infinite-dimensional problems into tractable finite-dimensional ones \n\nChakraverty et al. (2019)\n\nAtkinson & Potra (1987).","type":"content","url":"/projection","position":1},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl2":"Testing Whether a Residual Vanishes"},"type":"lvl2","url":"/projection#testing-whether-a-residual-vanishes","position":2},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl2":"Testing Whether a Residual Vanishes"},"content":"Consider a functional equation \\Residual(f) = 0, where \\Residual is an operator and the unknown f is an entire function (in our case, the Bellman optimality equation \\Bellman v = v, which we can write as \\Residual(v) \\equiv \\Bellman v - v = 0). Suppose we have found a candidate approximate solution \\hat{f}. To verify it satisfies \\Residual(\\hat{f}) = 0, we compute the residual function R(s) = \\Residual(\\hat{f})(s). For a true solution, this residual should be the zero function: R(s) = 0 for every state s.\n\nHow might we test whether a function is zero? One approach: sample many input points \\{s_1, s_2, \\ldots, s_m\\}, check whether R(s_i) = 0 at each, and summarize the results into a single scalar test by computing a weighted sum \\sum_{i=1}^m w_i R(s_i) with weights w_i > 0. If R is zero everywhere, this sum is zero. If R is nonzero somewhere, we can choose points and weights to make the sum nonzero. For vectors in finite dimensions, the inner product \\langle \\mathbf{r}, \\mathbf{y} \\rangle = \\sum_{i=1}^n r_i y_i implements exactly this idea: it tests \\mathbf{r} by weighting and summing. Indeed, a vector \\mathbf{r} \\in \\mathbb{R}^n equals zero if and only if \\langle \\mathbf{r}, \\mathbf{y} \\rangle = 0 for every vector \\mathbf{y} \\in \\mathbb{R}^n. To see why, suppose \\mathbf{r} \\neq \\mathbf{0}. Choosing \\mathbf{y} = \\mathbf{r} gives \\langle \\mathbf{r}, \\mathbf{r} \\rangle = \\|\\mathbf{r}\\|^2 > 0, contradicting the claim that all inner products vanish.\n\nThe same principle extends to functions. A function R equals the zero function if and only if its “inner product” with every “test function” p vanishes:R = 0 \\quad \\text{if and only if} \\quad \\langle R, p \\rangle_w = \\int_{\\mathcal{S}} R(s) p(s) w(s) ds = 0 \\quad \\text{for all test functions } p,\n\nwhere w(s) > 0 is a weight function that is part of the inner product definition. Why does this work? For the same reason as in finite dimensions: if R is not the zero function, there must be some region where R(s) \\neq 0. We can then choose a test function p that is nonzero in that same region (for instance, p(s) = R(s) itself), which will produce \\langle R, p \\rangle_w = \\int R(s) p(s) w(s) ds > 0, witnessing that R is nonzero. Conversely, if R is the zero function, then \\langle R, p \\rangle_w = 0 for any test function p.\n\nThis ability to distinguish between different functions using inner products is a fundamental principle from functional analysis. Just as we can test a vector by taking inner products with other vectors, we can test a function by taking inner products with other functions.\n\nConnection to Functional Analysis\n\nThe principle that “a function equals zero if and only if it has zero inner product with all test functions” is a consequence of the Hahn-Banach theorem, one of the cornerstones of functional analysis. The theorem guarantees that for any nonzero function R in a suitable function space, there exists a continuous linear functional (which can be represented as an inner product with some test function p) that produces a nonzero value when applied to R. This is often phrased as “the dual space separates points.”\n\nWhile you don’t need to know the Hahn-Banach theorem to use weighted residual methods, it provides the rigorous mathematical foundation ensuring that our inner product tests are theoretically sound. The constructive argument we gave above (choosing p = R) works in simple cases with well-behaved functions, but the Hahn-Banach theorem extends this guarantee to much more general settings.\n\nWhy is this useful? It transforms the pointwise condition “R(s) = 0 for all s” (infinitely many conditions, one per state) into an equivalent condition about inner products. We still cannot test against all possible test functions, since there are infinitely many of those too. But the inner product perspective suggests a natural computational strategy: choose a finite collection of test functions \\{p_1, \\ldots, p_n\\} and use them to construct n conditions that we can actually compute.\n\nThis suggests the weighted residual (or minimum residual) approach: choose n test functions \\{p_1, \\ldots, p_n\\} and a weight function w(s), then require the residual to have zero weighted inner product with each test function:\\langle R, p_i \\rangle_w = \\int_{\\mathcal{S}} R(s; \\theta) p_i(s) w(s) ds = 0, \\quad i = 1, \\ldots, n,\n\nwhere the residual is R(s; \\theta) = \\Residual(\\hat{f}(\\cdot; \\theta))(s) and the approximation is \\hat{f}(s; \\theta) = \\sum_{j=1}^n \\theta_j \\varphi_j(s). For the Bellman equation, \\Residual(v) = \\Bellman v - v, so R(s; \\theta) = \\Bellman\\hat{v}(s; \\theta) - \\hat{v}(s; \\theta). This transforms the impossible task of verifying “R(s) = 0 for all s” into a finite-dimensional problem: find n coefficients \\theta = (\\theta_1, \\ldots, \\theta_n) satisfying n weighted integral conditions.\n\nThe weight function w(s) is part of the inner product definition and serves important purposes: it can emphasize certain regions of the state space, or represent a natural probability measure over states. In unweighted problems, we simply take w(s) = 1. In reinforcement learning applications, w(s) is often chosen as the stationary distribution d^\\pi(s) under a policy \\pi. This is exactly what happens in methods like LSTD (Least Squares Temporal Difference), which can be viewed as a Galerkin method with w(s) = d^\\pi(s).\n\nDifferent choices of test functions give different methods, each with different computational and theoretical properties. When p_i = \\varphi_i (test functions equal basis functions), the method is called Galerkin, and can be interpreted geometrically as a projection of the residual onto the orthogonal complement of the approximation space. The rest of this chapter develops this framework systematically: how to choose basis functions \\{\\varphi_j\\}, how to select test functions \\{p_i\\}, and how to solve the resulting systems.","type":"content","url":"/projection#testing-whether-a-residual-vanishes","position":3},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl2":"The General Framework"},"type":"lvl2","url":"/projection#the-general-framework","position":4},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl2":"The General Framework"},"content":"Consider an operator equation of the form\\Residual(f) = 0,\n\nwhere \\Residual: B_1 \\to B_2 is a continuous operator between complete normed vector spaces B_1 and B_2. For the Bellman equation, we have \\Residual(v) = \\Bellman v - v, so that solving \\Residual(v) = 0 is equivalent to finding the fixed point v = \\Bellman v.\n\nJust as we transcribed infinite-dimensional continuous optimal control problems into finite-dimensional discrete optimal control problems in earlier chapters, we seek a finite-dimensional approximation to this infinite-dimensional functional equation. Recall that for continuous optimal control, we adopted control parameterization: we represented the control trajectory using a finite set of basis functions (piecewise constants, polynomials, splines) and searched over the finite-dimensional coefficient space instead of the infinite-dimensional function space. For integrals in the objective and constraints, we used numerical quadrature to approximate them with finite sums.\n\nWe follow the same strategy here. We parameterize the value function using a finite set of basis functions \\{\\varphi_1, \\ldots, \\varphi_n\\}, commonly polynomials (Chebyshev, Legendre), though other function classes (splines, radial basis functions, neural networks) are possible, and search for coefficients \\theta = (\\theta_1, \\ldots, \\theta_n) in \\mathbb{R}^n. When integrals appear in the Bellman operator or projection conditions, we approximate them using numerical quadrature. The projection method approach consists of several conceptual steps that accomplish this transcription.","type":"content","url":"/projection#the-general-framework","position":5},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl3":"Step 1: Choose a Finite-Dimensional Approximation Space","lvl2":"The General Framework"},"type":"lvl3","url":"/projection#step-1-choose-a-finite-dimensional-approximation-space","position":6},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl3":"Step 1: Choose a Finite-Dimensional Approximation Space","lvl2":"The General Framework"},"content":"We begin by selecting a basis \\Phi = \\{\\varphi_1, \\varphi_2, \\ldots, \\varphi_n\\} and approximating the unknown function as a linear combination:\\hat{f}(x) = \\sum_{i=1}^n \\theta_i \\varphi_i(x).\n\nThe choice of basis functions \\varphi_i is problem-dependent. Common choices include:\n\nPolynomials: For smooth problems, we might use Chebyshev polynomials or other orthogonal polynomial families\n\nSplines: For problems where we expect the solution to have regions of different smoothness\n\nRadial basis functions: For high-dimensional problems where tensor product methods become intractable\n\nThe number of basis functions n determines the flexibility of our approximation. In practice, we start with small n and increase it until the approximation quality is satisfactory. The only unknowns now are the coefficients \\theta = (\\theta_1, \\ldots, \\theta_n).\n\nWhile the classical presentation of projection methods focuses on polynomial bases, the framework applies equally well to other function classes. Neural networks, for instance, can be viewed through this lens: a neural network \\hat{f}(x; \\theta) with parameters \\theta defines a flexible function class, and many training procedures can be interpreted as projection methods with specific choices of test functions or residual norms. The distinction is that classical methods typically use predetermined basis functions with linear coefficients, while neural networks use adaptive nonlinear features. Throughout this chapter, we focus on the classical setting to develop the core concepts, but the principles extend naturally to modern function approximators.","type":"content","url":"/projection#step-1-choose-a-finite-dimensional-approximation-space","position":7},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl3":"Step 2: Define the Residual Function","lvl2":"The General Framework"},"type":"lvl3","url":"/projection#step-2-define-the-residual-function","position":8},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl3":"Step 2: Define the Residual Function","lvl2":"The General Framework"},"content":"Since we are approximating f with \\hat{f}, the operator \\Residual will generally not vanish exactly. Instead, we obtain a residual function:R(x; \\theta) = \\Residual(\\hat{f}(\\cdot; \\theta))(x).\n\nThis residual measures how far our candidate solution is from satisfying the equation at each point x. As we discussed in the introduction, we will assess whether this residual is “close to zero” by testing its inner products against chosen test functions.","type":"content","url":"/projection#step-2-define-the-residual-function","position":9},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl3":"Step 3: Impose Conditions to Determine Coefficients","lvl2":"The General Framework"},"type":"lvl3","url":"/projection#step-3-impose-conditions-to-determine-coefficients","position":10},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl3":"Step 3: Impose Conditions to Determine Coefficients","lvl2":"The General Framework"},"content":"Having chosen our basis and defined the residual, we must decide how to make the residual “close to zero.” As discussed in the introduction, we test the residual using weighted integrals. We select test functions \\{p_1, \\ldots, p_n\\} and a weight function w(x) (often w(x) = 1 for unweighted problems), then impose weighted residual conditions:\\int_{\\mathcal{S}} R(x; \\theta) p_i(x) w(x) \\, dx = 0, \\quad i = 1, \\ldots, n.\n\nThese n integral conditions provide n equations to determine the n unknown coefficients in \\theta. Different choices of test functions p_i give different methods:\n\nGalerkin: p_i = \\varphi_i (test with the basis functions themselves)\n\nCollocation: p_i = \\delta(x - x_i) (test at specific points)\n\nMethod of moments: p_i = x^{i-1} (test with monomials)\n\nAn alternative is the least squares approach, which minimizes the weighted norm of the residual:\\min_\\theta \\int_{\\mathcal{S}} R(x; \\theta)^2 w(x) dx.\n\nWe focus primarily on methods distinguished by their choice of test functions p_i. The Galerkin method, where p_i = \\varphi_i, can be interpreted geometrically as a projection when working in a Hilbert space with a weighted inner product.\n\nLet us examine the standard choices of test functions and what they tell us about the residual:","type":"content","url":"/projection#step-3-impose-conditions-to-determine-coefficients","position":11},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl4":"Galerkin Method: Test Against the Basis","lvl3":"Step 3: Impose Conditions to Determine Coefficients","lvl2":"The General Framework"},"type":"lvl4","url":"/projection#galerkin-method-test-against-the-basis","position":12},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl4":"Galerkin Method: Test Against the Basis","lvl3":"Step 3: Impose Conditions to Determine Coefficients","lvl2":"The General Framework"},"content":"The Galerkin method chooses test functions p_i = \\varphi_i, the same basis functions used to approximate \\hat{f}:\\int_{\\mathcal{S}} R(x; \\theta) \\varphi_i(x) w(x) dx = 0, \\quad i = 1, \\ldots, n.\n\nTo understand what this means, recall that in finite dimensions, two vectors are orthogonal when their inner product is zero. For functions, \\langle R, \\varphi_i \\rangle_w = \\int R(x) \\varphi_i(x) w(x) dx = 0 expresses the same concept: R and \\varphi_i are orthogonal as functions with respect to the weighted inner product. But there’s more to this than just testing against individual basis functions.\n\nConsider our approximation space \\text{span}\\{\\varphi_1, \\ldots, \\varphi_n\\} as an n-dimensional subspace within the infinite-dimensional space of all functions. Any function g in this space can be written as g = \\sum_{i=1}^n c_i \\varphi_i for some coefficients c_i. If the residual R is orthogonal to all basis functions \\varphi_i, then by linearity of the inner product, for any such function g:\\langle R, g \\rangle = \\left\\langle R, \\sum_{i=1}^n c_i \\varphi_i \\right\\rangle = \\sum_{i=1}^n c_i \\langle R, \\varphi_i \\rangle = 0.\n\nThis shows that R is orthogonal to every function we can represent with our basis. The residual has “zero overlap” with our approximation space: we cannot express any part of it using our basis functions. In this sense, the residual is as “invisible” to our approximation as possible.\n\nThis condition is the defining property of optimality. By choosing our approximation \\hat{f} so that the residual R = \\Residual(\\hat{f}) is orthogonal to the entire approximation space, we ensure that \\hat{f} is the orthogonal projection of the true solution onto \\text{span}{\\varphi_1, \\ldots, \\varphi_n}. Within this n-dimensional space, no better choice is possible: any other coefficients would yield a residual with a nonzero component inside the space, and therefore a larger norm.\n\nThe finite-dimensional analogy makes this concrete. Suppose you want to approximate a vector \\mathbf{v} \\in \\mathbb{R}^3 using only the xy-plane (a 2D subspace). The best approximation is to project \\mathbf{v} onto the plane, giving \\hat{\\mathbf{v}} = (v_1, v_2, 0). The error is \\mathbf{r} = \\mathbf{v} - \\hat{\\mathbf{v}} = (0, 0, v_3), which points purely in the z-direction, orthogonal to the entire xy-plane. We see the Galerkin condition in action: the error is orthogonal to the approximation space.","type":"content","url":"/projection#galerkin-method-test-against-the-basis","position":13},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl4":"Method of Moments: Test Against Monomials","lvl3":"Step 3: Impose Conditions to Determine Coefficients","lvl2":"The General Framework"},"type":"lvl4","url":"/projection#method-of-moments-test-against-monomials","position":14},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl4":"Method of Moments: Test Against Monomials","lvl3":"Step 3: Impose Conditions to Determine Coefficients","lvl2":"The General Framework"},"content":"The method of moments, for problems on D \\subset \\mathbb{R}, chooses test functions p_i(x) = x^{i-1} for i = 1, \\ldots, n:\\langle R(\\cdot; \\theta), x^{i-1} \\rangle = 0, \\quad i = 1, \\ldots, n.\n\nThis requires the first n moments of the residual function to vanish, ensuring the residual is “balanced” in the sense that it has no systematic trend captured by low-order polynomials. The moments \\int x^k R(x; \\theta) w(x) dx measure weighted averages of the residual, with increasing powers of x giving more weight to larger values. Setting these to zero ensures the residual doesn’t grow systematically with x. This approach is particularly useful when w(x) is chosen as a probability measure, making the conditions natural moment restrictions familiar from statistics and econometrics.","type":"content","url":"/projection#method-of-moments-test-against-monomials","position":15},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl4":"Collocation Method: Test Against Delta Functions","lvl3":"Step 3: Impose Conditions to Determine Coefficients","lvl2":"The General Framework"},"type":"lvl4","url":"/projection#collocation-method-test-against-delta-functions","position":16},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl4":"Collocation Method: Test Against Delta Functions","lvl3":"Step 3: Impose Conditions to Determine Coefficients","lvl2":"The General Framework"},"content":"The collocation method chooses test functions p_i(x) = \\delta(x - x_i), the Dirac delta functions at points \\{x_1, \\ldots, x_n\\}:\\langle R(\\cdot; \\theta), \\delta(\\cdot - x_i) \\rangle = R(x_i; \\theta) = 0, \\quad i = 1, \\ldots, n.\n\nThis is projection against the most localized test functions possible: delta functions that “sample” the residual at specific points, requiring the residual to vanish exactly where we test it. The computational advantage is significant: collocation avoids numerical integration entirely, requiring only pointwise evaluation of R.\n\nOrthogonal collocation combines collocation with spectral basis functions (orthogonal polynomials like Chebyshev, Legendre, or Hermite) for smooth problems. We choose collocation points at the zeros of the n-th polynomial in the family. For example, with Chebyshev polynomials T_0, T_1, \\ldots, T_{n-1}, we place collocation points at the zeros of T_n(x).\n\nThese zeros are also optimal nodes for Gauss quadrature with the associated weight function. This coordination means:\n\nWe get the computational simplicity of collocation: just pointwise evaluation R(x_i) = 0\n\nWhen we need integrals (inside the Bellman operator), the collocation points double as quadrature nodes with exactness for polynomials up to degree 2n-1\n\nFor smooth problems, spectral approximations achieve exponential convergence: the error decreases like O(e^{-cn}) as we add basis functions, compared to O(h^{p+1}) for piecewise polynomials\n\nThis approach is often called a pseudospectral method or spectral collocation method. The Chebyshev interpolation theorem guarantees that forcing R(x_i; \\theta) = 0 at these carefully chosen points makes R(x; \\theta) small everywhere, with well-conditioned systems and near-optimal interpolation error.","type":"content","url":"/projection#collocation-method-test-against-delta-functions","position":17},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl4":"Subdomain Method: Test Against Indicator Functions","lvl3":"Step 3: Impose Conditions to Determine Coefficients","lvl2":"The General Framework"},"type":"lvl4","url":"/projection#subdomain-method-test-against-indicator-functions","position":18},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl4":"Subdomain Method: Test Against Indicator Functions","lvl3":"Step 3: Impose Conditions to Determine Coefficients","lvl2":"The General Framework"},"content":"The subdomain method partitions the domain into n subregions \\{D_1, \\ldots, D_n\\} and chooses test functions p_i = I_{D_i}, the indicator functions:\\langle R(\\cdot; \\theta), I_{D_i} \\rangle_w = \\int_{D_i} R(x; \\theta) w(x) dx = 0, \\quad i = 1, \\ldots, n.\n\nThis requires the residual to have zero average over each subdomain, ensuring the approximation is good “on average” over each piece of the domain. This approach is particularly natural for finite element methods where the domain is divided into elements, ensuring local balance of the residual within each element.","type":"content","url":"/projection#subdomain-method-test-against-indicator-functions","position":19},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl4":"Least Squares","lvl3":"Step 3: Impose Conditions to Determine Coefficients","lvl2":"The General Framework"},"type":"lvl4","url":"/projection#least-squares","position":20},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl4":"Least Squares","lvl3":"Step 3: Impose Conditions to Determine Coefficients","lvl2":"The General Framework"},"content":"The least squares approach appears different at first glance, but it also fits the test function framework. We minimize:\\min_\\theta \\int_{\\mathcal{S}} R(x; \\theta)^2 w(x) dx = \\min_\\theta \\langle R(\\cdot; \\theta), R(\\cdot; \\theta) \\rangle_w.\n\nThe first-order conditions for this minimization problem are:\\left\\langle R(\\cdot; \\theta), \\frac{\\partial R(\\cdot; \\theta)}{\\partial \\theta_i} \\right\\rangle_w = 0, \\quad i = 1, \\ldots, n.\n\nThus least squares implicitly uses test functions p_i = \\partial R / \\partial \\theta_i, the gradients of the residual with respect to parameters. Unlike other methods where test functions are chosen a priori, here they depend on the current guess for \\theta and on the structure of our approximation.\n\nWe can now see the unifying structure of weighted residual methods: whether we use projection conditions or least squares minimization, all these methods follow the same template of restricting the search to an n-dimensional function space and imposing n conditions on the residual. For projection methods specifically, we pick n test functions and require \\langle R, p_i \\rangle = 0. They differ only in their philosophy about which test functions best detect whether the residual is “nearly zero.” Galerkin tests against the approximation basis itself (natural for orthogonal bases), the method of moments tests against monomials (ensuring polynomial balance), collocation tests against delta functions (pointwise satisfaction), subdomain tests against indicators (local average satisfaction), and least squares tests against residual gradients (global norm minimization). Each choice reflects different priorities: computational efficiency, theoretical optimality, ease of implementation, or sensitivity to errors in different regions of the domain.","type":"content","url":"/projection#least-squares","position":21},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl3":"Step 4: Solve the Finite-Dimensional Problem","lvl2":"The General Framework"},"type":"lvl3","url":"/projection#step-4-solve-the-finite-dimensional-problem","position":22},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl3":"Step 4: Solve the Finite-Dimensional Problem","lvl2":"The General Framework"},"content":"The projection conditions give us a system to solve for the coefficients \\theta. For test function methods (Galerkin, collocation, moments, subdomain), we solve:P_i(\\theta) \\equiv \\langle R(\\cdot; \\theta), p_i \\rangle_w = 0, \\quad i = 1, \\ldots, n.\n\nThis is a system of n (generally nonlinear) equations in n unknowns. For least squares, we solve the optimization problem \\min_\\theta \\langle R(\\cdot; \\theta), R(\\cdot; \\theta) \\rangle_w.","type":"content","url":"/projection#step-4-solve-the-finite-dimensional-problem","position":23},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl4":"Computational Cost and Conditioning","lvl3":"Step 4: Solve the Finite-Dimensional Problem","lvl2":"The General Framework"},"type":"lvl4","url":"/projection#computational-cost-and-conditioning","position":24},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl4":"Computational Cost and Conditioning","lvl3":"Step 4: Solve the Finite-Dimensional Problem","lvl2":"The General Framework"},"content":"The computational cost per iteration varies significantly across methods:\n\nCollocation: Cheapest to evaluate since P_i(\\theta) = R(x_i; \\theta) requires only pointwise evaluation (no integration). The Jacobian is also cheap: J_{ij} = \\frac{\\partial R(x_i; \\theta)}{\\partial \\theta_j}.\n\nGalerkin and moments: More expensive due to integration. Computing P_i(\\theta) = \\int R(x; \\theta) p_i(x) w(x) dx requires numerical quadrature. Each Jacobian entry requires integrating \\frac{\\partial R}{\\partial \\theta_j} p_i w.\n\nLeast squares: Most expensive when done via the objective function, which requires integrating R^2 w. However, the first-order conditions reduce it to a system like Galerkin, with test functions p_i = \\partial R / \\partial \\theta_i.\n\nFor methods requiring integration, the choice of quadrature rule should match the basis. Gaussian quadrature with nodes at orthogonal polynomial zeros is efficient. When combined with collocation at those same points, the quadrature is exact for polynomials up to a certain degree. This coordination between quadrature and collocation makes orthogonal collocation effective.\n\nThe conditioning of the system depends on the choice of test functions. The Jacobian matrix has entries:J_{ij} = \\frac{\\partial P_i}{\\partial \\theta_j} = \\left\\langle \\frac{\\partial R(\\cdot; \\theta)}{\\partial \\theta_j}, p_i \\right\\rangle_w.\n\nWhen test functions are orthogonal (or nearly so), the Jacobian tends to be well-conditioned. This is why orthogonal polynomial bases are preferred in Galerkin methods: they produce Jacobians with controlled condition numbers. Poorly chosen basis functions or collocation points can lead to nearly singular Jacobians, causing numerical instability. Orthogonal bases and carefully chosen collocation points (like Chebyshev nodes) help maintain good conditioning.","type":"content","url":"/projection#computational-cost-and-conditioning","position":25},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl4":"Two Main Solution Approaches","lvl3":"Step 4: Solve the Finite-Dimensional Problem","lvl2":"The General Framework"},"type":"lvl4","url":"/projection#two-main-solution-approaches","position":26},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl4":"Two Main Solution Approaches","lvl3":"Step 4: Solve the Finite-Dimensional Problem","lvl2":"The General Framework"},"content":"We have two fundamentally different ways to solve the projection equations: function iteration (exploiting fixed-point structure) and Newton’s method (exploiting smoothness). The choice depends on whether the original operator equation has contraction properties and how well those properties are preserved by the finite-dimensional approximation.","type":"content","url":"/projection#two-main-solution-approaches","position":27},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl5":"Method 1: Function Iteration (Successive Approximation)","lvl4":"Two Main Solution Approaches","lvl3":"Step 4: Solve the Finite-Dimensional Problem","lvl2":"The General Framework"},"type":"lvl5","url":"/projection#method-1-function-iteration-successive-approximation","position":28},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl5":"Method 1: Function Iteration (Successive Approximation)","lvl4":"Two Main Solution Approaches","lvl3":"Step 4: Solve the Finite-Dimensional Problem","lvl2":"The General Framework"},"content":"When the operator equation has the form f = \\Contraction f where \\Contraction is a contraction, the most natural approach is to iterate the operator directly:\\hat{f}^{(k+1)} = \\Contraction \\hat{f}^{(k)}.\n\nThe infinite-dimensional iteration becomes a finite-dimensional iteration in coefficient space once we choose our weighted residual method. Given a current approximation \\hat{f}^{(k)}(x; \\theta^{(k)}), how do we find the coefficients \\theta^{(k+1)} for the next iterate \\hat{f}^{(k+1)}?\n\nDifferent weighted residual methods answer this differently. For collocation, we proceed in two steps:\n\nEvaluate the operator: At each collocation point x_i, compute what the next iterate should be: t_i^{(k)} = (\\Contraction \\hat{f}^{(k)})(x_i). These n target values tell us what \\hat{f}^{(k+1)} should equal at the collocation points.\n\nFind matching coefficients: Determine \\theta^{(k+1)} so that \\hat{f}^{(k+1)}(x_i; \\theta^{(k+1)}) = t_i^{(k)} for all i. This is a linear system: \\sum_j \\theta_j^{(k+1)} \\varphi_j(x_i) = t_i^{(k)}.\n\nIn matrix form: \\boldsymbol{\\Phi} \\theta^{(k+1)} = t^{(k)}, where \\boldsymbol{\\Phi} is the collocation matrix with entries \\Phi_{ij} = \\varphi_j(x_i). Solving this system gives \\theta^{(k+1)} = \\boldsymbol{\\Phi}^{-1} t^{(k)}.\n\nFor Galerkin, the projection condition \\langle \\hat{f}^{(k+1)} - \\Contraction \\hat{f}^{(k+1)}, \\varphi_i \\rangle_w = 0 directly gives a system for \\theta^{(k+1)}. When \\Contraction is linear in its argument (as in many integral equations), this is a linear system. When \\Contraction is nonlinear (as in the Bellman equation), we must solve a nonlinear system at each iteration, though each solution still only involves n unknowns rather than an infinite-dimensional function.\n\nWhen \\Contraction is a contraction in the infinite-dimensional space with constant \\gamma < 1, iterating it pulls any starting function toward the unique fixed point. The hope is that the finite-dimensional operator, evaluating \\Contraction and projecting back onto the span of the basis functions, inherits this contraction property. When it does, function iteration converges globally from any initial guess, with each iteration reducing the error by a factor of roughly \\gamma. This is computationally attractive: we only evaluate the operator and solve a linear system (for collocation) or a relatively simple system (for other methods).\n\nHowever, the finite-dimensional approximation doesn’t always preserve contraction. High-order polynomial bases, in particular, can create oscillations between basis functions that amplify rather than contract errors. Even when contraction is preserved, convergence can be painfully slow when \\gamma is close to 1, the “weak contraction” regime common in economic problems with patient agents (\\gamma \\approx 0.95 or higher). Finally, not all operator equations naturally present themselves as contractions; some require reformulation (like f = f - \\alpha \\Residual(f)), and finding a good \\alpha can be problem-specific.","type":"content","url":"/projection#method-1-function-iteration-successive-approximation","position":29},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl5":"Method 2: Newton’s Method","lvl4":"Two Main Solution Approaches","lvl3":"Step 4: Solve the Finite-Dimensional Problem","lvl2":"The General Framework"},"type":"lvl5","url":"/projection#method-2-newtons-method","position":30},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl5":"Method 2: Newton’s Method","lvl4":"Two Main Solution Approaches","lvl3":"Step 4: Solve the Finite-Dimensional Problem","lvl2":"The General Framework"},"content":"Alternatively, we can treat the projection equations as a rootfinding problem G(\\theta) = 0 where G_i(\\theta) = P_i(\\theta) for test function methods, or solve the first-order conditions for least squares. Newton’s method uses the update:\\theta^{(k+1)} = \\theta^{(k)} - J_G(\\theta^{(k)})^{-1} G(\\theta^{(k)}),\n\nwhere J_G(\\theta) is the Jacobian of G at \\theta.\n\nTo apply this update, we must compute the Jacobian entries J_{ij} = \\frac{\\partial G_i}{\\partial \\theta_j}. For collocation, G_i(\\theta) = \\hat{f}(x_i; \\theta) - (\\Contraction \\hat{f}(\\cdot; \\theta))(x_i), so:\\frac{\\partial G_i}{\\partial \\theta_j} = \\frac{\\partial \\hat{f}(x_i; \\theta)}{\\partial \\theta_j} - \\frac{\\partial (\\Contraction \\hat{f}(\\cdot; \\theta))(x_i)}{\\partial \\theta_j}.\n\nThe first term is straightforward (it’s just \\varphi_j(x_i) for a linear approximation). The second term requires differentiating the operator \\Contraction with respect to the parameters.\n\nWhen \\Contraction involves optimization (as in the Bellman operator \\Bellman v = \\max_a \\{r(s,a) + \\gamma \\mathbb{E}[v(s')]\\}), computing this derivative appears problematic because the max operator is not differentiable. However, the Envelope Theorem resolves this difficulty.\n\nThe Envelope Theorem\n\nSetup: Consider a smooth objective function f(\\mathbf{x}, \\boldsymbol{\\theta}) and define the optimal value:v(\\boldsymbol{\\theta}) = \\max_{\\mathbf{x}} f(\\mathbf{x}, \\boldsymbol{\\theta}).\n\nLet \\mathbf{x}^*(\\boldsymbol{\\theta}) denote the maximizer, satisfying the first-order condition \\nabla_{\\mathbf{x}} f(\\mathbf{x}^*(\\boldsymbol{\\theta}), \\boldsymbol{\\theta}) = \\mathbf{0}.\n\nThe Result: To find how the optimal value changes with \\boldsymbol{\\theta}, we can compute:\\nabla_{\\boldsymbol{\\theta}} v(\\boldsymbol{\\theta}) = \\nabla_{\\boldsymbol{\\theta}} f(\\mathbf{x}^*(\\boldsymbol{\\theta}), \\boldsymbol{\\theta}).\n\nThat is, differentiate the objective with respect to \\boldsymbol{\\theta} while treating the maximizer \\mathbf{x}^* as constant. We don’t need to compute \\frac{\\partial \\mathbf{x}^*}{\\partial \\boldsymbol{\\theta}} because at the optimum, small changes in \\mathbf{x} don’t affect the value (first-order condition), so the direct effect dominates.\n\nWhy it works: By the chain rule, \\nabla_{\\boldsymbol{\\theta}} v = \\nabla_{\\boldsymbol{\\theta}} f + \\underbrace{(\\nabla_{\\mathbf{x}} f)^{\\top}}_{\\mathbf{0} \\text{ at optimum}} \\frac{\\partial \\mathbf{x}^*}{\\partial \\boldsymbol{\\theta}}.\n\nApplication to Bellman equations: For [\\Bellman v](s) = \\max_a \\{r(s,a) + \\gamma \\mathbb{E}[v(s')]\\}, the derivative with respect to parameters in v can be computed by treating the optimal action as constant. For example, if v(s; \\theta) = \\sum_j \\theta_j \\varphi_j(s):\\frac{\\partial [\\Bellman v](s)}{\\partial \\theta_j} = \\gamma \\mathbb{E}[\\varphi_j(s') \\mid s, a^*(s; \\theta)],\n\nwhere a^*(s; \\theta) is the optimal action given parameters \\theta.\n\nImportant assumptions: The objective f is smooth, the maximizer is unique and in the interior (or constraints are smooth with stable active sets), and the first-order condition holds.\n\nWith the Envelope Theorem providing a tractable way to compute Jacobians for problems involving optimization, Newton’s method becomes practical for weighted residual methods applied to Bellman equations and similar problems. The method offers quadratic convergence near the solution. Once in the neighborhood of the true fixed point, Newton’s method typically converges in just a few iterations. Unlike function iteration, it doesn’t rely on the finite-dimensional approximation preserving any contraction property, making it applicable to a broader range of problems, particularly those with high-order polynomial bases or large discount factors where function iteration struggles.\n\nHowever, Newton’s method demands more from both the algorithm and the user. Each iteration requires computing and solving a full Jacobian system, making the per-iteration cost significantly higher than function iteration. The method is also sensitive to initialization: started far from the solution, Newton’s method may diverge or converge to spurious fixed points that the finite-dimensional problem introduces but the original infinite-dimensional problem lacks. When applying the Envelope Theorem, implementation becomes more complex. We must track the optimal action at each evaluation point and compute the Jacobian entries using the formula above (expected basis function values at next states under optimal actions), though the economic interpretation (tracking how value propagates through optimal decisions) often makes the computation conceptually clearer than explicit derivative calculations would be.","type":"content","url":"/projection#method-2-newtons-method","position":31},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl5":"Comparison and Practical Recommendations","lvl4":"Two Main Solution Approaches","lvl3":"Step 4: Solve the Finite-Dimensional Problem","lvl2":"The General Framework"},"type":"lvl5","url":"/projection#comparison-and-practical-recommendations","position":32},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl5":"Comparison and Practical Recommendations","lvl4":"Two Main Solution Approaches","lvl3":"Step 4: Solve the Finite-Dimensional Problem","lvl2":"The General Framework"},"content":"Method\n\nConvergence\n\nPer-iteration cost\n\nInitial guess sensitivity\n\nFunction iteration\n\nLinear (when contraction holds)\n\nLow\n\nRobust\n\nNewton’s method\n\nQuadratic (near solution)\n\nModerate (Jacobian + solve)\n\nRequires good initial guess\n\nWhich method to use? When the problem has strong contraction (small \\gamma, well-conditioned bases, shape-preserving approximations like linear interpolation or splines), function iteration is simple and robust. For weak contraction (large \\gamma, high-order polynomials), a hybrid approach works well: run function iteration for several iterations to enter the basin of attraction, then switch to Newton’s method for rapid final convergence. When the finite-dimensional approximation destroys contraction entirely (common with non-monotone bases), Newton’s method may be necessary from the start, though careful initialization (from a coarser approximation or perturbation methods) is required.\n\nQuasi-Newton methods like BFGS or Broyden offer a middle ground. They approximate the Jacobian using function evaluations only, avoiding explicit derivative computations while maintaining superlinear convergence. This can be useful when computing the exact Jacobian via the Envelope Theorem is expensive or when the approximation quality is acceptable.","type":"content","url":"/projection#comparison-and-practical-recommendations","position":33},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl3":"Step 5: Verify the Solution","lvl2":"The General Framework"},"type":"lvl3","url":"/projection#step-5-verify-the-solution","position":34},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl3":"Step 5: Verify the Solution","lvl2":"The General Framework"},"content":"Once we have computed a candidate solution \\hat{f}, we must verify its quality. Projection methods optimize \\hat{f} with respect to specific criteria (specific test functions or collocation points), but we should check that the residual is small everywhere, including directions or points we did not optimize over.\n\nTypical diagnostic checks include:\n\nComputing \\|R(\\cdot; \\theta)\\| using a more accurate quadrature rule than was used in the optimization\n\nEvaluating R(x; \\theta) at many points not used in the fitting process\n\nIf using Galerkin with the first n basis functions, checking orthogonality against higher-order basis functions","type":"content","url":"/projection#step-5-verify-the-solution","position":35},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl2":"Application to the Bellman Equation"},"type":"lvl2","url":"/projection#application-to-the-bellman-equation","position":36},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl2":"Application to the Bellman Equation"},"content":"Consider the Bellman optimality equation v(s) = \\Bellman v(s) = \\max_{a \\in \\mathcal{A}_s} \\{ r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j) \\}. For a candidate approximation \\hat{v}(s) = \\sum_{i=1}^n \\theta_i \\varphi_i(s), the residual is:R(s; \\theta) = \\Bellman\\hat{v}(s) - \\hat{v}(s) = \\max_{a \\in \\mathcal{A}_s} \\left\\{ r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) \\hat{v}(j) \\right\\} - \\sum_{i=1}^n \\theta_i \\varphi_i(s).\n\nWe examine how collocation and Galerkin, the two most common weighted residual methods for Bellman equations, specialize the general solution approaches from Step 4.","type":"content","url":"/projection#application-to-the-bellman-equation","position":37},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl3":"Collocation","lvl2":"Application to the Bellman Equation"},"type":"lvl3","url":"/projection#collocation","position":38},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl3":"Collocation","lvl2":"Application to the Bellman Equation"},"content":"For collocation, we choose n states \\{s_1, \\ldots, s_n\\} and require the Bellman equation to hold exactly at these points:\\sum_{j=1}^n \\theta_j \\varphi_j(s_i) = \\max_{a \\in \\mathcal{A}_{s_i}} \\left\\{ r(s_i,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s_i,a) \\sum_{\\ell=1}^n \\theta_\\ell \\varphi_\\ell(j) \\right\\}, \\quad i = 1, \\ldots, n.\n\nIt helps to define the parametric Bellman operator \\mathrm{L}_\\varphi: \\mathbb{R}^n \\to \\mathbb{R}^n by [\\mathrm{L}_\\varphi(\\theta)]_i = [\\Bellman\\hat{v}(\\cdot; \\theta)](s_i), the Bellman operator evaluated at collocation point s_i. Let \\boldsymbol{\\Phi} be the n \\times n matrix with entries \\Phi_{ij} = \\varphi_j(s_i). Then the collocation equations become \\boldsymbol{\\Phi} \\theta = \\mathrm{L}_\\varphi(\\theta).\n\nFunction iteration for collocation proceeds as follows. Given current coefficients \\theta^{(k)}, we evaluate the Bellman operator at each collocation point to get target values t_i^{(k)} = [\\mathrm{L}_\\varphi(\\theta^{(k)})]_i. We then find new coefficients by solving the linear system \\boldsymbol{\\Phi} \\theta^{(k+1)} = t^{(k)}. This is parametric value iteration: apply the Bellman operator, fit the result.\n\nCollocation with Function Iteration\n\nInput Collocation points \\{s_1, \\ldots, s_n\\}, basis functions \\{\\varphi_1, \\ldots, \\varphi_n\\}, initial \\theta^{(0)}, tolerance \\varepsilon > 0\n\nOutput Converged coefficients \\theta^*\n\nForm collocation matrix \\boldsymbol{\\Phi} with \\Phi_{ij} = \\varphi_j(s_i)\n\nk \\leftarrow 0\n\nrepeat\n\nFor each i = 1, \\ldots, n:\n\nt_i^{(k)} \\leftarrow \\max_{a \\in \\mathcal{A}_{s_i}} \\left\\{ r(s_i, a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s_i, a) \\sum_{\\ell=1}^n \\theta_\\ell^{(k)} \\varphi_\\ell(j) \\right\\}\n\nSolve \\boldsymbol{\\Phi} \\theta^{(k+1)} = t^{(k)}\n\nk \\leftarrow k + 1\n\nuntil \\|\\theta^{(k)} - \\theta^{(k-1)}\\| < \\varepsilon\n\nreturn \\theta^{(k)}\n\nWhen the state space is continuous, we approximate expectations using numerical quadrature (Gauss-Hermite for normal shocks, etc.). The method is simple and robust when the finite-dimensional approximation preserves contraction, but can be slow for large discount factors.\n\nNewton’s method for collocation treats the problem as rootfinding: G(\\theta) = \\boldsymbol{\\Phi} \\theta - \\mathrm{L}_\\varphi(\\theta) = 0. The Jacobian is J_G = \\boldsymbol{\\Phi} - J_{\\mathrm{L}_\\varphi}, where the Envelope Theorem (Step 4) gives us [J_{\\mathrm{L}_\\varphi}]_{ij} = \\gamma \\sum_{s'} p(s'|s_i, a_i^*(\\theta)) \\varphi_j(s'). Here a_i^*(\\theta) is the optimal action at collocation point s_i given the current coefficients.\n\nCollocation with Newton’s Method\n\nInput Collocation points \\{s_1, \\ldots, s_n\\}, basis functions \\{\\varphi_1, \\ldots, \\varphi_n\\}, initial \\theta^{(0)}, tolerance \\varepsilon > 0\n\nOutput Converged coefficients \\theta^*\n\nForm collocation matrix \\boldsymbol{\\Phi} with \\Phi_{ij} = \\varphi_j(s_i)\n\nk \\leftarrow 0\n\nrepeat\n\nFor each i = 1, \\ldots, n:\n\nt_i^{(k)} \\leftarrow \\max_{a \\in \\mathcal{A}_{s_i}} \\left\\{ r(s_i, a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s_i, a) \\sum_{\\ell=1}^n \\theta_\\ell^{(k)} \\varphi_\\ell(j) \\right\\}\n\nStore a_i^* \\in \\arg\\max achieving the maximum\n\nCompute Jacobian: [J_{\\mathrm{L}_\\varphi}]_{ij} = \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s_i, a_i^*) \\varphi_j(j) for all i,j\n\nSolve (\\boldsymbol{\\Phi} - J_{\\mathrm{L}_\\varphi}) \\Delta\\theta = \\boldsymbol{\\Phi} \\theta^{(k)} - t^{(k)}\n\n\\theta^{(k+1)} \\leftarrow \\theta^{(k)} - \\Delta\\theta\n\nk \\leftarrow k + 1\n\nuntil \\|\\Delta\\theta\\| < \\varepsilon\n\nreturn \\theta^{(k)}\n\nThis converges rapidly near the solution but requires good initialization and more computation per iteration than function iteration. The method is equivalent to policy iteration: each step evaluates the value of the current greedy policy, then improves it.\n\nWhy is collocation popular for Bellman equations? Because it avoids integration when testing the residual. We only evaluate the Bellman operator at n discrete points. In contrast, Galerkin requires integrating the residual against each basis function.","type":"content","url":"/projection#collocation","position":39},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl3":"Galerkin","lvl2":"Application to the Bellman Equation"},"type":"lvl3","url":"/projection#galerkin","position":40},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl3":"Galerkin","lvl2":"Application to the Bellman Equation"},"content":"For Galerkin, we use the basis functions themselves as test functions. The conditions are:\\int_{\\mathcal{S}} [\\Bellman\\hat{v}(s; \\theta) - \\hat{v}(s; \\theta)] \\varphi_i(s) w(s) ds = 0, \\quad i = 1, \\ldots, n.\n\nwhere w(s) is a weight function (often the stationary distribution d^\\pi(s) in RL applications, or simply w(s) = 1). Expanding this:\\int_{\\mathcal{S}} \\left[ \\max_a \\left\\{ r(s,a) + \\gamma \\mathbb{E}[v(s')] \\right\\} - \\sum_j \\theta_j \\varphi_j(s) \\right] \\varphi_i(s) w(s) ds = 0.\n\nFunction iteration for Galerkin works differently than for collocation. Given \\theta^{(k)}, we cannot simply evaluate the Bellman operator and fit. Instead, we must solve an integral equation. At each iteration, we seek \\theta^{(k+1)} satisfying:\\int_{\\mathcal{S}} \\sum_j \\theta_j^{(k+1)} \\varphi_j(s) \\varphi_i(s) w(s) ds = \\int_{\\mathcal{S}} [\\Bellman\\hat{v}(s; \\theta^{(k)})] \\varphi_i(s) w(s) ds.\n\nGalerkin with Function Iteration\n\nInput Basis functions \\{\\varphi_1, \\ldots, \\varphi_n\\}, weight function w(s), initial \\theta^{(0)}, tolerance \\varepsilon > 0\n\nOutput Converged coefficients \\theta^*\n\nCompute mass matrix M_{ij} = \\int_{\\mathcal{S}} \\varphi_i(s) \\varphi_j(s) w(s) ds via numerical integration\n\nk \\leftarrow 0\n\nrepeat\n\nFor each i = 1, \\ldots, n:\n\nb_i^{(k)} \\leftarrow \\int_{\\mathcal{S}} [\\Bellman\\hat{v}(s; \\theta^{(k)})] \\varphi_i(s) w(s) ds via numerical integration\n\nSolve M \\theta^{(k+1)} = b^{(k)}\n\nk \\leftarrow k + 1\n\nuntil \\|\\theta^{(k)} - \\theta^{(k-1)}\\| < \\varepsilon\n\nreturn \\theta^{(k)}\n\nThe left side is a linear system (the “mass matrix” M_{ij} = \\int \\varphi_i \\varphi_j w), and the right side requires integrating the Bellman operator output against each test function. When the basis functions are orthogonal polynomials with matching weight w, the mass matrix is diagonal, simplifying the solve. But we still need numerical integration to evaluate the right side. This makes Galerkin substantially more expensive than collocation per iteration.\n\nNewton’s method for Galerkin similarly requires integration. The residual is R(s; \\theta) = \\Bellman\\hat{v}(s; \\theta) - \\hat{v}(s; \\theta), and we need G_i(\\theta) = \\int R(s; \\theta) \\varphi_i(s) w(s) ds = 0. The Jacobian entry is:J_{ij} = \\int \\left[ \\frac{\\partial \\Bellman\\hat{v}(s; \\theta)}{\\partial \\theta_j} - \\varphi_j(s) \\right] \\varphi_i(s) w(s) ds.\n\nGalerkin with Newton’s Method\n\nInput Basis functions \\{\\varphi_1, \\ldots, \\varphi_n\\}, weight function w(s), initial \\theta^{(0)}, tolerance \\varepsilon > 0\n\nOutput Converged coefficients \\theta^*\n\nk \\leftarrow 0\n\nrepeat\n\nFor each i = 1, \\ldots, n:\n\nG_i^{(k)} \\leftarrow \\int_{\\mathcal{S}} [\\Bellman\\hat{v}(s; \\theta^{(k)}) - \\hat{v}(s; \\theta^{(k)})] \\varphi_i(s) w(s) ds\n\nFor each j = 1, \\ldots, n:\n\nJ_{ij} \\leftarrow \\int_{\\mathcal{S}} \\left[ \\gamma \\mathbb{E}[\\varphi_j(s') \\mid s, a^*(s;\\theta^{(k)})] - \\varphi_j(s) \\right] \\varphi_i(s) w(s) ds\n\nSolve J \\Delta\\theta = G^{(k)}\n\n\\theta^{(k+1)} \\leftarrow \\theta^{(k)} - \\Delta\\theta\n\nk \\leftarrow k + 1\n\nuntil \\|\\Delta\\theta\\| < \\varepsilon\n\nreturn \\theta^{(k)}\n\nThe Envelope Theorem gives \\frac{\\partial \\Bellman\\hat{v}(s; \\theta)}{\\partial \\theta_j} = \\gamma \\mathbb{E}[\\varphi_j(s') \\mid s, a^*(s;\\theta)], so we must integrate expected basis function values (under optimal actions) against test functions and weight. This requires both numerical integration and careful tracking of optimal actions across the state space, making it substantially more complex than collocation’s pointwise evaluation.\n\nThe advantage of Galerkin over collocation lies in its theoretical properties: when using orthogonal polynomials, Galerkin provides optimal approximation in the weighted L^2 norm. For smooth problems, this can yield better accuracy per degree of freedom than collocation. In practice, collocation’s computational simplicity usually outweighs Galerkin’s theoretical optimality for Bellman equations, especially in high-dimensional problems where integration becomes prohibitively expensive.","type":"content","url":"/projection#galerkin","position":41},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl3":"Galerkin for Discrete MDPs: LSTD and LSPI","lvl2":"Application to the Bellman Equation"},"type":"lvl3","url":"/projection#galerkin-for-discrete-mdps-lstd-and-lspi","position":42},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl3":"Galerkin for Discrete MDPs: LSTD and LSPI","lvl2":"Application to the Bellman Equation"},"content":"When the state space is discrete and finite, the Galerkin conditions simplify dramatically. The integrals become sums, and we can write everything in matrix form. This specialization shows the connection to algorithms widely used in reinforcement learning.\n\nFor a discrete state space \\mathcal{S} = \\{s_1, \\ldots, s_m\\}, the Galerkin orthogonality conditions\\int_{\\mathcal{S}} [\\Bellman\\hat{v}(s; \\theta) - \\hat{v}(s; \\theta)] \\varphi_i(s) w(s) ds = 0\n\nbecome weighted sums over states:\\sum_{s \\in \\mathcal{S}} \\xi(s) [\\Bellman\\hat{v}(s; \\theta) - \\hat{v}(s; \\theta)] \\varphi_i(s) = 0, \\quad i = 1, \\ldots, n,\n\nwhere \\xi(s) \\geq 0 with \\sum_s \\xi(s) = 1 is a probability distribution over states. Define the feature matrix \\boldsymbol{\\Phi} \\in \\mathbb{R}^{m \\times n} with entries \\Phi_{si} = \\varphi_i(s) (each row contains the features for one state), and let \\boldsymbol{\\Xi} = \\text{diag}(\\xi) be the diagonal matrix with the state distribution on the diagonal.","type":"content","url":"/projection#galerkin-for-discrete-mdps-lstd-and-lspi","position":43},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl4":"Policy Evaluation: LSTD","lvl3":"Galerkin for Discrete MDPs: LSTD and LSPI","lvl2":"Application to the Bellman Equation"},"type":"lvl4","url":"/projection#policy-evaluation-lstd","position":44},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl4":"Policy Evaluation: LSTD","lvl3":"Galerkin for Discrete MDPs: LSTD and LSPI","lvl2":"Application to the Bellman Equation"},"content":"For policy evaluation with a fixed policy \\pi, the Bellman operator is linear:[\\BellmanPi \\hat{v}](s) = r(s, \\pi(s)) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s, \\pi(s)) \\hat{v}(j).\n\nWith linear function approximation \\hat{v}(s) = \\boldsymbol{\\varphi}(s)^\\top \\theta = \\sum_i \\theta_i \\varphi_i(s), this becomes:[\\BellmanPi \\hat{v}](s) = r(s, \\pi(s)) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s, \\pi(s)) \\sum_i \\theta_i \\varphi_i(j).\n\nLet \\mathbf{r}_\\pi \\in \\mathbb{R}^m be the vector of rewards [\\mathbf{r}_\\pi]_s = r(s, \\pi(s)), and \\mathbf{P}_\\pi \\in \\mathbb{R}^{m \\times m} be the transition matrix with [\\mathbf{P}_\\pi]_{sj} = p(j|s, \\pi(s)). Then \\BellmanPi \\hat{v} = \\mathbf{r}_\\pi + \\gamma \\mathbf{P}_\\pi \\boldsymbol{\\Phi} \\theta in vector form.\n\nThe Galerkin conditions require \\langle \\BellmanPi \\hat{v} - \\hat{v}, \\varphi_i \\rangle_\\xi = 0 for all basis functions, which in matrix form is:\\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} (\\mathbf{r}_\\pi + \\gamma \\mathbf{P}_\\pi \\boldsymbol{\\Phi} \\theta - \\boldsymbol{\\Phi} \\theta) = \\mathbf{0}.\n\nRearranging:\\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} (\\boldsymbol{\\Phi} - \\gamma \\mathbf{P}_\\pi \\boldsymbol{\\Phi}) \\theta = \\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} \\mathbf{r}_\\pi.\n\nThis is the LSTD (Least Squares Temporal Difference) solution. The matrix \\mathbf{A} = \\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} (\\boldsymbol{\\Phi} - \\gamma \\mathbf{P}_\\pi \\boldsymbol{\\Phi}) and vector \\mathbf{b} = \\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} \\mathbf{r}_\\pi give the linear system \\mathbf{A} \\theta = \\mathbf{b}.\n\nWhen \\xi is the stationary distribution of policy \\pi (so \\xi^\\top \\mathbf{P}_\\pi = \\xi^\\top), this system has a unique solution, and the projected Bellman operator \\Proj \\BellmanPi is a contraction in the weighted L^2 norm \\|\\cdot\\|_\\xi. This is the theoretical foundation for TD learning with linear function approximation.","type":"content","url":"/projection#policy-evaluation-lstd","position":45},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl4":"The Bellman Optimality Equation: Function Iteration and Newton’s Method","lvl3":"Galerkin for Discrete MDPs: LSTD and LSPI","lvl2":"Application to the Bellman Equation"},"type":"lvl4","url":"/projection#the-bellman-optimality-equation-function-iteration-and-newtons-method","position":46},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl4":"The Bellman Optimality Equation: Function Iteration and Newton’s Method","lvl3":"Galerkin for Discrete MDPs: LSTD and LSPI","lvl2":"Application to the Bellman Equation"},"content":"For the Bellman optimality equation, the max operator introduces nonlinearity:[\\Bellman\\hat{v}](s) = \\max_{a \\in \\mathcal{A}_s} \\left\\{ r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) \\hat{v}(j) \\right\\}.\n\nThe Galerkin conditions become:F(\\theta) \\equiv \\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} (\\Bellman\\hat{v}(\\cdot; \\theta) - \\boldsymbol{\\Phi} \\theta) = \\mathbf{0},\n\nwhere the Bellman operator must be evaluated at each state s to find the optimal action and compute the target value. This is a system of n nonlinear equations in n unknowns.\n\nFunction iteration applies the Bellman operator and projects back. Given \\theta^{(k)}, compute the greedy policy \\pi^{(k)}(s) = \\arg\\max_a \\{r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) \\boldsymbol{\\varphi}(j)^\\top \\theta^{(k)}\\} at each state, then solve:\\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} (\\boldsymbol{\\Phi} - \\gamma \\mathbf{P}_{\\pi^{(k)}} \\boldsymbol{\\Phi}) \\theta^{(k+1)} = \\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} \\mathbf{r}_{\\pi^{(k)}}.\n\nThis evaluates the current greedy policy using LSTD, then implicitly improves by computing a new greedy policy at the next iteration. However, convergence can be slow when the finite-dimensional approximation poorly preserves contraction.\n\nNewton’s method treats G(\\theta) = 0 as a rootfinding problem and uses the Jacobian to accelerate convergence. The Jacobian of G is:J_G(\\theta) = \\frac{\\partial G}{\\partial \\theta} = \\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} \\left( \\frac{\\partial \\Bellman\\hat{v}(\\cdot; \\theta)}{\\partial \\theta} - \\boldsymbol{\\Phi} \\right).\n\nTo compute \\frac{\\partial \\Bellman\\hat{v}(s; \\theta)}{\\partial \\theta_j}, we use the Envelope Theorem from Step 4. At the current \\theta^{(k)}, let a^*(s; \\theta^{(k)}) be the optimal action at state s. Then:\\frac{\\partial [\\Bellman\\hat{v}](s; \\theta^{(k)})}{\\partial \\theta_j} = \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s, a^*(s; \\theta^{(k)})) \\varphi_j(j).\n\nDefine the policy \\pi^{(k)}(s) = a^*(s; \\theta^{(k)}). The Jacobian becomes:J_G(\\theta^{(k)}) = \\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} (\\gamma \\mathbf{P}_{\\pi^{(k)}} \\boldsymbol{\\Phi} - \\boldsymbol{\\Phi}) = -\\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} (\\boldsymbol{\\Phi} - \\gamma \\mathbf{P}_{\\pi^{(k)}} \\boldsymbol{\\Phi}).\n\nThe Newton update \\theta^{(k+1)} = \\theta^{(k)} - J_G(\\theta^{(k)})^{-1} G(\\theta^{(k)}) simplifies. We have:G(\\theta^{(k)}) = \\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} (\\Bellman\\hat{v}(\\cdot; \\theta^{(k)}) - \\boldsymbol{\\Phi} \\theta^{(k)}).\n\nAt each state s, the greedy value is [\\Bellman\\hat{v}(\\cdot; \\theta^{(k)})](s) = r(s, \\pi^{(k)}(s)) + \\gamma \\sum_j p(j|s, \\pi^{(k)}(s)) \\boldsymbol{\\varphi}(j)^\\top \\theta^{(k)}, which equals [\\mathrm{L}_{\\pi^{(k)}} \\hat{v}(\\cdot; \\theta^{(k)})](s). Thus:G(\\theta^{(k)}) = \\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} (\\mathbf{r}_{\\pi^{(k)}} + \\gamma \\mathbf{P}_{\\pi^{(k)}} \\boldsymbol{\\Phi} \\theta^{(k)} - \\boldsymbol{\\Phi} \\theta^{(k)}).\n\nThe Newton step becomes:\\theta^{(k+1)} = \\theta^{(k)} + [\\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} (\\boldsymbol{\\Phi} - \\gamma \\mathbf{P}_{\\pi^{(k)}} \\boldsymbol{\\Phi})]^{-1} \\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} (\\mathbf{r}_{\\pi^{(k)}} + \\gamma \\mathbf{P}_{\\pi^{(k)}} \\boldsymbol{\\Phi} \\theta^{(k)} - \\boldsymbol{\\Phi} \\theta^{(k)}).\n\nMultiplying through and simplifying:\\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} (\\boldsymbol{\\Phi} - \\gamma \\mathbf{P}_{\\pi^{(k)}} \\boldsymbol{\\Phi}) \\theta^{(k+1)} = \\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} \\mathbf{r}_{\\pi^{(k)}}.\n\nThis is LSPI (Least Squares Policy Iteration). Each Newton step:\n\nComputes the greedy policy \\pi^{(k)}(s) = \\arg\\max_a \\{r(s,a) + \\gamma \\sum_j p(j|s,a) \\boldsymbol{\\varphi}(j)^\\top \\theta^{(k)}\\}\n\nSolves the LSTD equation for this policy to get \\theta^{(k+1)}\n\nNewton’s method for the Galerkin-projected Bellman optimality equation is equivalent to policy iteration in the function approximation setting. Just as Newton’s method for collocation corresponded to policy iteration (Step 4), Newton’s method for discrete Galerkin gives LSPI.\n\nGalerkin projection with linear function approximation reduces policy iteration to a sequence of linear systems, each solvable in closed form. For discrete MDPs, we can compute the matrices \\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} \\boldsymbol{\\Phi} and \\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} \\mathbf{P}_\\pi \\boldsymbol{\\Phi} exactly.","type":"content","url":"/projection#the-bellman-optimality-equation-function-iteration-and-newtons-method","position":47},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl2":"Extension to Nonlinear Approximators"},"type":"lvl2","url":"/projection#extension-to-nonlinear-approximators","position":48},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl2":"Extension to Nonlinear Approximators"},"content":"The weighted residual methods developed so far have focused on linear function classes: polynomial bases, piecewise linear interpolants, and linear combinations of fixed basis functions. Neural networks, kernel methods, and decision trees do not fit this template. How does the framework extend to nonlinear approximators?\n\nRecall the Galerkin approach for linear approximation v_{\\boldsymbol{\\theta}} = \\sum_{i=1}^d \\theta_i \\varphi_i. The orthogonality conditions \\langle v - \\Bellman v, \\varphi_i \\rangle_w = 0 for all i define a linear system with a closed-form solution. These equations arise from minimizing \\|v - \\Bellman v\\|_w^2 over the subspace, since at the minimum, the gradient with respect to each coefficient must vanish. The connection between norm minimization and orthogonality holds generally. For any norm \\|\\cdot\\|_w induced by an inner product \\langle \\cdot, \\cdot \\rangle_w, minimizing \\|f(\\boldsymbol{\\theta})\\|_w^2 with respect to parameters requires \\frac{\\partial}{\\partial \\theta_i} \\|f(\\boldsymbol{\\theta})\\|_w^2 = 0. Since \\|f\\|_w^2 = \\langle f, f \\rangle_w, the chain rule gives 2\\langle f, \\frac{\\partial f}{\\partial \\theta_i} \\rangle_w = 0. Minimizing the residual norm is thus equivalent to requiring orthogonality \\langle f, \\frac{\\partial f}{\\partial \\theta_i} \\rangle_w = 0 for all i. The equivalence holds for any choice of inner product: weighted L^2 integrals for Galerkin, sums over collocation points for collocation, or sampled expectations for neural networks.\n\nFor nonlinear function classes parameterized by \\boldsymbol{\\theta} \\in \\mathbb{R}^p (neural networks, kernel expansions), the same minimization principle applies:\\boldsymbol{\\theta}^* = \\arg\\min_{\\boldsymbol{\\theta}} \\|v_{\\boldsymbol{\\theta}} - \\Bellman v_{\\boldsymbol{\\theta}}\\|_w^2.\n\nThe first-order stationarity condition yields orthogonality:\\Big\\langle v_{\\boldsymbol{\\theta}} - \\Bellman v_{\\boldsymbol{\\theta}}, \\frac{\\partial v_{\\boldsymbol{\\theta}}}{\\partial \\theta_i} \\Big\\rangle_w = 0 \\quad \\text{for all } i.\n\nThe test functions are now the partial derivatives \\frac{\\partial v_{\\boldsymbol{\\theta}}}{\\partial \\theta_i}, which span the tangent space to the manifold \\{v_{\\boldsymbol{\\theta}} : \\boldsymbol{\\theta} \\in \\mathbb{R}^p\\} at the current parameters. In the linear case v_{\\boldsymbol{\\theta}} = \\sum_i \\theta_i \\varphi_i, the partial derivative \\frac{\\partial v_{\\boldsymbol{\\theta}}}{\\partial \\theta_i} = \\varphi_i recovers the fixed basis functions of Galerkin. For nonlinear parameterizations, the test functions change with \\boldsymbol{\\theta}, and the orthogonality conditions define a nonlinear system solved by iterative gradient descent.\n\nThe dual pairing formulation \n\nLegrand & Junca (2025) extends this framework to settings where test objects need not be regular functions. We have been informal about this distinction in our treatment of collocation, but the Dirac deltas \\delta(x - x_i) used there are not classical functions. They are distributions, defined rigorously only through their action on test functions via \\langle \\Residual(v), \\delta(x - x_i) \\rangle = (\\Residual v)(x_i). The simple calculus argument for orthogonality does not apply directly to such objects; the dual pairing framework provides the proper mathematical foundation. The induced dual norm \\|\\Residual(v)\\|_* = \\sup_{\\|w\\|=1} |\\langle \\Residual(v), w \\rangle| measures residuals by their worst-case effect on test functions, a perspective that has inspired adversarial formulations \n\nZang et al. (2020) where both trial and test functions are learned.\n\nThe minimum residual framework thus connects classical projection methods to modern function approximation. The unifying principle is orthogonality of residuals to test functions. Linear methods use fixed test functions and admit closed-form solutions. Nonlinear methods use parameter-dependent test functions and require iterative optimization.\n\nWe now turn to the question of convergence: when does the iteration v_{k+1} = \\Proj \\Bellman v_k converge?","type":"content","url":"/projection#extension-to-nonlinear-approximators","position":49},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl2":"Monotone Projection and the Preservation of Contraction"},"type":"lvl2","url":"/projection#monotone-projection-and-the-preservation-of-contraction","position":50},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl2":"Monotone Projection and the Preservation of Contraction"},"content":"The informal discussion of shape preservation hints at a deeper theoretical question: when does the function iteration method converge? Recall from our discussion of collocation that function iteration proceeds in two steps:\n\nApply the Bellman operator at collocation points: t^{(k)} = v(\\theta^{(k)}) where t_i^{(k)} = \\Bellman\\hat{v}^{(k)}(s_i)\n\nFit new coefficients to match these targets: \\boldsymbol{\\Phi} \\theta^{(k+1)} = t^{(k)}, giving \\theta^{(k+1)} = \\boldsymbol{\\Phi}^{-1} v(\\theta^{(k)})\n\nWe can reinterpret this iteration in function space rather than coefficient space. Let \\Proj be the projection operator that takes any function f and returns its approximation in \\text{span}\\{\\varphi_1, \\ldots, \\varphi_n\\}. For collocation, \\Proj is the interpolation operator: (\\Proj f)(s) is the unique linear combination of basis functions that matches f at the collocation points. Then Step 2 can be written as: fit \\hat{v}^{(k+1)} so that \\hat{v}^{(k+1)}(s_i) = \\Bellman\\hat{v}^{(k)}(s_i) for all collocation points, which means \\hat{v}^{(k+1)} = \\Proj(\\Bellman\\hat{v}^{(k)}).\n\nIn other words, function iteration is equivalent to projected value iteration in function space:\\hat{v}^{(k+1)} = \\Proj \\Bellman \\hat{v}^{(k)}.\n\nWe know that standard value iteration v_{k+1} = \\Bellman v_k converges because \\Bellman is a \\gamma-contraction in the sup norm. But now we’re iterating with the composed operator \\Proj \\Bellman instead of \\Bellman alone.\n\nThis \\Proj \\Bellman structure is not specific to collocation. It is inherent in all projection methods. The general pattern is always the same: apply the Bellman operator to get a target function \\Bellman\\hat{v}^{(k)}, then project it back onto our approximation space to get \\hat{v}^{(k+1)}. The projection step defines an operator \\Proj that depends on our choice of test functions:\n\nFor collocation, \\Proj interpolates values at collocation points\n\nFor Galerkin, \\Proj is orthogonal projection with respect to \\langle \\cdot, \\cdot \\rangle_w\n\nFor least squares, \\Proj minimizes the weighted residual norm\n\nBut regardless of which projection method we use, iteration takes the form \\hat{v}^{(k+1)} = \\Proj \\Bellman\\hat{v}^{(k)}.\n\nThe central question is whether the composition \\Proj \\Bellman inherits the contraction property of \\Bellman. If not, the iteration may diverge, oscillate, or converge to a spurious fixed point even though the original problem is well-posed.","type":"content","url":"/projection#monotone-projection-and-the-preservation-of-contraction","position":51},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl3":"Monotone Approximators and Stability","lvl2":"Monotone Projection and the Preservation of Contraction"},"type":"lvl3","url":"/projection#monotone-approximators-and-stability","position":52},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl3":"Monotone Approximators and Stability","lvl2":"Monotone Projection and the Preservation of Contraction"},"content":"The answer turns out to depend on specific properties of the approximation operator \\Proj. This theory was developed independently across multiple research communities: computational economics \n\nJudd (1992)\n\nJudd (1996)\n\nMcGrattan (1997)\n\nSantos & Vigo-Aguiar (1998), economic dynamics \n\nStachurski (2009), and reinforcement learning \n\nGordon (1995)\n\nGordon (1999). These communities arrived at essentially the same mathematical conditions.","type":"content","url":"/projection#monotone-approximators-and-stability","position":53},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl4":"Monotonicity Implies Nonexpansiveness","lvl3":"Monotone Approximators and Stability","lvl2":"Monotone Projection and the Preservation of Contraction"},"type":"lvl4","url":"/projection#monotonicity-implies-nonexpansiveness","position":54},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl4":"Monotonicity Implies Nonexpansiveness","lvl3":"Monotone Approximators and Stability","lvl2":"Monotone Projection and the Preservation of Contraction"},"content":"It turns out that approximation operators satisfying simple structural properties automatically preserve contraction.\n\nMonotone operators are nonexpansive (Stachurski)\n\nLet \\Proj: \\mathcal{V} \\to \\mathcal{V} be a linear operator on the space \\mathcal{V} of bounded real-valued functions on \\mathcal{S}. If \\Proj satisfies:\n\nMonotonicity: f \\leq g pointwise implies \\Proj f \\leq \\Proj g\n\nConstant preservation: \\Proj\\mathbf{1} = \\mathbf{1} where \\mathbf{1} is the constant function equal to 1\n\nThen \\Proj is nonexpansive in the sup norm: \\|\\Proj f - \\Proj g\\|_\\infty \\leq \\|f - g\\|_\\infty for all f, g \\in \\mathcal{V}.\n\nLet M = \\|f - g\\|_\\infty. Then -M \\leq f(s) - g(s) \\leq M for all s, which can be written as g - M\\mathbf{1} \\leq f \\leq g + M\\mathbf{1}. By monotonicity, \\Proj(g - M\\mathbf{1}) \\leq \\Proj f \\leq \\Proj(g + M\\mathbf{1}). By linearity and constant preservation, \\Proj g - M\\mathbf{1} \\leq \\Proj f \\leq \\Proj g + M\\mathbf{1}, which means |\\Proj f(s) - \\Proj g(s)| \\leq M for all s. Therefore \\|\\Proj f - \\Proj g\\|_\\infty \\leq \\|f - g\\|_\\infty.\n\nThis proposition shows that monotonicity and constant preservation automatically imply nonexpansiveness. There is no need to verify this separately. The intuition is that a monotone, constant-preserving operator acts like a weighted average that respects order structure and cannot amplify differences between functions.","type":"content","url":"/projection#monotonicity-implies-nonexpansiveness","position":55},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl4":"Preservation of Contraction","lvl3":"Monotone Approximators and Stability","lvl2":"Monotone Projection and the Preservation of Contraction"},"type":"lvl4","url":"/projection#preservation-of-contraction","position":56},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl4":"Preservation of Contraction","lvl3":"Monotone Approximators and Stability","lvl2":"Monotone Projection and the Preservation of Contraction"},"content":"Combining nonexpansiveness with the contraction property of the Bellman operator yields the main stability result.\n\nStability of projected value iteration (Santos-Vigo-Aguiar)\n\nLet \\Bellman: \\mathcal{V} \\to \\mathcal{V} be a \\gamma-contraction on the space \\mathcal{V} of bounded functions with respect to the sup norm. Let \\Proj: \\mathcal{V} \\to \\mathcal{V} be a linear approximation operator satisfying monotonicity and constant preservation.\n\nThen the composed operator \\Proj \\Bellman is a \\gamma-contraction, and projected value iteration v_{k+1} = \\Proj \\Bellman v_k converges globally to a unique fixed point v_\\Proj \\in \\text{Range}(\\Proj) with approximation error:\\|v_\\Proj - v^*\\|_\\infty \\leq \\frac{1}{1-\\gamma} \\|\\Proj v^* - v^*\\|_\\infty,\n\nwhere v^* is the true value function.\n\nBy \n\nProposition 1, \\Proj is nonexpansive since it satisfies monotonicity and constant preservation. Since \\Bellman is a \\gamma-contraction, we have \\|\\Bellman f - \\Bellman g\\|_\\infty \\leq \\gamma\\|f-g\\|_\\infty. Therefore:\\|\\Proj \\Bellman f - \\Proj \\Bellman g\\|_\\infty \\leq \\|\\Bellman f - \\Bellman g\\|_\\infty \\leq \\gamma\\|f-g\\|_\\infty,\n\nshowing that \\Proj \\Bellman is a \\gamma-contraction. The error bound follows from fixed-point analysis: v^* - v_\\Proj = (I - \\Proj \\Bellman)^{-1}(v^* - \\Proj v^*), and since \\Proj \\Bellman is a \\gamma-contraction, \\|(I - \\Proj \\Bellman)^{-1}\\| \\leq (1-\\gamma)^{-1}.\n\nThis error bound tells us that the fixed-point error is controlled by how well \\Proj can represent v^*. If v^* \\in \\text{Range}(\\Proj), then \\Proj v^* = v^* and the error vanishes. Otherwise, the error is proportional to the approximation error \\|\\Proj v^* - v^*\\|_\\infty, amplified by the factor (1-\\gamma)^{-1}.","type":"content","url":"/projection#preservation-of-contraction","position":57},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl4":"Averagers in Discrete-State Problems","lvl3":"Monotone Approximators and Stability","lvl2":"Monotone Projection and the Preservation of Contraction"},"type":"lvl4","url":"/projection#averagers-in-discrete-state-problems","position":58},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl4":"Averagers in Discrete-State Problems","lvl3":"Monotone Approximators and Stability","lvl2":"Monotone Projection and the Preservation of Contraction"},"content":"For discrete-state problems, the monotonicity conditions have a natural interpretation as averaging with nonnegative weights. This characterization was developed by Gordon in the context of reinforcement learning.\n\nAverager (Gordon)\n\nAn operator \\Proj: \\mathbb{R}^{|\\mathcal{S}|} \\to \\mathbb{R}^{|\\mathcal{S}|} is an averager if \\Proj v = Wv where W is a |\\mathcal{S}| \\times |\\mathcal{S}| stochastic matrix: w_{ij} \\geq 0 and \\sum_j w_{ij} = 1 for all i.\n\nAveragers automatically satisfy the monotonicity conditions: linearity follows from matrix multiplication, monotonicity follows from nonnegativity of entries, and constant preservation follows from row sums equaling one.\n\nStability with averagers (Gordon)\n\nIf \\Proj is an averager and \\Bellman is the Bellman operator (a \\gamma-contraction), then \\Proj \\Bellman is a \\gamma-contraction, and value iteration v_{k+1} = \\Proj \\Bellman v_k converges to a unique fixed point.\n\nThis specializes the Santos-Vigo-Aguiar theorem to discrete states, expressed in the probabilistic language of stochastic matrices. The stochastic matrix characterization connects to Markov chain theory: \\Proj v represents expected values after one transition, and the monotonicity property reflects the fact that expectations preserve order.\n\nExamples of averagers include state aggregation (averaging values within groups), K-nearest neighbors (averaging over nearest states), kernel smoothing with positive kernels, and multilinear interpolation on grids (barycentric weights are nonnegative and sum to one). Counterexamples include linear least squares regression (projection matrix may have negative entries) and high-order polynomial interpolation (Runge phenomenon produces negative weights).\n\nThe following table summarizes which common approximation operators satisfy the monotonicity conditions:\n\nMethod\n\nMonotone?\n\nNotes\n\nPiecewise linear interpolation\n\nYes\n\nAlways an averager; guaranteed stability\n\nMultilinear interpolation (grid)\n\nYes\n\nBarycentric weights are nonnegative and sum to one\n\nShape-preserving splines (Schumaker)\n\nYes\n\nDesigned to maintain monotonicity\n\nState aggregation\n\nYes\n\nExact averaging within groups\n\nKernel smoothing (positive kernels)\n\nYes\n\nIf kernel integrates to one\n\nHigh-order polynomial interpolation\n\nNo\n\nOscillations violate monotonicity (Runge phenomenon)\n\nLeast squares projection (arbitrary basis)\n\nNo\n\nProjection matrix may have negative entries\n\nFourier/spectral methods\n\nNo\n\nNot monotone-preserving in general\n\nNeural networks\n\nNo\n\nHighly flexible but no monotonicity guarantees\n\nThe distinction between “safe” (monotone) and “potentially unstable” (non-monotone) approximators provides rigorous foundation for the folk wisdom that linear interpolation is reliable while high-order polynomials can be dangerous for value iteration. But notice that the table’s verdict on “least squares projection” is somewhat abstract. It doesn’t specifically address the three weighted residual methods we introduced at the start of this chapter.\n\nThe choice of solution method determines which approximation operators are safe to use. Successive approximation (fixed-point iteration) requires monotone approximators to guarantee convergence. Rootfinding methods like Newton’s method do not require monotonicity. Stability depends on numerical properties of the Jacobian rather than contraction preservation. These considerations suggest hybrid strategies. One approach runs a few iterations with a monotone method to generate an initial guess, then switches to Newton’s method with a smooth approximation for rapid final convergence.","type":"content","url":"/projection#averagers-in-discrete-state-problems","position":59},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl3":"Connecting Back to Collocation, Galerkin, and Least Squares","lvl2":"Monotone Projection and the Preservation of Contraction"},"type":"lvl3","url":"/projection#connecting-back-to-collocation-galerkin-and-least-squares","position":60},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl3":"Connecting Back to Collocation, Galerkin, and Least Squares","lvl2":"Monotone Projection and the Preservation of Contraction"},"content":"We have now developed a general stability theory for projected value iteration and surveyed which approximation operators are monotone. But what does this mean for the three specific weighted residual methods we introduced at the start of this chapter: collocation, Galerkin, and least squares? Each method defines a different projection operator \\Proj, and we now need to determine which satisfy the monotonicity conditions that guarantee convergence.\n\nCollocation with piecewise linear interpolation is monotone. When we use collocation with piecewise linear basis functions on a grid, the projection operator performs linear interpolation between grid points. At any state s between grid points s_i and s_{i+1}, the interpolated value is:(\\Proj v)(s) = \\frac{s_{i+1} - s}{s_{i+1} - s_i} v(s_i) + \\frac{s - s_i}{s_{i+1} - s_i} v(s_{i+1}).\n\nThe interpolation weights (barycentric coordinates) are nonnegative and sum to one, making this an averager in Gordon’s sense. Therefore collocation with piecewise linear bases satisfies the monotonicity conditions and the Santos-Vigo-Aguiar stability theorem applies. The folk wisdom that “linear interpolation is safe for value iteration” has rigorous theoretical foundation.\n\nGalerkin projection is generally not monotone. The Galerkin projection operator for a general basis \\{\\varphi_1, \\ldots, \\varphi_n\\} has the form:\\Proj = \\boldsymbol{\\Phi}(\\boldsymbol{\\Phi}^\\top \\mathbf{W} \\boldsymbol{\\Phi})^{-1} \\boldsymbol{\\Phi}^\\top \\mathbf{W},\n\nwhere \\mathbf{W} is a diagonal weight matrix and \\boldsymbol{\\Phi} contains the basis function evaluations. This projection matrix typically has negative entries. To see why, consider a simple example with polynomial basis functions \\{1, x, x^2\\} on [-1, 1]. The projection of a function onto this space involves computing (\\boldsymbol{\\Phi}^\\top \\mathbf{W} \\boldsymbol{\\Phi})^{-1}, and the resulting operator can map nonnegative functions to functions with negative values. This is the same phenomenon underlying the Runge phenomenon in high-order polynomial interpolation: the projection weights oscillate in sign.\n\nSince Galerkin projection is not monotone, the sup norm contraction theory does not guarantee convergence of projected value iteration v_{k+1} = \\Proj \\Bellman v_k with Galerkin.\n\nLeast squares methods share the non-monotonicity issue. The least squares projection operator minimizes \\|\\Residual(\\hat{f})\\|_w^2 and has the same mathematical form as Galerkin projection. It is a linear projection onto \\text{span}\\{\\varphi_1, \\ldots, \\varphi_n\\} with respect to a weighted inner product. Like Galerkin, the projection matrix typically contains negative entries and violates monotonicity.\n\nThe monotone approximator framework successfully covers collocation with simple bases, but leaves two important methods, Galerkin and least squares, without convergence guarantees. These methods are used in least-squares temporal difference learning (LSTD) and modern reinforcement learning with linear function approximation. We need a different analytical framework to understand when these non-monotone projections lead to convergent algorithms.","type":"content","url":"/projection#connecting-back-to-collocation-galerkin-and-least-squares","position":61},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl2":"Beyond Monotone Approximators"},"type":"lvl2","url":"/projection#beyond-monotone-approximators","position":62},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl2":"Beyond Monotone Approximators"},"content":"The monotone approximator theory gives us a clean sufficient condition for convergence: if \\Proj is monotone (and constant-preserving), then \\Proj is non-expansive in the sup norm \\|\\cdot\\|_\\infty. Since \\Bellman is a \\gamma-contraction in the sup norm, their composition \\Proj \\Bellman is also a \\gamma-contraction in the sup norm, guaranteeing convergence of projected value iteration.\n\nBut what if \\Proj is not monotone? Can we still guarantee convergence? Galerkin and least squares projections typically violate monotonicity, yet they are widely used in practice, particularly in reinforcement learning through least-squares temporal difference learning (LSTD). In general, proving convergence for non-monotone projections is difficult. However, for the special case of policy evaluation, computing the value function v_\\pi of a fixed policy \\pi, we can establish convergence by working in a different norm.","type":"content","url":"/projection#beyond-monotone-approximators","position":63},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl3":"The Policy Evaluation Problem and LSTD","lvl2":"Beyond Monotone Approximators"},"type":"lvl3","url":"/projection#the-policy-evaluation-problem-and-lstd","position":64},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl3":"The Policy Evaluation Problem and LSTD","lvl2":"Beyond Monotone Approximators"},"content":"Consider the policy evaluation problem: given policy \\pi, we want to solve the policy Bellman equation v_\\pi = r_\\pi + \\gamma \\mathbf{P}_\\pi v_\\pi, where r_\\pi and \\mathbf{P}_\\pi are the reward vector and transition matrix under \\pi. This is the core computational task in policy iteration, actor-critic algorithms, and temporal difference learning. In reinforcement learning, we typically learn from sampled experience: trajectories (s_0, a_0, r_1, s_1, a_1, r_2, s_2, \\ldots) generated by following \\pi. If the Markov chain induced by \\pi is ergodic, the state distribution converges to a stationary distribution \\xi satisfying \\xi^\\top \\mathbf{P}_\\pi = \\xi^\\top.\n\nThis distribution determines which states appear frequently in our data. States visited often contribute more samples and have more influence on any learned approximation. States visited rarely contribute little. For a linear approximation v_\\theta(s) = \\sum_j \\theta_j \\varphi_j(s), the least-squares temporal difference (LSTD) algorithm computes coefficients by solving:\\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} (\\boldsymbol{\\Phi} - \\gamma \\mathbf{P}_\\pi \\boldsymbol{\\Phi}) \\boldsymbol{\\theta} = \\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} \\mathbf{r}_\\pi,\n\nwhere \\boldsymbol{\\Phi} is the matrix of basis function evaluations and \\boldsymbol{\\Xi} = \\text{diag}(\\xi). We write this matrix equation for analysis purposes, but the actual algorithm does not compute it this way. For large state spaces, we cannot enumerate all states to form \\boldsymbol{\\Phi} or explicitly represent the transition matrix \\mathbf{P}_\\pi. Instead, the practical algorithm accumulates sums from sampled transitions (s, r, s'), incrementally building the matrices \\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} \\boldsymbol{\\Phi} and \\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} \\mathbf{P}_\\pi \\boldsymbol{\\Phi} without ever forming the full objects. The algorithm is derived from first principles through temporal difference learning, and the Galerkin perspective provides an interpretation of what it computes.","type":"content","url":"/projection#the-policy-evaluation-problem-and-lstd","position":65},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl3":"LSTD as Projected Bellman Equation","lvl2":"Beyond Monotone Approximators"},"type":"lvl3","url":"/projection#lstd-as-projected-bellman-equation","position":66},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl3":"LSTD as Projected Bellman Equation","lvl2":"Beyond Monotone Approximators"},"content":"To see what this equation means, let \\hat{v} = \\boldsymbol{\\Phi} \\boldsymbol{\\theta} be the solution. Expanding the parentheses:\\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} \\boldsymbol{\\Phi} \\boldsymbol{\\theta} - \\gamma \\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} \\mathbf{P}_\\pi \\boldsymbol{\\Phi} \\boldsymbol{\\theta} = \\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} \\mathbf{r}_\\pi.\n\nMoving all terms to the left side and factoring out \\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi}:\\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} (\\boldsymbol{\\Phi} \\boldsymbol{\\theta} - \\gamma \\mathbf{P}_\\pi \\boldsymbol{\\Phi} \\boldsymbol{\\theta} - \\mathbf{r}_\\pi) = \\mathbf{0}.\n\nSince \\hat{v} = \\boldsymbol{\\Phi} \\boldsymbol{\\theta} and the policy Bellman operator is \\BellmanPi \\hat{v} = \\mathbf{r}_\\pi + \\gamma \\mathbf{P}_\\pi \\hat{v}, we can write:\\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} (\\hat{v} - \\BellmanPi \\hat{v}) = \\mathbf{0}.\n\nLet \\boldsymbol{\\varphi}_j denote the j-th column of \\boldsymbol{\\Phi}, which contains the evaluations of the j-th basis function at all states. The equation above says that for each j:\\boldsymbol{\\varphi}_j^\\top \\boldsymbol{\\Xi} (\\hat{v} - \\BellmanPi \\hat{v}) = 0.\n\nBut \\boldsymbol{\\varphi}_j^\\top \\boldsymbol{\\Xi} (\\hat{v} - \\BellmanPi \\hat{v}) is exactly the \\xi-weighted inner product \\langle \\boldsymbol{\\varphi}_j, \\hat{v} - \\BellmanPi \\hat{v} \\rangle_\\xi. So the residual \\hat{v} - \\BellmanPi \\hat{v} is orthogonal to every basis function, and therefore orthogonal to the entire subspace \\text{span}(\\boldsymbol{\\Phi}).\n\nBy definition, the orthogonal projection \\Proj y of a vector y onto a subspace is the unique vector in that subspace such that y - \\Proj y is orthogonal to the subspace. Here, \\hat{v} lies in \\text{span}(\\boldsymbol{\\Phi}) (since \\hat{v} = \\boldsymbol{\\Phi} \\boldsymbol{\\theta}), and we have just shown that \\BellmanPi \\hat{v} - \\hat{v} is orthogonal to \\text{span}(\\boldsymbol{\\Phi}). Therefore, \\hat{v} = \\Proj \\BellmanPi \\hat{v}, where \\Proj is orthogonal projection onto \\text{span}(\\boldsymbol{\\Phi}) with respect to the \\xi-weighted inner product:\\langle u, v \\rangle_\\xi = u^\\top \\boldsymbol{\\Xi} v, \\qquad \\|v\\|_\\xi = \\sqrt{v^\\top \\boldsymbol{\\Xi} v}, \\qquad \\Proj = \\boldsymbol{\\Phi}(\\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi} \\boldsymbol{\\Phi})^{-1} \\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Xi}.\n\nThe weighting by \\xi is not arbitrary. Temporal difference learning performs stochastic updates using individual transitions: \\theta_{k+1} = \\theta_k + \\alpha_k (r + \\gamma v_{\\theta_k}(s') - v_{\\theta_k}(s)) \\nabla v_{\\theta_k}(s), with states sampled from \\xi. The ODE analysis of this stochastic process (Borkar-Meyn theory) shows convergence to a fixed point, which can be expressed in closed form as the \\xi-weighted projected Bellman operator. LSTD is an algorithm that computes this analytical fixed point.","type":"content","url":"/projection#lstd-as-projected-bellman-equation","position":67},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl3":"Orthogonal Projection is Non-Expansive","lvl2":"Beyond Monotone Approximators"},"type":"lvl3","url":"/projection#orthogonal-projection-is-non-expansive","position":68},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl3":"Orthogonal Projection is Non-Expansive","lvl2":"Beyond Monotone Approximators"},"content":"Suppose \\xi is the steady-state distribution: \\xi^\\top \\mathbf{P}_\\pi = \\xi^\\top. Our goal is to establish that \\Proj \\BellmanPi is a contraction in \\|\\cdot\\|_\\xi. If we can establish that \\Proj is non-expansive in this norm and that \\BellmanPi is a \\gamma-contraction in \\|\\cdot\\|_\\xi, then their composition will be a \\gamma-contraction:\\|\\Proj \\BellmanPi v - \\Proj \\BellmanPi w\\|_\\xi \\leq \\|\\BellmanPi v - \\BellmanPi w\\|_\\xi \\leq \\gamma \\|v - w\\|_\\xi.\n\nFirst, we establish that orthogonal projection is non-expansive. For any vector v, we can decompose v = \\Proj v + (v - \\Proj v), where (v - \\Proj v) is orthogonal to the subspace \\text{span}(\\boldsymbol{\\Phi}). By the Pythagorean theorem in the \\|\\cdot\\|_\\xi inner product:\\|v\\|_\\xi^2 = \\|\\Proj v\\|_\\xi^2 + \\|v - \\Proj v\\|_\\xi^2.\n\nSince \\|v - \\Proj v\\|_\\xi^2 \\geq 0, we have:\\|v\\|_\\xi^2 \\geq \\|\\Proj v\\|_\\xi^2.\n\nTaking square roots of both sides (which preserves the inequality since both norms are non-negative):\\|\\Proj v\\|_\\xi \\leq \\|v\\|_\\xi.\n\nThis holds for all v, so \\Proj is non-expansive in \\|\\cdot\\|_\\xi.","type":"content","url":"/projection#orthogonal-projection-is-non-expansive","position":69},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl3":"Contraction of \\BellmanPi in \\|\\cdot\\|_\\xi","lvl2":"Beyond Monotone Approximators"},"type":"lvl3","url":"/projection#contraction-of-bellmanpi-in-cdot-xi","position":70},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl3":"Contraction of \\BellmanPi in \\|\\cdot\\|_\\xi","lvl2":"Beyond Monotone Approximators"},"content":"To show \\BellmanPi = r_\\pi + \\gamma \\mathbf{P}_\\pi is a \\gamma-contraction, we need to verify:\\|\\BellmanPi v - \\BellmanPi w\\|_\\xi = \\|\\gamma \\mathbf{P}_\\pi (v - w)\\|_\\xi = \\gamma \\|\\mathbf{P}_\\pi (v - w)\\|_\\xi.\n\nThis will be at most \\gamma \\|v - w\\|_\\xi if \\mathbf{P}_\\pi is non-expansive, meaning \\|\\mathbf{P}_\\pi z\\|_\\xi \\leq \\|z\\|_\\xi for any vector z. We therefore need to establish that \\mathbf{P}_\\pi is non-expansive in \\|\\cdot\\|_\\xi.\n\nConsider the squared norm of \\mathbf{P}_\\pi z. By definition of the weighted norm:\\|\\mathbf{P}_\\pi z\\|_\\xi^2 = \\sum_s \\xi(s) [(\\mathbf{P}_\\pi z)(s)]^2.\n\nThe s-th component of \\mathbf{P}_\\pi z is (\\mathbf{P}_\\pi z)(s) = \\sum_{s'} p(s'|s,\\pi(s)) z(s'). This is a weighted average of the values z(s') with weights p(s'|s,\\pi(s)) that sum to one. Therefore:\\|\\mathbf{P}_\\pi z\\|_\\xi^2 = \\sum_s \\xi(s) \\left[\\sum_{s'} p(s'|s,\\pi(s)) z(s')\\right]^2.\n\nSince the function x \\mapsto x^2 is convex, Jensen’s inequality applied to the probability distribution p(\\cdot|s,\\pi(s)) gives:\\left[\\sum_{s'} p(s'|s,\\pi(s)) z(s')\\right]^2 \\leq \\sum_{s'} p(s'|s,\\pi(s)) z(s')^2.\n\nSubstituting this into the norm expression:\\|\\mathbf{P}_\\pi z\\|_\\xi^2 \\leq \\sum_s \\xi(s) \\sum_{s'} p(s'|s,\\pi(s)) z(s')^2 = \\sum_{s'} z(s')^2 \\sum_s \\xi(s) p(s'|s,\\pi(s)).\n\nThe stationarity condition \\xi^\\top \\mathbf{P}_\\pi = \\xi^\\top means \\sum_s \\xi(s) p(s'|s,\\pi(s)) = \\xi(s') for all s'. Therefore:\\|\\mathbf{P}_\\pi z\\|_\\xi^2 \\leq \\sum_{s'} z(s')^2 \\xi(s') = \\|z\\|_\\xi^2.\n\nTaking square roots, \\|\\mathbf{P}_\\pi z\\|_\\xi \\leq \\|z\\|_\\xi, so \\mathbf{P}_\\pi is non-expansive in \\|\\cdot\\|_\\xi. This makes \\BellmanPi = r_\\pi + \\gamma \\mathbf{P}_\\pi a \\gamma-contraction in \\|\\cdot\\|_\\xi. Composing with the non-expansive projection:\\|\\Proj \\BellmanPi v - \\Proj \\BellmanPi w\\|_\\xi \\leq \\|\\BellmanPi v - \\BellmanPi w\\|_\\xi \\leq \\gamma \\|v - w\\|_\\xi.\n\nBy Banach’s fixed-point theorem, \\Proj \\BellmanPi has a unique fixed point and iterates converge from any initialization.","type":"content","url":"/projection#contraction-of-bellmanpi-in-cdot-xi","position":71},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl3":"Interpretation: The On-Policy Condition","lvl2":"Beyond Monotone Approximators"},"type":"lvl3","url":"/projection#interpretation-the-on-policy-condition","position":72},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl3":"Interpretation: The On-Policy Condition","lvl2":"Beyond Monotone Approximators"},"content":"The result shows that convergence depends on matching the weighting to the operator. We cannot choose an arbitrary weighted L^2 norm and expect \\Proj \\BellmanPi to be a contraction. Instead, the weighting \\xi must have a specific relationship with the transition matrix \\mathbf{P}_\\pi in the operator \\BellmanPi: namely, \\xi must be the stationary distribution of \\mathbf{P}_\\pi. This is what makes the weighted geometry compatible with the operator’s structure. When this match holds, Jensen’s inequality gives us non-expansiveness of \\mathbf{P}_\\pi in the \\|\\cdot\\|_\\xi norm, and the composition \\Proj \\BellmanPi inherits the contraction property.\n\nIn reinforcement learning, this has a practical interpretation. When we learn by following policy \\pi and collecting transitions (s, a, r, s'), the states we visit are distributed according to the stationary distribution of \\pi. This is on-policy learning. The LSTD algorithm uses data sampled from this distribution, which means the empirical weighting naturally matches the operator structure. Our analysis shows that the iterative algorithm v_{k+1} = \\Proj \\BellmanPi v_k converges to the same fixed point that LSTD computes in closed form.\n\nThis is fundamentally different from the monotone approximator theory. There, we required structural properties of \\Proj itself (monotonicity, constant preservation) to guarantee that \\Proj preserves the sup-norm contraction property of \\Bellman. Here, we place no such restriction on \\Proj. Galerkin projection is not monotone. Instead, convergence depends on matching the norm to the operator. When \\xi does not match the stationary distribution, as in off-policy learning where data comes from a different behavior policy, the Jensen inequality argument breaks down. The operator \\mathbf{P}_\\pi need not be non-expansive in \\|\\cdot\\|_\\xi, and \\Proj \\BellmanPi may fail to contract. This explains divergence phenomena such as Baird’s counterexample \n\nBaird (1995).","type":"content","url":"/projection#interpretation-the-on-policy-condition","position":73},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl3":"The Bellman Optimality Case","lvl2":"Beyond Monotone Approximators"},"type":"lvl3","url":"/projection#the-bellman-optimality-case","position":74},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl3":"The Bellman Optimality Case","lvl2":"Beyond Monotone Approximators"},"content":"Can we extend this weighted L^2 analysis to the Bellman optimality operator \\Bellman v = \\max_a [r_a + \\gamma \\mathbf{P}_a v]? The answer is no, at least not with this approach. The obstacle appears at the Jensen inequality step. For policy evaluation, we had:\\|\\mathbf{P}_\\pi z\\|_\\xi^2 = \\sum_s \\xi(s) \\left[\\sum_{s'} p(s'|s,\\pi(s)) z(s')\\right]^2.\n\nThe inner term is a convex combination of the values z(s'), which allowed us to apply Jensen’s inequality to the convex function x \\mapsto x^2. For the optimal Bellman operator, we would need to bound:\\left[\\max_{a} \\sum_{s'} p(s'|s,a) z(s')\\right]^2.\n\nBut the maximum of convex combinations is not itself a convex combination. It is a pointwise maximum. Jensen’s inequality does not apply. We cannot conclude that \\max_a [\\mathbf{P}_a z] is non-expansive in any weighted L^2 norm.\n\nIs convergence of \\Proj \\Bellman with Galerkin projection impossible, or merely difficult to prove? The situation is subtle. In practice, fitted Q-iteration and approximate value iteration with neural networks often work well, suggesting that some form of stability exists. But there are also well-documented divergence examples (e.g., Q-learning with linear function approximation can diverge). The theoretical picture remains incomplete. Some results exist for restricted function classes or under strong assumptions on the MDP structure, but no general convergence guarantee like the policy evaluation result is available. The interplay between the max operator, the projection, and the norm geometry is not well understood. This is an active area of research in reinforcement learning theory.\n\nDespite these theoretical gaps, the practical algorithm template is straightforward. We now present fitted-value iteration as a meta-algorithm that combines any supervised learning method with the Bellman operator.","type":"content","url":"/projection#the-bellman-optimality-case","position":75},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl2":"Fitted-Value/Q Iteration (FVI/FQI)"},"type":"lvl2","url":"/projection#fitted-value-q-iteration-fvi-fqi","position":76},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl2":"Fitted-Value/Q Iteration (FVI/FQI)"},"content":"We have developed weighted residual methods through abstract functional equations: choose test functions, impose orthogonality conditions \\langle R, p_i \\rangle_w = 0, solve for coefficients. What are we actually computing when we solve these equations by successive approximation? The answer is simpler than the formalism suggests: function iteration with a fitting step.\n\nRecall that the weighted residual conditions \\langle v - \\Bellman v, p_i \\rangle_w = 0 define a fixed-point problem v = \\Proj \\Bellman v, where \\Proj is a projection operator onto \\text{span}(\\boldsymbol{\\Phi}). We can solve this by iteration: v_{k+1} = \\Proj \\Bellman v_k. Under appropriate conditions (monotonicity of \\Proj, or matching the weight to the operator for policy evaluation), this converges to a solution.\n\nIn parameter space, this iteration becomes a fitting procedure. Consider Galerkin projection with a finite state space of n states. Let \\boldsymbol{\\Phi} be the n \\times d matrix of basis evaluations, \\mathbf{W} the diagonal weight matrix, and \\mathbf{y} the vector of Bellman operator evaluations: y_i = (\\Bellman v_k)(s_i). The projection is:\\boldsymbol{\\theta}_{k+1} = (\\boldsymbol{\\Phi}^\\top \\mathbf{W} \\boldsymbol{\\Phi})^{-1} \\boldsymbol{\\Phi}^\\top \\mathbf{W} \\mathbf{y}.\n\nThis is weighted least-squares regression: fit \\boldsymbol{\\Phi} \\boldsymbol{\\theta} to targets \\mathbf{y}. For collocation, we require exact interpolation \\boldsymbol{\\Phi} \\boldsymbol{\\theta}_{k+1} = \\mathbf{y} at chosen collocation points. For continuous state spaces, we approximate the Galerkin integrals using sampled states, reducing to the same finite-dimensional fitting problem. The abstraction remains consistent: function iteration in the abstract becomes generate targets, fit to targets, repeat in the implementation.\n\nThis extends beyond linear basis functions. Neural networks, decision trees, and kernel methods all implement variants of this procedure. Given data \\{(s_i, y_i)\\} where y_i = (\\Bellman v_k)(s_i), each method produces a function v_{k+1}: \\mathcal{S} \\to \\mathbb{R} by fitting to the targets. The projection operator \\Proj is simply one instantiation of a fitting procedure. Galerkin and collocation correspond to specific choices of approximation class and loss function.\n\nFitted-Value Iteration\n\nInputs: Finite state set \\mathcal{S} (or sample \\{s_i\\}_{i=1}^n), discount factor \\gamma, function class \\mathcal{F}, fitting procedure \\mathtt{fit}, convergence tolerance \\epsilon\n\nOutput: Approximate value function \\hat{v} \\approx v^*\n\nInitialize v_0 \\in \\mathcal{F} arbitrarily\n\nSet k \\leftarrow 0\n\nrepeat\n\n\\quad for each state s_i \\in \\mathcal{S} do\n\n\\quad\\quad Compute target: y_i \\leftarrow \\displaystyle\\max_{a \\in \\mathcal{A}} \\Big\\{ r(s_i, a) + \\gamma \\sum_{s'} p(s' \\mid s_i, a) v_k(s') \\Big\\}\n\n\\quad end for\n\n\\quad Fit new approximation: v_{k+1} \\leftarrow \\mathtt{fit}\\big(\\{(s_i, y_i)\\}_{i=1}^n; \\mathcal{F}\\big)\n\n\\quad k \\leftarrow k+1\n\nuntil \\|v_k - v_{k-1}\\| < \\epsilon (or maximum iterations reached)\n\nreturn v_k\n\nThe abstraction \\mathtt{fit} encapsulates all the complexity of function approximation, whether that involves solving a linear system, running gradient descent, or training an ensemble. Any regression model with a fit(X, y) interface works: LinearRegression, RandomForestRegressor, GradientBoostingRegressor, MLPRegressor, or custom neural networks. The projection operator \\Proj is one instantiation: when \\mathcal{F} is a linear subspace and we minimize weighted squared error, we recover Galerkin or collocation. The broader view is that FVI reduces dynamic programming to repeated calls to a supervised learning subroutine.\n\nA limitation of FVI/FQI is that it assumes we can evaluate the Bellman operator exactly. Computing y_i = (\\Bellman v_k)(s_i) requires knowing transition probabilities and summing over all next states. In practice, we often have only a simulator or observed data. The next chapter shows how to approximate these expectations from samples, connecting the fitted-value iteration framework to simulation-based methods.","type":"content","url":"/projection#fitted-value-q-iteration-fvi-fqi","position":77},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl2":"Summary"},"type":"lvl2","url":"/projection#summary","position":78},{"hierarchy":{"lvl1":"Weighted Residual Methods for Functional Equations","lvl2":"Summary"},"content":"This chapter developed weighted residual methods for solving functional equations like the Bellman equation. We approximate the value function using a finite basis, then impose conditions that make the residual orthogonal to chosen test functions. Different choices of test functions yield different methods: Galerkin tests against the basis itself, collocation tests at specific points, and least squares minimizes the residual norm. All reduce to the same computational pattern: generate Bellman targets, fit a function approximator, repeat.\n\nConvergence depends on how the projection interacts with the Bellman operator. For monotone projections (piecewise linear interpolation, state aggregation), the composition \\Proj \\Bellman inherits the contraction property and iteration converges. For non-monotone projections like Galerkin, convergence requires matching the weighting to the stationary distribution, which holds in on-policy settings. The Bellman optimality case remains theoretically incomplete.\n\nThroughout this chapter, we assumed access to the transition model: computing \\Bellman v(s) requires summing over all next states weighted by transition probabilities. In practice, we often have only a simulator or observed trajectories, not an explicit model. The next chapter addresses this gap. Monte Carlo methods estimate expectations from sampled transitions, replacing exact Bellman operator evaluations with sample averages. This connects the projection framework developed here to the simulation-based algorithms used in reinforcement learning.","type":"content","url":"/projection#summary","position":79},{"hierarchy":{"lvl1":"Programs as Models"},"type":"lvl1","url":"/simulation","position":0},{"hierarchy":{"lvl1":"Programs as Models"},"content":"","type":"content","url":"/simulation","position":1},{"hierarchy":{"lvl1":"Programs as Models"},"type":"lvl1","url":"/simulation#programs-as-models","position":2},{"hierarchy":{"lvl1":"Programs as Models"},"content":"Up to this point, we have described models using systems of equations—either differential or difference equations—that express how a system evolves over time. These analytical models define the transition structure explicitly. For instance, in discrete time, the evolution of the state is governed by a known function:\\mathbf{x}_{k+1} = f(\\mathbf{x}_k, \\mathbf{u}_k)\n\nGiven access to f, we can construct trajectories, analyze system behavior, and design control policies. The important feature here is not that the model evolves one step at a time, but that we are given the local dynamics function f itself.\n\nIn contrast, simulation-based models do not expose f directly. Instead, they define a procedure—implemented in code—that takes an initial state and input sequence and returns the resulting trajectory:\\{\\mathbf{x}_0, \\mathbf{x}_1, \\dots, \\mathbf{x}_T\\} = \\mathcal{S}(\\mathbf{x}_0, \\{\\mathbf{u}_t\\}_{t=0}^{T-1})\n\nHere, \\mathcal{S} represents the full simulator. Internally, it may apply numerical integration, scheduling logic, branching rules, or other computations. But these details are encapsulated. From the outside, we can only query the simulator by running it.\n\nThis distinction is subtle but important. Both types of models can generate trajectories. What matters is the interface: analytical models provide direct access to f; simulation models do not. They offer a trajectory-generation interface, but hide the internal structure that produces it.\n\nCase Study: Robotics — MuJoCo\n\nMuJoCo illustrates this distinction well. It simulates the dynamics of articulated rigid bodies under contact constraints. The equations it solves include:\n\n\nM(\\mathbf{q})\\ddot{\\mathbf{q}} + C(\\mathbf{q}, \\dot{\\mathbf{q}}) = \\boldsymbol{\\tau} + J^\\top \\boldsymbol{\\lambda}\n\n\n\\phi(\\mathbf{q}) = 0, \\quad \\boldsymbol{\\lambda} \\geq 0, \\quad \\boldsymbol{\\lambda}^\\top \\phi = 0\n\nHere \\mathbf{q} are joint positions, M is the mass matrix, and \\boldsymbol{\\lambda} are contact forces enforcing non-penetration. But these physical equations are part of a larger simulator that also includes:\n\ncollision detection,\n\ncontact force models,\n\nsensor and actuator emulation,\n\nand visual rendering.\n\nThe full behavior of a robot interacting with its environment emerges only when the simulator is executed. While the underlying physics are well-understood, the complexity of contact dynamics, collision detection, and sensor modeling makes it impractical to expose the local dynamics function f directly.","type":"content","url":"/simulation#programs-as-models","position":3},{"hierarchy":{"lvl1":"Programs as Models","lvl2":"Systems with Discrete Events"},"type":"lvl2","url":"/simulation#systems-with-discrete-events","position":4},{"hierarchy":{"lvl1":"Programs as Models","lvl2":"Systems with Discrete Events"},"content":"Many simulation models arise when a system’s dynamics are driven not by time-continuous evolution, but by the occurrence of events. These discrete-event systems (DES) change state only at specific, often asynchronous points in time. Between events, the state remains fixed.\n\nA discrete-event system can be described by:\n\na set of discrete states \\mathcal{X},\n\na set of events \\mathcal{E},\n\na transition function f: \\mathcal{X} \\times \\mathcal{E} \\rightarrow \\mathcal{X},\n\nand a time-advance function t_a: \\mathcal{X} \\rightarrow \\mathbb{R}_{\\geq 0}.\n\nAt each point, the system checks which events are enabled and advances to the next scheduled one.\n\nExample: Network Traffic Control System\n\nConsider a software-defined networking (SDN) controller managing traffic routing in a data center. The system must make real-time decisions about packet forwarding paths based on network conditions and service requirements.\n\nThe discrete states \\mathcal{X} represent the current network configuration: active routing tables, link utilization levels, and quality-of-service priority queues at each switch.\n\nThe events \\mathcal{E} include:\n\nNew flow requests arriving (video streaming, database queries, file transfers)\n\nLink failures or congestion threshold violations\n\nFlow completion notifications\n\nLoad balancing triggers when servers exceed capacity\n\nNetwork policy updates from administrators\n\nThe transition function f captures how routing decisions change the network state. When a high-priority video conference flow arrives while a link is congested, the controller might transition to a new state where low-priority background traffic is rerouted through alternative paths.\n\nThe time-advance function t_a determines when the next routing decision occurs. Flow arrivals follow traffic patterns (bursty during business hours), while link failures are rare but unpredictable events.\n\nBetween events, packets follow the established routing rules—the same forwarding tables remain active across all switches. The control problem here is to adapt routing decisions to discrete network events, balancing throughput, latency, and reliability constraints.","type":"content","url":"/simulation#systems-with-discrete-events","position":5},{"hierarchy":{"lvl1":"Programs as Models","lvl2":"Hybrid Systems"},"type":"lvl2","url":"/simulation#hybrid-systems","position":6},{"hierarchy":{"lvl1":"Programs as Models","lvl2":"Hybrid Systems"},"content":"Some systems evolve continuously most of the time but undergo discrete jumps in response to certain conditions. These hybrid systems are common in control applications.\n\nThe system consists of:\n\na set of discrete modes q \\in \\mathcal{Q},\n\ncontinuous dynamics in each mode: \\dot{\\mathbf{x}} = f_q(\\mathbf{x}),\n\nguards that specify when transitions between modes occur,\n\nand reset maps that update the state during such transitions.\n\nExample: Thermostat Control\n\nAn HVAC system can be in one of several modes: heating, cooling, or off. The temperature evolves continuously according to physical laws, but when it crosses certain thresholds, the system switches modes:if x < setpoint - delta:\n    mode = \"heating\"\nelif x > setpoint + delta:\n    mode = \"cooling\"\nelse:\n    mode = \"off\"\n\nWithin each mode, a different differential equation applies. This results in a piecewise-smooth trajectory with mode-dependent dynamics.\n\nCase Study: Building Energy — EnergyPlus\n\nEnergyPlus provides a sophisticated example of hybrid systems in building energy simulation. At its core are physical equations describing heat flows:C_i \\frac{dT_i}{dt} = \\sum_j h_{ij} A_{ij}(T_j - T_i) + Q_i\n\nIt also solves implicit equations representing HVAC component behavior:0 = f(T, \\dot{m}, P)\n\nBut the actual simulator includes hundreds of thousands of lines of code handling:\n\ninterpolated weather data,\n\noccupancy schedules,\n\nequipment performance curves,\n\nand control logic implemented as finite-state machines.\n\nThe result is a program that emulates how a building behaves over time, given environmental inputs and schedules. The hybrid nature emerges from the interaction between continuous thermal dynamics and discrete control decisions made by thermostats, occupancy sensors, and HVAC equipment.","type":"content","url":"/simulation#hybrid-systems","position":7},{"hierarchy":{"lvl1":"Programs as Models","lvl2":"Agent-Based Models"},"type":"lvl2","url":"/simulation#agent-based-models","position":8},{"hierarchy":{"lvl1":"Programs as Models","lvl2":"Agent-Based Models"},"content":"Some simulation models do not describe systems via global state transitions, but instead simulate the behavior of many individual components or agents, each following local rules. These agent-based models (ABMs) are widely used in epidemiology, ecology, and social modeling.\n\nEach agent maintains its own internal state and acts according to probabilistic or rule-based logic. The system’s behavior arises from the interactions among agents.\n\nExample: Residential Energy Consumption under Dynamic Pricing\n\nConsider a neighborhood where each household is an agent making energy consumption decisions based on real-time electricity pricing and thermal comfort preferences. Each household agent has:\n\nInternal state: current temperature, HVAC settings, comfort preferences, price sensitivity\n\nLocal decision rules: MPC algorithms that optimize the trade-off between energy cost and thermal comfort\n\nUnique characteristics: different utility functions, thermal mass, occupancy patterns\n\nThe simulator might execute something like:for household in neighborhood:\n    # Each household solves its own MPC optimization\n    current_price = utility.get_current_price()\n    comfort_weight = household.comfort_preference\n    \n    # Optimize over prediction horizon\n    optimal_setpoint = household.mpc_controller.optimize(\n        current_temp=household.temperature,\n        price_forecast=utility.price_forecast,\n        comfort_weight=comfort_weight\n    )\n    \n    household.set_hvac_setpoint(optimal_setpoint)\n    \n    # Update shared grid load\n    neighborhood.total_demand += household.power_consumption\n\nThe macro-level demand patterns—peak shifting, load leveling, rebound effects—emerge from individual household optimization decisions. No single equation describes the neighborhood’s energy consumption; it arises from the collective behavior of autonomous agents each solving their own control problems.\n\nCase Study: Traffic Simulation — SUMO\n\nSUMO demonstrates agent-based modeling in transportation systems. Each vehicle is an agent with its own route, driving behavior, and decision-making logic. The Krauss car-following rule shows how individual vehicle agents behave:def update_vehicle(v, v_leader, gap, dt):\n    v_safe = v_leader + (gap - min_gap) / tau\n    v_desired = min(v_max, v_safe)\n    ε = random.uniform(0, 1)\n    v_new = max(0, v_desired - ε * a_max * dt)\n    x_new = x + v_new * dt\n    return x_new, v_new\n\nBeyond car-following, each vehicle agent also:\n\nplans routes through the network based on travel time estimates,\n\nresponds to traffic signals and road conditions,\n\nmakes lane-changing decisions based on utility functions,\n\nand exhibits individual driving characteristics (aggressiveness, reaction time).\n\nThe emergent traffic patterns—congestion formation, traffic waves, bottlenecks—arise from the collective behavior of thousands of individual vehicle agents, each following local rules and making autonomous decisions.\n\nCase Study: Modeling Curbside Access at Montréal–Trudeau (YUL)\n\nAfternoon traffic at Montréal–Trudeau airport regularly backs up along the two-lane ramp leading to the departures curb. As passenger volumes rebound, the mix of private drop-offs, taxis, and shuttles converging in a confined space produces frequent delays. When curb dwell times rise—especially around wide-body departures—queues can spill back onto the access road and interfere with other flows on the airport campus.\n\nTo manage the situation, the airport operator relies on a dense sensor network. Cameras and license plate readers track vehicle trajectories across virtual gates, generating a real-time stream of entry points, curb interactions, and exit times. According to public statements, AI-based forecasting solutions have been deployed to anticipate congestion and suggest alternative routing options for passengers and drivers. While no technical details have been disclosed, this is a typical instance of a traffic prediction and control problem that lends itself to agent-based modeling.\n\nIn such a model, each vehicle is treated as an individual agent with internal state:s_t = \\bigl(\\text{lane},\\;x_t,\\;v_t,\\;\\text{intent}\\bigr),\n\nwhere x_t and v_t denote longitudinal position and speed, and lane and intent capture higher-level behavioural traits. The simulation proceeds in discrete time. At each step, agents update their acceleration based on local traffic density (e.g., via a car-following model like IDM), evaluate potential lane changes (e.g., using a utility or incentive rule), and advance position accordingly.\n\nThe layout of the ramp—its geometry, merges, and constraints—is fixed. What changes are the traffic patterns and driver behaviours. These can be estimated from historical trajectories and updated as new data arrives. In a real-time setting, a filtering step adjusts the simulation so that its predicted flows remain consistent with current observations.\n\nWhile the behaviour of each individual agent is governed by program logic and heuristics—such as car-following rules, desired speeds, or gap acceptance—some parameters are identified offline from historical data, while others are estimated online. This adaptation helps the model track observed conditions. But even with such adjustments, not all effects are easily captured.\n\nConstruction activity, weather disturbances, and irregular flight scheduling can introduce sudden shifts in flow that lie outside the scope of the structural model. To account for these, one can overlay a data-driven correction on top of the simulation. Suppose the simulator produces a queue length forecast q^{\\text{sim}}_{t+h} over horizon h. A statistical model can be trained to predict the residual between this forecast and the observed outcome:r_{t+h} = q^{\\text{obs}}_{t+h} - q^{\\text{sim}}_{t+h},\n\nas a function of exogenous features z_t, such as weather, incident flags, or scheduled arrivals. The final forecast then becomes:\\widehat{q}_{t+h} = q^{\\text{sim}}_{t+h} + \\phi(z_t),\n\nwhere \\phi is a learned mapping from external conditions to the expected correction. The result is a hybrid model: the simulation enforces physical structure and agent-level behaviour, while the residual model compensates for aspects of the system that are harder to express analytically.","type":"content","url":"/simulation#agent-based-models","position":9},{"hierarchy":{"lvl1":"Looking Ahead"},"type":"lvl1","url":"/simulation#looking-ahead","position":10},{"hierarchy":{"lvl1":"Looking Ahead"},"content":"There’s no universally correct way to model a system. Your choice depends on what you know, what you can observe, what you care about, and what tools you have.\n\nThis chapter laid out a spectrum—from explicit, mechanistic models to black-box simulators and learned dynamics. In every case, modeling choices define the structure of the problem and the space of possible solutions. In the next chapters, we’ll see how they anchor learning, optimization, and decision-making.","type":"content","url":"/simulation#looking-ahead","position":11},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations"},"type":"lvl1","url":"/smoothing","position":0},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations"},"content":"Dynamic programming methods suffer from the curse of dimensionality and can quickly become difficult to apply in practice. We may also be dealing with large or continuous state or action spaces. We have seen so far that we could address this problem using discretization, or interpolation. These were already examples of approximate dynamic programming. In this chapter, we will see other forms of approximations meant to facilitate the optimization problem, either by approximating the optimality equations, the value function, or the policy itself.\nApproximation theory is at the heart of learning methods, and fundamentally, this chapter will be about the application of learning ideas to solve complex decision-making problems.","type":"content","url":"/smoothing","position":1},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations"},"type":"lvl1","url":"/smoothing#smooth-bellman-optimality-equations","position":2},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations"},"content":"While the standard Bellman optimality equations use the max operator to determine the best action, an alternative formulation known as the smooth or soft Bellman optimality equations replaces this with a softmax operator. This approach originated from \n\nRust (1987) and was later rediscovered in the context of maximum entropy inverse reinforcement learning \n\nZiebart et al. (2008), which then led to soft Q-learning \n\nHaarnoja et al. (2017) and soft actor-critic \n\nHaarnoja et al. (2018), a state-of-the-art deep reinforcement learning algorithm.\n\nIn the infinite-horizon setting, the smooth Bellman optimality equations take the form:v_\\gamma^\\star(s) = \\frac{1}{\\beta} \\log \\sum_{a \\in A_s} \\exp\\left(\\beta\\left(r(s, a) + \\gamma \\sum_{j \\in S} p(j | s, a) v_\\gamma^\\star(j)\\right)\\right)\n\nAdopting an operator-theoretic perspective, we can define a nonlinear operator \\mathrm{L}_\\beta such that the smooth value function of an MDP is then the solution to the following fixed-point equation:(\\mathrm{L}_\\beta \\mathbf{v})(s) = \\frac{1}{\\beta} \\log \\sum_{a \\in \\mathcal{A}_s} \\exp\\left(\\beta\\left(r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j)\\right)\\right)\n\nAs \\beta \\to \\infty, \\mathrm{L}_\\beta converges to the standard Bellman operator \\Bellman. Furthermore, it can be shown that the smooth Bellman operator is a contraction mapping in the supremum norm, and therefore has a unique fixed point. However, as opposed to the usual “hard” setting, the fixed point of \\mathrm{L}_\\beta is associated with the value function of an optimal stochastic policy defined by the softmax distribution:\\pi(a|s) = \\frac{\\exp\\left(\\beta\\left(r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v_\\gamma^\\star(j)\\right)\\right)}{\\sum_{a' \\in \\mathcal{A}_s} \\exp\\left(\\beta\\left(r(s,a') + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a') v_\\gamma^\\star(j)\\right)\\right)}\n\nDespite the confusing terminology, the above “softmax” policy is simply the smooth counterpart to the argmax operator in the original optimality equation: it acts as a soft-argmax.\n\nThis formulation is interesting for several reasons. First, smoothness is a desirable property from an optimization standpoint. Unlike \\gamma, we view \\beta as a hyperparameter of our algorithm, which we can control to achieve the desired level of accuracy.\n\nSecond, while presented from an intuitive standpoint where we replace the max by the log-sum-exp (a smooth maximum) and the argmax by the softmax (a smooth argmax), this formulation can also be obtained from various other perspectives, offering theoretical tools and solution methods. For example, \n\nRust (1987) derived this algorithm by considering a setting in which the rewards are stochastic and perturbed by a Gumbel noise variable. When considering the corresponding augmented state space and integrating the noise, we obtain smooth equations. This interpretation is leveraged by Rust for modeling purposes.","type":"content","url":"/smoothing#smooth-bellman-optimality-equations","position":3},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Smooth Value Iteration Algorithm"},"type":"lvl3","url":"/smoothing#smooth-value-iteration-algorithm","position":4},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Smooth Value Iteration Algorithm"},"content":"The smooth value iteration algorithm replaces the max operator in standard value iteration with the logsumexp operator. Here’s the algorithm structure:\n\nSmooth Value Iteration\n\nInput: MDP (S, A, r, p, \\gamma), inverse temperature \\beta > 0, tolerance \\epsilon > 0\n\nOutput: Approximate optimal value function v and stochastic policy \\pi\n\nInitialize v(s) \\leftarrow 0 for all s \\in S\n\nrepeat\n\n\\quad \\Delta \\leftarrow 0\n\n\\quad for each state s \\in S do\n\n\\quad\\quad for each action a \\in A_s do\n\n\\quad\\quad\\quad q(s,a) \\leftarrow r(s,a) + \\gamma \\sum_{j \\in S} p(j|s,a) v(j)\n\n\\quad\\quad end for\n\n\\quad\\quad v_{\\text{new}}(s) \\leftarrow \\frac{1}{\\beta} \\log \\sum_{a \\in A_s} \\exp(\\beta \\cdot q(s,a))\n\n\\quad\\quad \\Delta \\leftarrow \\max(\\Delta, |v_{\\text{new}}(s) - v(s)|)\n\n\\quad\\quad v(s) \\leftarrow v_{\\text{new}}(s)\n\n\\quad end for\n\nuntil \\Delta < \\epsilon\n\nExtract policy: for each state s \\in S do\n\n\\quad Compute q(s,a) for all a \\in A_s as in lines 5-7\n\n\\quad \\pi(a|s) \\leftarrow \\frac{\\exp(\\beta \\cdot q(s,a))}{\\sum_{a' \\in A_s} \\exp(\\beta \\cdot q(s,a'))} for all a \\in A_s\n\nend for\n\nreturn v, \\pi\n\nDifferences from standard value iteration:\n\nLine 8 uses \\frac{1}{\\beta} \\log \\sum_a \\exp(\\beta \\cdot q(s,a)) instead of \\max_a q(s,a)\n\nLine 15 extracts a stochastic policy using softmax instead of a deterministic argmax policy\n\nAs \\beta \\to \\infty, the algorithm converges to standard value iteration\n\nLower \\beta values produce more stochastic policies with higher entropy\n\nThere is also a way to obtain this equation by starting from the energy-based formulation often used in supervised learning, in which we convert an unnormalized probability distribution into a distribution using the softmax transformation. This is essentially what \n\nZiebart et al. (2008) did in their paper. Furthermore, this perspective bridges with the literature on probabilistic graphical models, in which we can now cast the problem of finding an optimal smooth policy into one of maximum likelihood estimation (an inference problem). This is the idea of control as inference, which also admits the converse - that of inference as control - used nowadays for deriving fast samples and amortized inference techniques using reinforcement learning \n\nLevine et al. (2018).\n\nFinally, it’s worth noting that we can also derive this form by considering an entropy-regularized formulation in which we penalize for the entropy of our policy in the reward function term. This formulation admits a solution that coincides with the smooth Bellman equations \n\nHaarnoja et al. (2017).","type":"content","url":"/smoothing#smooth-value-iteration-algorithm","position":5},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl2":"Alternative Soft Maximum: Gaussian Uncertainty-Weighted Aggregation"},"type":"lvl2","url":"/smoothing#alternative-soft-maximum-gaussian-uncertainty-weighted-aggregation","position":6},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl2":"Alternative Soft Maximum: Gaussian Uncertainty-Weighted Aggregation"},"content":"The logsumexp operator provides one way to soften the hard maximum, but alternative approaches exist. When Q-value estimates have heterogeneous uncertainty (some actions estimated more precisely than others), we can weight actions by the probability they are optimal under a Gaussian uncertainty model. \n\nD'Eramo et al. (2016) proposed computing weights as:w_{a'} = \\int_{-\\infty}^{+\\infty} \\phi\\left(\\frac{x - \\hat{\\mu}_{a'}}{\\hat{\\sigma}_{a'}/\\sqrt{n}}\\right) \\prod_{b \\neq a'} \\Phi\\left(\\frac{x - \\hat{\\mu}_b}{\\hat{\\sigma}_b/\\sqrt{n}}\\right) dx\n\nwhere \\hat{\\mu}_{a'} and \\hat{\\sigma}_{a'} are the sample mean and standard deviation of Q-value estimates, n is the sample size, and \\phi, \\Phi are the standard normal PDF and CDF. The soft Bellman target becomes v(s') = \\sum_{a'} w_{a'} q(s', a'), a probability-weighted expectation rather than a hard maximum.\n\nThis differs from logsumexp in that it adapts to state-action-specific uncertainty (actions with tighter confidence intervals receive more weight), whereas logsumexp applies uniform smoothing via temperature \\beta. The Gaussian-weighted approach requires maintaining variance estimates and computing integrals, making it more expensive than logsumexp. However, it provides a principled way to reduce overestimation bias in Q-learning while avoiding the pessimism of double Q-learning. We return to this estimator in the \n\nsimulation-based methods chapter when discussing overestimation bias mitigation strategies.","type":"content","url":"/smoothing#alternative-soft-maximum-gaussian-uncertainty-weighted-aggregation","position":7},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl2":"Gumbel Noise on the Rewards"},"type":"lvl2","url":"/smoothing#gumbel-noise-on-the-rewards","position":8},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl2":"Gumbel Noise on the Rewards"},"content":"We can obtain the smooth Bellman equation by considering a setting in which we have Gumbel noise added to the reward function. This derivation provides both theoretical insight and connects to practical modeling scenarios where rewards have random perturbations.","type":"content","url":"/smoothing#gumbel-noise-on-the-rewards","position":9},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Step 1: Define the Augmented MDP with Gumbel Noise","lvl2":"Gumbel Noise on the Rewards"},"type":"lvl3","url":"/smoothing#step-1-define-the-augmented-mdp-with-gumbel-noise","position":10},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Step 1: Define the Augmented MDP with Gumbel Noise","lvl2":"Gumbel Noise on the Rewards"},"content":"At each time period and state s, we draw an action-indexed shock vector:\\boldsymbol{\\epsilon}_t(s) = \\big(\\epsilon_t(s,a)\\big)_{a \\in \\mathcal{A}_s}, \\quad \\text{where } \\epsilon_t(s,a) \\text{ i.i.d.} \\sim \\mathrm{Gumbel}(\\mu_\\epsilon, 1/\\beta)\n\nThese shocks are independent across time periods, states, and actions, and are independent of the MDP transition dynamics p(\\cdot | s, a).\n\nThe Gumbel distribution with location parameter \\mu and scale parameter 1/\\beta has probability density function:f(x; \\mu, \\beta) = \\beta\\exp\\left(-\\beta(x-\\mu)-\\exp(-\\beta(x-\\mu))\\right)\n\nTo generate a Gumbel-distributed random variable, we can use inverse transform sampling: X = \\mu - \\frac{1}{\\beta} \\ln(-\\ln(U)) where U is uniform on (0,1).\n\nZero-Mean Shocks\n\nTo ensure the shocks have zero mean, we set \\mu_\\epsilon = -\\gamma_E/\\beta where \\gamma_E \\approx 0.5772 is the Euler-Mascheroni constant. This choice eliminates an additive constant that would otherwise appear in the smooth Bellman equation. For simplicity, we will adopt this convention throughout.\n\nWe now define an augmented MDP with:\n\nAugmented state: \\tilde{s} = (s, \\boldsymbol{\\epsilon}) where s \\in \\mathcal{S} and \\boldsymbol{\\epsilon} \\in \\mathbb{R}^{|\\mathcal{A}_s|}\n\nAugmented reward: \\tilde{r}(\\tilde{s}, a) = r(s,a) + \\epsilon(a)\n\nAugmented transition: \\tilde{p}(\\tilde{s}' | \\tilde{s}, a) = p(s' | s, a) \\cdot p(\\boldsymbol{\\epsilon}')\n\nThe transition factorizes because the next shock vector \\boldsymbol{\\epsilon}' is drawn independently of the current state and action (conditional independence).\n\nThe Augmented State Space is Infinite-Dimensional\n\nEven if the original state space \\mathcal{S} and action space \\mathcal{A} are finite, the augmented state space \\tilde{\\mathcal{S}} = \\mathcal{S} \\times \\mathbb{R}^{|\\mathcal{A}|} is uncountably infinite because each shock vector \\boldsymbol{\\epsilon} is a continuous random variable. Therefore:\n\nWe cannot enumerate the augmented states\n\nTabular dynamic programming methods do not apply directly\n\nThe augmented value function \\tilde{v}(s, \\boldsymbol{\\epsilon}) maps a continuous space to \\mathbb{R}\n\nThis motivates why we immediately marginalize over the shocks to obtain a finite-dimensional representation.","type":"content","url":"/smoothing#step-1-define-the-augmented-mdp-with-gumbel-noise","position":11},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Step 2: The Hard Bellman Equation on the Augmented State Space","lvl2":"Gumbel Noise on the Rewards"},"type":"lvl3","url":"/smoothing#step-2-the-hard-bellman-equation-on-the-augmented-state-space","position":12},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Step 2: The Hard Bellman Equation on the Augmented State Space","lvl2":"Gumbel Noise on the Rewards"},"content":"The Bellman optimality equation for the augmented MDP is:\\tilde{v}(s, \\boldsymbol{\\epsilon}) = \\max_{a \\in \\mathcal{A}_s} \\left\\{ r(s,a) + \\epsilon(a) + \\gamma \\mathbb{E}_{s', \\boldsymbol{\\epsilon}'}\\left[\\tilde{v}(s', \\boldsymbol{\\epsilon}') \\mid s, a\\right] \\right\\}\n\nHere the expectation is over the next augmented state (s', \\boldsymbol{\\epsilon}'), which includes both the next state s' \\sim p(\\cdot | s, a) and the next shock vector \\boldsymbol{\\epsilon}' \\sim p(\\cdot).\n\nThis is a perfectly well-defined Bellman equation, and an optimal stationary policy exists:\\pi(s, \\boldsymbol{\\epsilon}) \\in \\operatorname{argmax}_{a \\in \\mathcal{A}_s} \\left\\{ r(s,a) + \\epsilon(a) + \\gamma \\mathbb{E}_{s', \\boldsymbol{\\epsilon}'}\\left[\\tilde{v}(s', \\boldsymbol{\\epsilon}') \\mid s, a\\right] \\right\\}\n\nHowever, this equation is computationally intractable because:\n\nThe state space is continuous and infinite-dimensional\n\nThe shocks are fresh each period\n\nWe would need to solve for \\tilde{v} over an uncountable domain\n\nWe never solve this equation directly. Instead, we use it as a mathematical device to derive the smooth Bellman equation.","type":"content","url":"/smoothing#step-2-the-hard-bellman-equation-on-the-augmented-state-space","position":13},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Step 3: Define the Ex-Ante (Inclusive) Value Function","lvl2":"Gumbel Noise on the Rewards"},"type":"lvl3","url":"/smoothing#step-3-define-the-ex-ante-inclusive-value-function","position":14},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Step 3: Define the Ex-Ante (Inclusive) Value Function","lvl2":"Gumbel Noise on the Rewards"},"content":"The idea here is to consider the expected value before observing the current shocks. We define what some authors in econometrics call the inclusive value or ex-ante value:v(s) := \\mathbb{E}_{\\boldsymbol{\\epsilon}}\\big[\\tilde{v}(s, \\boldsymbol{\\epsilon})\\big]\n\nThis is the value of being in state s before we observe the current-period shock vector \\boldsymbol{\\epsilon}.\n\nTwo Different Value Functions\n\nIt is crucial to distinguish:\n\n\\tilde{v}(s, \\boldsymbol{\\epsilon}): the value after observing shocks (conditional on \\boldsymbol{\\epsilon}), defined on the augmented state space\n\nv(s): the value before observing shocks (marginalizing over \\boldsymbol{\\epsilon}), defined on the original state space\n\nThe function v(s) is what we actually compute and care about. The augmented value \\tilde{v} exists only as a proof device.","type":"content","url":"/smoothing#step-3-define-the-ex-ante-inclusive-value-function","position":15},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Step 4: Separate the Deterministic and Random Components","lvl2":"Gumbel Noise on the Rewards"},"type":"lvl3","url":"/smoothing#step-4-separate-the-deterministic-and-random-components","position":16},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Step 4: Separate the Deterministic and Random Components","lvl2":"Gumbel Noise on the Rewards"},"content":"Now we take the expectation of the augmented Bellman equation with respect to the current shocks only (everything that does not depend on the current \\boldsymbol{\\epsilon} can be pulled out).\n\nFirst, note that by the law of iterated expectations and independence of shocks across time:\\mathbb{E}_{\\boldsymbol{\\epsilon}'}\\big[\\tilde{v}(s', \\boldsymbol{\\epsilon}')\\big] = v(s')\n\nThis follows from our definition of v and the fact that the next shock is independent of everything else.\n\nNow define the deterministic part of the right-hand side:x_a(s) := r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j)\n\nThis is the expected return from taking action a in state s without the shock. Using this notation, the augmented Bellman equation becomes:\\tilde{v}(s, \\boldsymbol{\\epsilon}) = \\max_{a \\in \\mathcal{A}_s} \\left\\{ x_a(s) + \\epsilon(a) \\right\\}\n\nTaking the expectation over \\boldsymbol{\\epsilon} on both sides:v(s) = \\mathbb{E}_{\\boldsymbol{\\epsilon}}\\left[\\max_{a \\in \\mathcal{A}_s} \\left\\{ x_a(s) + \\epsilon(a) \\right\\}\\right]\n\nExpectation of a Max, Not Max of an Expectation\n\nNotice carefully: we have \\mathbb{E}[\\max(\\cdot)], not \\max \\mathbb{E}[\\cdot]. We are not swapping max and expectation.\n\nThe expression \\mathbb{E}_{\\boldsymbol{\\epsilon}}[\\max_a \\{x_a + \\epsilon(a)\\}] is the expected value of the maximum of Gumbel-perturbed utilities. The Gumbel random utility identity evaluates this quantity in closed form.","type":"content","url":"/smoothing#step-4-separate-the-deterministic-and-random-components","position":17},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Step 5: Apply the Gumbel Random Utility Identity","lvl2":"Gumbel Noise on the Rewards"},"type":"lvl3","url":"/smoothing#step-5-apply-the-gumbel-random-utility-identity","position":18},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Step 5: Apply the Gumbel Random Utility Identity","lvl2":"Gumbel Noise on the Rewards"},"content":"We now invoke a result from extreme value theory:\n\nGumbel Random Utility Identity\n\nLet \\epsilon_1, \\ldots, \\epsilon_m be i.i.d. \\mathrm{Gumbel}(\\mu_\\epsilon, 1/\\beta) random variables. For any deterministic values x_1, \\ldots, x_m \\in \\mathbb{R}:\\max_{i=1,\\ldots,m} \\{x_i + \\epsilon_i\\} \\overset{d}{=} \\frac{1}{\\beta} \\log \\sum_{i=1}^m \\exp(\\beta x_i) + \\zeta\n\nwhere \\zeta \\sim \\mathrm{Gumbel}(\\mu_\\epsilon, 1/\\beta) (same distribution as the original shocks).\n\nTaking expectations:\\mathbb{E}\\left[\\max_{i=1,\\ldots,m} \\{x_i + \\epsilon_i\\}\\right] = \\frac{1}{\\beta} \\log \\sum_{i=1}^m \\exp(\\beta x_i) + \\mu_\\epsilon + \\frac{\\gamma_E}{\\beta}\n\nwhere \\gamma_E \\approx 0.5772 is the Euler-Mascheroni constant.\n\nWith mean-zero shocks (\\mu_\\epsilon = -\\gamma_E/\\beta), the constant term vanishes:\\mathbb{E}\\left[\\max_{i=1,\\ldots,m} \\{x_i + \\epsilon_i\\}\\right] = \\frac{1}{\\beta} \\log \\sum_{i=1}^m \\exp(\\beta x_i)\n\nApplying this identity to our problem (with mean-zero shocks):v(s) = \\mathbb{E}_{\\boldsymbol{\\epsilon}}\\left[\\max_{a \\in \\mathcal{A}_s} \\{x_a(s) + \\epsilon(a)\\}\\right] = \\frac{1}{\\beta} \\log \\sum_{a \\in \\mathcal{A}_s} \\exp(\\beta x_a(s))\n\nSubstituting the definition of x_a(s):v(s) = \\frac{1}{\\beta} \\log \\sum_{a \\in \\mathcal{A}_s} \\exp\\left(\\beta\\left(r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j)\\right)\\right)\n\nWe have arrived at the smooth Bellman equation.","type":"content","url":"/smoothing#step-5-apply-the-gumbel-random-utility-identity","position":19},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Step 6: Summary of the Derivation","lvl2":"Gumbel Noise on the Rewards"},"type":"lvl3","url":"/smoothing#step-6-summary-of-the-derivation","position":20},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Step 6: Summary of the Derivation","lvl2":"Gumbel Noise on the Rewards"},"content":"To recap the logical flow:\n\nWe constructed an augmented MDP with state (s, \\boldsymbol{\\epsilon}) where shocks perturb rewards\n\nWe wrote the standard Bellman equation for this augmented MDP (hard max, but over an infinite-dimensional state space)\n\nWe defined the ex-ante value v(s) = \\mathbb{E}_{\\boldsymbol{\\epsilon}}[\\tilde{v}(s, \\boldsymbol{\\epsilon})] to eliminate the continuous shock component\n\nWe separated deterministic and random terms: \\tilde{v}(s, \\boldsymbol{\\epsilon}) = \\max_a \\{x_a(s) + \\epsilon(a)\\}\n\nWe applied the Gumbel identity to evaluate \\mathbb{E}_{\\boldsymbol{\\epsilon}}[\\max_a \\{\\cdots\\}] in closed form as a log-sum-exp\n\nThe augmented MDP with shocks exists only as a mathematical device. We never approximate \\tilde{v}, never discretize \\boldsymbol{\\epsilon}, and never enumerate the augmented state space. The only computational object we work with is v(s) on the original (finite) state space, which satisfies the smooth Bellman equation.","type":"content","url":"/smoothing#step-6-summary-of-the-derivation","position":21},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Deriving the Optimal Smooth Policy","lvl2":"Gumbel Noise on the Rewards"},"type":"lvl3","url":"/smoothing#deriving-the-optimal-smooth-policy","position":22},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Deriving the Optimal Smooth Policy","lvl2":"Gumbel Noise on the Rewards"},"content":"Now that we have derived the smooth value function, we can also obtain the corresponding optimal policy. The question is: what policy should we follow in the original MDP (without explicitly conditioning on shocks)?\n\nIn the augmented MDP, the optimal policy is deterministic but depends on the shock realization:\\pi(s, \\boldsymbol{\\epsilon}) \\in \\operatorname{argmax}_{a \\in \\mathcal{A}_s} \\left\\{ r(s,a) + \\epsilon(a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j) \\right\\}\n\nHowever, we want a policy for the original state space s (not the augmented state). We obtain this by marginalizing over the current shocks, essentially asking: “what is the probability that action a is optimal when we average over all possible shock realizations?”\n\nDefine an indicator function:I_a(\\boldsymbol{\\epsilon}) = \\begin{cases} \n   1 & \\text{if } a \\in \\operatorname{argmax}_{a' \\in \\mathcal{A}_s} \\left\\{ x_{a'}(s) + \\epsilon(a') \\right\\} \\\\\n   0 & \\text{otherwise}\n   \\end{cases}\n\nwhere x_a(s) = r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j) as before.\n\nThe ex-ante probability that action a is optimal at state s is:\\pi(a|s) = \\mathbb{E}_{\\boldsymbol{\\epsilon}}[I_a(\\boldsymbol{\\epsilon})] = \\mathbb{P}_{\\boldsymbol{\\epsilon}}\\left(a \\in \\operatorname{argmax}_{a'} \\left\\{ x_{a'}(s) + \\epsilon(a') \\right\\}\\right)\n\nThis is the probability that action a achieves the maximum when utilities are perturbed by Gumbel noise.\n\nGumbel-Max Probability (Softmax)\n\nLet \\epsilon_1, \\ldots, \\epsilon_m be i.i.d. \\mathrm{Gumbel}(\\mu_\\epsilon, 1/\\beta) random variables. For any deterministic values x_1, \\ldots, x_m \\in \\mathbb{R}, the probability that index i achieves the maximum is:\\mathbb{P}\\left(i \\in \\operatorname{argmax}_j \\{x_j + \\epsilon_j\\}\\right) = \\frac{\\exp(\\beta x_i)}{\\sum_{j=1}^m \\exp(\\beta x_j)}\n\nThis holds regardless of the location parameter \\mu_\\epsilon.\n\nApplying this result to our problem:\\pi(a|s) = \\frac{\\exp\\left(\\beta x_a(s)\\right)}{\\sum_{a' \\in \\mathcal{A}_s} \\exp\\left(\\beta x_{a'}(s)\\right)} = \\frac{\\exp\\left(\\beta\\left(r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a)v(j)\\right)\\right)}{\\sum_{a' \\in \\mathcal{A}_s} \\exp\\left(\\beta\\left(r(s,a') + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a')v(j)\\right)\\right)}\n\nThis is the softmax policy or Gibbs/Boltzmann policy with inverse temperature \\beta.\n\nProperties:\n\nAs \\beta \\to \\infty: the policy becomes deterministic, concentrating on the action(s) with highest x_a(s) (recovers standard greedy policy)\n\nAs \\beta \\to 0: the policy becomes uniform over all actions (maximum entropy)\n\nFor finite \\beta > 0: the policy is stochastic, with probability mass proportional to exponentiated Q-values\n\nThis completes the derivation: the smooth Bellman equation yields a value function v(s), and the corresponding optimal policy is the softmax over Q-values. ## Control as Inference Perspective\n\nThe smooth Bellman optimality equations can also be derived from probabilistic inference perspective. To see this, let's go back to the idea from the previous section in which we introduced an indicator function $I_a(\\epsilon)$ to represent whether an action $a$ is optimal given a particular realization of the noise $\\epsilon$:\n\n$$ I_a(\\epsilon) = \\begin{cases} \n   1 & \\text{if } a \\in \\operatorname{argmax}_{a' \\in \\mathcal{A}_s} \\left\\{ r(s,a') + \\epsilon(a') + \\gamma \\mathbb{E}_{s', \\epsilon'}\\left[v_\\gamma^\\star(s',\\epsilon')\\mid s, a'\\right] \\right\\} \\\\\n   0 & \\text{otherwise}\n   \\end{cases} $$\n\nWhen we took the expectation over the noise $\\epsilon$, we obtained a soft version of this indicator:\n\n$$ \\begin{align*}\n\\mathbb{E}_\\epsilon[I_a(\\epsilon)] &= \\mathbb{P}\\left(a \\in \\operatorname{argmax}_{a' \\in \\mathcal{A}_s} \\left\\{ r(s,a') + \\epsilon(a') + \\gamma \\mathbb{E}_{s', \\epsilon'}\\left[v_\\gamma^\\star(s',\\epsilon')\\mid s, \\epsilon, a'\\right] \\right\\}\\right) \\\\\n&= \\frac{\\exp\\left(\\beta\\left(r(s,a) + \\gamma \\mathbb{E}_{s'}\\left[v_\\gamma^\\star(s')\\mid s, a\\right]\\right)\\right)}{\\sum_{a' \\in \\mathcal{A}_s} \\exp\\left(\\beta\\left(r(s,a') + \\gamma \\mathbb{E}_{s'}\\left[v_\\gamma^\\star(s')\\mid s, a'\\right]\\right)\\right)}\n\\end{align*} $$\n\nGiven this indicator function, we can \"infer\" the optimal action in any state. This is the intuition and starting point behind the control as inference perspective in which we directly define a continuous-valued \"optimality\" variable $O_t$ at each time step $t$. We define the probability of optimality given a state-action pair as:\n\n$$ p(O_t = 1 | s_t, a_t) = \\exp(\\beta r(s_t, a_t)) $$\n\nBuilding on this notion of soft optimality, we can formulate the MDP as a probabilistic graphical model. We define the following probabilities:\n\n1. State transition probability: $p(s_{t+1} | s_t, a_t)$ (given by the MDP dynamics)\n2. Prior policy: $p(a_t | s_t)$ (which we'll assume to be uniform for simplicity)\n3. Optimality probability: $p(O_t = 1 | s_t, a_t) = \\exp(\\beta r(s_t, a_t))$\n\nThis formulation encodes the idea that more rewarding state-action pairs are more likely to be \"optimal,\" which directly parallels the soft assignment of optimality we obtained by taking the expectation over the Gumbel noise.\n\nThe control problem can now be framed as an inference problem: we want to find the posterior distribution over actions given that all time steps are optimal:\n\n$$ p(a_t | s_t, O_{1:T} = 1) $$\n\nwhere $O_{1:T} = 1$ means $O_t = 1$ for all $t$ from 1 to T. \n\n### Message Passing \n\nTo solve this inference problem, we can use a technique from probabilistic graphical models called message passing, specifically the belief propagation algorithm. Message passing is a way to efficiently compute marginal distributions in a graphical model by passing local messages between nodes. Messages are passed between nodes in both forward and backward directions. Each message represents a belief about the distribution of a variable, based on the information available to the sending node. After messages have been passed, each node updates its belief about its associated variable by combining all incoming messages.\n\nIn our specific case, we're particularly interested in the backward messages, which propagate information about future optimality backwards in time. Let's define the backward message $\\beta_t(s_t)$ as:\n\n$$ \\beta_t(s_t) = p(O_{t:T} = 1 | s_t) $$\n\nThis represents the probability of optimality for all future time steps given the current state. We can compute this recursively:\n\n$$ \\beta_t(s_t) = \\sum_{a_t} p(a_t | s_t) p(O_t = 1 | s_t, a_t) \\sum_{s_{t+1}} p(s_{t+1} | s_t, a_t) \\beta_{t+1}(s_{t+1}) $$\n\n\nTaking the log and assuming a uniform prior over actions, we get:\n\n$$ \\log \\beta_t(s_t) = \\log \\sum_{a_t} \\sum_{s_{t+1}} p(s_{t+1} | s_t, a_t) \\exp(\\beta (r(s_t, a_t) + \\gamma v(_{t+1}) + \\frac{1}{\\beta} \\log \\beta_{t+1}(s_{t+1}))) $$\n\nIf we define the soft value function as $V_t(s_t) = \\frac{1}{\\beta} \\log \\beta_t(s_t)$, we can rewrite the above equation as:\n\n$$ V_t(s_t) = \\frac{1}{\\beta} \\log \\sum_{a_t} \\sum_{s_{t+1}} p(s_{t+1} | s_t, a_t) \\exp(\\beta (r(s_t, a_t) + \\gamma V_{t+1}(s_{t+1}))) $$\n\nThis is exactly the smooth Bellman equation we derived earlier, but now interpreted as the result of probabilistic inference in a graphical model.\n\n### Deriving the Optimal Policy\n\nThe backward message recursion we derived earlier assumes a uniform prior policy $p(a_t | s_t)$. However, our goal is to find an optimal policy. We can extract this optimal policy efficiently by computing the posterior distribution over actions given our backward messages.\n\nStarting from the definition of conditional probability and applying Bayes' rule, we can write:\n\n$$ \\begin{align}\np(a_t | s_t, O_{1:T} = 1) &= \\frac{p(O_{1:T} = 1 | s_t, a_t) p(a_t | s_t)}{p(O_{1:T} = 1 | s_t)} \\\\\n&\\propto p(a_t | s_t) p(O_t = 1 | s_t, a_t) p(O_{t+1:T} = 1 | s_t, a_t) \\\\\n&= p(a_t | s_t) p(O_t = 1 | s_t, a_t) \\sum_{s_{t+1}} p(s_{t+1} | s_t, a_t) \\beta_{t+1}(s_{t+1})\n\\end{align} $$\n\nHere, $\\beta_{t+1}(s_{t+1}) = p(O_{t+1:T} = 1 | s_{t+1})$ is our backward message.\n\nNow, let's substitute our definitions for the optimality probability and the soft value function:\n\n$$ \\begin{align}\np(a_t | s_t, O_{1:T} = 1) &\\propto p(a_t | s_t) \\exp(\\beta r(s_t, a_t)) \\sum_{s_{t+1}} p(s_{t+1} | s_t, a_t) \\exp(\\beta \\gamma V_{t+1}(s_{t+1})) \\\\\n&= p(a_t | s_t) \\exp(\\beta (r(s_t, a_t) + \\gamma \\sum_{s_{t+1}} p(s_{t+1} | s_t, a_t) V_{t+1}(s_{t+1})))\n\\end{align} $$\n\nAfter normalization, and assuming a uniform prior $p(a_t | s_t)$, we obtain the randomized decision rule:\n\n$$ d(a_t | s_t) = \\frac{\\exp(\\beta (r(s_t, a_t) + \\gamma \\sum_{s_{t+1}} p(s_{t+1} | s_t, a_t) V_{t+1}(s_{t+1})))}{\\sum_{a'_t} \\exp(\\beta (r(s_t, a'_t) + \\gamma \\sum_{s_{t+1}} p(s_{t+1} | s_t, a'_t) V_{t+1}(s_{t+1})))} $$ ","type":"content","url":"/smoothing#deriving-the-optimal-smooth-policy","position":23},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl2":"Regularized Markov Decision Processes"},"type":"lvl2","url":"/smoothing#regularized-markov-decision-processes","position":24},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl2":"Regularized Markov Decision Processes"},"content":"Regularized MDPs \n\nGeist et al. (2019) provide another perspective on how the smooth Bellman equations come to be. This framework offers a more general approach in which we seek to find optimal policies under the infinite horizon criterion while also accounting for a regularizer that influences the kind of policies we try to obtain.\n\nLet’s set up some necessary notation. First, recall that the policy evaluation operator for a stationary policy with decision rule \\pi is defined as:\\BellmanPi \\mathbf{v} = \\mathbf{r}_\\pi + \\gamma \\mathbf{P}_\\pi \\mathbf{v}\n\nwhere \\mathbf{r}_\\pi is the expected reward vector under policy \\pi, \\gamma is the discount factor, and \\mathbf{P}_\\pi is the state transition probability matrix under \\pi. A complementary object to the value function is the q-function (or Q-factor) representation:\\begin{align*}\nq_\\gamma^{\\pi}(s, a) &= r(s, a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v_\\gamma^{\\pi}(j) \\\\\nv_\\gamma^{\\pi}(s) &= \\sum_{a \\in \\mathcal{A}_s} \\pi(a | s) q_\\gamma^{\\pi}(s, a) \n\\end{align*}\n\nThe policy evaluation operator can then be written in terms of the q-function as:[\\BellmanPi v](s) = \\langle \\pi(\\cdot | s), q(s, \\cdot) \\rangle","type":"content","url":"/smoothing#regularized-markov-decision-processes","position":25},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Legendre-Fenchel Transform","lvl2":"Regularized Markov Decision Processes"},"type":"lvl3","url":"/smoothing#legendre-fenchel-transform","position":26},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Legendre-Fenchel Transform","lvl2":"Regularized Markov Decision Processes"},"content":"The workhorse behind the theory of regularized MDPs is the Legendre-Fenchel transform, also known as the convex conjugate. For a strongly convex function \\Omega: \\Delta_{\\mathcal{A}} \\rightarrow \\mathbb{R}, its Legendre-Fenchel transform \\Omega^*: \\mathbb{R}^{\\mathcal{A}} \\rightarrow \\mathbb{R} is defined as:\\Omega^*(q(s, \\cdot)) = \\max_{\\pi(\\cdot|s) \\in \\Delta_{\\mathcal{A}}} \\langle \\pi(\\cdot | s), q(s, \\cdot) \\rangle - \\Omega(\\pi(\\cdot | s))\n\nAn important property of this transform is that it has a unique maximizing argument, given by the gradient of \\Omega^*. This gradient is Lipschitz and satisfies:\\nabla \\Omega^*(q(s, \\cdot)) = \\arg\\max_\\pi \\langle \\pi(\\cdot | s), q(s, \\cdot) \\rangle - \\Omega(\\pi(\\cdot | s))\n\nAn important example of a regularizer is the negative entropy, which gives rise to the smooth Bellman equations as we are about to see.","type":"content","url":"/smoothing#legendre-fenchel-transform","position":27},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl2":"Regularized Bellman Operators"},"type":"lvl2","url":"/smoothing#regularized-bellman-operators","position":28},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl2":"Regularized Bellman Operators"},"content":"With these concepts in place, we can now define the regularized Bellman operators:\n\nRegularized Policy Evaluation Operator (\\mathrm{L}_{\\pi,\\Omega}):[\\mathrm{L}_{\\pi,\\Omega} v](s) = \\langle q(s,\\cdot), \\pi(\\cdot | s) \\rangle - \\Omega(\\pi(\\cdot | s))\n\nRegularized Bellman Optimality Operator (\\mathrm{L}_\\Omega):[\\mathrm{L}_\\Omega v](s) = [\\max_\\pi \\mathrm{L}_{\\pi,\\Omega} v ](s) = \\Omega^*(q(s, \\cdot))\n\nIt can be shown that the addition of a regularizer in these regularized operators still preserves the contraction properties, and therefore the existence of a solution to the optimality equations and the convergence of successive approximation.\n\nThe regularized value function of a stationary policy with decision rule \\pi, denoted by v_{\\pi,\\Omega}, is the unique fixed point of the operator equation:\\text{find $v$ such that } \\enspace v = \\mathrm{L}_{\\pi,\\Omega} v\n\nUnder the usual assumptions on the discount factor and the boundedness of the reward, the value of a policy can also be found in closed form by solving for \\mathbf{v} in the linear system of equations:(\\mathbf{I} - \\gamma \\mathbf{P}_\\pi) \\mathbf{v} =  \\mathbf{r}_\\pi - \\boldsymbol{\\Omega}_\\pi\n\nwhere [\\boldsymbol{\\Omega}_\\pi](s) = \\Omega(\\pi(\\cdot|s)) is the vector of regularization terms at each state.\n\nThe associated state-action value function q_{\\pi,\\Omega} is given by:\\begin{align*}\nq_{\\pi,\\Omega}(s, a) &= r(s, a) + \\sum_{j \\in \\mathcal{S}} \\gamma p(j|s,a) v_{\\pi,\\Omega}(j) \\\\\nv_{\\pi,\\Omega}(s) &= \\sum_{a \\in \\mathcal{A}_s} \\pi(a | s) q_{\\pi,\\Omega}(s, a) - \\Omega(\\pi(\\cdot | s))\n\\end{align*}\n\nThe regularized optimal value function v^*_\\Omega is then the unique fixed point of \\mathrm{L}_\\Omega in the fixed point equation:\\text{find $v$ such that } v = \\mathrm{L}_\\Omega v\n\nThe associated state-action value function q^*_\\Omega is given by:\\begin{align*}\nq^*_\\Omega(s, a) &= r(s, a) + \\sum_{j \\in \\mathcal{S}} \\gamma p(j|s,a) v^*_\\Omega(j) \\\\\nv^*_\\Omega(s) &= \\Omega^*(q^*_\\Omega(s, \\cdot))\\end{align*}\n\nAn important result in the theory of regularized MDPs is that there exists a unique optimal regularized policy. Specifically, if \\pi^*_\\Omega is a conserving decision rule (i.e., \\pi^*_\\Omega = \\arg\\max_\\pi \\mathrm{L}_{\\pi,\\Omega} v^*_\\Omega), then the randomized stationary policy \\boldsymbol{\\pi} = \\mathrm{const}(\\pi^*_\\Omega) is the unique optimal regularized policy.\n\nIn practice, once we have found v^*_\\Omega, we can derive the optimal decision rule by taking the gradient of the convex conjugate evaluated at the optimal action-value function:\\pi^*(\\cdot | s) = \\nabla \\Omega^*(q^*_\\Omega(s, \\cdot))","type":"content","url":"/smoothing#regularized-bellman-operators","position":29},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Recovering the Smooth Bellman Equations","lvl2":"Regularized Bellman Operators"},"type":"lvl3","url":"/smoothing#recovering-the-smooth-bellman-equations","position":30},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Recovering the Smooth Bellman Equations","lvl2":"Regularized Bellman Operators"},"content":"Under this framework, we can recover the smooth Bellman equations by choosing \\Omega to be the negative entropy, and obtain the softmax policy as the gradient of the convex conjugate. Let’s show this explicitly:\n\nUsing the negative entropy regularizer:\\Omega(d(\\cdot|s)) = \\sum_{a \\in \\mathcal{A}_s} d(a|s) \\ln d(a|s)\n\nThe convex conjugate:\\Omega^*(q(s, \\cdot)) = \\ln \\sum_{a \\in \\mathcal{A}_s} \\exp q(s,a)\n\nNow, let’s write out the regularized Bellman optimality equation:v^*_\\Omega(s) = \\Omega^*(q^*_\\Omega(s, \\cdot))\n\nSubstituting the expressions for \\Omega^* and q^*_\\Omega:v^*_\\Omega(s) = \\ln \\sum_{a \\in \\mathcal{A}_s} \\exp \\left(r(s, a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v^*_\\Omega(j)\\right)\n\nThis matches the form of the smooth Bellman equation we derived earlier, with the log-sum-exp operation replacing the max operation of the standard Bellman equation.\n\nFurthermore, the optimal policy is given by the gradient of \\Omega^*:d^*(a|s) = \\nabla \\Omega^*(q^*_\\Omega(s, \\cdot)) = \\frac{\\exp(q^*_\\Omega(s,a))}{\\sum_{a' \\in \\mathcal{A}_s} \\exp(q^*_\\Omega(s,a'))}\n\nThis is the familiar softmax policy we encountered in the smooth MDP setting.","type":"content","url":"/smoothing#recovering-the-smooth-bellman-equations","position":31},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Smooth Policy Iteration Algorithm","lvl2":"Regularized Bellman Operators"},"type":"lvl3","url":"/smoothing#smooth-policy-iteration-algorithm","position":32},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Smooth Policy Iteration Algorithm","lvl2":"Regularized Bellman Operators"},"content":"Now that we’ve seen how the regularized MDP framework leads to smooth Bellman equations, we present smooth policy iteration. Unlike value iteration which directly iterates the Bellman operator, policy iteration alternates between policy evaluation and policy improvement steps.\n\nSmooth Policy Evaluation\n\nInput: MDP (S, A, r, p, \\gamma), policy \\pi, inverse temperature \\beta > 0, tolerance \\epsilon > 0\n\nOutput: Value function v^\\pi for policy \\pi\n\nInitialize v(s) \\leftarrow 0 for all s \\in S\n\nSet \\alpha \\leftarrow 1/\\beta\n\nrepeat\n\n\\quad \\Delta \\leftarrow 0\n\n\\quad for each state s \\in S do\n\n\\quad\\quad v_{\\text{old}} \\leftarrow v(s)\n\n\\quad\\quad for each action a \\in A_s do\n\n\\quad\\quad\\quad q(s,a) \\leftarrow r(s,a) + \\gamma \\sum_{j \\in S} p(j|s,a) v(j)\n\n\\quad\\quad end for\n\n\\quad\\quad Compute expected Q-value: \\bar{q} \\leftarrow \\sum_{a \\in A_s} \\pi(a|s) \\cdot q(s,a)\n\n\\quad\\quad Compute policy entropy: H \\leftarrow -\\sum_{a \\in A_s} \\pi(a|s) \\log \\pi(a|s)\n\n\\quad\\quad v(s) \\leftarrow \\bar{q} + \\alpha H\n\n\\quad\\quad \\Delta \\leftarrow \\max(\\Delta, |v(s) - v_{\\text{old}}|)\n\n\\quad end for\n\nuntil \\Delta < \\epsilon\n\nreturn v\n\nSmooth Policy Iteration\n\nInput: MDP (S, A, r, p, \\gamma), inverse temperature \\beta > 0, tolerance \\epsilon > 0\n\nOutput: Approximate optimal value function v and stochastic policy \\pi\n\nInitialize \\pi(a|s) \\leftarrow 1/|A_s| for all s \\in S, a \\in A_s (uniform policy)\n\nrepeat\n\n\\quad Policy Evaluation:\n\n\\quad\\quad v \\leftarrow SmoothPolicyEvaluation(S, A, r, p, \\gamma, \\pi, \\beta, \\epsilon)\n\n\\quad Policy Improvement:\n\n\\quad policy_stable \\leftarrow true\n\n\\quad for each state s \\in S do\n\n\\quad\\quad \\pi_{\\text{old}}(\\cdot|s) \\leftarrow \\pi(\\cdot|s)\n\n\\quad\\quad for each action a \\in A_s do\n\n\\quad\\quad\\quad q(s,a) \\leftarrow r(s,a) + \\gamma \\sum_{j \\in S} p(j|s,a) v(j)\n\n\\quad\\quad end for\n\n\\quad\\quad for each action a \\in A_s do\n\n\\quad\\quad\\quad \\pi(a|s) \\leftarrow \\frac{\\exp(\\beta \\cdot q(s,a))}{\\sum_{a' \\in A_s} \\exp(\\beta \\cdot q(s,a'))}\n\n\\quad\\quad end for\n\n\\quad\\quad if \\|\\pi(\\cdot|s) - \\pi_{\\text{old}}(\\cdot|s)\\| > \\epsilon then\n\n\\quad\\quad\\quad policy_stable \\leftarrow false\n\n\\quad\\quad end if\n\n\\quad end for\n\nuntil policy_stable\n\nreturn v, \\pi\n\nKey properties of smooth policy iteration:\n\nEntropy-regularized evaluation: The policy evaluation step (line 12 of Algorithm \n\nAlgorithm 2) accounts for the entropy bonus \\alpha H(\\pi(\\cdot|s)) where \\alpha = 1/\\beta\n\nStochastic policy improvement: The policy improvement step (lines 12-14 of Algorithm \n\nAlgorithm 3) uses softmax instead of deterministic argmax, producing a stochastic policy\n\nTemperature parameter:\n\nHigher \\beta → policies closer to deterministic (lower entropy)\n\nLower \\beta → more stochastic policies (higher entropy)\n\nAs \\beta \\to \\infty → recovers standard policy iteration\n\nConvergence: Like standard policy iteration, this algorithm converges to the unique optimal regularized value function and policy","type":"content","url":"/smoothing#smooth-policy-iteration-algorithm","position":33},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Equivalence Between Smooth Bellman Equations and Entropy-Regularized MDPs","lvl2":"Regularized Bellman Operators"},"type":"lvl3","url":"/smoothing#equivalence-between-smooth-bellman-equations-and-entropy-regularized-mdps","position":34},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Equivalence Between Smooth Bellman Equations and Entropy-Regularized MDPs","lvl2":"Regularized Bellman Operators"},"content":"We have now seen two distinct ways to arrive at smooth Bellman equations. Earlier in this chapter, we introduced the logsumexp operator as a smooth approximation to the max operator, motivated by analytical tractability and the desire for differentiability. Just now, we derived the same equations through the lens of regularized MDPs, where we explicitly penalize the entropy of policies. These two perspectives are mathematically equivalent: solving the smooth Bellman equation with inverse temperature parameter \\beta yields exactly the same optimal value function and optimal policy as solving the entropy-regularized MDP with regularization strength \\alpha = 1/\\beta. The two formulations are not merely similar. They describe identical optimization problems.\n\nTo see this equivalence clearly, consider the standard MDP problem with rewards r(s,a) and transition probabilities p(j|s,a). The regularized MDP framework tells us to solve:\\max_\\pi \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t r(s_t, a_t) \\right] + \\alpha \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t H(\\pi(\\cdot|s_t)) \\right],\n\nwhere H(\\pi(\\cdot|s)) = -\\sum_a \\pi(a|s) \\ln \\pi(a|s) is the entropy of the policy at state s, and \\alpha > 0 is the entropy regularization strength.\n\nWe can rewrite this objective by absorbing the entropy term into a modified reward function. Define the entropy-augmented reward:\\tilde{r}(s,a,\\pi) = r(s,a) + \\alpha H(\\pi(\\cdot|s)).\n\nHowever, this formulation makes the reward depend on the entire policy at each state, which is awkward. We can reformulate this more cleanly by expanding the entropy term. Recall that the entropy is:H(\\pi(\\cdot|s)) = -\\sum_a \\pi(a|s) \\ln \\pi(a|s).\n\nWhen we take the expectation over actions drawn from \\pi, we have:\\mathbb{E}_{a \\sim \\pi(\\cdot|s)} [H(\\pi(\\cdot|s))] = \\sum_a \\pi(a|s) \\left[-\\sum_{a'} \\pi(a'|s) \\ln \\pi(a'|s)\\right] = -\\sum_{a'} \\pi(a'|s) \\ln \\pi(a'|s),\n\nsince the entropy doesn’t depend on which action is actually sampled. But we can also write this as:H(\\pi(\\cdot|s)) = -\\sum_a \\pi(a|s) \\ln \\pi(a|s) = \\mathbb{E}_{a \\sim \\pi(\\cdot|s)}[-\\ln \\pi(a|s)].\n\nThis shows that adding \\alpha H(\\pi(\\cdot|s)) to the expected reward at state s is equivalent to adding -\\alpha \\ln \\pi(a|s) to the reward of taking action a at state s. More formally:\\begin{align*}\n&\\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t r(s_t, a_t) \\right] + \\alpha \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t H(\\pi(\\cdot|s_t)) \\right] \\\\\n&= \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t r(s_t, a_t) \\right] + \\alpha \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t \\mathbb{E}_{a_t \\sim \\pi(\\cdot|s_t)}[-\\ln \\pi(a_t|s_t)] \\right] \\\\\n&= \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t \\left( r(s_t, a_t) - \\alpha \\ln \\pi(a_t|s_t) \\right) \\right].\n\\end{align*}\n\nThe entropy bonus at each state, when averaged over the policy, becomes a per-action penalty proportional to the negative log probability of the action taken. This reformulation is more useful because the modified reward now depends only on the state, the action taken, and the probability assigned to that specific action by the policy, not on the entire distribution over actions.\n\nThis expression shows that entropy regularization is equivalent to adding a state-action dependent penalty term -\\alpha \\ln \\pi(a|s) to the reward. Intuititively, this terms amounts to paying a cost for low-entropy (deterministic) policies.\n\nNow, when we write down the Bellman equation for this entropy-regularized problem, at each state s we need to find the decision rule d(\\cdot|s) \\in \\Delta(\\mathcal{A}_s) (a probability distribution over actions) that maximizes:v(s) = \\max_{d(\\cdot|s) \\in \\Delta(\\mathcal{A}_s)} \\sum_a d(a|s) \\left[ r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j) - \\alpha \\ln d(a|s) \\right].\n\nHere \\Delta(\\mathcal{A}_s) = \\{d(\\cdot|s) : d(a|s) \\geq 0, \\sum_a d(a|s) = 1\\} denotes the probability simplex over actions available at state s. The optimization is over randomized decision rules at each state, constrained to be valid probability distributions.\n\nThis is a convex optimization problem with a linear constraint. We form the Lagrangian:\\mathcal{L}(d, \\lambda) = \\sum_a d(a|s) \\left[ r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j) - \\alpha \\ln d(a|s) \\right] - \\lambda \\left(\\sum_a d(a|s) - 1\\right),\n\nwhere \\lambda is the Lagrange multiplier enforcing the normalization constraint. Taking the derivative with respect to d(a|s) and setting it to zero:r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j) - \\alpha(1 + \\ln d^*(a|s)) - \\lambda = 0.\n\nSolving for d^*(a|s):d^*(a|s) = \\exp\\left(\\frac{1}{\\alpha}\\left(r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j) - \\lambda\\right)\\right).\n\nUsing the normalization constraint \\sum_a d^*(a|s) = 1 to solve for \\lambda:\\sum_a \\exp\\left(\\frac{1}{\\alpha}\\left(r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j)\\right)\\right) = \\exp\\left(\\frac{\\lambda}{\\alpha}\\right).\n\nTherefore:\\lambda = \\alpha \\ln \\sum_a \\exp\\left(\\frac{1}{\\alpha}\\left(r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j)\\right)\\right).\n\nSubstituting this back into the Bellman equation and simplifying:v(s) = \\alpha \\ln \\sum_a \\exp\\left(\\frac{1}{\\alpha}\\left(r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j)\\right)\\right).\n\nSetting \\beta = 1/\\alpha (the inverse temperature), this becomes:v(s) = \\frac{1}{\\beta} \\ln \\sum_a \\exp\\left(\\beta\\left(r(s,a) + \\gamma \\sum_{j \\in \\mathcal{S}} p(j|s,a) v(j)\\right)\\right).\n\nWe recover the smooth Bellman equation we derived earlier using the logsumexp operator. The inverse temperature parameter \\beta controls how closely the logsumexp approximates the max: as \\beta \\to \\infty, we recover the standard Bellman equation, while for finite \\beta, we have a smooth approximation that corresponds to optimizing with entropy regularization strength \\alpha = 1/\\beta.\n\nThe optimal policy is:\\pi^*(a|s) = \\frac{\\exp\\left(\\beta q^*(s,a)\\right)}{\\sum_{a'} \\exp\\left(\\beta q^*(s,a')\\right)} = \\text{softmax}_\\beta(q^*(s,\\cdot))(a),\n\nwhich is exactly the softmax policy parametrized by inverse temperature.\n\nThe derivation establishes the complete equivalence: the value function v^* that solves the smooth Bellman equation is identical to the optimal value function v^*_\\Omega of the entropy-regularized MDP (with \\Omega being negative entropy and \\alpha = 1/\\beta), and the softmax policy that is greedy with respect to this value function achieves the maximum of the entropy-regularized objective. Both approaches yield the same numerical solution: the same values at every state and the same policy prescriptions. The only difference is how we conceptualize the problem: as smoothing the Bellman operator for computational tractability, or as explicitly trading off reward maximization against policy entropy.\n\nThis equivalence has important implications. When we use smooth Bellman equations with a logsumexp operator, we are implicitly solving an entropy-regularized MDP. Conversely, when we explicitly add entropy regularization to an MDP objective, we arrive at smooth Bellman equations as the natural description of optimality. This dual perspective will prove valuable in understanding various algorithms and theoretical results. For instance, in soft actor-critic methods and other maximum entropy reinforcement learning algorithms, the connection between smooth operators and entropy regularization provides both computational benefits (differentiability) and conceptual clarity (why we want stochastic policies).","type":"content","url":"/smoothing#equivalence-between-smooth-bellman-equations-and-entropy-regularized-mdps","position":35},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Entropy-Regularized Dynamic Programming Algorithms","lvl2":"Regularized Bellman Operators"},"type":"lvl3","url":"/smoothing#entropy-regularized-dynamic-programming-algorithms","position":36},{"hierarchy":{"lvl1":"Smooth Bellman Optimality Equations","lvl3":"Entropy-Regularized Dynamic Programming Algorithms","lvl2":"Regularized Bellman Operators"},"content":"While the smooth Bellman equations (using logsumexp) and entropy-regularized formulations are mathematically equivalent, it is instructive to present the algorithms explicitly in the entropy-regularized form, where the entropy bonus appears directly in the update equations.\n\nEntropy-Regularized Value Iteration\n\nInput: MDP (S, A, r, p, \\gamma), entropy weight \\alpha > 0, tolerance \\epsilon > 0\n\nOutput: Approximate optimal value function v and stochastic policy \\pi\n\nInitialize \\pi(a|s) \\leftarrow 1/|A_s| for all s \\in S, a \\in A_s (uniform policy)\n\nInitialize v(s) \\leftarrow 0 for all s \\in S\n\nrepeat\n\n\\quad \\Delta \\leftarrow 0\n\n\\quad for each state s \\in S do\n\n\\quad\\quad Policy Improvement: Update policy for current value estimate\n\n\\quad\\quad for each action a \\in A_s do\n\n\\quad\\quad\\quad q(s,a) \\leftarrow r(s,a) + \\gamma \\sum_{j \\in S} p(j|s,a) v(j)\n\n\\quad\\quad end for\n\n\\quad\\quad for each action a \\in A_s do\n\n\\quad\\quad\\quad \\pi_{\\text{new}}(a|s) \\leftarrow \\frac{\\exp(q(s,a)/\\alpha)}{\\sum_{a' \\in A_s} \\exp(q(s,a')/\\alpha)}\n\n\\quad\\quad end for\n\n\\quad\\quad Value Update: Compute regularized value\n\n\\quad\\quad v_{\\text{new}}(s) \\leftarrow \\sum_{a \\in A_s} \\pi_{\\text{new}}(a|s) \\cdot q(s,a) + \\alpha H(\\pi_{\\text{new}}(\\cdot|s))\n\n\\quad\\quad where H(\\pi_{\\text{new}}(\\cdot|s)) = -\\sum_{a \\in A_s} \\pi_{\\text{new}}(a|s) \\log \\pi_{\\text{new}}(a|s)\n\n\\quad\\quad \\Delta \\leftarrow \\max(\\Delta, |v_{\\text{new}}(s) - v(s)|)\n\n\\quad\\quad v(s) \\leftarrow v_{\\text{new}}(s)\n\n\\quad\\quad \\pi(\\cdot|s) \\leftarrow \\pi_{\\text{new}}(\\cdot|s)\n\n\\quad end for\n\nuntil \\Delta < \\epsilon\n\nreturn v, \\pi\n\nFeatures:\n\nLine 11 updates the policy using the softmax of Q-values, with temperature \\alpha\n\nLine 14 explicitly computes the entropy-regularized value: expected Q-value plus entropy bonus\n\nThe algorithm maintains and updates a stochastic policy throughout\n\nAs \\alpha \\to 0 (or equivalently \\beta \\to \\infty), this recovers standard value iteration\n\nEntropy-Regularized Policy Iteration\n\nInput: MDP (S, A, r, p, \\gamma), entropy weight \\alpha > 0, tolerance \\epsilon > 0\n\nOutput: Approximate optimal value function v and stochastic policy \\pi\n\nInitialize \\pi(a|s) \\leftarrow 1/|A_s| for all s \\in S, a \\in A_s (uniform policy)\n\nrepeat\n\n\\quad Policy Evaluation: Solve for v^\\pi such that for all s \\in S:\n\n\\quad\\quad Option 1 (Iterative):\n\n\\quad\\quad Initialize v(s) \\leftarrow 0 for all s \\in S\n\n\\quad\\quad repeat\n\n\\quad\\quad\\quad for each state s \\in S do\n\n\\quad\\quad\\quad\\quad Compute q^\\pi(s,a) \\leftarrow r(s,a) + \\gamma \\sum_{j \\in S} p(j|s,a) v(j) for all a \\in A_s\n\n\\quad\\quad\\quad\\quad v_{\\text{new}}(s) \\leftarrow \\sum_{a \\in A_s} \\pi(a|s) \\cdot q^\\pi(s,a) + \\alpha H(\\pi(\\cdot|s))\n\n\\quad\\quad\\quad end for\n\n\\quad\\quad\\quad if \\max_s |v_{\\text{new}}(s) - v(s)| < \\epsilon then break\n\n\\quad\\quad\\quad v \\leftarrow v_{\\text{new}}\n\n\\quad\\quad until convergence\n\n\\quad\\quad Option 2 (Direct): Solve linear system (\\mathbf{I} - \\gamma \\mathbf{P}_\\pi) \\mathbf{v} = \\mathbf{r}_\\pi + \\alpha \\mathbf{H}_\\pi\n\n\\quad\\quad where [\\mathbf{r}_\\pi](s) = \\sum_a \\pi(a|s) r(s,a) and [\\mathbf{H}_\\pi](s) = H(\\pi(\\cdot|s))\n\n\\quad Policy Improvement:\n\n\\quad policy_changed \\leftarrow false\n\n\\quad for each state s \\in S do\n\n\\quad\\quad \\pi_{\\text{old}}(\\cdot|s) \\leftarrow \\pi(\\cdot|s)\n\n\\quad\\quad for each action a \\in A_s do\n\n\\quad\\quad\\quad q(s,a) \\leftarrow r(s,a) + \\gamma \\sum_{j \\in S} p(j|s,a) v(j)\n\n\\quad\\quad end for\n\n\\quad\\quad for each action a \\in A_s do\n\n\\quad\\quad\\quad \\pi(a|s) \\leftarrow \\frac{\\exp(q(s,a)/\\alpha)}{\\sum_{a' \\in A_s} \\exp(q(s,a')/\\alpha)}\n\n\\quad\\quad end for\n\n\\quad\\quad if \\|\\pi(\\cdot|s) - \\pi_{\\text{old}}(\\cdot|s)\\| > \\epsilon then\n\n\\quad\\quad\\quad policy_changed \\leftarrow true\n\n\\quad\\quad end if\n\n\\quad end for\n\nuntil policy_changed = false\n\nreturn v, \\pi\n\nFeatures:\n\nPolicy Evaluation (lines 3-15): Computes the value of the current policy including entropy bonus\n\nOption 1: Iterative method (successive approximation)\n\nOption 2: Direct solution via linear system\n\nPolicy Improvement (lines 16-29): Updates policy to softmax over Q-values\n\nLine 14 shows the vector form: the linear system includes the entropy vector \\mathbf{H}_\\pi\n\nThe algorithm alternates between evaluating the current stochastic policy and improving it\n\nConverges to the unique optimal entropy-regularized policy","type":"content","url":"/smoothing#entropy-regularized-dynamic-programming-algorithms","position":37},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization"},"type":"lvl1","url":"/trajectories","position":0},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization"},"content":"In the previous chapter, we examined different ways to represent dynamical systems: continuous versus discrete time, deterministic versus stochastic, fully versus partially observable, and even simulation-based views such as agent-based or programmatic models. Our focus was on the structure of models: how they capture evolution, uncertainty, and information.\n\nIn this chapter, we turn to what makes these models useful for decision-making. The goal is no longer just to describe how a system behaves, but to leverage that description to compute actions over time. This doesn’t mean the model prescribes actions on its own. Rather, it provides the scaffolding for optimization: given a model and an objective, we can derive the control inputs that make the modeled system behave well according to a chosen criterion.\n\nOur entry point will be trajectory optimization. By a trajectory, we mean the time-indexed sequence of states and controls that the system follows under a plan: the states (\\mathbf{x}_1, \\dots, \\mathbf{x}_T) together with the controls (\\mathbf{u}_1, \\dots, \\mathbf{u}_{T-1}). In this chapter, we focus on an open-loop viewpoint: starting from a known initial state, we compute the entire sequence of controls in advance and then apply it as-is. This is appealing because, for discrete-time problems, it yields a finite-dimensional optimization over a vector of decisions and cleanly exposes the structure of the constraints. In continuous time, the base formulation is infinite-dimensional; in this course we will rely on direct methods—time discretization and parameterization—to transform it into a finite-dimensional nonlinear program.\n\nOpen loop also has a clear limitation: if reality deviates from the model—due to disturbances, model mismatch, or unanticipated events—the state you actually reach may differ from the predicted one. The precomputed controls that were optimal for the nominal trajectory can then lead you further off course, and errors can compound over time.\n\nLater, we will study closed-loop (feedback) strategies, where the choice of action at time t can depend on the state observed at time t. Instead of a single sequence, we optimize a policy \\pi_t mapping states to controls, \\mathbf{u}_t = \\pi_t(\\mathbf{x}_t). Feedback makes plans resilient to unforeseen situations by adapting on the fly, but it leads to a more challenging problem class. We start with open-loop trajectory optimization to build core concepts and tools before tackling feedback design.","type":"content","url":"/trajectories","position":1},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"Discrete-Time Optimal Control Problems (DOCPs)"},"type":"lvl3","url":"/trajectories#discrete-time-optimal-control-problems-docps","position":2},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"Discrete-Time Optimal Control Problems (DOCPs)"},"content":"Consider a system described by a state \\mathbf{x}_t \\in \\mathbb{R}^n, summarizing everything needed to predict its evolution. At each stage t, we can influence the system through a control input \\mathbf{u}_t \\in \\mathbb{R}^m. The dynamics specify how the state evolves:\\mathbf{x}_{t+1} = \\mathbf{f}_t(\\mathbf{x}_t, \\mathbf{u}_t),\n\nwhere \\mathbf{f}_t may be nonlinear or time-varying. We assume the initial state \\mathbf{x}_1 is known.\n\nThe goal is to pick a sequence of controls \\mathbf{u}_1,\\dots,\\mathbf{u}_{T-1} that makes the trajectory desirable. But desirable in what sense? That depends on an objective function, which often includes two components:\\text{(i) stage cost: } c_t(\\mathbf{x}_t,\\mathbf{u}_t), \\qquad \\text{(ii) terminal cost: } c_T(\\mathbf{x}_T).\n\nThe stage cost reflects ongoing penalties—energy, delay, risk. The terminal cost measures the value (or cost) of ending in a particular state. Together, these give a discrete-time Bolza problem with path constraints and bounds:\\begin{aligned}\n    \\text{minimize} \\quad & c_T(\\mathbf{x}_T) + \\sum_{t=1}^{T-1} c_t(\\mathbf{x}_t,\\mathbf{u}_t) \\\\\n    \\text{subject to} \\quad & \\mathbf{x}_{t+1} = \\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t) \\\\\n                            & \\mathbf{g}_t(\\mathbf{x}_t,\\mathbf{u}_t) \\leq \\mathbf{0} \\\\\n                            & \\mathbf{x}_{\\text{min}} \\leq \\mathbf{x}_t \\leq \\mathbf{x}_{\\text{max}} \\\\\n                            & \\mathbf{u}_{\\text{min}} \\leq \\mathbf{u}_t \\leq \\mathbf{u}_{\\text{max}} \\\\\n    \\text{given} \\quad & \\mathbf{x}_1 = \\mathbf{x}_0 \\enspace .\n\\end{aligned}\n\nWritten this way, it may seem obvious that the decision variables are the controls \\mathbf{u}_t. After all, in most intuitive descriptions of control, we think of choosing inputs to influence the system. But notice that in the program above, the entire state trajectory also appears as a set of variables, linked to the controls by the dynamics constraints. This is intentional: it reflects one way of writing the problem that makes the constraints explicit.\n\nWhy introduce \\mathbf{x}_t as decision variables if they can be simulated forward from the controls? Many readers hesitate here, and the question is natural: If the model is deterministic and \\mathbf{x}_1 is known, why not pick \\mathbf{u}_{1:T-1} and compute \\mathbf{x}_{2:T} on the fly? That instinct leads to single shooting, a method we will return to shortly.\n\nAlready in this formulation, though, we see an important theme: the structure of the problem matters. Ignoring it can make our life much harder. The reason is twofold:\n\nDimensionality grows with the horizon. For a horizon of length T, the program has roughly (T-1)(m+n) decision variables.\n\nTemporal coupling. Each control affects all future states and costs. The feasible set is not a simple box but a narrow manifold defined by the dynamics.\n\nTogether, these features explain why specialized methods exist and why the way we write the problem influences the algorithms we can use. Whether we keep states explicit or eliminate them through forward simulation determines not just the problem size, but also its conditioning and the trade-offs between robustness and computational effort.","type":"content","url":"/trajectories#discrete-time-optimal-control-problems-docps","position":3},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"Existence of Solutions and Optimality Conditions"},"type":"lvl3","url":"/trajectories#existence-of-solutions-and-optimality-conditions","position":4},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"Existence of Solutions and Optimality Conditions"},"content":"Now that we have the optimization problem written down, a natural question arises: does this program always have a solution? And if it does, how can we recognize one when we see it? These questions bring us into the territory of feasibility and optimality conditions.","type":"content","url":"/trajectories#existence-of-solutions-and-optimality-conditions","position":5},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"When does a solution exist?","lvl3":"Existence of Solutions and Optimality Conditions"},"type":"lvl4","url":"/trajectories#when-does-a-solution-exist","position":6},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"When does a solution exist?","lvl3":"Existence of Solutions and Optimality Conditions"},"content":"Notice first that nothing in the problem statement required the dynamics\\mathbf{x}_{t+1} = \\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t)\n\nto be stable. In fact, many problems of interest involve unstable systems; think of balancing a pole or steering a spacecraft. What matters is that the dynamics are well defined: given a state–control pair, the rule \\mathbf{f}_t produces a valid next state.\n\nIn continuous time, one usually requires \\mathbf{f} to be continuous (often Lipschitz continuous) in \\mathbf{x} so that the ODE has a unique solution on the horizon of interest. In discrete time, the requirement is lighter—we only need the update map to be well posed.\n\nExistence also hinges on feasibility. A candidate control sequence must generate a trajectory that respects all constraints: the dynamics, any bounds on state and control, and any terminal requirements. If no such sequence exists, the feasible set is empty and the problem has no solution. This can happen if the constraints are overly strict, or if the system is uncontrollable from the given initial condition.","type":"content","url":"/trajectories#when-does-a-solution-exist","position":7},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"What does optimality look like?","lvl3":"Existence of Solutions and Optimality Conditions"},"type":"lvl4","url":"/trajectories#what-does-optimality-look-like","position":8},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"What does optimality look like?","lvl3":"Existence of Solutions and Optimality Conditions"},"content":"Assume the feasible set is nonempty. To characterize a point that is not only feasible but locally optimal, we use the Lagrange multiplier machinery from nonlinear programming. For a smooth problem\\begin{aligned}\n\\min_{\\mathbf{z}}\\quad & F(\\mathbf{z})\\\\\n\\text{s.t.}\\quad & G(\\mathbf{z})=\\mathbf{0},\\\\\n& H(\\mathbf{z})\\ge \\mathbf{0},\n\\end{aligned}\n\ndefine the Lagrangian\\mathcal{L}(\\mathbf{z},\\boldsymbol{\\lambda},\\boldsymbol{\\mu})\n= F(\\mathbf{z})+\\boldsymbol{\\lambda}^{\\top}G(\\mathbf{z})+\\boldsymbol{\\mu}^{\\top}H(\\mathbf{z}),\\qquad \\boldsymbol{\\mu}\\ge \\mathbf{0}.\n\nFor an inequality system H(\\mathbf{z})\\ge \\mathbf{0} and a candidate point \\mathbf{z}, the active set is\\mathcal{A}(\\mathbf{z}) \\;=\\; \\{\\, i \\;:\\; H_i(\\mathbf{z})=0 \\,\\},\n\nwhile indices with H_i(\\mathbf{z})>0 are inactive. Only active inequalities can carry positive multipliers.\n\nWe now make a constraint qualification assumption. In plain language, it says the constraints near the solution intersect in a regular way so that the feasible set has a well-defined tangent space and the multipliers exist. Algebraically, this amounts to a full row rank condition on the Jacobian of the equalities together with the active inequalities:\\text{rows of }\\big[\\nabla G(\\mathbf{z}^\\star);\\ \\nabla H_{\\mathcal{A}}(\\mathbf{z}^\\star)\\big]\\ \\text{are linearly independent.}\n\nThis is the LICQ (Linear Independence Constraint Qualification). In convex problems, Slater’s condition (existence of a strictly feasible point) plays a similar role. You can think of these as the assumptions that let the linearized KKT equations be solvable; we do not literally invert that Jacobian, but the full-rank property is the key ingredient that would make such an inversion possible in principle.\n\nUnder such a constraint qualification, any local minimizer \\mathbf{z}^\\star admits multipliers (\\boldsymbol{\\lambda}^\\star,\\boldsymbol{\\mu}^\\star) that satisfy the Karush–Kuhn–Tucker (KKT) conditions:\\begin{aligned}\n&\\text{stationarity:} && \\nabla_{\\mathbf{z}}\\mathcal{L}(\\mathbf{z}^\\star,\\boldsymbol{\\lambda}^\\star,\\boldsymbol{\\mu}^\\star)=\\mathbf{0},\\\\\n&\\text{primal feasibility:} && G(\\mathbf{z}^\\star)=\\mathbf{0},\\quad H(\\mathbf{z}^\\star)\\ge \\mathbf{0},\\\\\n&\\text{dual feasibility:} && \\boldsymbol{\\mu}^\\star\\ge \\mathbf{0},\\\\\n&\\text{complementarity:} && \\mu_i^\\star\\,H_i(\\mathbf{z}^\\star)=0\\quad \\text{for all } i.\n\\end{aligned}\n\nOnly constraints that are active at \\mathbf{z}^\\star can have \\mu_i^\\star>0; inactive ones have \\mu_i^\\star=0. The multipliers quantify marginal costs: \\lambda_j^\\star measures how the optimal value changes if the j-th equality is relaxed, and \\mu_i^\\star does the same for the i-th inequality. (If you prefer h(\\mathbf{z})\\le 0, signs flip accordingly.)\n\nIn our trajectory problems, \\mathbf{z} stacks state and control trajectories, G enforces the dynamics, and H collects bounds and path constraints. The equalities’ multipliers act as costates or shadow prices for the dynamics. Writing the KKT system stage by stage yields the discrete-time Pontryagin principle, derived next. For convex programs these conditions are also sufficient.\n\nWhat fails without a CQ? If the active gradients are dependent (for example duplicated or nearly parallel), the Jacobian loses rank; multipliers may then be nonunique or fail to exist, and the linearized equations become ill-posed. In transcribed trajectory problems this shows up as dependent dynamic constraints or redundant path constraints, which leads to fragile solver behavior.","type":"content","url":"/trajectories#what-does-optimality-look-like","position":9},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"From KKT to algorithms","lvl3":"Existence of Solutions and Optimality Conditions"},"type":"lvl4","url":"/trajectories#from-kkt-to-algorithms","position":10},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"From KKT to algorithms","lvl3":"Existence of Solutions and Optimality Conditions"},"content":"The KKT system can be read as the first-order optimality conditions of a saddle-point problem. With equalities G(\\mathbf{z})=\\mathbf{0} and inequalities H(\\mathbf{z})\\ge \\mathbf{0}, define the Lagrangian\\mathcal{L}(\\mathbf{z},\\boldsymbol{\\lambda},\\boldsymbol{\\mu})\n= F(\\mathbf{z})+\\boldsymbol{\\lambda}^{\\top}G(\\mathbf{z})+\\boldsymbol{\\mu}^{\\top}H(\\mathbf{z}),\\quad \\boldsymbol{\\mu}\\ge \\mathbf{0}.\n\nOptimality corresponds to a saddle: minimize in \\mathbf{z}, maximize in (\\boldsymbol{\\lambda},\\boldsymbol{\\mu}) (with \\boldsymbol{\\mu} constrained to the nonnegative orthant).","type":"content","url":"/trajectories#from-kkt-to-algorithms","position":11},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl5":"Primal–dual gradient dynamics (Arrow–Hurwicz)","lvl4":"From KKT to algorithms","lvl3":"Existence of Solutions and Optimality Conditions"},"type":"lvl5","url":"/trajectories#primal-dual-gradient-dynamics-arrow-hurwicz","position":12},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl5":"Primal–dual gradient dynamics (Arrow–Hurwicz)","lvl4":"From KKT to algorithms","lvl3":"Existence of Solutions and Optimality Conditions"},"content":"The simplest algorithm mirrors this saddle structure by descending in the primal variables and ascending in the dual variables, with a projection for the inequalities:\\begin{aligned}\n\\mathbf{z}^{k+1} &= \\mathbf{z}^{k}-\\alpha_k\\big(\\nabla F(\\mathbf{z}^{k})+\\nabla G(\\mathbf{z}^{k})^{\\top}\\boldsymbol{\\lambda}^{k}+\\nabla H(\\mathbf{z}^{k})^{\\top}\\boldsymbol{\\mu}^{k}\\big),\\\\[2mm]\n\\boldsymbol{\\lambda}^{k+1} &= \\boldsymbol{\\lambda}^{k}+\\beta_k\\,G(\\mathbf{z}^{k}),\\\\[1mm]\n\\boldsymbol{\\mu}^{k+1} &= \\Pi_{\\ge 0}\\!\\big(\\boldsymbol{\\mu}^{k}+\\beta_k\\,H(\\mathbf{z}^{k})\\big).\n\\end{aligned}\n\nHere \\Pi_{\\ge 0} is the projection onto \\{\\boldsymbol{\\mu}\\ge 0\\}. In convex settings and with suitable step sizes, these iterates converge to a saddle point. In nonconvex problems (our trajectory optimizations after transcription), these updates are often used inside augmented Lagrangian or penalty frameworks to improve robustness, for example by replacing \\mathcal{L} with\\mathcal{L}_\\rho(\\mathbf{z},\\boldsymbol{\\lambda},\\boldsymbol{\\mu})\n= \\mathcal{L}(\\mathbf{z},\\boldsymbol{\\lambda},\\boldsymbol{\\mu})\n+\\tfrac{\\rho}{2}\\|G(\\mathbf{z})\\|^2\n+\\tfrac{\\rho}{2}\\|\\min\\{0,H(\\mathbf{z})\\}\\|^2,\n\nwhich stabilizes the dual ascent when constraints are not yet well satisfied.","type":"content","url":"/trajectories#primal-dual-gradient-dynamics-arrow-hurwicz","position":13},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl5":"SQP as Newton on the KKT system (equality case)","lvl4":"From KKT to algorithms","lvl3":"Existence of Solutions and Optimality Conditions"},"type":"lvl5","url":"/trajectories#sqp-as-newton-on-the-kkt-system-equality-case","position":14},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl5":"SQP as Newton on the KKT system (equality case)","lvl4":"From KKT to algorithms","lvl3":"Existence of Solutions and Optimality Conditions"},"content":"With only equality constraints G(\\mathbf{z})=\\mathbf{0}, write first-order conditions\\nabla_{\\mathbf{z}}\\mathcal{L}(\\mathbf{z},\\boldsymbol{\\lambda})=\\mathbf{0},\n\\qquad\nG(\\mathbf{z})=\\mathbf{0},\n\\quad \\text{where }\\mathcal{L}=F+\\boldsymbol{\\lambda}^{\\top}G.\n\nApplying Newton’s method to this system gives the linear KKT solve\\begin{bmatrix}\n\\nabla_{\\mathbf{z}\\mathbf{z}}^2\\mathcal{L}(\\mathbf{z}^k,\\boldsymbol{\\lambda}^k) & \\nabla G(\\mathbf{z}^k)^{\\top}\\\\\n\\nabla G(\\mathbf{z}^k) & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\Delta \\mathbf{z}\\\\ \\Delta \\boldsymbol{\\lambda}\n\\end{bmatrix}\n=\n-\n\\begin{bmatrix}\n\\nabla_{\\mathbf{z}}\\mathcal{L}(\\mathbf{z}^k,\\boldsymbol{\\lambda}^k)\\\\\nG(\\mathbf{z}^k)\n\\end{bmatrix}.\n\nThis is exactly the step computed by Sequential Quadratic Programming (SQP) in the equality-constrained case: it is Newton’s method on the KKT equations. For general problems with inequalities, SQP forms a quadratic subproblem by quadratically modeling F with \\nabla_{\\mathbf{z}\\mathbf{z}}^2\\mathcal{L} and linearizing the constraints, then solves that QP with line search or trust region. In least-squares-like problems one often uses Gauss–Newton (or a Levenberg–Marquardt trust region) as a positive-definite approximation to the Lagrangian Hessian.\n\nIn trajectory optimization. After transcription, the KKT matrix inherits banded/sparse structure from the dynamics. Newton/SQP steps can be computed efficiently by exploiting this structure; in the special case of quadratic models and linearized dynamics, the QP reduces to an LQR solve along the horizon (this is the backbone of iLQR/DDP-style methods). Primal–dual updates provide simpler iterations and are easy to implement; augmented terms are typically needed to obtain stable progress when constraints couple stages.\n\nWhen to use which. Primal–dual gradients give lightweight iterations and are good for warm starts or as inner loops with penalties. SQP/Newton gives rapid local convergence when you are close to a solution and LICQ holds; use trust regions or line search to globalize.","type":"content","url":"/trajectories#sqp-as-newton-on-the-kkt-system-equality-case","position":15},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"Examples of DOCPs"},"type":"lvl3","url":"/trajectories#examples-of-docps","position":16},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"Examples of DOCPs"},"content":"To make things concrete, here are three problems that are naturally posed as discrete-time OCPs. In each case, we seek an optimal trajectory of states and controls over a finite horizon.","type":"content","url":"/trajectories#examples-of-docps","position":17},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"Periodic Inventory Control","lvl3":"Examples of DOCPs"},"type":"lvl4","url":"/trajectories#periodic-inventory-control","position":18},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"Periodic Inventory Control","lvl3":"Examples of DOCPs"},"content":"Decisions are made once per period: choose order quantity u_k \\ge 0 to meet forecast demand d_k. The state x_k is on-hand inventory with dynamics x_{k+1} = x_k + u_k - d_k. A typical stage cost is c_k(x_k,u_k) = h\\,[x_k]_+ + p\\,[-x_k]_+ + c\\,u_k, trading off holding, backorder, and ordering costs. The horizon objective is \\min \\sum_{k=0}^{T-1} c_k(x_k,u_k) subject to bounds.","type":"content","url":"/trajectories#periodic-inventory-control","position":19},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"End-of-Day Portfolio Rebalancing","lvl3":"Examples of DOCPs"},"type":"lvl4","url":"/trajectories#end-of-day-portfolio-rebalancing","position":20},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"End-of-Day Portfolio Rebalancing","lvl3":"Examples of DOCPs"},"content":"At each trading day k, choose trades u_k to adjust holdings h_k before next-day returns r_{k} realize. Deterministic planning uses predicted returns \\mu_k, with dynamics h_{k+1} = (h_k + u_k) \\odot (\\mathbf{1} + \\mu_k) and budget/box constraints. The stage cost can capture transaction costs and risk, e.g., c_k(h_k,u_k) = \\tau\\lVert u_k \\rVert_1 + \\tfrac{\\lambda}{2}\\,h_k^\\top \\Sigma_k h_k, with a terminal utility or wealth objective.","type":"content","url":"/trajectories#end-of-day-portfolio-rebalancing","position":21},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"Daily Ad-Budget Allocation with Carryover","lvl3":"Examples of DOCPs"},"type":"lvl4","url":"/trajectories#daily-ad-budget-allocation-with-carryover","position":22},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"Daily Ad-Budget Allocation with Carryover","lvl3":"Examples of DOCPs"},"content":"Allocate spend u_k \\in [0, U_{\\max}] to build awareness s_k with carryover dynamics s_{k+1} = \\alpha s_k + \\beta u_k. Conversions/revenue at day k follow a response curve g(s_k,u_k); the goal is \\max \\sum_{k=0}^{T-1} g(s_k,u_k) - c\\,u_k subject to spend limits. This is naturally discrete because decisions and measurements occur daily.","type":"content","url":"/trajectories#daily-ad-budget-allocation-with-carryover","position":23},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"DOCPs Arising from the Discretization of Continuous-Time OCPs","lvl3":"Examples of DOCPs"},"type":"lvl4","url":"/trajectories#docps-arising-from-the-discretization-of-continuous-time-ocps","position":24},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"DOCPs Arising from the Discretization of Continuous-Time OCPs","lvl3":"Examples of DOCPs"},"content":"Although many applications are natively discrete-time, it is also common to obtain a DOCP by discretizing a continuous-time formulation. Consider a system on [0, T_c] given by\\dot{\\mathbf{x}}(t) = \\mathbf{f}(t, \\mathbf{x}(t), \\mathbf{u}(t)), \\qquad \\mathbf{x}(0) = \\mathbf{x}_0.\n\nChoose a step size \\Delta > 0 and grid t_k = k\\,\\Delta. A one-step integration scheme induces a discrete map \\mathbf{F}_\\Delta so that\\mathbf{x}_{k+1} = \\mathbf{F}_\\Delta(\\mathbf{x}_k, \\mathbf{u}_k, t_k),\\qquad k=0,\\dots, T-1,\n\nwhere, for example, explicit Euler gives \\mathbf{F}_\\Delta(\\mathbf{x},\\mathbf{u},t) = \\mathbf{x} + \\Delta\\,\\mathbf{f}(t,\\mathbf{x},\\mathbf{u}). The resulting discrete-time optimal control problem takes the Bolza form with these induced dynamics:\\begin{aligned}\n\\min_{\\{\\mathbf{x}_k,\\mathbf{u}_k\\}}\\; & c_T(\\mathbf{x}_T) + \\sum_{k=0}^{T-1} c_k(\\mathbf{x}_k,\\mathbf{u}_k) \\\\\n\\text{s.t.}\\; & \\mathbf{x}_{k+1} - \\mathbf{F}_\\Delta(\\mathbf{x}_k,\\mathbf{u}_k, t_k) = 0,\\quad k=0,\\dots,T-1, \\\\\n& \\mathbf{x}_0 = \\mathbf{x}_\\mathrm{init}.\n\\end{aligned}","type":"content","url":"/trajectories#docps-arising-from-the-discretization-of-continuous-time-ocps","position":25},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"Programs as DOCPs and Differentiable Programming","lvl3":"Examples of DOCPs"},"type":"lvl4","url":"/trajectories#programs-as-docps-and-differentiable-programming","position":26},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"Programs as DOCPs and Differentiable Programming","lvl3":"Examples of DOCPs"},"content":"It is often useful to view a computer program itself as a discrete-time dynamical system. Let the program state collect memory, buffers, and intermediate variables, and let the control represent inputs or tunable decisions at each step. A single execution step defines a transition map\\mathbf{x}_{k+1}=\\Phi_k(\\mathbf{x}_k,\\mathbf{u}_k),\n\nand a scalar objective (e.g., loss, error, runtime, energy) yields a DOCP:\\min_{\\{\\mathbf{u}_k\\}} \\; c_T(\\mathbf{x}_T)+\\sum_{k=0}^{T-1} c_k(\\mathbf{x}_k,\\mathbf{u}_k)\n\\quad\\text{s.t.}\\quad \\mathbf{x}_{k+1}=\\Phi_k(\\mathbf{x}_k,\\mathbf{u}_k).\n\nIn differentiable programming (e.g., JAX, PyTorch), the composed map \\Phi_{T-1}\\circ\\cdots\\circ\\Phi_0 is differentiable, enabling reverse-mode automatic differentiation and efficient gradient-based trajectory optimization. When parts of the program are non-differentiable (discrete branches, simulators with events), DOCPs can still be solved using derivative-free or weak-gradient methods (eg. finite differences, SPSA, Nelder–Mead, CMA-ES, or evolutionary strategies) optionally combined with smoothing, relaxations, or stochastic estimators to navigate non-smooth regions.","type":"content","url":"/trajectories#programs-as-docps-and-differentiable-programming","position":27},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl5":"Example: HTTP Retrier Optimization","lvl4":"Programs as DOCPs and Differentiable Programming","lvl3":"Examples of DOCPs"},"type":"lvl5","url":"/trajectories#example-http-retrier-optimization","position":28},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl5":"Example: HTTP Retrier Optimization","lvl4":"Programs as DOCPs and Differentiable Programming","lvl3":"Examples of DOCPs"},"content":"As an example we cast the problem of optimizing a “HTTP retrier with backoff” as a DOCP where the state tracks wall-clock time, attempt index, success, last code, and jitter; the control is the chosen wait time before the next request (the backoff schedule); the transition encapsulates waiting and a probabilistic request outcome; and the objective penalizes latency and failure. We then optimize the schedule either directly (per-step SPSA) or via a two-parameter exponential policy using common random numbers for variance reduction.\n\n#  label: fig-ocp-http-retrier\n#  caption: Console output comparing the baseline backoff schedule with SPSA-optimized schedules for the HTTP retrier DOCP, including costs, attempts, and success codes.\n\n%config InlineBackend.figure_format = 'retina'\nfrom dataclasses import dataclass\nimport math, random\nimport matplotlib.pyplot as plt\n\n# Apply book style\ntry:\n    import scienceplots\n    plt.style.use(['science', 'notebook'])\nexcept (ImportError, OSError):\n    pass  # Use matplotlib defaults\n\n# ---------------------------\n# PROGRAM = \"HTTP retrier with backoff\"\n# ---------------------------\n\n@dataclass\nclass State:\n    t: float            # wall-clock time (s)\n    k: int              # attempt index\n    done: bool          # success flag\n    code: int | None    # last HTTP code or None\n    jitter: float       # per-run jitter (simulates clock/socket noise)\n\n# Controls (decision variables): per-step wait times (backoff schedule)\n# u[k] can be optimized; in a fixed policy you'd set u[k] = base * gamma**k\n# We'll keep them bounded for realism.\ndef clamp(x, lo, hi): return max(lo, min(hi, x))\n\n# Simulated environment: availability is time-varying (spiky outage)\ndef server_success_prob(t: float) -> float:\n    # Low availability for the first 2 seconds, then rebounds\n    base = 0.15 if t < 2.0 else 0.85\n    # Some diurnal-like wobble (toy)\n    wobble = 0.1 * math.sin(2 * math.pi * (t / 3.0))\n    return clamp(base + wobble, 0.01, 0.99)\n\ndef http_request():\n    # Just returns a code; success = 200, failure = 503\n    return 200 if random.random() < 0.5 else 503\n\n# -------- DOCP ingredients --------\n# State x_k = (t, k, done, code, jitter)\n# Control u_k = wait time before next attempt (our backoff schedule entry)\n# Transition Phi_k: one \"program step\" = (optional wait) + (one request) + (branch)\ndef Phi(state: State, u_k: float) -> State:\n    if state.done:\n        # No-ops after success (absorbing state)\n        return State(state.t, state.k, True, state.code, state.jitter)\n\n    # 1) Wait according to control (backoff schedule) + jitter\n    wait = clamp(u_k + 0.02 * state.jitter, 0.0, 3.0)\n    t = state.t + wait\n\n    # 2) Environment: success probability depends on time t\n    p = server_success_prob(t)\n\n    # 3) \"Perform request\": success with prob p; otherwise 503\n    code = 200 if random.random() < p else 503\n    done = (code == 200)\n\n    # 4) Advance attempt counter and wall clock\n    return State(t=t, k=state.k + 1, done=done, code=code, jitter=state.jitter)\n\n# Stage cost: latency penalty each step; heavy penalty if still failing late\ndef stage_cost(state: State, u_k: float) -> float:\n    # Latency/energy per unit wait + small per-step overhead when not done\n    return 0.20 * u_k + (0.00 if state.done else 0.002)\n\n# Terminal cost: if failed after horizon, big penalty; if succeeded, pay total time\ndef terminal_cost(state: State, max_attempts: int) -> float:\n    # Pay for elapsed time; fail late incurs extra penalty\n    return 0.3 * state.t + (5.0 if (not state.done and state.k >= max_attempts) else 0.0)\n\ndef rollout(u, max_attempts=8, seed=0):\n    random.seed(seed)\n    s = State(t=0.0, k=0, done=False, code=None, jitter=random.uniform(-1,1))\n    J = 0.0\n    for k in range(max_attempts):\n        J += stage_cost(s, u[k])\n        s = Phi(s, u[k])\n        if s.done:  # early stop like a real program\n            break\n    J += terminal_cost(s, max_attempts)\n    return J, s  # return final state for debugging if needed\n\n# ---------- helpers for SPSA with common random numbers ----------\ndef eval_policy(u, seeds, max_attempts=8):\n    # Average over a fixed set of seeds (CRN helps SPSA a lot)\n    Js = []\n    for sd in seeds:\n        J, _ = rollout(u, max_attempts=max_attempts, seed=sd)\n        Js.append(J)\n    return sum(Js) / len(Js)\n\ndef project_waits(u):\n    # Keep waits in [0, 3] for realism\n    return [max(0.0, min(3.0, x)) for x in u]\n\n# ---------- schedule parameterizations ----------\ndef schedule_exp(base, gamma, K):\n    # u[k] = base * gamma**k\n    return [base * (gamma ** k) for k in range(K)]\n\n# If you prefer per-step but monotone nonnegative waits, use softplus increments:\ndef schedule_softplus(z, K):\n    # z in R^K -> u monotone via cumulative softplus increments\n    def softplus(x):\n        return math.log1p(math.exp(-abs(x))) + max(x, 0.0)\n    inc = [softplus(zi) for zi in z]\n    u = []\n    s_accum = 0.0\n    for i in range(K):\n        s_accum += inc[i]\n        u.append(s_accum)\n    return u\n\n# ---------------------------\n# Black-box optimization (SPSA) of the schedule u[0:K]\n# ---------------------------\ndef spsa_optimize(K=8, iters=200, seed=0):\n    random.seed(seed)\n    # Initialize a conservative schedule (small linear backoff)\n    u = [0.05 + 0.1*k for k in range(K)]\n    alpha = 0.2      # learning rate\n    c0 = 0.1         # perturbation scale\n    for t in range(1, iters+1):\n        c = c0 / (t ** 0.101)\n        # Rademacher perturbation\n        delta = [1.0 if random.random() < 0.5 else -1.0 for _ in range(K)]\n        u_plus  = [clamp(u[i] + c * delta[i], 0.0, 3.0) for i in range(K)]\n        u_minus = [clamp(u[i] - c * delta[i], 0.0, 3.0) for i in range(K)]\n\n        Jp, _ = rollout(u_plus, seed=seed + 10*t + 1)\n        Jm, _ = rollout(u_minus, seed=seed + 10*t + 2)\n\n        # SPSA gradient estimate\n        g = [(Jp - Jm) / (2.0 * c * delta[i]) for i in range(K)]\n        # Update (project back to bounds)\n        u = [clamp(u[i] - alpha * g[i], 0.0, 3.0) for i in range(K)]\n    return u\n\n# ---------- SPSA over 2 parameters (base, gamma) with CRN ----------\ndef spsa_optimize_exp(K=8, iters=200, seed=0, Nmc=16):\n    random.seed(seed)\n    # fixed seeds reused every iteration (CRN)\n    seeds = [seed + 1000 + i for i in range(Nmc)]\n\n    # init: small base, mild growth\n    base, gamma = 0.05, 1.4\n    alpha0, c0 = 0.15, 0.2  # learning rate and perturbation scales\n\n    for t in range(1, iters + 1):\n        a_t = alpha0 / (t ** 0.602)   # standard SPSA decay\n        c_t = c0 / (t ** 0.101)\n\n        # Rademacher perturbations for 2 params\n        d_base = 1.0 if random.random() < 0.5 else -1.0\n        d_gamma = 1.0 if random.random() < 0.5 else -1.0\n\n        base_plus  = base  + c_t * d_base\n        base_minus = base  - c_t * d_base\n        gamma_plus  = gamma + c_t * d_gamma\n        gamma_minus = gamma - c_t * d_gamma\n\n        u_plus  = project_waits(schedule_exp(base_plus,  gamma_plus,  K))\n        u_minus = project_waits(schedule_exp(base_minus, gamma_minus, K))\n\n        Jp = eval_policy(u_plus, seeds, max_attempts=K)\n        Jm = eval_policy(u_minus, seeds, max_attempts=K)\n\n        # SPSA gradient estimate\n        g_base  = (Jp - Jm) / (2.0 * c_t * d_base)\n        g_gamma = (Jp - Jm) / (2.0 * c_t * d_gamma)\n\n        # Update\n        base  = max(0.0, base  - a_t * g_base)\n        gamma = max(0.5, gamma - a_t * g_gamma)  # keep reasonable\n\n    return base, gamma\n\nK = 8\n# Baseline linear schedule\nu0 = [0.05 + 0.1*k for k in range(K)]\nJ0, s0 = rollout(u0, seed=42)\n\n# Optimize per-step waits (K-dim SPSA)\nu_opt = spsa_optimize(K=K, iters=200, seed=123)\nJ1, s1 = rollout(u_opt, seed=999)\n\n# Optimize exponential schedule parameters (2-dim SPSA with CRN)\nbase_opt, gamma_opt = spsa_optimize_exp(K=K, iters=200, seed=321, Nmc=16)\nu_exp = project_waits(schedule_exp(base_opt, gamma_opt, K))\nJ2, s2 = rollout(u_exp, seed=777)\n\nprint(\"Initial schedule:\", [round(x,3) for x in u0], \"  Cost ≈\", round(J0,3))\nprint(\"Optimized (per-step SPSA):\", [round(x,3) for x in u_opt], \"  Cost ≈\", round(J1,3))\nprint(\"Optimized (exp base, gamma): base=\", round(base_opt,3), \" gamma=\", round(gamma_opt,3),\n      \"  schedule=\", [round(x,3) for x in u_exp], \"  Cost ≈\", round(J2,3))\nprint(\"Attempts (init → per-step → exp):\", s0.k, \"→\", s1.k, \"→\", s2.k,\n      \"  Success codes:\", s0.code, s1.code, s2.code)\n\nstrategies = {\n    \"Baseline\": u0,\n    \"Per-step SPSA\": u_opt,\n    \"Exp SPSA\": u_exp,\n}\ncosts = {\"Baseline\": J0, \"Per-step SPSA\": J1, \"Exp SPSA\": J2}\n\nfig, axes = plt.subplots(2, 1, figsize=(9, 7), sharex=True)\n\nk = range(1, K + 1)\nfor name, waits in strategies.items():\n    axes[0].step(k, waits, where=\"mid\", label=name)\naxes[0].set_ylabel(\"Wait time (s)\")\naxes[0].set_title(\"Backoff Schedules\")\naxes[0].grid(alpha=0.3)\naxes[0].legend()\n\naxes[1].bar(list(costs.keys()), [costs[key] for key in costs], color=[\"#4a90e2\", \"#f5a623\", \"#7ed321\"])\naxes[1].set_ylabel(\"Mean rollout cost\")\naxes[1].set_title(\"Objective Values (lower is better)\")\naxes[1].grid(axis=\"y\", alpha=0.3)\naxes[1].set_xlabel(\"Strategy\")\n\nfig.tight_layout()\n\n","type":"content","url":"/trajectories#example-http-retrier-optimization","position":29},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl5":"Example: Gradient Descent with Momentum as DOCP","lvl4":"Programs as DOCPs and Differentiable Programming","lvl3":"Examples of DOCPs"},"type":"lvl5","url":"/trajectories#example-gradient-descent-with-momentum-as-docp","position":30},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl5":"Example: Gradient Descent with Momentum as DOCP","lvl4":"Programs as DOCPs and Differentiable Programming","lvl3":"Examples of DOCPs"},"content":"To connect this lens to familiar practice—and to hyperparameter optimization—treat the learning rate and momentum (or their schedules) as controls. Rather than fixing them a priori, we can optimize them as part of a trajectory optimization. The optimizer itself becomes the dynamical system whose execution we shape to minimize final loss.\n\nProgram: gradient descent with momentum on a quadratic loss. We fit \\boldsymbol{\\theta}\\in\\mathbb{R}^p to data (\\mathbf{A},\\mathbf{b}) by minimizing\\ell(\\boldsymbol{\\theta})=\\tfrac{1}{2}\\,\\lVert\\mathbf{A}\\boldsymbol{\\theta}-\\mathbf{b}\\rVert_2^2.\n\nThe program maintains parameters \\boldsymbol{\\theta}_k and momentum \\mathbf{m}_k. Each iteration does:\n\ncompute gradient  \\mathbf{g}_k=\\nabla_{\\boldsymbol{\\theta}}\\ell(\\boldsymbol{\\theta}_k)=\\mathbf{A}^\\top(\\mathbf{A}\\boldsymbol{\\theta}_k-\\mathbf{b})\n\nupdate momentum  \\mathbf{m}_{k+1}=\\beta_k \\, \\mathbf{m}_k + \\mathbf{g}_k\n\nupdate parameters  \\boldsymbol{\\theta}_{k+1}=\\boldsymbol{\\theta}_k - \\alpha_k \\, \\mathbf{m}_{k+1}\n\nState, control, and transition. Define the state \\mathbf{x}_k=\\begin{bmatrix}\\boldsymbol{\\theta}_k\\\\ \\mathbf{m}_k\\end{bmatrix}\\in\\mathbb{R}^{2p} and the control \\mathbf{u}_k=\\begin{bmatrix}\\alpha_k\\\\ \\beta_k\\end{bmatrix}. One program step is\\Phi_k(\\mathbf{x}_k,\\mathbf{u}_k)=\n\\begin{bmatrix}\n\\boldsymbol{\\theta}_k - \\alpha_k\\!\\left(\\beta_k \\, \\mathbf{m}_k + \\mathbf{A}^\\top(\\mathbf{A}\\boldsymbol{\\theta}_k-\\mathbf{b})\\right)\\\\[2mm]\n\\beta_k \\, \\mathbf{m}_k + \\mathbf{A}^\\top(\\mathbf{A}\\boldsymbol{\\theta}_k-\\mathbf{b})\n\\end{bmatrix}.\n\nExecuting the program for T iterations gives the trajectory\\mathbf{x}_{k+1}=\\Phi_k(\\mathbf{x}_k,\\mathbf{u}_k),\\quad k=0,\\dots,T-1,\\qquad\n\\mathbf{x}_0=\\begin{bmatrix}\\boldsymbol{\\theta}_0\\\\ \\mathbf{m}_0\\end{bmatrix}.\n\nObjective as a DOCP. Choose terminal cost c_T(\\mathbf{x}_T)=\\ell(\\boldsymbol{\\theta}_T) and (optionally) stage costs c_k(\\mathbf{x}_k,\\mathbf{u}_k)=\\rho_\\alpha \\, \\alpha_k^2+\\rho_\\beta\\,(\\beta_k- \\bar\\beta)^2. The program-as-control problem is\\min_{\\{\\alpha_k,\\beta_k\\}} \\; \\ell(\\boldsymbol{\\theta}_T)+\\sum_{k=0}^{T-1}\\big(\\rho_\\alpha \\, \\alpha_k^2+\\rho_\\beta\\,(\\beta_k-\\bar\\beta)^2\\big)\n\\quad\\text{s.t.}\\quad \\mathbf{x}_{k+1}=\\Phi_k(\\mathbf{x}_k,\\mathbf{u}_k).\n\nBackpropagation = reverse-time costate recursion. Because \\Phi_k is differentiable, reverse-mode AD computes \\nabla_{\\mathbf{u}_{0:T-1}} \\big(c_T+\\sum c_k\\big) by propagating a costate \\boldsymbol{\\lambda}_k=\\partial \\mathcal{J}/\\partial \\mathbf{x}_k backward:\\boldsymbol{\\lambda}_T=\\nabla_{\\mathbf{x}_T} c_T,\\qquad\n\\boldsymbol{\\lambda}_k=\\nabla_{\\mathbf{x}_k} c_k + \\left(\\nabla_{\\mathbf{x}_k}\\Phi_k\\right)^\\top \\boldsymbol{\\lambda}_{k+1},\n\nand the gradients with respect to controls are\\nabla_{\\mathbf{u}_k}\\mathcal{J}=\\nabla_{\\mathbf{u}_k} c_k + \\left(\\nabla_{\\mathbf{u}_k}\\Phi_k\\right)^\\top \\boldsymbol{\\lambda}_{k+1}.\n\nUnrolling a tiny horizon (T=3) to see the composition:\\begin{aligned}\n\\mathbf{x}_1&=\\Phi_0(\\mathbf{x}_0,\\mathbf{u}_0),\\\\\n\\mathbf{x}_2&=\\Phi_1(\\mathbf{x}_1,\\mathbf{u}_1),\\\\\n\\mathbf{x}_3&=\\Phi_2(\\mathbf{x}_2,\\mathbf{u}_2),\\qquad\n\\mathcal{J}=c_T(\\mathbf{x}_3)+\\sum_{k=0}^{2} c_k(\\mathbf{x}_k,\\mathbf{u}_k).\n\\end{aligned}\n\nWhat if the program branches? Suppose we insert a “skip-small-gradients” branch\\boldsymbol{\\theta}_{k+1}=\\boldsymbol{\\theta}_k - \\alpha_k\\,\\mathbf{m}_{k+1}\\,\\mathbf{1}\\{ \\lVert\\mathbf{g}_k\\rVert>\\tau\\},\n\nwhich is non-differentiable because of the indicator. The DOCP view still applies, but gradients are unreliable. Two practical paths: smooth the branch (e.g., replace \\mathbf{1}\\{\\cdot\\} with \\sigma((\\lVert\\mathbf{g}_k\\rVert-\\tau)/\\epsilon) for small \\epsilon) and use autodiff; or go derivative-free on \\{\\alpha_k,\\beta_k,\\tau\\} (e.g., SPSA or CMA-ES) while keeping the inner dynamics exact.","type":"content","url":"/trajectories#example-gradient-descent-with-momentum-as-docp","position":31},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"Variants: Lagrange and Mayer Problems"},"type":"lvl3","url":"/trajectories#variants-lagrange-and-mayer-problems","position":32},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"Variants: Lagrange and Mayer Problems"},"content":"The Bolza form is general enough to cover most situations, but two common special cases are worth noting:\n\nLagrange problem (no terminal cost)\nIf the objective only accumulates stage costs:\\min_{\\mathbf{u}_{1:T-1}} \\sum_{t=1}^{T-1} c_t(\\mathbf{x}_t,\\mathbf{u}_t).\n\nExample: Energy minimization for a delivery drone. The concern is total battery use, regardless of the final position.\n\nMayer problem (terminal cost only)\nIf the objective depends only on the final state:\\min_{\\mathbf{u}_{1:T-1}} c_T(\\mathbf{x}_T).\n\nExample: Satellite orbital transfer. The only goal is to reach a specified orbit, no matter the fuel spent along the way.\n\nThese distinctions matter when deriving optimality conditions, but conceptually they fit in the same framework: the system evolves over time, and we choose controls to shape the trajectory.","type":"content","url":"/trajectories#variants-lagrange-and-mayer-problems","position":33},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"Reducing to Mayer Form by State Augmentation","lvl3":"Variants: Lagrange and Mayer Problems"},"type":"lvl4","url":"/trajectories#reducing-to-mayer-form-by-state-augmentation","position":34},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl4":"Reducing to Mayer Form by State Augmentation","lvl3":"Variants: Lagrange and Mayer Problems"},"content":"Although Bolza, Lagrange, and Mayer problems look different, they are equivalent in expressive power. Any problem with running costs can be rewritten as a Mayer problem (one whose objective depends only on the final state) through a simple trick: augment the state with a running sum of costs.\n\nThe idea is straightforward. Introduce a new variable, y_t, that keeps track of the cumulative cost so far. At each step, we update this running sum along with the system state:\\tilde{\\mathbf{x}}_{t+1} =\n\\begin{pmatrix}\n\\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t) \\\\\ny_t + c_t(\\mathbf{x}_t,\\mathbf{u}_t)\n\\end{pmatrix},\n\nwhere \\tilde{\\mathbf{x}}_t = (\\mathbf{x}_t, y_t). The terminal cost then becomes:\\tilde{c}_T(\\tilde{\\mathbf{x}}_T) = c_T(\\mathbf{x}_T) + y_T.\n\nThe overall effect is that the explicit sum \\sum_{t=1}^{T-1} c_t(\\mathbf{x}_t,\\mathbf{u}_t) disappears from the objective and is captured implicitly by the augmented state. This lets us write every optimal control problem in Mayer form.\n\nWhy do this? Two reasons. First, it often simplifies mathematical derivations, as we will see later when deriving necessary conditions. Second, it can streamline algorithmic implementation: instead of writing separate code paths for Mayer, Lagrange, and Bolza problems, we can reduce everything to one canonical form. That said, this “one size fits all” approach isn’t always best in practice—specialized formulations can sometimes be more efficient computationally, especially when the running cost has simple structure.\n\nThe unifying theme is that a DOCP may look like a generic NLP on paper, but its structure matters. Ignoring that structure often leads to impractical solutions, whereas formulations that expose sparsity and respect temporal coupling allow modern solvers to scale effectively. In the following sections, we will examine how these choices play out in practice through single shooting, multiple shooting, and collocation methods, and why different formulations strike different trade-offs between robustness and computational effort.","type":"content","url":"/trajectories#reducing-to-mayer-form-by-state-augmentation","position":35},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl2":"Numerical Methods for Solving DOCPs"},"type":"lvl2","url":"/trajectories#numerical-methods-for-solving-docps","position":36},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl2":"Numerical Methods for Solving DOCPs"},"content":"Before we discuss specific algorithms, it is useful to clarify the goal: we want to recast a discrete-time optimal control problem as a standard nonlinear program (NLP). Collect all decision variables—states, controls, and any auxiliary variables—into a single vector \\mathbf{z}\\in\\mathbb{R}^{n_z} and write\\begin{aligned}\n\\min_{\\mathbf{z}\\in\\mathbb{R}^{n_z}} \\quad & F(\\mathbf{z}) \\\\\n\\text{s.t.} \\quad & G(\\mathbf{z}) = 0, \\\\\n& H(\\mathbf{z}) \\ge 0,\n\\end{aligned}\n\nwith maps F:\\mathbb{R}^{n_z}\\to\\mathbb{R}, G:\\mathbb{R}^{n_z}\\to\\mathbb{R}^{r_e}, and H:\\mathbb{R}^{n_z}\\to\\mathbb{R}^{r_h}. In optimal control, G typically encodes dynamics and boundary conditions, while H captures path and box constraints.\n\nThere are multiple ways to arrive at (and benefit from) this NLP:\n\nSimultaneous (direct transcription / full discretization): keep all states and controls as variables and impose the dynamics as equality constraints. This is straightforward and exposes sparsity, but the problem can be large unless solver-side techniques (e.g., condensing) are exploited.\n\nSequential (recursive elimination / single shooting): eliminate states by forward propagation from the initial condition, leaving controls as the main decision variables. This reduces dimension and constraints, but can be sensitive to initialization and longer horizons.\n\nMultiple shooting: introduce state variables at segment boundaries and enforce continuity between simulated segments. This compromises between size and conditioning and is often more robust than pure single shooting.\n\nThe next sections work through these formulations—starting with simultaneous methods, then sequential methods, and finally multiple shooting—before discussing how generic NLP solvers and specialized algorithms leverage the resulting structure in practice.","type":"content","url":"/trajectories#numerical-methods-for-solving-docps","position":37},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"Simultaneous Methods","lvl2":"Numerical Methods for Solving DOCPs"},"type":"lvl3","url":"/trajectories#simultaneous-methods","position":38},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"Simultaneous Methods","lvl2":"Numerical Methods for Solving DOCPs"},"content":"In the simultaneous (also called direct transcription or full discretization) approach, we keep the entire trajectory explicit and enforce the dynamics as equality constraints. Starting from the Bolza DOCP,\\min_{\\{\\mathbf{x}_t,\\mathbf{u}_t\\}}\\; c_T(\\mathbf{x}_T) + \\sum_{t=1}^{T-1} c_t(\\mathbf{x}_t,\\mathbf{u}_t)\n\\quad\\text{s.t.}\\quad \\mathbf{x}_{t+1} - \\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t) = 0,\\; t=1,\\dots,T-1,\n\ncollect all variables into a single vector\\mathbf{z} := \\begin{bmatrix}\n\\mathbf{x}_1^\\top & \\cdots & \\mathbf{x}_T^\\top & \\mathbf{u}_1^\\top & \\cdots & \\mathbf{u}_{T-1}^\\top\n\\end{bmatrix}^\\top \\in \\mathbb{R}^{n_z}.\n\nPath constraints typically apply only at selected times. Let \\mathscr{E} index additional equality constraints g_i and \\mathscr{I} index inequality constraints h_i. For each constraint i, define the set of time indices K_i \\subseteq \\{1,\\dots,T\\} where it is enforced (e.g., terminal constraints use K_i = \\{T\\}). The simultaneous transcription is the NLP\\begin{aligned}\n\\min_{\\mathbf{z}}\\quad & F(\\mathbf{z}) := c_T(\\mathbf{x}_T) + \\sum_{t=1}^{T-1} c_t(\\mathbf{x}_t,\\mathbf{u}_t) \\\\\n\\text{s.t.}\\quad & G(\\mathbf{z}) = \\begin{bmatrix}\n\\big[\\, g_i(\\mathbf{x}_k,\\mathbf{u}_k) \\big]_{i\\in\\mathscr{E},\\, k\\in K_i} \\\\\n\\big[\\, \\mathbf{x}_{t+1} - \\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t) \\big]_{t=1: T-1} \\\\\n\\mathbf{x}_1 - \\mathbf{x}_\\mathrm{init}\n\\end{bmatrix} = \\mathbf{0}, \\\\\n& H(\\mathbf{z}) = \\big[\\, h_i(\\mathbf{x}_k,\\mathbf{u}_k) \\big]_{i\\in\\mathscr{I},\\, k\\in K_i} \\; \\ge \\; \\mathbf{0},\n\\end{aligned}\n\noptionally with simple bounds \\mathbf{x}_{\\mathrm{lb}} \\le \\mathbf{x}_t \\le \\mathbf{x}_{\\mathrm{ub}} and \\mathbf{u}_{\\mathrm{lb}} \\le \\mathbf{u}_t \\le \\mathbf{u}_{\\mathrm{ub}} folded into H or provided to the solver separately. For notational convenience, some constraints may not depend on \\mathbf{u}_k at times in K_i; the indexing still helps specify when each condition is active.\n\nThis direct transcription is attractive because it is faithful to the model and exposes sparsity. The Jacobian of G has a block bi-diagonal structure induced by the dynamics, and the KKT matrix is sparse and structured—properties exploited by interior-point and SQP methods. The trade-off is size: with state dimension n and control dimension m, the decision vector has (T\\!\\cdot\\!n) + ((T\\!-\n1)\\cdot m) entries, and there are roughly (T\\!-\n1)\\cdot n dynamic equalities plus any path and boundary conditions. Techniques such as partial or full condensing eliminate state variables to reduce the equality set (at the cost of denser matrices), while keeping states explicit preserves sparsity and often improves robustness on long horizons and in the presence of state constraints.\n\nCompared to alternatives, simultaneous methods avoid the long nonlinear dependency chains of single shooting and make it easier to impose state/path constraints. They can, however, demand more memory and per-iteration linear algebra, so practical performance hinges on exploiting sparsity and good initialization.\n\nThe same logic applies when selecting an optimizer. For small-scale problems, it is common to rely on general-purpose routines such as those in scipy.optimize.minimize. Derivative-free methods like Nelder–Mead require no gradients but scale poorly as dimensionality increases. Quasi-Newton schemes such as BFGS work well for moderate dimensions and can approximate gradients by finite differences, while large-scale trajectory optimization often calls for gradient-based constrained solvers such as interior-point or sequential quadratic programming methods that can exploit sparse Jacobians and benefit from automatic differentiation. Stochastic techniques, including genetic algorithms, simulated annealing, or particle swarm optimization, occasionally appear when gradients are unavailable, but their cost grows rapidly with dimension and they are rarely competitive for structured optimal control problems. ### On the Choice of Optimizer\n\nAlthough the code example uses SLSQP, many alternatives exist. `scipy.optimize.minimize` provides a menu of options, and each has implications for speed, robustness, and scalability:\n\n* **Derivative-free methods** such as Nelder–Mead avoid gradients altogether. They are attractive when gradients are unavailable or noisy, but they scale poorly with dimension.\n* **Quasi-Newton methods** like BFGS approximate gradients by finite differences. They work well for moderate-scale problems and often outperform derivative-free schemes when the objective is smooth.\n* **Gradient-based constrained solvers** such as interior-point or SQP methods exploit derivatives—exact or automatic—and are typically the most efficient for large structured problems like trajectory optimization.\n\nBeyond these, **stochastic optimizers** occasionally appear in practice, especially when gradients are unreliable or the loss landscape is rugged. Random search is the simplest example, while genetic algorithms, simulated annealing, and particle swarm optimization introduce mechanisms for global exploration at the cost of significant computational effort.\n\nWhich method to choose depends on the context: problem size, availability of derivatives, and computational resources. When automatic differentiation is accessible, first-order methods like L-BFGS or Adam often dominate, particularly for single-shooting formulations where the objective is smooth and unconstrained except for simple bounds. This is why researchers with a machine learning background tend to gravitate toward these techniques: they integrate seamlessly with existing frameworks and run efficiently on GPUs.  \n### Example: Direct Solution to the Eco-cruise Problem\n\nMany modern vehicles include features that aim to improve energy efficiency without requiring extra effort from the driver. One such feature is Eco-Cruise. Unlike traditional cruise control, which keeps the car at a fixed speed regardless of conditions, Eco-Cruise adjusts speed within small margins to reduce energy consumption. The reasoning is straightforward: holding speed up a hill by applying full throttle uses more energy than allowing the car to slow slightly and regain speed later. Some systems go further by using map data, anticipating slopes and curves to plan ahead. These ideas are no longer experimental; several manufacturers already deploy predictive cruise systems based on navigation input.\n\nThe setup we will use is slightly idealized, but not unrealistic. It assumes that the driver provides a destination and an acceptable time target, something that most navigation systems already require. With that information, the controller can decide how fast to go and when to accelerate while ensuring the trip remains on schedule. Framing the problem in this way allows us to cast Eco-Cruise as a trajectory optimization exercise and to explore the structure of a discrete-time optimal control problem.\n\nConsider a 1 km segment of road that must be completed in exactly 60 seconds. We divide this horizon into 60 steps of one second each. At step $t$, the state consists of the cumulative distance $s_t$ and the speed $v_t$. The control input is the longitudinal acceleration $u_t$. With a time step of one second, the dynamics are written as\n\n$$\ns_{t+1} = s_t + v_t, \\qquad\nv_{t+1} = v_t + u_t.\n$$\n\nThe trip starts from rest, so $s_1 = 0$ and $v_1 = 0$, and it must end at $s_{T+1} = 1000$ m with $v_{T+1} = 0$.\n\nEnergy consumption depends on both acceleration and speed. Rather than model the details of rolling resistance, drivetrain losses, and aerodynamics, we adopt a simple quadratic approximation. Each stage incurs a cost\n\n$$\nc_t(v_t, u_t) = \\tfrac{1}{2}\\beta u_t^2 + \\tfrac{1}{2}\\gamma v_t^2,\n$$\n\nwhere the first term penalizes strong accelerations and the second discourages high cruising speed. Reasonable values are $\\beta = 1.0$ and $\\gamma = 0.1$. The objective is to minimize the sum of these stage costs across the horizon:\n\n$$\n\\min \\sum_{t=1}^{T} \\bigl( \\tfrac{\\beta}{2}u_t^2 + \\tfrac{\\gamma}{2}v_t^2 \\bigr).\n$$\n\nThe optimization must also respect physical limits. Speeds must remain between zero and $20\\ \\text{m/s}$ (about 72 km/h), and accelerations are bounded by $|u_t| \\le 3\\ \\text{m/s}^2$ for comfort and safety.\n\n\nThe complete formulation is\n\n$$\n\\begin{aligned}\n\\min_{\\{s_t,v_t,u_t\\}} \\ & \\sum_{t=1}^{T} \\bigl( \\tfrac{\\beta}{2}u_t^2 + \\tfrac{\\gamma}{2}v_t^2 \\bigr) \\\\\n\\text{subject to}\\ & s_{t+1}-s_t-v_t = 0,\\ \\ v_{t+1}-v_t-u_t = 0,\\ t=1,\\dots,T, \\\\\n& s_1 = 0,\\ v_1 = 0,\\ s_{T+1} = 1000,\\ v_{T+1} = 0, \\\\\n& 0 \\le v_t \\le 20,\\ \\ |u_t|\\le 3.\n\\end{aligned}\n$$\n\n#### Solution\n\nOnce the objective and constraints are expressed as Python functions, the problem can be passed to a generic optimizer with very little extra work. Here is a direct implementation using `scipy.optimize.minimize` with the SLSQP method:\n\n```{code-cell} python\n:tags: [remove-input, remove-output]\n\nimport numpy as np\nimport json\nimport matplotlib.pyplot as plt\n\n# Apply book style\ntry:\n    import scienceplots\n    plt.style.use(['science', 'notebook'])\nexcept (ImportError, OSError):\n    pass  # Use matplotlib defaults\nfrom scipy.optimize import minimize, Bounds\n\ndef solve_eco_cruise(beta=1.0, gamma=0.05, T=60, v_max=20.0, a_max=3.0, distance=1000.0):\n    \"\"\"Solve the eco-cruise optimization problem.\"\"\"\n    \n    n_state, n_control = T + 1, T\n    \n    def unpack(z):\n        s, v, u = z[:n_state], z[n_state:2*n_state], z[2*n_state:]\n        return s, v, u\n\n    def objective(z):\n        _, v, u = unpack(z)\n        return 0.5 * beta * np.sum(u**2) + 0.5 * gamma * np.sum(v[:-1]**2)\n\n    def dynamics(z):\n        s, v, u = unpack(z)\n        ceq = np.empty(2*T)\n        ceq[0::2] = s[1:] - s[:-1] - v[:-1]  # position dynamics\n        ceq[1::2] = v[1:] - v[:-1] - u        # velocity dynamics\n        return ceq\n\n    def boundary(z):\n        s, v, _ = unpack(z)\n        return np.array([s[0], v[0], s[-1]-distance, v[-1]])  # start/end conditions\n\n    # Optimization setup\n    cons = [{'type':'eq', 'fun': dynamics}, {'type':'eq', 'fun': boundary}]\n    bounds = Bounds(\n        lb=np.concatenate([np.full(n_state,-1e4), np.zeros(n_state), np.full(n_control,-a_max)]),\n        ub=np.concatenate([np.full(n_state,1e4), v_max*np.ones(n_state), np.full(n_control,a_max)])\n    )\n\n    # Initial guess: triangular velocity profile\n    accel_time = int(0.3 * T)\n    decel_time = int(0.3 * T)\n    cruise_time = T - accel_time - decel_time\n    peak_v = min(1.2 * distance/T, 0.8 * v_max)\n    \n    v0 = np.zeros(n_state)\n    v0[:accel_time+1] = np.linspace(0, peak_v, accel_time+1)\n    v0[accel_time:accel_time+cruise_time+1] = peak_v\n    v0[accel_time+cruise_time:] = np.linspace(peak_v, 0, decel_time+1)\n    \n    s0 = np.cumsum(np.concatenate([[0], v0[:-1]]))\n    scale = distance / s0[-1]\n    s0, v0 = s0 * scale, v0 * scale\n    u0 = np.diff(v0)\n    \n    z0 = np.concatenate([s0, v0, u0])\n    \n    # Solve optimization\n    print(f\"Solving eco-cruise optimization (β={beta}, γ={gamma})...\")\n    res = minimize(objective, z0, method=\"SLSQP\", bounds=bounds, constraints=cons,\n                   options={\"maxiter\": 1000, \"ftol\": 1e-9})\n    \n    if not res.success:\n        print(f\"Optimization failed: {res.message}\")\n        return None\n        \n    s_opt, v_opt, u_opt = unpack(res.x)\n    \n    # Create trajectory data\n    eco_trajectory = []\n    cumulative_energy = 0\n    \n    for t in range(T + 1):\n        if t < T:\n            stage_cost = 0.5 * beta * u_opt[t]**2 + 0.5 * gamma * v_opt[t]**2\n            cumulative_energy += stage_cost\n        else:\n            stage_cost = 0\n            \n        eco_trajectory.append({\n            \"time\": float(t), \"position\": float(s_opt[t]), \"velocity\": float(v_opt[t]),\n            \"acceleration\": float(u_opt[t]) if t < T else 0.0,\n            \"stageCost\": float(stage_cost), \"cumulativeEnergy\": float(cumulative_energy)\n        })\n    \n    return {\n        \"eco_trajectory\": eco_trajectory,\n        \"total_energy\": float(cumulative_energy),\n        \"optimization_success\": True,\n        \"parameters\": {\"beta\": beta, \"gamma\": gamma, \"T\": T, \"v_max\": v_max, \"a_max\": a_max, \"distance\": distance}\n    }\n\ndef generate_naive_trajectory(T=60, distance=1000.0, gamma=0.05):\n    \"\"\"Generate naive constant-speed trajectory for comparison.\"\"\"\n    \n    # Simple triangular profile: accelerate, cruise, decelerate\n    accel_time = decel_time = 4\n    cruise_time = T - accel_time - decel_time\n    cruise_speed = distance / (0.5 * accel_time + cruise_time + 0.5 * decel_time)\n    \n    naive_trajectory = []\n    cumulative_energy = 0\n    \n    for t in range(T + 1):\n        if t <= accel_time:\n            velocity = (cruise_speed / accel_time) * t\n            acceleration = cruise_speed / accel_time\n        elif t <= accel_time + cruise_time:\n            velocity = cruise_speed\n            acceleration = 0.0\n        else:\n            remaining_time = T - t\n            velocity = (cruise_speed / decel_time) * remaining_time\n            acceleration = -cruise_speed / decel_time\n        \n        # Calculate position by integration\n        position = 0 if t == 0 else naive_trajectory[t-1]['position'] + naive_trajectory[t-1]['velocity']\n        \n        # Calculate costs\n        if t < T:\n            stage_cost = 0.5 * 1.0 * acceleration**2 + 0.5 * gamma * velocity**2\n            cumulative_energy += stage_cost\n        else:\n            stage_cost = 0.0\n            \n        naive_trajectory.append({\n            \"time\": float(t), \"position\": float(position), \"velocity\": float(velocity),\n            \"acceleration\": float(acceleration), \"stageCost\": float(stage_cost),\n            \"cumulativeEnergy\": float(cumulative_energy)\n        })\n    \n    return {\"naive_trajectory\": naive_trajectory, \"total_energy\": float(cumulative_energy)}\n\ndef plot_comparison(eco_data, naive_data=None, save_plot=True):\n    \"\"\"Create visualization plots comparing eco-cruise and naive trajectories.\"\"\"\n    \n    eco_traj = eco_data['eco_trajectory']\n    times = [p['time'] for p in eco_traj]\n    positions = [p['position'] for p in eco_traj]\n    velocities = [p['velocity'] for p in eco_traj]\n    accelerations = [p['acceleration'] for p in eco_traj]\n    energy_costs = [p['stageCost'] for p in eco_traj]\n    cumulative_energy = [p['cumulativeEnergy'] for p in eco_traj]\n    \n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    fig.suptitle('Eco-Cruise vs Naive Trajectory Comparison', fontsize=16, fontweight='bold')\n    \n    # Plot 1: Position vs Time\n    axes[0, 0].plot(times, positions, 'b-', linewidth=2, label='Eco-Cruise')\n    if naive_data:\n        naive_traj = naive_data['naive_trajectory']\n        naive_times = [p['time'] for p in naive_traj]\n        naive_positions = [p['position'] for p in naive_traj]\n        axes[0, 0].plot(naive_times, naive_positions, 'r--', linewidth=2, label='Naive')\n    axes[0, 0].set_xlabel('Time (s)'); axes[0, 0].set_ylabel('Position (m)')\n    axes[0, 0].set_title('Position vs Time'); axes[0, 0].grid(True, alpha=0.3); axes[0, 0].legend()\n    \n    # Plot 2: Velocity vs Time\n    axes[0, 1].plot(times, velocities, 'b-', linewidth=2, label='Eco-Cruise')\n    if naive_data:\n        naive_velocities = [p['velocity'] for p in naive_traj]\n        axes[0, 1].plot(naive_times, naive_velocities, 'r--', linewidth=2, label='Naive')\n    axes[0, 1].set_xlabel('Time (s)'); axes[0, 1].set_ylabel('Velocity (m/s)')\n    axes[0, 1].set_title('Velocity vs Time'); axes[0, 1].grid(True, alpha=0.3); axes[0, 1].legend()\n    \n    # Plot 3: Acceleration vs Time\n    axes[0, 2].plot(times[:-1], accelerations[:-1], 'b-', linewidth=2, label='Eco-Cruise')\n    axes[0, 2].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n    axes[0, 2].set_xlabel('Time (s)'); axes[0, 2].set_ylabel('Acceleration (m/s²)')\n    axes[0, 2].set_title('Acceleration vs Time'); axes[0, 2].grid(True, alpha=0.3); axes[0, 2].legend()\n    \n    # Plot 4: Stage Cost vs Time\n    axes[1, 0].plot(times[:-1], energy_costs[:-1], 'b-', linewidth=2, label='Eco-Cruise')\n    if naive_data:\n        naive_costs = [p['stageCost'] for p in naive_traj[:-1]]\n        axes[1, 0].plot(naive_times[:-1], naive_costs, 'r--', linewidth=2, label='Naive')\n    axes[1, 0].set_xlabel('Time (s)'); axes[1, 0].set_ylabel('Stage Cost')\n    axes[1, 0].set_title('Stage Cost vs Time'); axes[1, 0].grid(True, alpha=0.3); axes[1, 0].legend()\n    \n    # Plot 5: Cumulative Energy vs Time\n    axes[1, 1].plot(times, cumulative_energy, 'b-', linewidth=2, label='Eco-Cruise')\n    if naive_data:\n        naive_cumulative = [p['cumulativeEnergy'] for p in naive_traj]\n        axes[1, 1].plot(naive_times, naive_cumulative, 'r--', linewidth=2, label='Naive')\n    axes[1, 1].set_xlabel('Time (s)'); axes[1, 1].set_ylabel('Cumulative Energy')\n    axes[1, 1].set_title('Cumulative Energy vs Time'); axes[1, 1].grid(True, alpha=0.3); axes[1, 1].legend()\n    \n    # Plot 6: Phase Space (Velocity vs Position)\n    axes[1, 2].plot(positions, velocities, 'b-', linewidth=2, label='Eco-Cruise')\n    if naive_data:\n        naive_positions = [p['position'] for p in naive_traj]\n        axes[1, 2].plot(naive_positions, naive_velocities, 'r--', linewidth=2, label='Naive')\n    axes[1, 2].set_xlabel('Position (m)'); axes[1, 2].set_ylabel('Velocity (m/s)')\n    axes[1, 2].set_title('Phase Space: Velocity vs Position'); axes[1, 2].grid(True, alpha=0.3); axes[1, 2].legend()\n    \n    plt.tight_layout()\n    \n    if save_plot:\n        plt.savefig('_static/eco_cruise_visualization.png', dpi=300, bbox_inches='tight')\n        print(\"Plot saved to _static/eco_cruise_visualization.png\")\n    \n    plt.show()\n    return fig\n\ndef demo():\n    \"\"\"Run complete eco-cruise demonstration with visualization.\"\"\"\n    \n    # Solve optimization\n    eco_data = solve_eco_cruise(beta=1.0, gamma=0.05, T=60, distance=1000.0)\n    if eco_data is None:\n        # Use Jupyter Book's gluing feature for error message\n        try:\n            from myst_nb import glue\n            glue(\"eco_cruise_output\", \"❌ Optimization failed!\", display=False)\n        except ImportError:\n            print(\"Optimization failed!\")\n        return None\n    \n    # Generate naive trajectory\n    naive_data = generate_naive_trajectory(T=60, distance=1000.0, gamma=0.05)\n    \n    # Create visualization\n    fig = plot_comparison(eco_data, naive_data, save_plot=True)\n    \n    # Use Jupyter Book's gluing feature to display the figure\n    try:\n        from myst_nb import glue\n        glue(\"eco_cruise_figure\", fig, display=False)\n    except ImportError:\n        # Fallback for when not running in Jupyter Book context\n        pass\n    \n    return eco_data, naive_data, fig\n\nif __name__ == \"__main__\":\n    demo()\n```\n\n```{glue:figure} eco_cruise_figure\n:figwidth: 100%\n:name: \"fig-eco-cruise\"\n\nEco-Cruise optimization results showing the comparison between energy-efficient and naive trajectory approaches.\n```\n\n``````{tab-set}\n:tags: [full-width]\n\n`````{tab-item} Visualization\n```{raw} html\n<script src=\"_static/iframe-modal.js\"></script>\n<div id=\"eco-cruise-container\"></div>\n<script>\ncreateIframeModal({\n  containerId: 'eco-cruise-container',\n  iframeSrc: '_static/eco-cruise-demo.html',\n  title: 'Eco-Cruise Optimization Visualization',\n  aspectRatio: '200%',\n  maxWidth: '1400px',\n  maxHeight: '900px'\n});\n</script>\n`````\n\n`````{tab-item} Code\n```{literalinclude} code/eco-cruise.py\n:language: python\n```\n`````\n``````\n\nThe function `scipy.optimize.minimize` expects three things: an objective function that returns a scalar cost, a set of constraints grouped as equality or inequality functions, and bounds on individual variables. Everything else is about bookkeeping.\n\nThe first step is to gather all decision variables—positions, speeds, and accelerations—into a single vector $\\mathbf{z}$. Helper routines like `unpack` then slice this vector back into its components so that the rest of the code reads naturally. The objective function mirrors the analytical form of the cost: it sums quadratic penalties on speeds and accelerations across the horizon.\n\nDynamics and boundary conditions appear as equality constraints. Each entry in `dynamics` enforces one of the discrete-time equations\n\n$$\ns_{t+1} - s_t - v_t = 0,\\qquad\nv_{t+1} - v_t - u_t = 0,\n$$\n\nwhile `boundary` pins down the start and end conditions. Together, these ensure that any candidate solution corresponds to a physically consistent trajectory.\n\nBounds serve two purposes: they impose physical limits on speed and acceleration and keep the otherwise unbounded position variables within a large but finite range. This prevents the optimizer from exploring meaningless regions of the search space during intermediate iterations.\n\nFinally, an initial guess is constructed by interpolating a straight line for the position, assigning a constant speed, and setting accelerations to zero. This is not intended to be optimal; it simply gives the solver a feasible starting point close enough to the constraint manifold to converge quickly.\n\nOnce these components are in place, the call to `minimize` does the rest. Internally, SLSQP linearizes the constraints, builds a quadratic subproblem, and iterates until both the Karush–Kuhn–Tucker conditions and the stopping tolerances are met. From the user's perspective, the heavy lifting reduces to providing functions that compute costs and residuals—everything else is handled by the solver. ","type":"content","url":"/trajectories#simultaneous-methods","position":39},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"Sequential Methods","lvl2":"Numerical Methods for Solving DOCPs"},"type":"lvl3","url":"/trajectories#sequential-methods","position":40},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"Sequential Methods","lvl2":"Numerical Methods for Solving DOCPs"},"content":"The previous section showed how a discrete-time optimal control problem can be solved by treating all states and controls as decision variables and enforcing the dynamics as equality constraints. This produces a nonlinear program that can be passed to solvers such as scipy.optimize.minimize with the SLSQP method. For short horizons, this approach is straightforward and works well; the code stays close to the mathematical formulation.\n\nIt also has a real advantage: by keeping the states explicit and imposing the dynamics through constraints, we anchor the trajectory at multiple points. This extra structure helps stabilize the optimization, especially for long horizons where small deviations in early steps can otherwise propagate and cause the optimizer to drift or diverge. In that sense, this formulation is better conditioned and more robust than approaches that treat the dynamics implicitly.\n\nThe drawback is scale. As the horizon grows, the number of variables and constraints grows with it, and all are coupled by the dynamics. Each iteration of a sequential quadratic programming (SQP) or interior-point method requires building and factorizing large Jacobians and Hessians. These methods have been embedded in reinforcement learning and differentiable programming pipelines—through implicit layers or differentiable convex solvers—but the cost is significant. They remain serial, rely on repeated linear algebra factorizations, and are difficult to parallelize efficiently. When thousands of such problems must be solved inside a learning loop, the overhead becomes prohibitive.\n\nThis motivates an alternative that aligns better with the computational model of machine learning. If the dynamics are deterministic and state constraints are absent (or reducible to simple bounds on controls), we can eliminate the equality constraints altogether by making the states implicit. Instead of solving for both states and controls, we fix the initial state and roll the system forward under a candidate control sequence. This is the essence of single shooting.\n\nThe term “shooting” comes from the idea of aiming and firing a trajectory from the initial state: you pick a control sequence, integrate (or step) the system forward, and see where it lands. If the final state misses the target, you adjust the controls and try again: like adjusting the angle of a shot until it hits the mark. It is called single shooting because we compute the entire trajectory in one pass from the starting point, without breaking it into segments. Later, we will contrast this with multiple shooting, where the horizon is divided into smaller arcs that are optimized jointly to improve stability and conditioning.\n\nThe analogy with deep learning is also immediate: the control sequence plays the role of parameters, the rollout is a forward pass, and the cost is a scalar loss. Gradients can be obtained with reverse-mode automatic differentiation. In the single shooting formulation of the DOCP, the constrained program\\min_{\\mathbf{x}_{1:T},\\,\\mathbf{u}_{1:T-1}} J(\\mathbf{x}_{1:T},\\mathbf{u}_{1:T-1})\n\\quad\\text{s.t.}\\quad \n\\mathbf{x}_{t+1}=\\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t)\n\ncollapses to\\min_{\\mathbf{u}_{1:T-1}}\\;\nc_T\\!\\bigl(\\boldsymbol{\\phi}_{T}(\\mathbf{u}, \\mathbf{x}_1)\\bigr)\n+\\sum_{t=1}^{T-1} c_t\\!\\bigl(\\boldsymbol{\\phi}_{t}(\\mathbf{u}, \\mathbf{x}_1), \\mathbf{u}_t\\bigr),\n\\qquad\n\\mathbf{u}_{\\mathrm{lb}}\\le\\mathbf{u}_{t}\\le\\mathbf{u}_{\\mathrm{ub}}.\n\nHere \\boldsymbol{\\phi}_t denotes the state reached at time t by recursively applying the dynamics to the previous state and current control. This recursion can be written as\\boldsymbol{\\phi}_{t+1}(\\mathbf{u},\\mathbf{x}_1)=\n\\mathbf{f}_{t}\\!\\bigl(\\boldsymbol{\\phi}_{t}(\\mathbf{u},\\mathbf{x}_1),\\mathbf{u}_t\\bigr),\\qquad\n\\boldsymbol{\\phi}_{1}=\\mathbf{x}_1.\n\nConcretely, here is JAX-style pseudocode for defining phi(u, x_0, t) using jax.lax.scan with a zero-based time index:def phi(u_seq, x0, t):\n    \"\"\"Return \\phi_t(u, x0) with 0-based t (\\phi_0 = x0).\n\n    u_seq: controls of length T (or T-1); only first t entries are used\n    x0: initial state at time 0\n    t: integer >= 0\n    \"\"\"\n    if t <= 0:\n        return x0\n\n    def step(carry, u):\n        x, t_idx = carry\n        x_next = f(x, u, t_idx)\n        return (x_next, t_idx + 1), None\n\n    (x_t, _), _ = lax.scan(step, (x0, 0), u_seq[:t])\n    return x_t\n\nThe pattern mirrors an RNN unroll: starting from an initial state (\\mathbf{x}^\\star_1) and a sequence of controls (\\mathbf{u}^*_{1:T-1}), we propagate forward through the dynamics, updating the state at each step and accumulating cost along the way. This structural similarity is why single shooting often feels natural to practitioners with a deep learning background: the rollout is a forward pass, and gradients propagate backward through time exactly as in backpropagation through an RNN.\n\nAlgorithmically:\n\nSingle Shooting: Forward Unroll\n\nInputs: Initial state \\mathbf{x}_1, horizon T, control bounds \\mathbf{u}_{\\mathrm{lb}}, \\mathbf{u}_{\\mathrm{ub}}, dynamics \\mathbf{f}_t, costs c_t\n\nOutput: Optimal control sequence \\mathbf{u}^*_{1:T-1}\n\nInitialize \\mathbf{u}_{1:T-1} within bounds\n\nDefine ComputeTrajectoryAndCost(\\mathbf{u}, \\mathbf{x}_1):\n\n\\mathbf{x} \\leftarrow \\mathbf{x}_1, J \\leftarrow 0\n\nFor t = 1 to T-1:\n\nJ \\leftarrow J + c_t(\\mathbf{x}, \\mathbf{u}_t)\n\n\\mathbf{x} \\leftarrow \\mathbf{f}_t(\\mathbf{x}, \\mathbf{u}_t)\n\nJ \\leftarrow J + c_T(\\mathbf{x})\n\nReturn J\n\nSolve \\min_{\\mathbf{u}} J(\\mathbf{u}) subject to \\mathbf{u}_{\\mathrm{lb}} \\le \\mathbf{u}_t \\le \\mathbf{u}_{\\mathrm{ub}}\n\nReturn \\mathbf{u}^*_{1:T-1}\n\nIn JAX or PyTorch, this loop can be JIT-compiled and differentiated automatically. Any gradient-based optimizer—L-BFGS, Adam, even SGD—can be applied, making the pipeline look very much like training a neural network. In effect, we are “backpropagating through the world model” when computing \\nabla J(\\mathbf{u}).\n\nSingle shooting is attractive for its simplicity and compatibility with differentiable programming, but it has limitations. The absence of intermediate constraints makes it sensitive to initialization and prone to numerical instability over long horizons. When state constraints or robustness matter, formulations that keep states explicit—such as multiple shooting or collocation—become preferable. These trade-offs are the focus of the next section.\n\n#  label: fig-ocp-single-shooting\n#  caption: Single-shooting EV example: the plot shows optimal state trajectories (battery charge and speed) plus the control sequence, while the console reports the optimized control inputs.\n\n%config InlineBackend.figure_format = 'retina'\nimport jax\nimport jax.numpy as jnp\nfrom jax import grad, jit\nfrom jax.example_libraries import optimizers\nimport matplotlib.pyplot as plt\n\n# Apply book style\ntry:\n    import scienceplots\n    plt.style.use(['science', 'notebook'])\nexcept (ImportError, OSError):\n    pass  # Use matplotlib defaults\n\ndef single_shooting_ev_optimization(T=20, num_iterations=1000, step_size=0.01):\n    \"\"\"\n    Implements the single shooting method for the electric vehicle energy optimization problem.\n    \n    Args:\n    T: time horizon\n    num_iterations: number of optimization iterations\n    step_size: step size for the optimizer\n    \n    Returns:\n    optimal_u: optimal control sequence\n    \"\"\"\n    \n    def f(x, u, t):\n        return jnp.array([\n            x[0] + 0.1 * x[1] + 0.05 * u,\n            x[1] + 0.1 * u\n        ])\n    \n    def c(x, u, t):\n        if t == T:\n            return x[0]**2 + x[1]**2\n        else:\n            return 0.1 * (x[0]**2 + x[1]**2 + u**2)\n    \n    def compute_trajectory_and_cost(u, x1):\n        x = x1\n        total_cost = 0\n        for t in range(1, T):\n            total_cost += c(x, u[t-1], t)\n            x = f(x, u[t-1], t)\n        total_cost += c(x, 0.0, T)  # No control at final step\n        return total_cost\n    \n    def objective(u):\n        return compute_trajectory_and_cost(u, x1)\n    \n    def clip_controls(u):\n        return jnp.clip(u, -1.0, 1.0)\n    \n    x1 = jnp.array([1.0, 0.0])  # Initial state: full battery, zero speed\n    \n    # Initialize controls\n    u_init = jnp.zeros(T-1)\n    \n    # Setup optimizer\n    optimizer = optimizers.adam(step_size)\n    opt_init, opt_update, get_params = optimizer\n    opt_state = opt_init(u_init)\n    \n    @jit\n    def step(i, opt_state):\n        u = get_params(opt_state)\n        value, grads = jax.value_and_grad(objective)(u)\n        opt_state = opt_update(i, grads, opt_state)\n        u = get_params(opt_state)\n        u = clip_controls(u)\n        opt_state = opt_init(u)\n        return value, opt_state\n    \n    # Run optimization\n    for i in range(num_iterations):\n        value, opt_state = step(i, opt_state)\n        if i % 100 == 0:\n            print(f\"Iteration {i}, Cost: {value}\")\n    \n    optimal_u = get_params(opt_state)\n    return optimal_u\n\ndef plot_results(optimal_u, T):\n    # Compute state trajectory\n    x1 = jnp.array([1.0, 0.0])\n    x_trajectory = [x1]\n    for t in range(T-1):\n        x_next = jnp.array([\n            x_trajectory[-1][0] + 0.1 * x_trajectory[-1][1] + 0.05 * optimal_u[t],\n            x_trajectory[-1][1] + 0.1 * optimal_u[t]\n        ])\n        x_trajectory.append(x_next)\n    x_trajectory = jnp.array(x_trajectory)\n\n    time = jnp.arange(T)\n    \n    plt.figure(figsize=(12, 8))\n    \n    plt.subplot(2, 1, 1)\n    plt.plot(time, x_trajectory[:, 0], label='Battery State of Charge')\n    plt.plot(time, x_trajectory[:, 1], label='Vehicle Speed')\n    plt.xlabel('Time Step')\n    plt.ylabel('State Value')\n    plt.title('Optimal State Trajectories')\n    plt.legend()\n    plt.grid(True)\n    \n    plt.subplot(2, 1, 2)\n    plt.plot(time[:-1], optimal_u, label='Motor Power Input')\n    plt.xlabel('Time Step')\n    plt.ylabel('Control Input')\n    plt.title('Optimal Control Inputs')\n    plt.legend()\n    plt.grid(True)\n    \n    plt.tight_layout()\n\n# Run the optimization\noptimal_u = single_shooting_ev_optimization()\nprint(\"Optimal control sequence:\", optimal_u)\n\n# Plot the results\nplot_results(optimal_u, T=20)\n\n","type":"content","url":"/trajectories#sequential-methods","position":41},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"In Between Sequential and Simultaneous","lvl2":"Numerical Methods for Solving DOCPs"},"type":"lvl3","url":"/trajectories#in-between-sequential-and-simultaneous","position":42},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"In Between Sequential and Simultaneous","lvl2":"Numerical Methods for Solving DOCPs"},"content":"The two formulations we have seen so far lie at opposite ends. The full discretization approach keeps every state explicit and enforces the dynamics through equality constraints, which makes the structure clear but leads to a large optimization problem. At the other end, single shooting removes these constraints by simulating forward from the initial state, leaving only the controls as decision variables. That makes the problem smaller, but it also introduces a long and highly nonlinear dependency from the first control to the last state.\n\nMultiple shooting sits in between. Instead of simulating the entire horizon in one shot, we divide it into smaller segments. For each segment, we keep its starting state as a decision variable and propagate forward using the dynamics for that segment. At the end, we enforce continuity by requiring that the simulated end state of one segment matches the decision variable for the next.\n\nFormally, suppose the horizon of T steps is divided into K segments of length L (with T = K \\cdot L for simplicity). We introduce:\n\nThe controls for each step: \\mathbf{u}_{1:T-1}.\n\nThe state at the start of each segment: \\mathbf{x}_1,\\dots,\\mathbf{x}_K.\n\nGiven \\mathbf{x}_k and the controls in its segment, we compute the predicted terminal state by simulating forward:\\hat{\\mathbf{x}}_{k+1} = \\Phi(\\mathbf{x}_k,\\mathbf{u}_{\\text{segment }k}),\n\nwhere \\Phi represents L applications of the dynamics. Continuity constraints enforce:\\mathbf{x}_{k+1} - \\hat{\\mathbf{x}}_{k+1} = 0, \\qquad k=1,\\dots,K-1.\n\nThe resulting nonlinear program looks like this:\\begin{aligned}\n\\min_{\\{\\mathbf{x}_k,\\mathbf{u}_t\\}} \\quad &\nc_T(\\mathbf{x}_T) + \\sum_{t=1}^{T-1} c_t(\\mathbf{x}_t,\\mathbf{u}_t) \\\\\n\\text{subject to} \\quad &\n\\mathbf{x}_{k+1} - \\Phi(\\mathbf{x}_k,\\mathbf{u}_{\\text{segment }k}) = 0,\\quad k = 1,\\dots,K-1, \\\\\n& \\mathbf{u}_{\\mathrm{lb}} \\le \\mathbf{u}_t \\le \\mathbf{u}_{\\mathrm{ub}}, \\\\\n& \\text{boundary conditions on } \\mathbf{x}_1 \\text{ and } \\mathbf{x}_K.\n\\end{aligned}\n\nCompared to the full NLP, we no longer introduce every intermediate state as a variable—only the anchors at segment boundaries. Inside each segment, states are reconstructed by simulation. Compared to single shooting, these anchors break the long dependency chain that makes optimization unstable: gradients only have to travel across L steps before they hit a decision variable, rather than the entire horizon. This is the same reason why exploding or vanishing gradients appear in deep recurrent networks: when the chain is too long, information either dies out or blows up. Multiple shooting shortens the chain and improves conditioning.\n\nBy adjusting the number of segments K, we can interpolate between the two extremes: K = 1 gives single shooting, while K = T recovers the full direct NLP. In practice, a moderate number of segments often strikes a good balance between robustness and complexity.\n\n#  label: fig-ocp-multiple-shooting\n#  caption: Multiple shooting ballistic BVP: the code produces an animation (and optional static plot) that shows how segment defects shrink while steering the projectile to the target.\n\n%config InlineBackend.figure_format = 'retina'\n\"\"\"\nMultiple Shooting as a Boundary-Value Problem (BVP) for a Ballistic Trajectory\n-----------------------------------------------------------------------------\nWe solve for the initial velocities (and total flight time) so that the terminal\nposition hits a target, enforcing continuity between shooting segments.\n\"\"\"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Apply book style\ntry:\n    import scienceplots\n    plt.style.use(['science', 'notebook'])\nexcept (ImportError, OSError):\n    pass  # Use matplotlib defaults\nfrom scipy.integrate import solve_ivp\nfrom scipy.optimize import minimize\nfrom IPython.display import HTML, display\n\n# -----------------------------\n# Physical parameters\n# -----------------------------\ng = 9.81          # gravity (m/s^2)\nm = 1.0           # mass (kg)\ndrag_coeff = 0.1  # quadratic drag coefficient\n\n\ndef dynamics(t, state):\n    \"\"\"Ballistic dynamics with quadratic drag. state = [x, y, vx, vy].\"\"\"\n    x, y, vx, vy = state\n    v = np.hypot(vx, vy)\n    drag_x = -drag_coeff * v * vx / m if v > 0 else 0.0\n    drag_y = -drag_coeff * v * vy / m if v > 0 else 0.0\n    dx  = vx\n    dy  = vy\n    dvx = drag_x\n    dvy = drag_y - g\n    return np.array([dx, dy, dvx, dvy])\n\n\ndef flow(y0, h):\n    \"\"\"One-segment flow map Φ(y0; h): integrate dynamics over duration h.\"\"\"\n    sol = solve_ivp(dynamics, (0.0, h), y0, method=\"RK45\", rtol=1e-7, atol=1e-9)\n    return sol.y[:, -1], sol\n\n# -----------------------------\n# Multiple-shooting BVP residuals\n# -----------------------------\n\ndef residuals(z, K, x_init, x_target):\n    \"\"\"\n    Unknowns z = [vx0, vy0, H, y1(4), y2(4), ..., y_{K-1}(4)]  (total len = 3 + 4*(K-1))\n    We define y0 from x_init and (vx0, vy0). Each segment has duration h = H/K.\n    Residual vector stacks:\n      - initial position constraints: y0[:2] - x_init[:2]\n      - continuity: y_{k+1} - Φ(y_k; h) for k=0..K-2\n      - terminal position constraint at end of last segment: Φ(y_{K-1}; h)[:2] - x_target[:2]\n    \"\"\"\n    n = 4\n    vx0, vy0, H = z[0], z[1], z[2]\n    if H <= 0:\n        # Strongly penalize nonpositive durations to keep solver away\n        return 1e6 * np.ones(2 + 4*(K-1) + 2)\n\n    h = H / K\n\n    # Build list of segment initial states y_0..y_{K-1}\n    ys = []\n    y0 = np.array([x_init[0], x_init[1], vx0, vy0], dtype=float)\n    ys.append(y0)\n    if K > 1:\n        rest = z[3:]\n        y_internals = rest.reshape(K-1, n)\n        ys.extend(list(y_internals))  # y1..y_{K-1}\n\n    res = []\n\n    # Initial position must match exactly\n    res.extend(ys[0][:2] - x_init[:2])\n\n    # Continuity across segments\n    for k in range(K-1):\n        yk = ys[k]\n        yk1_pred, _ = flow(yk, h)\n        res.extend(ys[k+1] - yk1_pred)\n\n    # Terminal position at the end of last segment equals target\n    y_last_end, _ = flow(ys[-1], h)\n    res.extend(y_last_end[:2] - x_target[:2])\n\n    # Optional soft \"stay above ground\" at knots (kept gentle)\n    # res.extend(np.minimum(0.0, np.array([y[1] for y in ys])).ravel())\n\n    return np.asarray(res)\n\n# -----------------------------\n# Solve BVP via optimization on 0.5*||residuals||^2\n# -----------------------------\n\ndef solve_bvp_multiple_shooting(K=5, x_init=np.array([0., 0.]), x_target=np.array([10., 0.])):\n    \"\"\"\n    K: number of shooting segments.\n    x_init: initial position (x0, y0). Initial velocities are unknown.\n    x_target: desired terminal position (xT, yT) at time H (unknown).\n    \"\"\"\n    # Heuristic initial guesses:\n    dx = x_target[0] - x_init[0]\n    dy = x_target[1] - x_init[1]\n    H0 = max(0.5, dx / 5.0)  # guess ~ 5 m/s horizontal\n    vx0_0 = dx / H0\n    vy0_0 = (dy + 0.5 * g * H0**2) / H0  # vacuum guess\n\n    # Intentionally disconnected internal knots to visualize defect shrinkage\n    internals = []\n    for k in range(1, K):  # y1..y_{K-1}\n        xk = x_init[0] + (dx * k) / K\n        yk = x_init[1] + (dy * k) / K + 2.0  # offset to create mismatch\n        internals.append(np.array([xk, yk, 0.0, 0.0]))\n    internals = np.array(internals) if K > 1 else np.array([])\n\n    z0 = np.concatenate(([vx0_0, vy0_0, H0], internals.ravel()))\n\n    # Variable bounds: H > 0, keep velocities within a reasonable range\n    # Use wide bounds to let the solver work; tune if needed.\n    lb = np.full_like(z0, -np.inf, dtype=float)\n    ub = np.full_like(z0,  np.inf, dtype=float)\n    lb[2] = 1e-2  # H lower bound\n    # Optional velocity bounds\n    lb[0], ub[0] = -50.0, 50.0\n    lb[1], ub[1] = -50.0, 50.0\n\n    # Objective and callback for L-BFGS-B\n    def objective(z):\n        r = residuals(z, K,\n                      np.array([x_init[0], x_init[1], 0., 0.]),\n                      np.array([x_target[0], x_target[1], 0., 0.]))\n        return 0.5 * np.dot(r, r)\n\n    iterate_history = []\n    def cb(z):\n        iterate_history.append(z.copy())\n\n    bounds = list(zip(lb.tolist(), ub.tolist()))\n    sol = minimize(objective, z0, method='L-BFGS-B', bounds=bounds,\n                   callback=cb, options={'maxiter': 300, 'ftol': 1e-12})\n\n    return sol, iterate_history\n\n# -----------------------------\n# Reconstruct and plot (optional static figure)\n# -----------------------------\n\ndef reconstruct_and_plot(sol, K, x_init, x_target):\n    n = 4\n    vx0, vy0, H = sol.x[0], sol.x[1], sol.x[2]\n    h = H / K\n\n    ys = []\n    y0 = np.array([x_init[0], x_init[1], vx0, vy0])\n    ys.append(y0)\n    if K > 1:\n        internals = sol.x[3:].reshape(K-1, n)\n        ys.extend(list(internals))\n\n    # Integrate each segment and stitch\n    traj_x, traj_y = [], []\n    for k in range(K):\n        yk = ys[k]\n        yend, seg = flow(yk, h)\n        traj_x.extend(seg.y[0, :].tolist() if k == 0 else seg.y[0, 1:].tolist())\n        traj_y.extend(seg.y[1, :].tolist() if k == 0 else seg.y[1, 1:].tolist())\n\n    # Plot\n    fig, ax = plt.subplots(figsize=(7, 4.2))\n    ax.plot(traj_x, traj_y, '-', label='Multiple-shooting solution')\n    ax.plot([x_init[0]], [x_init[1]], 'go', label='Start')\n    ax.plot([x_target[0]], [x_target[1]], 'r*', ms=12, label='Target')\n    total_pts = len(traj_x)\n    for k in range(1, K):\n        idx = int(k * total_pts / K)\n        ax.axvline(traj_x[idx], color='k', ls='--', alpha=0.3, lw=1)\n\n    ax.set_xlabel('x (m)')\n    ax.set_ylabel('y (m)')\n    ax.set_title(f'Multiple Shooting BVP (K={K})   H={H:.3f}s   v0=({vx0:.2f},{vy0:.2f}) m/s')\n    ax.grid(True, alpha=0.3)\n    ax.legend(loc='best')\n    plt.tight_layout()\n\n    # Report residual norms\n    res = residuals(sol.x, K, np.array([x_init[0], x_init[1], 0., 0.]), np.array([x_target[0], x_target[1], 0., 0.]))\n    print(f\"\\nFinal residual norm: {np.linalg.norm(res):.3e}\")\n    print(f\"vx0={vx0:.4f} m/s, vy0={vy0:.4f} m/s, H={H:.4f} s\")\n\n# -----------------------------\n# Create JS animation for notebooks\n# -----------------------------\n\ndef create_animation_progress(iter_history, K, x_init, x_target):\n    \"\"\"Return a JS animation (to_jshtml) showing defect shrinkage across segments.\"\"\"\n    import matplotlib.pyplot as plt\n\n# Apply book style\ntry:\n    import scienceplots\n    plt.style.use(['science', 'notebook'])\nexcept (ImportError, OSError):\n    pass  # Use matplotlib defaults\n    from matplotlib.animation import FuncAnimation\n\n    n = 4\n\n    def unpack(z):\n        vx0, vy0, H = z[0], z[1], z[2]\n        ys = [np.array([x_init[0], x_init[1], vx0, vy0])]\n        if K > 1 and len(z) > 3:\n            internals = z[3:].reshape(K-1, n)\n            ys.extend(list(internals))\n        return H, ys\n\n    fig, ax = plt.subplots(figsize=(7, 4.2))\n    ax.set_xlabel('Segment index (normalized time)')\n    ax.set_ylabel('y (m)')\n    ax.set_title('Multiple Shooting: Defect Shrinkage (Fixed Boundaries)')\n    ax.grid(True, alpha=0.3)\n\n    # Start/target markers at fixed indices\n    ax.plot([0], [x_init[1]], 'go', label='Start')\n    ax.plot([K], [x_target[1]], 'r*', ms=12, label='Target')\n    # Vertical dashed lines at boundaries\n    for k in range(1, K):\n        ax.axvline(k, color='k', ls='--', alpha=0.35, lw=1)\n    ax.legend(loc='best')\n\n    # Pre-create line artists\n    colors = plt.cm.plasma(np.linspace(0, 1, K))\n    segment_lines = [ax.plot([], [], '-', color=colors[k], lw=2, alpha=0.9)[0] for k in range(K)]\n    connector_lines = [ax.plot([], [], 'r-', lw=1.4, alpha=0.75)[0] for _ in range(K-1)]\n\n    text_iter = ax.text(0.02, 0.98, '', transform=ax.transAxes,\n                        va='top', fontsize=9,\n                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n\n    def animate(i):\n        idx = min(i, len(iter_history)-1)\n        z = iter_history[idx]\n        H, ys = unpack(z)\n        h = H / K\n\n        all_y = [x_init[1], x_target[1]]\n        total_defect = 0.0\n        for k in range(K):\n            yk = ys[k]\n            yend, seg = flow(yk, h)\n            # Map local time to [k, k+1]\n            t_local = seg.t\n            x_vals = k + (t_local / t_local[-1])\n            y_vals = seg.y[1, :]\n            segment_lines[k].set_data(x_vals, y_vals)\n            all_y.extend(y_vals.tolist())\n            if k < K-1:\n                y_next = ys[k+1]\n                # Vertical connector at boundary x=k+1\n                connector_lines[k].set_data([k+1, k+1], [yend[1], y_next[1]])\n                total_defect += abs(y_next[1] - yend[1])\n\n        # Fixed x-limits in index space\n        ax.set_xlim(-0.1, K + 0.1)\n        ymin, ymax = min(all_y), max(all_y)\n        margin_y = 0.10 * max(1.0, ymax - ymin)\n        ax.set_ylim(ymin - margin_y, ymax + margin_y)\n\n        text_iter.set_text(f'Iterate {idx+1}/{len(iter_history)}  |  Sum vertical defect: {total_defect:.3e}')\n        return segment_lines + connector_lines + [text_iter]\n\n    anim = FuncAnimation(fig, animate, frames=len(iter_history), interval=600, blit=False, repeat=True)\n    plt.tight_layout()\n    js_anim = anim.to_jshtml()\n    plt.close(fig)\n    return js_anim\n\n\ndef main():\n    # Problem definition\n    x_init = np.array([0.0, 0.0])      # start at origin\n    x_target = np.array([10.0, 0.0])   # hit ground at x=10 m\n    K = 6                               # number of shooting segments\n\n    sol, iter_hist = solve_bvp_multiple_shooting(K=K, x_init=x_init, x_target=x_target)\n    # Optionally show static reconstruction (commented for docs cleanliness)\n    # reconstruct_and_plot(sol, K, x_init, x_target)\n\n    # Animate progression (defect shrinkage across segments) and display as JS\n    js_anim = create_animation_progress(iter_hist, K, x_init, x_target)\n    display(HTML(js_anim))\n\n\nif __name__ == \"__main__\":\n    main()\n\n","type":"content","url":"/trajectories#in-between-sequential-and-simultaneous","position":43},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl2":"The Discrete-Time Pontryagin Principle"},"type":"lvl2","url":"/trajectories#the-discrete-time-pontryagin-principle","position":44},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl2":"The Discrete-Time Pontryagin Principle"},"content":"If we take the Bolza formulation of the DOCP and apply the KKT conditions directly, we obtain an optimization system with many multipliers and constraints. Written in raw form, it looks like any other nonlinear program. But in control, this structure has a long history and a name of its own: the Pontryagin principle. In fact, the discrete-time version can be seen as the structured KKT system that results from introducing multipliers for the dynamics and collecting terms stage by stage.\n\nWe work with the Bolza program\\begin{aligned}\n\\min_{\\{\\mathbf{x}_t,\\mathbf{u}_t\\}} \\quad & c_T(\\mathbf{x}_T)\\;+\\;\\sum_{t=1}^{T-1} c_t(\\mathbf{x}_t,\\mathbf{u}_t) \\\\\n\\text{s.t.}\\quad & \\mathbf{x}_{t+1}=\\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t),\\quad t=1,\\dots,T-1,\\\\\n& \\mathbf{g}_t(\\mathbf{x}_t,\\mathbf{u}_t)\\le \\mathbf{0},\\quad \\mathbf{u}_t\\in \\mathcal{U}_t,\\\\\n& \\mathbf{h}(\\mathbf{x}_T)=\\mathbf{0}\\quad\\text{(optional terminal equalities)}.\n\\end{aligned}\n\nIntroduce costates \\boldsymbol{\\lambda}_{t+1}\\in\\mathbb{R}^n for the dynamics, multipliers \\boldsymbol{\\mu}_t\\ge \\mathbf{0} for path inequalities, and \\boldsymbol{\\nu} for terminal equalities. The Lagrangian is\\mathcal{L}\n= c_T(\\mathbf{x}_T) + \\sum_{t=1}^{T-1} c_t(\\mathbf{x}_t,\\mathbf{u}_t)\n+ \\sum_{t=1}^{T-1} \\boldsymbol{\\lambda}_{t+1}^\\top\\!\\big(\\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t)-\\mathbf{x}_{t+1}\\big)\n+ \\sum_{t=1}^{T-1} \\boldsymbol{\\mu}_t^\\top \\mathbf{g}_t(\\mathbf{x}_t,\\mathbf{u}_t)\n+ \\boldsymbol{\\nu}^\\top \\mathbf{h}(\\mathbf{x}_T).\n\nIt is convenient to package the stagewise terms in a HamiltonianH_t(\\mathbf{x}_t,\\mathbf{u}_t,\\boldsymbol{\\lambda}_{t+1},\\boldsymbol{\\mu}_t)\n:= c_t(\\mathbf{x}_t,\\mathbf{u}_t)\n+ \\boldsymbol{\\lambda}_{t+1}^\\top \\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t)\n+ \\boldsymbol{\\mu}_t^\\top \\mathbf{g}_t(\\mathbf{x}_t,\\mathbf{u}_t).\n\nThen\\mathcal{L} = c_T(\\mathbf{x}_T)+\\boldsymbol{\\nu}^\\top \\mathbf{h}(\\mathbf{x}_T)\n+ \\sum_{t=1}^{T-1}\\Big[H_t(\\mathbf{x}_t,\\mathbf{u}_t,\\boldsymbol{\\lambda}_{t+1},\\boldsymbol{\\mu}_t)\n- \\boldsymbol{\\lambda}_{t+1}^\\top \\mathbf{x}_{t+1}\\Big].","type":"content","url":"/trajectories#the-discrete-time-pontryagin-principle","position":45},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"Necessary conditions","lvl2":"The Discrete-Time Pontryagin Principle"},"type":"lvl3","url":"/trajectories#necessary-conditions","position":46},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"Necessary conditions","lvl2":"The Discrete-Time Pontryagin Principle"},"content":"Gradient convention\n\nThroughout this section, we use the denominator layout (gradient layout) convention:\n\n\\nabla_{\\mathbf{x}} f(\\mathbf{x}) produces a column vector (gradient)\n\n\\frac{\\partial f}{\\partial \\mathbf{x}} produces the Jacobian matrix\n\nFor scalar functions: \\nabla_{\\mathbf{x}} f = \\left(\\frac{\\partial f}{\\partial \\mathbf{x}}\\right)^\\top\n\nThis is the standard convention in optimization and control theory.\n\nTaking first-order variations and collecting terms gives the discrete-time adjoint system, control stationarity, and complementarity. At a local minimum \\{\\mathbf{x}_t^\\star,\\mathbf{u}_t^\\star\\} with multipliers \\{\\boldsymbol{\\lambda}_t^\\star,\\boldsymbol{\\mu}_t^\\star,\\boldsymbol{\\nu}^\\star\\}:\n\nState dynamics (primal feasibility)\\mathbf{x}_{t+1}^\\star=\\mathbf{f}_t(\\mathbf{x}_t^\\star,\\mathbf{u}_t^\\star),\\quad t=1,\\dots,T-1.\n\nCostate recursion (backward “adjoint” equation)\\boldsymbol{\\lambda}_t^\\star\n= \\nabla_{\\mathbf{x}} H_t\\big(\\mathbf{x}_t^\\star,\\mathbf{u}_t^\\star,\\boldsymbol{\\lambda}_{t+1}^\\star,\\boldsymbol{\\mu}_t^\\star\\big)\n= \\nabla_{\\mathbf{x}} c_t(\\mathbf{x}_t^\\star,\\mathbf{u}_t^\\star)\n+ \\big[\\nabla_{\\mathbf{x}} \\mathbf{f}_t(\\mathbf{x}_t^\\star,\\mathbf{u}_t^\\star)\\big]^\\top \\boldsymbol{\\lambda}_{t+1}^\\star\n+ \\big[\\nabla_{\\mathbf{x}} \\mathbf{g}_t(\\mathbf{x}_t^\\star,\\mathbf{u}_t^\\star)\\big]^\\top \\boldsymbol{\\mu}_t^\\star,\n\nwith the terminal condition\\boldsymbol{\\lambda}_T^\\star\n= \\nabla_{\\mathbf{x}} c_T(\\mathbf{x}_T^\\star) + \\big[\\nabla_{\\mathbf{x}} \\mathbf{h}(\\mathbf{x}_T^\\star)\\big]^\\top \\boldsymbol{\\nu}^\\star\n\\quad\\text{(and \\(\\boldsymbol{\\nu}^\\star=\\mathbf{0}\\) if there are no terminal equalities).}\n\nControl stationarity (first-order optimality in \\mathbf{u}_t)\nIf \\mathcal{U}_t=\\mathbb{R}^m (no explicit set constraint), then\\nabla_{\\mathbf{u}} H_t\\big(\\mathbf{x}_t^\\star,\\mathbf{u}_t^\\star,\\boldsymbol{\\lambda}_{t+1}^\\star,\\boldsymbol{\\mu}_t^\\star\\big)=\\mathbf{0}.\n\nIf \\mathcal{U}_t imposes bounds or a convex set, the condition becomes the variational inequality\\mathbf{0}\\in \\nabla_{\\mathbf{u}} H_t(\\cdot)\\;+\\;N_{\\mathcal{U}_t}(\\mathbf{u}_t^\\star),\n\nwhere N_{\\mathcal{U}_t}(\\cdot) is the normal cone to \\mathcal{U}_t. For simple box bounds, this reduces to standard KKT sign and complementarity conditions on the components of \\mathbf{u}_t^\\star.\n\nPath-constraint multipliers (primal/dual feasibility and complementarity)\\mathbf{g}_t(\\mathbf{x}_t^\\star,\\mathbf{u}_t^\\star)\\le \\mathbf{0},\\quad\n\\boldsymbol{\\mu}_t^\\star\\ge \\mathbf{0},\\quad\n\\mu_{t,i}^\\star\\, g_{t,i}(\\mathbf{x}_t^\\star,\\mathbf{u}_t^\\star)=0\\quad \\text{for all }i,t.\n\nTerminal equalities (if present)\\mathbf{h}(\\mathbf{x}_T^\\star)=\\mathbf{0}.\n\nThe triplet “forward state, backward costate, control stationarity” is the discrete-time Euler–Lagrange system tailored to control with dynamics. It is the same KKT logic as before, but organized stagewise through the Hamiltonian.\n\nDiscrete-time Pontryagin necessary conditions (summary)\n\nAt a local minimum of the DOCP\\min_{\\{\\mathbf{x}_t,\\mathbf{u}_t\\}}\\ c_T(\\mathbf{x}_T)+\\sum_{t=1}^{T-1} c_t(\\mathbf{x}_t,\\mathbf{u}_t)\n\\quad\\text{s.t.}\\quad \\mathbf{x}_{t+1}=\\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t),\\ \\mathbf{g}_t(\\mathbf{x}_t,\\mathbf{u}_t)\\le \\mathbf{0},\\ \\mathbf{h}(\\mathbf{x}_T)=\\mathbf{0},\n\nthere exist multipliers \\{\\boldsymbol{\\lambda}_{t+1}\\}, \\{\\boldsymbol{\\mu}_t\\ge\\mathbf{0}\\}, and (if present) \\boldsymbol{\\nu} such that, for t=1,\\dots,T-1:\n\nState dynamics: \\ \\mathbf{x}_{t+1}=\\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t).\n\nBackward costate recursion:\\boldsymbol{\\lambda}_t = \\nabla_{\\mathbf{x}} c_t(\\mathbf{x}_t,\\mathbf{u}_t)\n  + \\big[\\nabla_{\\mathbf{x}} \\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t)\\big]^\\top \\boldsymbol{\\lambda}_{t+1}\n  + \\big[\\nabla_{\\mathbf{x}} \\mathbf{g}_t(\\mathbf{x}_t,\\mathbf{u}_t)\\big]^\\top \\boldsymbol{\\mu}_t.\n\nTerminal condition: \\ \\boldsymbol{\\lambda}_T = \\nabla_{\\mathbf{x}} c_T(\\mathbf{x}_T) + \\big[\\nabla_{\\mathbf{x}} \\mathbf{h}(\\mathbf{x}_T)\\big]^\\top \\boldsymbol{\\nu}.\n\nControl stationarity (unconstrained control): \\ \\nabla_{\\mathbf{u}} H_t(\\cdot)=\\mathbf{0}; with a convex control set \\mathcal{U}_t, \\ \\mathbf{0}\\in \\nabla_{\\mathbf{u}} H_t(\\cdot)+N_{\\mathcal{U}_t}(\\mathbf{u}_t).\n\nPath inequalities: \\ \\mathbf{g}_t(\\mathbf{x}_t,\\mathbf{u}_t)\\le \\mathbf{0}, \\ \\boldsymbol{\\mu}_t\\ge\\mathbf{0}, and complementarity \\ \\mu_{t,i}\\,g_{t,i}(\\mathbf{x}_t,\\mathbf{u}_t)=0 for all i.\n\nTerminal equalities (if present): \\ \\mathbf{h}(\\mathbf{x}_T)=\\mathbf{0}.\n\nHere H_t(\\mathbf{x}_t,\\mathbf{u}_t,\\boldsymbol{\\lambda}_{t+1},\\boldsymbol{\\mu}_t):=c_t(\\mathbf{x}_t,\\mathbf{u}_t)+\\boldsymbol{\\lambda}_{t+1}^\\top\\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t)+\\boldsymbol{\\mu}_t^\\top\\mathbf{g}_t(\\mathbf{x}_t,\\mathbf{u}_t) is the stage Hamiltonian.","type":"content","url":"/trajectories#necessary-conditions","position":47},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"The adjoint equation as reverse accumulation","lvl2":"The Discrete-Time Pontryagin Principle"},"type":"lvl3","url":"/trajectories#the-adjoint-equation-as-reverse-accumulation","position":48},{"hierarchy":{"lvl1":"Discrete-Time Trajectory Optimization","lvl3":"The adjoint equation as reverse accumulation","lvl2":"The Discrete-Time Pontryagin Principle"},"content":"Optimization needs sensitivities. In trajectory problems we adjust decisions—controls or parameters—to reduce an objective while respecting dynamics and constraints. First‑order methods in the unconstrained case (e.g., gradient descent, L‑BFGS, Adam) require the gradient of the objective with respect to all controls, and constrained methods (SQP, interior‑point) require gradients of the Lagrangian, i.e., of costs and constraints. The discrete‑time adjoint equations provide these derivatives in a way that scales to long horizons and many decision variables.\n\nConsiderJ = c_T(\\mathbf{x}_T) + \\sum_{t=1}^{T-1} c_t(\\mathbf{x}_t,\\mathbf{u}_t),\n\\qquad \\mathbf{x}_{t+1}=\\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t).\n\nA single forward rollout computes and stores the trajectory \\mathbf{x}_{1:T}. A single backward sweep then applies the reverse‑mode chain rule stage by stage. Defining the costate by\\boldsymbol{\\lambda}_T = \\nabla_{\\mathbf{x}} c_T(\\mathbf{x}_T),\\qquad\n\\boldsymbol{\\lambda}_t = \\nabla_{\\mathbf{x}} c_t(\\mathbf{x}_t,\\mathbf{u}_t) + \\big[\\nabla_{\\mathbf{x}} \\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t)\\big]^\\top \\boldsymbol{\\lambda}_{t+1},\\quad t=T-1,\\dots,1,\n\nyields exactly the discrete‑time adjoint (PMP) recursion. The gradient with respect to each control follows from the same reverse pass:\\nabla_{\\mathbf{u}_t} J = \\nabla_{\\mathbf{u}} c_t(\\mathbf{x}_t,\\mathbf{u}_t) + \\big[\\nabla_{\\mathbf{u}} \\mathbf{f}_t(\\mathbf{x}_t,\\mathbf{u}_t)\\big]^\\top \\boldsymbol{\\lambda}_{t+1}.\n\nTwo points are worth emphasizing. Computationally, this reverse accumulation produces all control gradients with one forward rollout and one backward adjoint pass; its cost is essentially a small constant multiple of simulating the system once. Conceptually, the costate \\boldsymbol{\\lambda}_t is the marginal effect of perturbing the state at time t on the total objective; the control gradient combines a direct contribution from c_t and an indirect contribution through how \\mathbf{u}_t changes the next state. This is the same structure that underlies backpropagation, expressed for dynamical systems.\n\nIt is instructive to contrast this with alternatives. Black‑box finite differences perturb one decision at a time and re‑roll the system, requiring on the order of p rollouts for p decision variables and suffering from step‑size and noise issues—prohibitive when p=(T-1)m for an m‑dimensional control over T steps. Forward‑mode (tangent) sensitivities propagate Jacobian–vector products for each parameter direction; their work also scales with p. Reverse‑mode (the adjoint) instead propagates a single vector \\boldsymbol{\\lambda}_t backward and then reads off all partial derivatives \\nabla_{\\mathbf{u}_t} J at once. For a scalar objective, its cost is effectively independent of p, at the price of storing (or checkpointing) the forward trajectory. This scalability is why the adjoint is the method of choice for gradient‑based trajectory optimization and for constrained transcriptions via the Hamiltonian.","type":"content","url":"/trajectories#the-adjoint-equation-as-reverse-accumulation","position":49}]}