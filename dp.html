
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>11. Dynamic Programming &#8212; Pragmatic Reinforcement Learning: Algorithms and Applications</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="_static/graphviz.css?v=fd3f3429" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=87e54e7c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"bm": ["{\\boldsymbol #1}", 1]}, "processEscapes": true}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'dp';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="13. Approximate Dynamic Programming" href="adp.html" />
    <link rel="prev" title="7. Model Predictive Control" href="mpc.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Pragmatic Reinforcement Learning: Algorithms and Applications</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ocp.html">1. Discrete-Time Trajectory Optimization</a></li>



<li class="toctree-l1"><a class="reference internal" href="cocp.html">5. Continuous-Time Trajectory Optimization</a></li>

<li class="toctree-l1"><a class="reference internal" href="mpc.html">7. Model Predictive Control</a></li>



<li class="toctree-l1 current active"><a class="current reference internal" href="#">11. Dynamic Programming</a></li>

<li class="toctree-l1"><a class="reference internal" href="adp.html">13. Approximate Dynamic Programming</a></li>




<li class="toctree-l1"><a class="reference internal" href="bibliography.html">18. Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/pierrelux/rlbook" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pierrelux/rlbook/edit/main/dp.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pierrelux/rlbook/issues/new?title=Issue%20on%20page%20%2Fdp.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/dp.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Dynamic Programming</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">11. Dynamic Programming</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#notation-in-operations-reseach">11.1. Notation in Operations Reseach</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-rules-and-policies">11.2. Decision Rules and Policies</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-an-optimal-policy">11.3. What is an Optimal Policy?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-sample-size-determination-in-pharmaceutical-development">11.4. Example: Sample Size Determination in Pharmaceutical Development</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#infinite-horizon-mdps">12. Infinite-Horizon MDPs</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-horizon-interpretation-of-discounting">12.1. Random Horizon Interpretation of Discounting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-representation-in-markov-decision-processes">12.2. Vector Representation in Markov Decision Processes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-operator-equations">12.3. Solving Operator Equations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#successive-approximation-method">12.3.1. Successive Approximation Method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#newton-kantorovich-method">12.3.2. Newton-Kantorovich Method</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimality-equations-for-infinite-horizon-mdps">12.4. Optimality Equations for Infinite-Horizon MDPs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#algorithms-for-solving-the-optimality-equations">12.5. Algorithms for Solving the Optimality Equations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#newton-kantorovich-applied-to-the-optimality-equations">12.5.1. Newton-Kantorovich applied to the Optimality Equations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-iteration">12.5.2. Policy Iteration</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="dynamic-programming">
<h1><span class="section-number">11. </span>Dynamic Programming<a class="headerlink" href="#dynamic-programming" title="Link to this heading">#</a></h1>
<p>Rather than expressing the stochasticity in our system through a disturbance term as a parameter to a deterministic difference equation, we often work with an alternative representation (more common in operations research) which uses the Markov Decision Process formulation. The idea is that when we model our system in this way with the disturbance term being drawn indepently of the previous stages, the induced trajectory are those of a Markov chain. Hence, we can re-cast our control problem in that language, leading to the so-called Markov Decision Process framework in which we express the system dynamics in terms of transition probabilities rather than explicit state equations. In this framework, we express the probability that the system is in a given state using the transition probability function:</p>
<div class="math notranslate nohighlight">
\[ p_t(\mathbf{x}_{t+1} | \mathbf{x}_t, \mathbf{u}_t) \]</div>
<p>This function gives the probability of transitioning to state <span class="math notranslate nohighlight">\(\mathbf{x}_{t+1}\)</span> at time <span class="math notranslate nohighlight">\(t+1\)</span>, given that the system is in state <span class="math notranslate nohighlight">\(\mathbf{x}_t\)</span> and action <span class="math notranslate nohighlight">\(\mathbf{u}_t\)</span> is taken at time <span class="math notranslate nohighlight">\(t\)</span>. Therefore, <span class="math notranslate nohighlight">\(p_t\)</span> specifies a conditional probability distribution over the next states: namely, the sum (for discrete state spaces) or integral over the next state should be 1.</p>
<p>Given the control theory formulation of our problem via a deterministic dynamics function and a noise term, we can derive the corresponding transition probability function through the following relationship:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
p_t(\mathbf{x}_{t+1} | \mathbf{x}_t, \mathbf{u}_t) &amp;= \mathbb{P}(\mathbf{W}_t \in \left\{\mathbf{w} \in \mathbf{W}: \mathbf{x}_{t+1} = f_t(\mathbf{x}_t, \mathbf{u}_t, \mathbf{w})\right\}) \\
&amp;= \sum_{\left\{\mathbf{w} \in \mathbf{W}: \mathbf{x}_{t+1} = f_t(\mathbf{x}_t, \mathbf{u}_t, \mathbf{w})\right\}} q_t(\mathbf{w})
\end{aligned}
\end{split}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(q_t(\mathbf{w})\)</span> represents the probability density or mass function of the disturbance <span class="math notranslate nohighlight">\(\mathbf{W}_t\)</span> (assuming discrete state spaces). When dealing with continuous spaces, the above expression simply contains an integral rather than a summation.</p>
<p>For a system with deterministic dynamics and no disturbance, the transition probabilities become much simpler and be expressed using the indicator function. Given a deterministic system with dynamics:</p>
<div class="math notranslate nohighlight">
\[ \mathbf{x}_{t+1} = f_t(\mathbf{x}_t, \mathbf{u}_t) \]</div>
<p>The transition probability function can be expressed as:</p>
<div class="math notranslate nohighlight">
\[\begin{split} p_t(\mathbf{x}_{t+1} | \mathbf{x}_t, \mathbf{u}_t) = \begin{cases}
1 &amp; \text{if } \mathbf{x}_{t+1} = f_t(\mathbf{x}_t, \mathbf{u}_t) \\
0 &amp; \text{otherwise}
\end{cases} \end{split}\]</div>
<p>With this transition probability function, we can recast our Bellman optimality equation:</p>
<div class="math notranslate nohighlight">
\[ J_t^\star(\mathbf{x}_t) = \max_{\mathbf{u}_t \in \mathbf{U}} \left\{ c_t(\mathbf{x}_t, \mathbf{u}_t) + \sum_{\mathbf{x}_{t+1}} p_t(\mathbf{x}_{t+1} | \mathbf{x}_t, \mathbf{u}_t) J_{t+1}^\star(\mathbf{x}_{t+1}) \right\} \]</div>
<p>Here, <span class="math notranslate nohighlight">\({c}(\mathbf{x}_t, \mathbf{u}_t)\)</span> represents the expected immediate reward (or negative cost) when in state <span class="math notranslate nohighlight">\(\mathbf{x}_t\)</span> and taking action <span class="math notranslate nohighlight">\(\mathbf{u}_t\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>. The summation term computes the expected optimal value for the future states, weighted by their transition probabilities.</p>
<p>This formulation offers several advantages:</p>
<ol class="arabic simple">
<li><p>It makes the Markovian nature of the problem explicit: the future state depends only on the current state and action, not on the history of states and actions.</p></li>
<li><p>For discrete-state problems, the entire system dynamics can be specified by a set of transition matrices, one for each possible action.</p></li>
<li><p>It allows us to bridge the gap with the wealth of methods in the field of probabilistic graphical models and statistical machine learning techniques for modelling and analysis.</p></li>
</ol>
<section id="notation-in-operations-reseach">
<h2><span class="section-number">11.1. </span>Notation in Operations Reseach<a class="headerlink" href="#notation-in-operations-reseach" title="Link to this heading">#</a></h2>
<p>The presentation above was intended to bridge the gap between the control-theoretic perspective and the world of closed-loop control through the idea of determining the value function of a parametric optimal control problem. We then saw how the backward induction procedure was applicable to both the deterministic and stochastic cases by taking the expectation over the disturbance variable. We then said that we can alternatively work with a representation of our system where instead of writing our model as a deterministic dynamics function taking a disturbance as an input, we would rather work directly via its transition probability function, which gives rise to the Markov chain interpretation of our system in simulation.</p>
<p>Now we should highlight that the notation used in control theory tends to differ from that found in operations research communities, in which the field of dynamic programming flourished. We summarize those (purely notational) differences in this section.</p>
<p>In operations research, the system state at each decision epoch is typically denoted by <span class="math notranslate nohighlight">\(s \in \mathcal{S}\)</span>, where <span class="math notranslate nohighlight">\(S\)</span> is the set of possible system states. When the system is in state <span class="math notranslate nohighlight">\(s\)</span>, the decision maker may choose an action <span class="math notranslate nohighlight">\(a\)</span> from the set of allowable actions <span class="math notranslate nohighlight">\(\mathcal{A}_s\)</span>. The union of all action sets is denoted as <span class="math notranslate nohighlight">\(\mathcal{A} = \bigcup_{s \in \mathcal{S}} \mathcal{A}_s\)</span>.</p>
<p>The dynamics of the system are described by a transition probability function <span class="math notranslate nohighlight">\(p_t(j | s, a)\)</span>, which represents the probability of transitioning to state <span class="math notranslate nohighlight">\(j \in \mathcal{S}\)</span> at time <span class="math notranslate nohighlight">\(t+1\)</span>, given that the system is in state <span class="math notranslate nohighlight">\(s\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> and action <span class="math notranslate nohighlight">\(a \in \mathcal{A}_s\)</span> is chosen. This transition probability function satisfies:</p>
<div class="math notranslate nohighlight">
\[\sum_{j \in \mathcal{S}} p_t(j | s, a) = 1\]</div>
<p>It’s worth noting that in operations research, we typically work with reward maximization rather than cost minimization, which is more common in control theory. However, we can easily switch between these perspectives by simply negating the quantity. That is, maximizing a reward function is equivalent to minimizing its negative, which we would then call a cost function.</p>
<p>The reward function is denoted by <span class="math notranslate nohighlight">\(r_t(s, a)\)</span>, representing the reward received at time <span class="math notranslate nohighlight">\(t\)</span> when the system is in state <span class="math notranslate nohighlight">\(s\)</span> and action <span class="math notranslate nohighlight">\(a\)</span> is taken. In some cases, the reward may also depend on the next state, in which case it is denoted as <span class="math notranslate nohighlight">\(r_t(s, a, j)\)</span>. The expected reward can then be computed as:</p>
<div class="math notranslate nohighlight">
\[r_t(s, a) = \sum_{j \in \mathcal{S}} r_t(s, a, j) p_t(j | s, a)\]</div>
<p>Combined together, these elemetns specify a Markov decision process, which is fully described by the tuple:</p>
<div class="math notranslate nohighlight">
\[\{T, S, \mathcal{A}_s, p_t(\cdot | s, a), r_t(s, a)\}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathrm{T}\)</span> represents the set of decision epochs (the horizon).</p>
</section>
<section id="decision-rules-and-policies">
<h2><span class="section-number">11.2. </span>Decision Rules and Policies<a class="headerlink" href="#decision-rules-and-policies" title="Link to this heading">#</a></h2>
<p>In the presentation provided so far, we directly assumed that the form of our feedback controller was of the form <span class="math notranslate nohighlight">\(u(\mathbf{x}, t)\)</span>. The idea is that rather than just looking at the stage as in open-loop control, we would now consider the current state to account for the presence of noise. We came to that conclusion by considering the parametric optimization problem corresponding to the trajectory optimization perspective and saw that the “argmax” counterpart to the value function (the max) was exactly this function <span class="math notranslate nohighlight">\(u(x, t)\)</span>. But this presentation was mostly for intuition and neglected the fact that we could consider other kinds of feedback controllers. In the context of MDPs and under the OR terminology, we should now rather talk of policies instead of controllers.</p>
<p>But to properly introduce the concept of policy, we first have to talk about decision rules. A decision rule is a prescription of a procedure for action selection in each state at a specified decision epoch. These rules can vary in their complexity due to their potential dependence on the history and ways in which actions are then selected. Decision rules can be classified based on two main criteria:</p>
<ol class="arabic simple">
<li><p>Dependence on history: Markovian or History-dependent</p></li>
<li><p>Action selection method: Deterministic or Randomized</p></li>
</ol>
<p>Markovian decision rules are those that depend only on the current state, while history-dependent rules consider the entire sequence of past states and actions. Formally, we can define a history <span class="math notranslate nohighlight">\(h_t\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> as:</p>
<div class="math notranslate nohighlight">
\[h_t = (s_1, a_1, \ldots, s_{t-1}, a_{t-1}, s_t)\]</div>
<p>where <span class="math notranslate nohighlight">\(s_u\)</span> and <span class="math notranslate nohighlight">\(a_u\)</span> denote the state and action at decision epoch <span class="math notranslate nohighlight">\(u\)</span>. The set of all possible histories at time <span class="math notranslate nohighlight">\(t\)</span>, denoted <span class="math notranslate nohighlight">\(H_t\)</span>, grows rapidly with <span class="math notranslate nohighlight">\(t\)</span>:</p>
<div class="math notranslate nohighlight">
\[H_1 = \mathcal{S}\]</div>
<div class="math notranslate nohighlight">
\[H_2 = \mathcal{S} \times A \times \mathcal{S}\]</div>
<div class="math notranslate nohighlight">
\[H_t = H_{t-1} \times A \times \mathcal{S} = \mathcal{S} \times (A \times \mathcal{S})^{t-1}\]</div>
<p>This exponential growth in the size of the history set motivates us to seek conditions under which we can avoid searching for history-dependent decision rules and instead focus on Markovian rules, which are much simpler to implement and evaluate.</p>
<p>Decision rules can be further classified as deterministic or randomized. A deterministic rule selects an action with certainty, while a randomized rule specifies a probability distribution over the action space.</p>
<p>These classifications lead to four types of decision rules:</p>
<ol class="arabic simple">
<li><p>Markovian Deterministic (MD): <span class="math notranslate nohighlight">\(d_t: \mathcal{S} \rightarrow \mathcal{A}_s\)</span></p></li>
<li><p>Markovian Randomized (MR): <span class="math notranslate nohighlight">\(d_t: \mathcal{S} \rightarrow \mathcal{P}(\mathcal{A}_s)\)</span></p></li>
<li><p>History-dependent Deterministic (HD): <span class="math notranslate nohighlight">\(d_t: H_t \rightarrow \mathcal{A}_s\)</span></p></li>
<li><p>History-dependent Randomized (HR): <span class="math notranslate nohighlight">\(d_t: H_t \rightarrow \mathcal{P}(\mathcal{A}_s)\)</span></p></li>
</ol>
<p>Where <span class="math notranslate nohighlight">\(\mathcal{P}(\mathcal{A}_s)\)</span> denotes the set of probability distributions over <span class="math notranslate nohighlight">\(\mathcal{A}_s\)</span>.</p>
<p>It’s important to note that decision rules are stage-wise objects. However, to solve an MDP, we need a strategy for the entire horizon. This is where we make a distinction and introduce the concept of a policy. A policy <span class="math notranslate nohighlight">\(\pi\)</span> is a sequence of decision rules, one for each decision epoch:</p>
<div class="math notranslate nohighlight">
\[\pi = (d_1, d_2, ..., d_{N-1})\]</div>
<p>Where <span class="math notranslate nohighlight">\(N\)</span> is the horizon length (possibly infinite). The set of all policies of class <span class="math notranslate nohighlight">\(K\)</span> (where <span class="math notranslate nohighlight">\(K\)</span> can be HR, HD, MR, or MD) is denoted as <span class="math notranslate nohighlight">\(\Pi^K\)</span>.</p>
<p>A special type of policy is a stationary policy, where the same decision rule is used at all epochs: <span class="math notranslate nohighlight">\(\pi = (d, d, ...)\)</span>, often denoted as <span class="math notranslate nohighlight">\(d^\infty\)</span>.</p>
<p>The relationships between these policy classes form a hierarchy:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
\Pi^{SD} \subset \Pi^{SR} \subset \Pi^{MR} \subset \Pi^{HR}\\
\Pi^{SD} \subset \Pi^{MD} \subset \Pi^{MR} \subset \Pi^{HR} \\
\Pi^{SD} \subset \Pi^{MD} \subset \Pi^{HD} \subset \Pi^{HR}
\end{align*}
\end{split}\]</div>
<p>Where SD stands for Stationary Deterministic and SR for Stationary Randomized. The largest set is by far the set of history randomized policies.</p>
<p>A fundamental question in MDP theory is: under what conditions can we avoid working with the set <span class="math notranslate nohighlight">\(\Pi^{HR}\)</span> and focus for example on the much simpler set of deterministic Markovian policy? Even more so, we will see that in the infinite horizon case, we can drop the dependance on time and simply consider stationary deterministic Markovian policies.</p>
</section>
<section id="what-is-an-optimal-policy">
<h2><span class="section-number">11.3. </span>What is an Optimal Policy?<a class="headerlink" href="#what-is-an-optimal-policy" title="Link to this heading">#</a></h2>
<p>Let’s go back to the starting point and define what it means for a policy to be optimal in a Markov Decision Problem. For this, we will be considering different possible search spaces (policy classes) and compare policies based on the ordering of their value from any possible start state. The value of a policy <span class="math notranslate nohighlight">\(\pi\)</span> (optimal or not) is defined as the expected total reward obtained by following that policy from a given starting state. Formally, for a finite-horizon MDP with <span class="math notranslate nohighlight">\(N\)</span> decision epochs, we define the value function <span class="math notranslate nohighlight">\(v_\pi(s, t)\)</span> as:</p>
<div class="math notranslate nohighlight">
\[
v_\pi(s, t) \triangleq \mathbb{E}\left[\sum_{k=t}^{N-1} r_t(S_k, A_k) + r_N(S_N) \mid S_t = s\right]
\]</div>
<p>where <span class="math notranslate nohighlight">\(S_t\)</span> is the state at time <span class="math notranslate nohighlight">\(t\)</span>, <span class="math notranslate nohighlight">\(A_t\)</span> is the action taken at time <span class="math notranslate nohighlight">\(t\)</span>, and <span class="math notranslate nohighlight">\(r_t\)</span> is the reward function. For simplicity, we write <span class="math notranslate nohighlight">\(v_\pi(s)\)</span> to denote <span class="math notranslate nohighlight">\(v_\pi(s, 1)\)</span>, the value of following policy <span class="math notranslate nohighlight">\(\pi\)</span> from state <span class="math notranslate nohighlight">\(s\)</span> at the first stage over the entire horizon <span class="math notranslate nohighlight">\(N\)</span>.</p>
<p>In finite-horizon MDPs, our goal is to identify an optimal policy, denoted by <span class="math notranslate nohighlight">\(\pi^*\)</span>, that maximizes total expected reward over the horizon <span class="math notranslate nohighlight">\(N\)</span>. Specifically:</p>
<div class="math notranslate nohighlight">
\[
v_{\pi^*}(s) \geq v_\pi(s), \quad \forall s \in \mathcal{S}, \quad \forall \pi \in \Pi^{\text{HR}}
\]</div>
<p>We call <span class="math notranslate nohighlight">\(\pi^*\)</span> an <strong>optimal policy</strong> because it yields the highest possible value across all states and all policies within the policy class <span class="math notranslate nohighlight">\(\Pi^{\text{HR}}\)</span>. We denote by <span class="math notranslate nohighlight">\(v^*\)</span> the maximum value achievable by any policy:</p>
<div class="math notranslate nohighlight">
\[
v^*(s) = \max_{\pi \in \Pi^{\text{HR}}} v_\pi(s), \quad \forall s \in \mathcal{S}
\]</div>
<p>In reinforcement learning literature, <span class="math notranslate nohighlight">\(v^*\)</span> is typically referred to as the “optimal value function,” while in some operations research references, it might be called the “value of an MDP.” An optimal policy <span class="math notranslate nohighlight">\(\pi^*\)</span> is one for which its value function equals the optimal value function:</p>
<div class="math notranslate nohighlight">
\[
v_{\pi^*}(s) = v^*(s), \quad \forall s \in \mathcal{S}
\]</div>
<p>It’s important to note that this notion of optimality applies to every state. Policies optimal in this sense are sometimes called “uniformly optimal policies.” A weaker notion of optimality, often encountered in reinforcement learning practice, is optimality with respect to an initial distribution of states. In this case, we seek a policy <span class="math notranslate nohighlight">\(\pi \in \Pi^{\text{HR}}\)</span> that maximizes:</p>
<div class="math notranslate nohighlight">
\[
\sum_{s \in \mathcal{S}} v_\pi(s) P_1(S_1 = s)
\]</div>
<p>where <span class="math notranslate nohighlight">\(P_1(S_1 = s)\)</span> is the probability of starting in state <span class="math notranslate nohighlight">\(s\)</span>.</p>
<p>A fundamental result in MDP theory states that the maximum value can be achieved by searching over the space of deterministic Markovian Policies. Consequently:</p>
<div class="math notranslate nohighlight">
\[ v^*(s) = \max_{\pi \in \Pi^{\mathrm{HR}}} v_\pi(s) = \max _{\pi \in \Pi^{M D}} v_\pi(s), \quad s \in S\]</div>
<p>This equality significantly simplifies the computational complexity of our algorithms, as the search problem can now be decomposed into <span class="math notranslate nohighlight">\(N\)</span> sub-problems in which we only have to search over the set of possible actions. This is the backward induction algorithm, which we present a second time, but departing this time from the control-theoretic notation and using the MDP formalism:</p>
<div class="proof algorithm admonition" id="backward-induction">
<p class="admonition-title"><span class="caption-number">Algorithm 11.1 </span> (Backward Induction)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> State space <span class="math notranslate nohighlight">\(S\)</span>, Action space <span class="math notranslate nohighlight">\(A\)</span>, Transition probabilities <span class="math notranslate nohighlight">\(p_t\)</span>, Reward function <span class="math notranslate nohighlight">\(r_t\)</span>, Time horizon <span class="math notranslate nohighlight">\(N\)</span></p>
<p><strong>Output:</strong> Optimal value functions <span class="math notranslate nohighlight">\(v^*\)</span></p>
<ol class="arabic">
<li><p>Initialize:</p>
<ul>
<li><p>Set <span class="math notranslate nohighlight">\(t = N\)</span></p></li>
<li><p>For all <span class="math notranslate nohighlight">\(s_N \in S\)</span>:</p>
<div class="math notranslate nohighlight">
\[v^*(s_N, N) = r_N(s_N)\]</div>
</li>
</ul>
</li>
<li><p>For <span class="math notranslate nohighlight">\(t = N-1\)</span> to <span class="math notranslate nohighlight">\(1\)</span>:</p>
<ul>
<li><p>For each <span class="math notranslate nohighlight">\(s_t \in S\)</span>:
a. Compute the optimal value function:</p>
<div class="math notranslate nohighlight">
\[v^*(s_t, t) = \max_{a \in A_{s_t}} \left\{r_t(s_t, a) + \sum_{j \in S} p_t(j | s_t, a) v^*(j, t+1)\right\}\]</div>
<p>b. Determine the set of optimal actions:</p>
<div class="math notranslate nohighlight">
\[A_{s_t,t}^* = \arg\max_{a \in A_{s_t}} \left\{r_t(s_t, a) + \sum_{j \in S} p_t(j | s_t, a) v^*(j, t+1)\right\}\]</div>
</li>
</ul>
</li>
<li><p>Return the optimal value functions <span class="math notranslate nohighlight">\(u_t^*\)</span> and optimal action sets <span class="math notranslate nohighlight">\(A_{s_t,t}^*\)</span> for all <span class="math notranslate nohighlight">\(t\)</span> and <span class="math notranslate nohighlight">\(s_t\)</span></p></li>
</ol>
</section>
</div><p>Note that the same procedure can also be used for finding the value of a policy with minor changes;</p>
<div class="proof algorithm admonition" id="backward-policy-evaluation">
<p class="admonition-title"><span class="caption-number">Algorithm 11.2 </span> (Policy Evaluation)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong></p>
<ul class="simple">
<li><p>State space <span class="math notranslate nohighlight">\(S\)</span></p></li>
<li><p>Action space <span class="math notranslate nohighlight">\(A\)</span></p></li>
<li><p>Transition probabilities <span class="math notranslate nohighlight">\(p_t\)</span></p></li>
<li><p>Reward function <span class="math notranslate nohighlight">\(r_t\)</span></p></li>
<li><p>Time horizon <span class="math notranslate nohighlight">\(N\)</span></p></li>
<li><p>A markovian deterministic policy <span class="math notranslate nohighlight">\(\pi\)</span></p></li>
</ul>
<p><strong>Output:</strong> Value function <span class="math notranslate nohighlight">\(v^\pi\)</span> for policy <span class="math notranslate nohighlight">\(\pi\)</span></p>
<ol class="arabic">
<li><p>Initialize:</p>
<ul>
<li><p>Set <span class="math notranslate nohighlight">\(t = N\)</span></p></li>
<li><p>For all <span class="math notranslate nohighlight">\(s_N \in S\)</span>:</p>
<div class="math notranslate nohighlight">
\[v_\pi(s_N, N) = r_N(s_N)\]</div>
</li>
</ul>
</li>
<li><p>For <span class="math notranslate nohighlight">\(t = N-1\)</span> to <span class="math notranslate nohighlight">\(1\)</span>:</p>
<ul>
<li><p>For each <span class="math notranslate nohighlight">\(s_t \in S\)</span>:
a. Compute the value function for the given policy:</p>
<div class="math notranslate nohighlight">
\[v_\pi(s_t, t) = r_t(s_t, d_t(s_t)) + \sum_{j \in S} p_t(j | s_t, d_t(s_t)) v_\pi(j, t+1)\]</div>
</li>
</ul>
</li>
<li><p>Return the value function <span class="math notranslate nohighlight">\(v^\pi(s_t, t)\)</span> for all <span class="math notranslate nohighlight">\(t\)</span> and <span class="math notranslate nohighlight">\(s_t\)</span></p></li>
</ol>
</section>
</div><p>This code could also finally be adapted to support randomized policies using:</p>
<div class="math notranslate nohighlight">
\[v_\pi(s_t, t) = \sum_{a_t \in \mathcal{A}_{s_t}} d_t(a_t \mid s_t) \left( r_t(s_t, a_t) + \sum_{j \in S} p_t(j | s_t, a_t) v_\pi(j, t+1) \right)\]</div>
</section>
<section id="example-sample-size-determination-in-pharmaceutical-development">
<h2><span class="section-number">11.4. </span>Example: Sample Size Determination in Pharmaceutical Development<a class="headerlink" href="#example-sample-size-determination-in-pharmaceutical-development" title="Link to this heading">#</a></h2>
<p>Pharmaceutical development is the process of bringing a new drug from initial discovery to market availability. This process is lengthy, expensive, and risky, typically involving several stages:</p>
<ol class="arabic simple">
<li><p><strong>Drug Discovery</strong>: Identifying a compound that could potentially treat a disease.</p></li>
<li><p><strong>Preclinical Testing</strong>: Laboratory and animal testing to assess safety and efficacy.
. <strong>Clinical Trials</strong>: Testing the drug in humans, divided into phases:</p>
<ul class="simple">
<li><p>Phase I: Testing for safety in a small group of healthy volunteers.</p></li>
<li><p>Phase II: Testing for efficacy and side effects in a larger group with the target condition.</p></li>
<li><p>Phase III: Large-scale testing to confirm efficacy and monitor side effects.</p></li>
</ul>
</li>
<li><p><strong>Regulatory Review</strong>: Submitting a New Drug Application (NDA) for approval.</p></li>
<li><p><strong>Post-Market Safety Monitoring</strong>: Continuing to monitor the drug’s effects after market release.</p></li>
</ol>
<p>This process can take 10-15 years and cost over $1 billion <span id="id1">[<a class="reference internal" href="bibliography.html#id20" title="Christopher Paul Adams and Van Vu Brantner. Spending on new drug development1. Health Economics, 19(2):130–141, February 2009. URL: http://dx.doi.org/10.1002/hec.1454, doi:10.1002/hec.1454.">1</a>]</span>. The high costs and risks involved call for a principled approach to decision making. We’ll focus on the clinical trial phases and NDA approval, per the MDP model presented by <span id="id2">[<a class="reference internal" href="bibliography.html#id21" title="Mark Chang. Monte Carlo Simulation for the Pharmaceutical Industry: Concepts, Algorithms, and Case Studies. CRC Press, September 2010. ISBN 9780429152382. URL: http://dx.doi.org/10.1201/EBK1439835920, doi:10.1201/ebk1439835920.">5</a>]</span>:</p>
<ol class="arabic simple">
<li><p><strong>States</strong> (<span class="math notranslate nohighlight">\(S\)</span>): Our state space is <span class="math notranslate nohighlight">\(S = \{s_1, s_2, s_3, s_4\}\)</span>, where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(s_1\)</span>: Phase I clinical trial</p></li>
<li><p><span class="math notranslate nohighlight">\(s_2\)</span>: Phase II clinical trial</p></li>
<li><p><span class="math notranslate nohighlight">\(s_3\)</span>: Phase III clinical trial</p></li>
<li><p><span class="math notranslate nohighlight">\(s_4\)</span>: NDA approval</p></li>
</ul>
</li>
<li><p><strong>Actions</strong> (<span class="math notranslate nohighlight">\(A\)</span>): At each state, the action is choosing the sample size <span class="math notranslate nohighlight">\(n_i\)</span> for the corresponding clinical trial. The action space is <span class="math notranslate nohighlight">\(A = \{10, 11, ..., 1000\}\)</span>, representing possible sample sizes.</p></li>
<li><p><strong>Transition Probabilities</strong> (<span class="math notranslate nohighlight">\(P\)</span>): The probability of moving from one state to the next depends on the chosen sample size and the inherent properties of the drug.
We define:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(s_2|s_1, n_1) = p_{12}(n_1) = \sum_{i=0}^{\lfloor\eta_1 n_1\rfloor} \binom{n_1}{i} p_0^i (1-p_0)^{n_1-i}\)</span>
where <span class="math notranslate nohighlight">\(p_0\)</span> is the true toxicity rate and <span class="math notranslate nohighlight">\(\eta_1\)</span> is the toxicity threshold for Phase I.</p></li>
</ul>
</li>
</ol>
<ul>
<li><p>Of particular interest is the transition from Phase II to Phase III which we model as:</p>
<p><span class="math notranslate nohighlight">\(P(s_3|s_2, n_2) = p_{23}(n_2) = \Phi\left(\frac{\sqrt{n_2}}{2}\delta - z_{1-\eta_2}\right)\)</span></p>
<p>where <span class="math notranslate nohighlight">\(\Phi\)</span> is the cumulative distribution function (CDF) of the standard normal distribution:</p>
<p><span class="math notranslate nohighlight">\(\Phi(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^x e^{-t^2/2} dt\)</span></p>
<p>This is giving us the probability that we would observe a treatment effect this large or larger if the null hypothesis (no treatment effect) were true. A higher probability indicates stronger evidence of a treatment effect, making it more likely that the drug will progress to Phase III.</p>
<p>In this expression, <span class="math notranslate nohighlight">\(\delta\)</span> is called the “normalized treatment effect”. In clinical trials, we’re often interested in the difference between the treatment and control groups. The “normalized” part means we’ve adjusted this difference for the variability in the data. Specifically <span class="math notranslate nohighlight">\(\delta = \frac{\mu_t - \mu_c}{\sigma}\)</span> where <span class="math notranslate nohighlight">\(\mu_t\)</span> is the mean outcome in the treatment group, <span class="math notranslate nohighlight">\(\mu_c\)</span> is the mean outcome in the control group, and <span class="math notranslate nohighlight">\(\sigma\)</span> is the standard deviation of the outcome. A larger <span class="math notranslate nohighlight">\(\delta\)</span> indicates a stronger treatment effect.</p>
<p>Furthermore, the term <span class="math notranslate nohighlight">\(z_{1-\eta_2}\)</span> is the <span class="math notranslate nohighlight">\((1-\eta_2)\)</span>-quantile of the standard normal distribution. In other words, it’s the value where the probability of a standard normal random variable being greater than this value is <span class="math notranslate nohighlight">\(\eta_2\)</span>. For example, if <span class="math notranslate nohighlight">\(\eta_2 = 0.05\)</span>, then <span class="math notranslate nohighlight">\(z_{1-\eta_2} \approx 1.645\)</span>. A smaller <span class="math notranslate nohighlight">\(\eta_2\)</span> makes the trial more conservative, requiring stronger evidence to proceed to Phase III.</p>
<p>Finally, <span class="math notranslate nohighlight">\(n_2\)</span> is the sample size for Phase II. The <span class="math notranslate nohighlight">\(\sqrt{n_2}\)</span> term reflects that the precision of our estimate of the treatment effect improves with the square root of the sample size.</p>
</li>
<li><p><span class="math notranslate nohighlight">\(P(s_4|s_3, n_3) = p_{34}(n_3) = \Phi\left(\frac{\sqrt{n_3}}{2}\delta - z_{1-\eta_3}\right)\)</span>
where <span class="math notranslate nohighlight">\(\eta_3\)</span> is the significance level for Phase III.</p></li>
</ul>
<ol class="arabic simple" start="4">
<li><p><strong>Rewards</strong> (<span class="math notranslate nohighlight">\(R\)</span>): The reward function captures the costs of running trials and the potential profit from a successful drug:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(r(s_i, n_i) = -c_i(n_i)\)</span> for <span class="math notranslate nohighlight">\(i = 1, 2, 3\)</span>, where <span class="math notranslate nohighlight">\(c_i(n_i)\)</span> is the cost of running a trial with sample size <span class="math notranslate nohighlight">\(n_i\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(r(s_4) = g_4\)</span>, where <span class="math notranslate nohighlight">\(g_4\)</span> is the expected profit from a successful drug.</p></li>
</ul>
</li>
<li><p><strong>Discount Factor</strong> (<span class="math notranslate nohighlight">\(\gamma\)</span>): We use a discount factor <span class="math notranslate nohighlight">\(0 &lt; \gamma \leq 1\)</span> to account for the time value of money and risk preferences.</p></li>
</ol>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">binom</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="k">def</span> <span class="nf">binomial_pmf</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">transition_prob_phase1</span><span class="p">(</span><span class="n">n1</span><span class="p">,</span> <span class="n">eta1</span><span class="p">,</span> <span class="n">p0</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">([</span><span class="n">binomial_pmf</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">n1</span><span class="p">,</span> <span class="n">p0</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">eta1</span> <span class="o">*</span> <span class="n">n1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)])</span>

<span class="k">def</span> <span class="nf">transition_prob_phase2</span><span class="p">(</span><span class="n">n2</span><span class="p">,</span> <span class="n">eta2</span><span class="p">,</span> <span class="n">delta</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n2</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">delta</span> <span class="o">-</span> <span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">eta2</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">transition_prob_phase3</span><span class="p">(</span><span class="n">n3</span><span class="p">,</span> <span class="n">eta3</span><span class="p">,</span> <span class="n">delta</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n3</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">delta</span> <span class="o">-</span> <span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">eta3</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">immediate_reward</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">n</span>  <span class="c1"># Negative to represent cost</span>

<span class="k">def</span> <span class="nf">backward_induction</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">g4</span><span class="p">,</span> <span class="n">p0</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">eta1</span><span class="p">,</span> <span class="n">eta2</span><span class="p">,</span> <span class="n">eta3</span><span class="p">):</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">S</span><span class="p">))</span>
    <span class="n">V</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">g4</span>  <span class="c1"># Value for NDA approval state</span>
    <span class="n">optimal_n</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span>  <span class="c1"># Store optimal n for each phase</span>

    <span class="c1"># Backward induction</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>  <span class="c1"># Iterate backwards from Phase III to Phase I</span>
        <span class="n">max_value</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">A</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Phase I</span>
                <span class="n">p</span> <span class="o">=</span> <span class="n">transition_prob_phase1</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">eta1</span><span class="p">,</span> <span class="n">p0</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># Phase II</span>
                <span class="n">p</span> <span class="o">=</span> <span class="n">transition_prob_phase2</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">eta2</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>  <span class="c1"># Phase III</span>
                <span class="n">p</span> <span class="o">=</span> <span class="n">transition_prob_phase3</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">eta3</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">immediate_reward</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">p</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">value</span> <span class="o">&gt;</span> <span class="n">max_value</span><span class="p">:</span>
                <span class="n">max_value</span> <span class="o">=</span> <span class="n">value</span>
                <span class="n">optimal_n</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">n</span>
        <span class="n">V</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_value</span>

    <span class="k">return</span> <span class="n">V</span><span class="p">,</span> <span class="n">optimal_n</span>

<span class="c1"># Set up the problem parameters</span>
<span class="n">S</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Phase I&#39;</span><span class="p">,</span> <span class="s1">&#39;Phase II&#39;</span><span class="p">,</span> <span class="s1">&#39;Phase III&#39;</span><span class="p">,</span> <span class="s1">&#39;NDA approval&#39;</span><span class="p">]</span>
<span class="n">A</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1001</span><span class="p">)</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.95</span>
<span class="n">g4</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">p0</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># Example toxicity rate for Phase I</span>
<span class="n">delta</span> <span class="o">=</span> <span class="mf">0.5</span>  <span class="c1"># Example normalized treatment difference</span>
<span class="n">eta1</span><span class="p">,</span> <span class="n">eta2</span><span class="p">,</span> <span class="n">eta3</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.025</span>

<span class="c1"># Run the backward induction algorithm</span>
<span class="n">V</span><span class="p">,</span> <span class="n">optimal_n</span> <span class="o">=</span> <span class="n">backward_induction</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">g4</span><span class="p">,</span> <span class="n">p0</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">eta1</span><span class="p">,</span> <span class="n">eta2</span><span class="p">,</span> <span class="n">eta3</span><span class="p">)</span>

<span class="c1"># Print results</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">state</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">S</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Value for </span><span class="si">{</span><span class="n">state</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">V</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Optimal sample sizes: Phase I: </span><span class="si">{</span><span class="n">optimal_n</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">, Phase II: </span><span class="si">{</span><span class="n">optimal_n</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">, Phase III: </span><span class="si">{</span><span class="n">optimal_n</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Sanity checks</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Sanity checks:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;1. NDA approval value: </span><span class="si">{</span><span class="n">V</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;2. All values non-positive and &lt;= NDA value: </span><span class="si">{</span><span class="nb">all</span><span class="p">(</span><span class="n">v</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">V</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">v</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">V</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;3. Optimal sample sizes in range: </span><span class="si">{</span><span class="nb">all</span><span class="p">(</span><span class="mi">10</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="mi">1000</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">optimal_n</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="ow">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="kc">None</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Value for Phase I: 7869.92
Value for Phase II: 8385.83
Value for Phase III: 9123.40
Value for NDA approval: 10000.00
Optimal sample sizes: Phase I: 75, Phase II: 239, Phase III: 326

Sanity checks:
1. NDA approval value: 10000.0
2. All values non-positive and &lt;= NDA value: True
3. Optimal sample sizes in range: True
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="infinite-horizon-mdps">
<h1><span class="section-number">12. </span>Infinite-Horizon MDPs<a class="headerlink" href="#infinite-horizon-mdps" title="Link to this heading">#</a></h1>
<p>It often makes sense to model control problems over infinite horizons. We extend the previous setting and define the expected total reward of policy <span class="math notranslate nohighlight">\(\pi \in \Pi^{\mathrm{HR}}\)</span>, <span class="math notranslate nohighlight">\(v^\pi\)</span> as:</p>
<div class="math notranslate nohighlight">
\[
v^\pi(s) = \mathbb{E}\left[\sum_{t=1}^{\infty} r(S_t, A_t)\right]
\]</div>
<p>One drawback of this model is that we could easily encounter values that are <span class="math notranslate nohighlight">\(+\infty\)</span> or <span class="math notranslate nohighlight">\(-\infty\)</span>, even in a setting as simple as a single-state MDP which loops back into itself and where the accrued reward is nonzero.</p>
<p>Therefore, it is often more convenient to work with an alternative formulation which guarantees the existence of a limit: the expected total discounted reward of policy <span class="math notranslate nohighlight">\(\pi \in \Pi^{\mathrm{HR}}\)</span> is defined to be:</p>
<div class="math notranslate nohighlight">
\[
v_\gamma^\pi(s) \equiv \lim_{N \rightarrow \infty} \mathbb{E}\left[\sum_{t=1}^N \gamma^{t-1} r(S_t, A_t)\right]
\]</div>
<p>for <span class="math notranslate nohighlight">\(0 \leq \gamma &lt; 1\)</span> and when <span class="math notranslate nohighlight">\(\max_{s \in \mathcal{S}} \max_{a \in \mathcal{A}_s}|r(s, a)| = R_{\max} &lt; \infty\)</span>, in which case, <span class="math notranslate nohighlight">\(|v_\gamma^\pi(s)| \leq (1-\gamma)^{-1} R_{\max}\)</span>.</p>
<p>Finally, another possibility for the infinite-horizon setting is the so-called average reward or gain of policy <span class="math notranslate nohighlight">\(\pi \in \Pi^{\mathrm{HR}}\)</span> defined as:</p>
<div class="math notranslate nohighlight">
\[
g^\pi(s) \equiv \lim_{N \rightarrow \infty} \frac{1}{N} \mathbb{E}\left[\sum_{t=1}^N r(S_t, A_t)\right]
\]</div>
<p>We won’t be working with this formulation in this course due to its inherent practical and theoretical complexities.</p>
<p>Extending the previous notion of optimality from finite-horizon models, a policy <span class="math notranslate nohighlight">\(\pi^*\)</span> is said to be discount optimal for a given <span class="math notranslate nohighlight">\(\gamma\)</span> if:</p>
<div class="math notranslate nohighlight">
\[
v_\gamma^{\pi^*}(s) \geq v_\gamma^\pi(s) \quad \text { for each } s \in S \text { and all } \pi \in \Pi^{\mathrm{HR}}
\]</div>
<p>Furthermore, the value of a discounted MDP <span class="math notranslate nohighlight">\(v_\gamma^*(s)\)</span>, is defined by:</p>
<div class="math notranslate nohighlight">
\[
v_\gamma^*(s) \equiv \max _{\pi \in \Pi^{\mathrm{HR}}} v_\gamma^\pi(s)
\]</div>
<p>More often, we refer to <span class="math notranslate nohighlight">\(v_\gamma\)</span> by simply calling it the optimal value function.</p>
<p>As for the finite-horizon setting, the infinite horizon discounted model does not require history-dependent policies, since for any <span class="math notranslate nohighlight">\(\pi \in \Pi^{HR}\)</span> there exists a <span class="math notranslate nohighlight">\(\pi^{\prime} \in \Pi^{MR}\)</span> with identical total discounted reward:
$<span class="math notranslate nohighlight">\(
v_\gamma^*(s) \equiv \max_{\pi \in \Pi^{HR}} v_\gamma^\pi(s)=\max_{\pi \in \Pi^{MR}} v_\gamma^\pi(s) .
\)</span>$</p>
<section id="random-horizon-interpretation-of-discounting">
<h2><span class="section-number">12.1. </span>Random Horizon Interpretation of Discounting<a class="headerlink" href="#random-horizon-interpretation-of-discounting" title="Link to this heading">#</a></h2>
<p>The use of discounting can be motivated both from a modeling perspective and as a means to ensure that the total reward remains bounded. From the modeling perspective, we can view discounting as a way to weight more or less importance on the immediate rewards vs. the long-term consequences. There is also another interpretation which stems from that of a finite horizon model but with an uncertain end time. More precisely:</p>
<p>Let <span class="math notranslate nohighlight">\(v_\nu^\pi(s)\)</span> denote the expected total reward obtained by using policy <span class="math notranslate nohighlight">\(\pi\)</span> when the horizon length <span class="math notranslate nohighlight">\(\nu\)</span> is random. We define it by:</p>
<div class="math notranslate nohighlight">
\[
v_\nu^\pi(s) \equiv \mathbb{E}_s^\pi\left[\mathbb{E}_\nu\left\{\sum_{t=1}^\nu r(S_t, A_t)\right\}\right]
\]</div>
<div class="proof theorem admonition" id="prop-5-3-1">
<p class="admonition-title"><span class="caption-number">Theorem 12.1 </span> (Random horizon interpretation of discounting)</p>
<section class="theorem-content" id="proof-content">
<p>Suppose that the horizon <span class="math notranslate nohighlight">\(\nu\)</span> follows a geometric distribution with parameter <span class="math notranslate nohighlight">\(\gamma\)</span>, <span class="math notranslate nohighlight">\(0 \leq \gamma &lt; 1\)</span>, independent of the policy such that
<span class="math notranslate nohighlight">\(P(\nu=n) = (1-\gamma) \gamma^{n-1}, \, n=1,2, \ldots\)</span>, then <span class="math notranslate nohighlight">\(v_\nu^\pi(s) = v_\gamma^\pi(s)\)</span> for all <span class="math notranslate nohighlight">\(s \in \mathcal{S}\)</span> .</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. See proposition 5.3.1 in <span id="id3">[<a class="reference internal" href="bibliography.html#id23" title="Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley &amp; Sons, New York, 1994. ISBN 978-0-471-61977-3. First published in 1994.">29</a>]</span></p>
<div class="math notranslate nohighlight">
\[
v_\nu^\pi(s) = E_s^\pi \left\{\sum_{n=1}^{\infty} \sum_{t=1}^n r(X_t, Y_t)(1-\gamma) \gamma^{n-1}\right\}.
\]</div>
<p>Under the bounded reward assumption and <span class="math notranslate nohighlight">\(\gamma &lt; 1\)</span>, the series converges and we can reverse the order of summation :</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
v_\nu^\pi(s) &amp;= E_s^\pi \left\{\sum_{t=1}^{\infty} \sum_{n=t}^{\infty} r(S_t, A_t)(1-\gamma) \gamma^{n-1}\right\} \\
&amp;= E_s^\pi \left\{\sum_{t=1}^{\infty} \gamma^{t-1} r(S_t, A_t)\right\} = v_\gamma^\pi(s)
\end{align*}\]</div>
<p>where the last line follows from the geometric series:</p>
<div class="math notranslate nohighlight">
\[
\sum_{n=1}^{\infty} \gamma^{n-1} = \frac{1}{1-\gamma}
\]</div>
</div>
</section>
<section id="vector-representation-in-markov-decision-processes">
<h2><span class="section-number">12.2. </span>Vector Representation in Markov Decision Processes<a class="headerlink" href="#vector-representation-in-markov-decision-processes" title="Link to this heading">#</a></h2>
<p>Let V be the set of bounded real-valued functions on a discrete state space S. This means any function <span class="math notranslate nohighlight">\( f \in V \)</span> satisfies the condition:</p>
<div class="math notranslate nohighlight">
\[
\|f\| = \max_{s \in S} |f(s)| &lt; \infty.
\]</div>
<p>where notation <span class="math notranslate nohighlight">\( \|f\| \)</span> represents the sup-norm (or <span class="math notranslate nohighlight">\( \ell_\infty \)</span>-norm) of the function <span class="math notranslate nohighlight">\( f \)</span>.</p>
<p>When working with discrete state spaces, we can interpret elements of V as vectors and linear operators on V as matrices, allowing us to leverage tools from linear algebra. The sup-norm (<span class="math notranslate nohighlight">\(\ell_\infty\)</span> norm) of matrix <span class="math notranslate nohighlight">\(\mathbf{H}\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[
\|\mathbf{H}\| \equiv \max_{s \in S} \sum_{j \in S} |\mathbf{H}_{s,j}|
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{H}_{s,j}\)</span> represents the <span class="math notranslate nohighlight">\((s, j)\)</span>-th component of the matrix <span class="math notranslate nohighlight">\(\mathbf{H}\)</span>.</p>
<p>For a Markovian decision rule <span class="math notranslate nohighlight">\(d \in D^{MD}\)</span>, we define:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{r}_d(s) &amp;\equiv r(s, d(s)), \quad \mathbf{r}_d \in \mathbb{R}^{|S|}, \\
[\mathbf{P}_d]_{s,j} &amp;\equiv p(j \mid s, d(s)), \quad \mathbf{P}_d \in \mathbb{R}^{|S| \times |S|}.
\end{align*}\]</div>
<p>For a randomized decision rule <span class="math notranslate nohighlight">\(d \in D^{MR}\)</span>, these definitions extend to:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{r}_d(s) &amp;\equiv \sum_{a \in A_s} d(a \mid s) \, r(s, a), \\
[\mathbf{P}_d]_{s,j} &amp;\equiv \sum_{a \in A_s} d(a \mid s) \, p(j \mid s, a).
\end{align*}\]</div>
<p>In both cases, <span class="math notranslate nohighlight">\(\mathbf{r}_d\)</span> denotes a reward vector in <span class="math notranslate nohighlight">\(\mathbb{R}^{|S|}\)</span>, with each component <span class="math notranslate nohighlight">\(\mathbf{r}_d(s)\)</span> representing the reward associated with state <span class="math notranslate nohighlight">\(s\)</span>. Similarly, <span class="math notranslate nohighlight">\(\mathbf{P}_d\)</span> is a transition probability matrix in <span class="math notranslate nohighlight">\(\mathbb{R}^{|S| \times |S|}\)</span>, capturing the transition probabilities under decision rule <span class="math notranslate nohighlight">\(d\)</span>.</p>
<p>For a nonstationary Markovian policy <span class="math notranslate nohighlight">\(\pi = (d_1, d_2, \ldots) \in \Pi^{MR}\)</span>, the expected total discounted reward is given by:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{v}_\gamma^{\pi}(s)=\mathbb{E}\left[\sum_{t=1}^{\infty} \gamma^{t-1} r\left(S_t, A_t\right) \,\middle|\, S_1 = s\right].
\]</div>
<p>Using vector notation, this can be expressed as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{v}_\gamma^{\pi} &amp;= \sum_{t=1}^{\infty} \gamma^{t-1} \mathbf{P}_\pi^{t-1} \mathbf{r}_{d_1} \\
&amp;= \mathbf{r}_{d_1} + \gamma \mathbf{P}_{d_1} \mathbf{r}_{d_2} + \gamma^2 \mathbf{P}_{d_1} \mathbf{P}_{d_2} \mathbf{r}_{d_3} + \cdots \\
&amp;= \mathbf{r}_{d_1} + \gamma \mathbf{P}_{d_1} \left( \mathbf{r}_{d_2} + \gamma \mathbf{P}_{d_2} \mathbf{r}_{d_3} + \gamma^2 \mathbf{P}_{d_2} \mathbf{P}_{d_3} \mathbf{r}_{d_4} + \cdots \right).
\end{aligned}
\end{split}\]</div>
<p>This formulation leads to a recursive relationship:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathbf{v}_\gamma^\pi &amp;= \mathbf{r}_{d_1} + \gamma \mathbf{P}_{d_1} \mathbf{v}_\gamma^{\pi^{\prime}}\\
&amp;=\sum_{t=1}^{\infty} \gamma^{t-1} \mathbf{P}_\pi^{t-1} \mathbf{r}_{d_t}
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\pi^{\prime} = (d_2, d_3, \ldots)\)</span>.</p>
<p>For stationary policies, where <span class="math notranslate nohighlight">\(\pi = d^{\infty} \equiv (d, d, \ldots)\)</span>, the total expected reward simplifies to:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathbf{v}_\gamma^{d^\infty} &amp;= \mathbf{r}_d+ \gamma \mathbf{P}_d \mathbf{v}_\gamma^{d^\infty} \\
&amp;=\sum_{t=1}^{\infty} \gamma^{t-1} \mathbf{P}_d^{t-1} \mathbf{r}_{d}
\end{align*}
\end{split}\]</div>
<p>This last expression is called a Neumann series expansion, and it’s guaranteed to exists under the assumptions of bounded reward and discount factor strictly less than one. Consequently, for a stationary policy, <span class="math notranslate nohighlight">\(\mathbf{v}_\gamma^{d^\infty}\)</span> can be determined as the solution to the linear equation:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{v} = \mathbf{r}_d+ \gamma \mathbf{P}_d\mathbf{v},
\]</div>
<p>which can be rearranged to:</p>
<div class="math notranslate nohighlight">
\[
(\mathbf{I} - \gamma \mathbf{P}_d) \mathbf{v} = \mathbf{r}_d.
\]</div>
<p>We can also characterize <span class="math notranslate nohighlight">\(\mathbf{v}_\gamma^{d^\infty}\)</span> as the solution to an operator equation. More specifically, define the linear transformation <span class="math notranslate nohighlight">\(\mathrm{L}_d\)</span> by</p>
<div class="math notranslate nohighlight">
\[
\mathrm{L}_d \mathbf{v} \equiv \mathbf{r}_d+\gamma \mathbf{P}_dv
\]</div>
<p>for any <span class="math notranslate nohighlight">\(v \in V\)</span>. Intuitively, <span class="math notranslate nohighlight">\(\mathrm{L}_d\)</span> takes a value function <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> as input and returns a new value function that combines immediate rewards (<span class="math notranslate nohighlight">\(\mathbf{r}_d\)</span>) with discounted future values (<span class="math notranslate nohighlight">\(\gamma \mathbf{P}_d\mathbf{v}\)</span>).</p>
<!-- 

A key property of $\mathrm{L}_d$ is that it maps bounded functions to bounded functions, provided certain conditions are met. This is formalized in the following lemma:


```{prf:lemma} Bounded Reward and Value Function
:label: bounded-reward-value

Let $S$ be a discrete state space, and assume that the reward function is bounded such that $|r(s, a)| \leq M$ for all actions $a \in A_s$ and states $s \in S$. For any discount factor $\gamma$ where $0 \leq \gamma \leq 1$, and for all value functions $v \in V$ and decision rules $d \in D^{MR}$, the following holds:

$$
\mathbf{r}_d+ \gamma \mathbf{P}_d \mathbf{v} \in V
$$

where $V$ is the space of bounded real-valued functions on $S$, $\mathbf{r}_d$ is the reward vector, and $\mathbf{P}_d$ is the transition probability matrix under decision rule $d$.
```

```{prf:proof}
We will prove this lemma in three steps:

1. First, we'll show that $\mathbf{r}_d\in V$.
2. Then, we'll prove that $\mathbf{P}_dv \in V$ for all $v \in V$.
3. Finally, we'll combine these results to show that $\mathbf{r}_d+ \gamma \mathbf{P}_dv \in V$.

**Step 1: Showing $\mathbf{r}_d\in V$**

Given that $|r(s, a)| \leq M$ for all $a \in A_s$ and $s \in S$, we can conclude that $\|\mathbf{r}_d\| \leq M$ for all $d \in D^{MR}$. This is because $\mathbf{r}_d$ is a vector whose components are weighted averages of $r(s, a)$ values, which are all bounded by $M$. Therefore, $\mathbf{r}_d$ is a bounded function on $S$, meaning $\mathbf{r}_d\in V$.

**Step 2: Showing $\mathbf{P}_d \mathbf{v} \in V$ for all $\mathbf{v} \in V$**

$\mathbf{P}_d$ is a probability matrix, which means that each row sums to 1. This property implies that $\|\mathbf{P}_d\| = 1$. Now, for any $v \in V$, we have:

$$
\|\mathbf{P}_d\mathbf{v}\| \leq \|\mathbf{P}_d\| \|\mathbf{v}\| = \|\mathbf{v}\|
$$

This inequality shows that if $v$ is bounded (which it is, since $v \in V$), then $\mathbf{P}_dv$ is also bounded by the same value. Therefore, $\mathbf{P}_d\mathbf{v} \in V$ for all $\mathbf{v} \in V$.

**Step 3: Combining the results**

We've shown that $\mathbf{r}_d\in V$ and $\mathbf{P}_d \mathbf{v} \in V$. Since $V$ is a vector space, it is closed under addition and scalar multiplication. Therefore, for any $\gamma$ where $0 \leq \gamma \leq 1$, we can conclude that:

$$
\mathbf{r}_d+ \gamma \mathbf{P}_d \mathbf{v} \in V
$$

This completes the proof.
``` -->
<p>Therefore, we view <span class="math notranslate nohighlight">\(\mathrm{L}_d\)</span> as an operator mapping elements of <span class="math notranslate nohighlight">\(V\)</span> to <span class="math notranslate nohighlight">\(V\)</span>: ie. <span class="math notranslate nohighlight">\(\mathrm{L}_d: V \rightarrow V\)</span>. The fact that the value function of a policy is the solution to a system of equations can then be expressed with the statement:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{v}_\gamma^{d^\infty}=\mathrm{L}_d \mathbf{v}_\gamma^{d^\infty} \text {. }
\]</div>
</section>
<section id="solving-operator-equations">
<h2><span class="section-number">12.3. </span>Solving Operator Equations<a class="headerlink" href="#solving-operator-equations" title="Link to this heading">#</a></h2>
<p>The operator equation we encountered in MDPs, <span class="math notranslate nohighlight">\(\mathbf{v}_\gamma^{d^\infty} = \mathrm{L}_d \mathbf{v}_\gamma^{d^\infty}\)</span>, is a specific instance of a more general class of problems known as operator equations. These equations appear in various fields of mathematics and applied sciences, ranging from differential equations to functional analysis.</p>
<p>Operator equations can take several forms, each with its own characteristics and solution methods:</p>
<ol class="arabic simple">
<li><p><strong>Fixed Point Form</strong>: <span class="math notranslate nohighlight">\(x = \mathrm{T}(x)\)</span>, where <span class="math notranslate nohighlight">\(\mathrm{T}: X \rightarrow X\)</span>.
Common in fixed-point problems, such as our MDP equation, we seek a fixed point <span class="math notranslate nohighlight">\(x^*\)</span> such that <span class="math notranslate nohighlight">\(x^* = \mathrm{T}(x^*)\)</span>.</p></li>
<li><p><strong>General Operator Equation</strong>: <span class="math notranslate nohighlight">\(\mathrm{T}(x) = y\)</span>, where <span class="math notranslate nohighlight">\(\mathrm{T}: X \rightarrow Y\)</span>.
Here, <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> can be different spaces. We seek an <span class="math notranslate nohighlight">\(x \in X\)</span> that satisfies the equation for a given <span class="math notranslate nohighlight">\(y \in Y\)</span>.</p></li>
<li><p><strong>Nonlinear Equation</strong>: <span class="math notranslate nohighlight">\(\mathrm{T}(x) = 0\)</span>, where <span class="math notranslate nohighlight">\(\mathrm{T}: X \rightarrow Y\)</span>.
A special case of the general operator equation where we seek roots or zeros of the operator.</p></li>
<li><p><strong>Variational Inequality</strong>: Find <span class="math notranslate nohighlight">\(x^* \in K\)</span> such that <span class="math notranslate nohighlight">\(\langle \mathrm{T}(x^*), x - x^* \rangle \geq 0\)</span> for all <span class="math notranslate nohighlight">\(x \in K\)</span>.
Here, <span class="math notranslate nohighlight">\(K\)</span> is a closed convex subset of <span class="math notranslate nohighlight">\(X\)</span>, and <span class="math notranslate nohighlight">\(\mathrm{T}: K \rightarrow X^*\)</span> (the dual space of <span class="math notranslate nohighlight">\(X\)</span>). These problems often arise in optimization, game theory, and partial differential equations.</p></li>
</ol>
<section id="successive-approximation-method">
<h3><span class="section-number">12.3.1. </span>Successive Approximation Method<a class="headerlink" href="#successive-approximation-method" title="Link to this heading">#</a></h3>
<p>For equations in fixed point form, a common numerical solution method is successive approximation, also known as fixed-point iteration:</p>
<div class="proof algorithm admonition" id="successive-approximation">
<p class="admonition-title"><span class="caption-number">Algorithm 12.1 </span> (Successive Approximation)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> An operator <span class="math notranslate nohighlight">\(\mathrm{T}: X \rightarrow X\)</span>, an initial guess <span class="math notranslate nohighlight">\(x_0 \in X\)</span>, and a tolerance <span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span><br />
<strong>Output:</strong> An approximate fixed point <span class="math notranslate nohighlight">\(x^*\)</span> such that <span class="math notranslate nohighlight">\(\|x^* - \mathrm{T}(x^*)\| &lt; \epsilon\)</span></p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(n = 0\)</span></p></li>
<li><p><strong>repeat</strong><br />
3. Compute <span class="math notranslate nohighlight">\(x_{n+1} = \mathrm{T}(x_n)\)</span><br />
4. If <span class="math notranslate nohighlight">\(\|x_{n+1} - x_n\| &lt; \epsilon\)</span>, <strong>return</strong> <span class="math notranslate nohighlight">\(x_{n+1}\)</span><br />
5. Set <span class="math notranslate nohighlight">\(n = n + 1\)</span></p></li>
<li><p><strong>until</strong> convergence or maximum iterations reached</p></li>
</ol>
</section>
</div><p>The convergence of successive approximation depends on the properties of the operator <span class="math notranslate nohighlight">\(\mathrm{T}\)</span>. In the simplest and most common setting, we assume <span class="math notranslate nohighlight">\(\mathrm{T}\)</span> is a contraction mapping. The Banach Fixed-Point Theorem then guarantees that <span class="math notranslate nohighlight">\(\mathrm{T}\)</span> has a unique fixed point, and the successive approximation method will converge to this fixed point from any starting point. Specifically, <span class="math notranslate nohighlight">\(\mathrm{T}\)</span> is a contraction if there exists a constant <span class="math notranslate nohighlight">\(q \in [0,1)\)</span> such that for all <span class="math notranslate nohighlight">\(x,y \in X\)</span>:</p>
<div class="math notranslate nohighlight">
\[
d(\mathrm{T}(x), \mathrm{T}(y)) \leq q \cdot d(x,y)
\]</div>
<p>where <span class="math notranslate nohighlight">\(d\)</span> is the metric on <span class="math notranslate nohighlight">\(X\)</span>. In this case, the rate of convergence is linear, with error bound:</p>
<div class="math notranslate nohighlight">
\[
d(x_n, x^*) \leq \frac{q^n}{1-q} d(x_1, x_0)
\]</div>
<p>However, the contraction mapping condition is not the only one that can lead to convergence. For instance, if <span class="math notranslate nohighlight">\(\mathrm{T}\)</span> is nonexpansive (i.e., Lipschitz continuous with Lipschitz constant 1) and <span class="math notranslate nohighlight">\(X\)</span> is a Banach space with certain geometrical properties (e.g., uniformly convex), then under additional conditions (e.g., <span class="math notranslate nohighlight">\(\mathrm{T}\)</span> has at least one fixed point), the successive approximation method can still converge, albeit potentially more slowly than in the contraction case.</p>
<p>In practice, when dealing with specific problems like MDPs or differential equations, the properties of the operator often naturally align with one of these convergence conditions. For example, in discounted MDPs, the Bellman operator is a contraction in the supremum norm, which guarantees the convergence of value iteration.</p>
</section>
<section id="newton-kantorovich-method">
<h3><span class="section-number">12.3.2. </span>Newton-Kantorovich Method<a class="headerlink" href="#newton-kantorovich-method" title="Link to this heading">#</a></h3>
<p>The Newton-Kantorovich method is a generalization of Newton’s method from finite dimensional vector spaces to infinite dimensional function spaces: rather than iterating in the space of vectors, we are iterating in the space of functions. Just as in the finite-dimensional counterpart, the idea is to improve the rate of convergence of our method by taking an “educated guess” on where to move next using a linearization of our operator at the current point. Now the concept of linearization, which is synonymous with derivative, will also require a generalization. Here we are in essence trying to quantify how the output of the operator <span class="math notranslate nohighlight">\(\mathrm{T}\)</span> – a function – varies as we perturb its input – also a function. The right generalization here is that of the Fréchet derivative.</p>
<p>Before we delve into the Fréchet derivative, it’s important to understand the context in which it operates: Banach spaces. A Banach space is a complete normed vector space: ie a vector space, that has a norm, and which every Cauchy sequence convergeces.   Banach spaces provide a natural generalization of finite-dimensional vector spaces to infinite-dimensional settings.
The norm in a Banach space allows us to quantify the “size” of functions and the “distance” between functions. This allows us to define notions of continuity, differentiability, and for analyzing the convergence of our method.
Furthermore, the completeness property of Banach spaces ensures that we have a well-defined notion of convergence.</p>
<p>In the context of the Newton-Kantorovich method, we typically work with an operator <span class="math notranslate nohighlight">\(\mathrm{T}: X \to Y\)</span>, where both <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are Banach spaces and whose Fréchet derivative at a point <span class="math notranslate nohighlight">\(x \in X\)</span>, denoted <span class="math notranslate nohighlight">\(\mathrm{T}'(x)\)</span>, is a bounded linear operator from <span class="math notranslate nohighlight">\(X\)</span> to <span class="math notranslate nohighlight">\(Y\)</span> such that:</p>
<div class="math notranslate nohighlight">
\[
\lim_{h \to 0} \frac{\|\mathrm{T}(x + h) - \mathrm{T}(x) - \mathrm{T}'(x)h\|_Y}{\|h\|_X} = 0
\]</div>
<p>where <span class="math notranslate nohighlight">\(\|\cdot\|_X\)</span> and <span class="math notranslate nohighlight">\(\|\cdot\|_Y\)</span> are the norms in <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> respectively. In other words, <span class="math notranslate nohighlight">\(\mathrm{T}'(x)\)</span> is the best linear approximation of <span class="math notranslate nohighlight">\(\mathrm{T}\)</span> near <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>Now apart from those mathematical technicalities, Newton-Kantorovich has in essence the same structure as that of the original Newton’s method. That is, it applies the following sequence of steps:</p>
<ol class="arabic">
<li><p><strong>Linearize the Operator</strong>:
Given an approximation <span class="math notranslate nohighlight">\( x_n \)</span>, we consider the Fréchet derivative of <span class="math notranslate nohighlight">\( \mathrm{T} \)</span>, denoted by <span class="math notranslate nohighlight">\( \mathrm{T}'(x_n) \)</span>. This derivative is a linear operator that provides a local approximation of  <span class="math notranslate nohighlight">\( \mathrm{T} \)</span>, near <span class="math notranslate nohighlight">\( x_n \)</span>.</p></li>
<li><p><strong>Set Up the Newton Step</strong>:
The method then solves the linearized equation for a correction <span class="math notranslate nohighlight">\( h_n \)</span>:</p>
<div class="math notranslate nohighlight">
\[
   \mathrm{T}(x_n) h_n = \mathrm{T}(x_n) - x_n.
   \]</div>
<p>This equation represents a linear system where <span class="math notranslate nohighlight">\( h_n \)</span> is chosen to minimize the difference between <span class="math notranslate nohighlight">\( x_n \)</span> and <span class="math notranslate nohighlight">\( \mathrm{T}(x_n) \)</span> with respect to the operator’s local behavior.</p>
</li>
<li><p><strong>Update the Solution</strong>:
The new approximation <span class="math notranslate nohighlight">\( x_{n+1} \)</span> is then given by:</p>
<div class="math notranslate nohighlight">
\[
   x_{n+1} = x_n - h_n.
   \]</div>
<p>This correction step refines <span class="math notranslate nohighlight">\( x_n \)</span>, bringing it closer to the true solution.</p>
</li>
<li><p><strong>Repeat Until Convergence</strong>:
We repeat the linearization and update steps until the solution <span class="math notranslate nohighlight">\( x_n \)</span> converges to the desired tolerance, which can be verified by checking that <span class="math notranslate nohighlight">\( \|\mathrm{T}(x_n) - x_n\| \)</span> is sufficiently small, or by monitoring the norm <span class="math notranslate nohighlight">\( \|x_{n+1} - x_n\| \)</span>.</p></li>
</ol>
<p>The convergence of Newton-Kantorovich does not hinge on <span class="math notranslate nohighlight">\( \mathrm{T} \)</span> being a contraction over the entire domain – as it could be the case for successive approximation. The convergence properties of the Newton-Kantorovich method are as follows:</p>
<ol class="arabic">
<li><p><strong>Local Convergence</strong>: Under mild conditions (e.g., <span class="math notranslate nohighlight">\(\mathrm{T}\)</span> is Fréchet differentiable and <span class="math notranslate nohighlight">\(\mathrm{T}'(x)\)</span> is invertible near the solution), the method converges locally. This means that if the initial guess is sufficiently close to the true solution, the method will converge.</p></li>
<li><p><strong>Global Convergence</strong>: Global convergence is not guaranteed in general. However, under stronger conditions (e.g.,  <span class="math notranslate nohighlight">\( \mathrm{T} \)</span>, is analytic and satisfies certain bounds), the method can converge globally.</p></li>
<li><p><strong>Rate of Convergence</strong>: When the method converges, it typically exhibits quadratic convergence. This means that the error at each step is proportional to the square of the error at the previous step:</p>
<div class="math notranslate nohighlight">
\[
   \|x_{n+1} - x^*\| \leq C\|x_n - x^*\|^2
   \]</div>
<p>where <span class="math notranslate nohighlight">\(x^*\)</span> is the true solution and <span class="math notranslate nohighlight">\(C\)</span> is some constant. This quadratic convergence is significantly faster than the linear convergence typically seen in methods like successive approximation.</p>
</li>
</ol>
</section>
</section>
<section id="optimality-equations-for-infinite-horizon-mdps">
<h2><span class="section-number">12.4. </span>Optimality Equations for Infinite-Horizon MDPs<a class="headerlink" href="#optimality-equations-for-infinite-horizon-mdps" title="Link to this heading">#</a></h2>
<p>Recall that in the finite-horizon setting, the optimality equations are:</p>
<div class="math notranslate nohighlight">
\[
v_n(s) = \max_{a \in A_s} \left\{r(s, a) + \gamma \sum_{j \in S} p(j | s, a) v_{n+1}(j)\right\}
\]</div>
<p>where <span class="math notranslate nohighlight">\(v_n(s)\)</span> is the value function at time step <span class="math notranslate nohighlight">\(n\)</span> for state <span class="math notranslate nohighlight">\(s\)</span>, <span class="math notranslate nohighlight">\(A_s\)</span> is the set of actions available in state <span class="math notranslate nohighlight">\(s\)</span>, <span class="math notranslate nohighlight">\(r(s, a)\)</span> is the reward function, <span class="math notranslate nohighlight">\(\gamma\)</span> is the discount factor, and <span class="math notranslate nohighlight">\(p(j | s, a)\)</span> is the transition probability from state <span class="math notranslate nohighlight">\(s\)</span> to state <span class="math notranslate nohighlight">\(j\)</span> given action <span class="math notranslate nohighlight">\(a\)</span>.</p>
<p>Intuitively, we would expect that by taking the limit of <span class="math notranslate nohighlight">\(n\)</span> to infinity, we might get the nonlinear equations:</p>
<div class="math notranslate nohighlight">
\[
v(s) = \max_{a \in A_s} \left\{r(s, a) + \gamma \sum_{j \in S} p(j | s, a) v(j)\right\}
\]</div>
<p>which are called the optimality equations or Bellman equations for infinite-horizon MDPs.</p>
<p>We can adopt an operator-theoretic perspective by defining a nonlinear operator <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> on the space <span class="math notranslate nohighlight">\(V\)</span> of bounded real-valued functions on the state space <span class="math notranslate nohighlight">\(S\)</span>. We can then show that the value of an MDP is the solution to the following fixed-point equation:</p>
<div class="math notranslate nohighlight">
\[
\mathrm{L} \mathbf{v} \equiv \max_{d \in D^{MD}} \left\{\mathbf{r}_d + \gamma \mathbf{P}_d \mathbf{v}\right\}
\]</div>
<p>where <span class="math notranslate nohighlight">\(D^{MD}\)</span> is the set of Markov deterministic decision rules, <span class="math notranslate nohighlight">\(\mathbf{r}_d\)</span> is the reward vector under decision rule <span class="math notranslate nohighlight">\(d\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{P}_d\)</span> is the transition probability matrix under decision rule <span class="math notranslate nohighlight">\(d\)</span>.</p>
<p>Note that while we write <span class="math notranslate nohighlight">\(\max_{d \in D^{MD}}\)</span>, we do not implement the above operator in this way. Written in this fashion, it would indeed imply that we first need to enumerate all Markov deterministic decision rules and pick the maximum. Now the fact that we compare policies based on their value functions in a componentwise fashion, maxing over the space of Markovian deterministic rules amounts to the following update in component form:</p>
<div class="math notranslate nohighlight">
\[
(\mathrm{L} \mathbf{v})(s) = \max_{a \in \mathcal{A}_s} \left\{r(s,a) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a) v(j)\right\}
\]</div>
<p>The equivalence between these two forms can be shown mathematically, as demonstrated in the following proposition and proof.</p>
<div class="proof proposition admonition" id="proposition-4">
<p class="admonition-title"><span class="caption-number">Proposition 12.1 </span></p>
<section class="proposition-content" id="proof-content">
<p>The operator <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> defined as a maximization over Markov deterministic decision rules:</p>
<div class="math notranslate nohighlight">
\[(\mathrm{L} \mathbf{v})(s) = \max_{d \in D^{MD}} \left\{r(s,d(s)) + \gamma \sum_{j \in \mathcal{S}} p(j|s,d(s)) v(j)\right\}\]</div>
<p>is equivalent to the componentwise maximization over actions:</p>
<div class="math notranslate nohighlight">
\[(\mathrm{L} \mathbf{v})(s) = \max_{a \in \mathcal{A}_s} \left\{r(s,a) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a) v(j)\right\}\]</div>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Let’s define the right-hand side of the first equation as <span class="math notranslate nohighlight">\(R_1\)</span> and the right-hand side of the second equation as <span class="math notranslate nohighlight">\(R_2\)</span>. We’ll prove that <span class="math notranslate nohighlight">\(R_1 \leq R_2\)</span> and <span class="math notranslate nohighlight">\(R_2 \leq R_1\)</span>, which will establish their equality.</p>
<p>Step 1: Proving <span class="math notranslate nohighlight">\(R_1 \leq R_2\)</span></p>
<p>For any <span class="math notranslate nohighlight">\(d \in D^{MD}\)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[r(s,d(s)) + \gamma \sum_{j \in \mathcal{S}} p(j|s,d(s)) v(j) \leq \max_{a \in \mathcal{A}_s} \left\{r(s,a) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a) v(j)\right\}\]</div>
<p>This is because <span class="math notranslate nohighlight">\(d(s) \in \mathcal{A}_s\)</span>, so the left-hand side is included in the set over which we’re maximizing on the right-hand side.</p>
<p>Taking the maximum over all <span class="math notranslate nohighlight">\(d \in D^{MD}\)</span> on the left-hand side doesn’t change this inequality:</p>
<div class="math notranslate nohighlight">
\[\max_{d \in D^{MD}} \left\{r(s,d(s)) + \gamma \sum_{j \in \mathcal{S}} p(j|s,d(s)) v(j)\right\} \leq \max_{a \in \mathcal{A}_s} \left\{r(s,a) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a) v(j)\right\}\]</div>
<p>Therefore, <span class="math notranslate nohighlight">\(R_1 \leq R_2\)</span>.</p>
<p>Step 2: Proving <span class="math notranslate nohighlight">\(R_2 \leq R_1\)</span></p>
<p>Let <span class="math notranslate nohighlight">\(a^* \in \mathcal{A}_s\)</span> be the action that achieves the maximum in <span class="math notranslate nohighlight">\(R_2\)</span>. We can construct a Markov deterministic decision rule <span class="math notranslate nohighlight">\(d^*\)</span> such that <span class="math notranslate nohighlight">\(d^*(s) = a^*\)</span> and <span class="math notranslate nohighlight">\(d^*(s')\)</span> is arbitrary for <span class="math notranslate nohighlight">\(s' \neq s\)</span>. Then:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
R_2 &amp;= r(s,a^*) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a^*) v(j) \\
&amp;= r(s,d^*(s)) + \gamma \sum_{j \in \mathcal{S}} p(j|s,d^*(s)) v(j) \\
&amp;\leq \max_{d \in D^{MD}} \left\{r(s,d(s)) + \gamma \sum_{j \in \mathcal{S}} p(j|s,d(s)) v(j)\right\} \\
&amp;= R_1
\end{align*}\end{split}\]</div>
<p>Conclusion:
Since we’ve shown <span class="math notranslate nohighlight">\(R_1 \leq R_2\)</span> and <span class="math notranslate nohighlight">\(R_2 \leq R_1\)</span>, we can conclude that <span class="math notranslate nohighlight">\(R_1 = R_2\)</span>, which proves the equivalence of the two forms of the operator.</p>
</div>
</section>
<section id="algorithms-for-solving-the-optimality-equations">
<h2><span class="section-number">12.5. </span>Algorithms for Solving the Optimality Equations<a class="headerlink" href="#algorithms-for-solving-the-optimality-equations" title="Link to this heading">#</a></h2>
<p>The optimality equations are operator equations. Therefore, we can apply general numerical methods to solve them. Applying the successive approximation method to the Bellman optimality equation yields a method known as “value iteration” in dynamic programming. A direct application of the blueprint for successive approximation yields the following algorithm:</p>
<div class="proof algorithm admonition" id="value-iteration">
<p class="admonition-title"><span class="caption-number">Algorithm 12.2 </span> (Value Iteration)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> Given an MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span> and tolerance <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span></p>
<p><strong>Output</strong> Compute an <span class="math notranslate nohighlight">\(\varepsilon\)</span>-optimal value function <span class="math notranslate nohighlight">\(V\)</span> and policy <span class="math notranslate nohighlight">\(\pi\)</span></p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(v_0(s) = 0\)</span> for all <span class="math notranslate nohighlight">\(s \in S\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow 0\)</span></p></li>
<li><p><strong>repeat</strong></p>
<ol class="arabic simple">
<li><p>For each <span class="math notranslate nohighlight">\(s \in S\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(v_{n+1}(s) \leftarrow \max_{a \in A} \left\{r(s,a) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a)v_n(s')\right\}\)</span></p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(\delta \leftarrow \|v_{n+1} - v_n\|_\infty\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow n + 1\)</span></p></li>
</ol>
</li>
<li><p><strong>until</strong> <span class="math notranslate nohighlight">\(\delta &lt; \frac{\varepsilon(1-\gamma)}{2\gamma}\)</span></p></li>
<li><p>For each <span class="math notranslate nohighlight">\(s \in S\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\pi(s) \leftarrow \arg\max_{a \in A} \left\{r(s,a) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a)v_n(s')\right\}\)</span></p></li>
</ol>
</li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(v_n, \pi\)</span></p></li>
</ol>
</section>
</div><p>The termination criterion in this algorithm is based on a specific bound that provides guarantees on the quality of the solution. This is in contrast to supervised learning, where we often use arbitrary termination criteria based on computational budget or early stopping when the learning curve flattens. This is because establishing implementable generalization bounds in supervised learning is challenging.</p>
<p>However, in the dynamic programming context, we can derive various bounds that can be implemented in practice. These bounds help us terminate our procedure with a guarantee on the precision of our value function and, correspondingly, on the optimality of the resulting policy.</p>
<div class="proof proposition admonition" id="value-iteration-convergence">
<p class="admonition-title"><span class="caption-number">Proposition 12.2 </span> (Convergence of Value Iteration)</p>
<section class="proposition-content" id="proof-content">
<p>(Adapted from <span id="id4">Puterman [<a class="reference internal" href="bibliography.html#id23" title="Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley &amp; Sons, New York, 1994. ISBN 978-0-471-61977-3. First published in 1994.">29</a>]</span> theorem 6.3.1)</p>
<p>Let <span class="math notranslate nohighlight">\(v_0\)</span> be any initial value function, <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span> a desired accuracy, and let <span class="math notranslate nohighlight">\(\{v_n\}\)</span> be the sequence of value functions generated by value iteration, i.e., <span class="math notranslate nohighlight">\(v_{n+1} = \mathrm{L}v_n\)</span> for <span class="math notranslate nohighlight">\(n \geq 1\)</span>, where <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> is the Bellman optimality operator. Then:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(v_n\)</span> converges to the optimal value function <span class="math notranslate nohighlight">\(v^*_\gamma\)</span>,</p></li>
<li><p>The algorithm terminates in finite time,</p></li>
<li><p>The resulting policy <span class="math notranslate nohighlight">\(\pi_\varepsilon\)</span> is <span class="math notranslate nohighlight">\(\varepsilon\)</span>-optimal, and</p></li>
<li><p>When the algorithm terminates, <span class="math notranslate nohighlight">\(v_{n+1}\)</span> is within <span class="math notranslate nohighlight">\(\varepsilon/2\)</span> of <span class="math notranslate nohighlight">\(v^*_\gamma\)</span>.</p></li>
</ol>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Parts 1. and 2. follow directly from the fact that <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> is a contraction mapping. Hence, by Banach’s fixed-point theorem, it has a unique fixed point (which is <span class="math notranslate nohighlight">\(v^*_\gamma\)</span>), and repeated application of <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> will converge to this fixed point. Moreover, this convergence happens at a linear/geometric rate, which ensures that we reach the termination condition in finite time.</p>
<p>To show that the Bellman optimality operator <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> is a contraction mapping, we need to prove that for any two value functions <span class="math notranslate nohighlight">\(v\)</span> and <span class="math notranslate nohighlight">\(u\)</span>:</p>
<div class="math notranslate nohighlight">
\[\|\mathrm{L}v - \mathrm{L}u\|_\infty \leq \gamma \|V - U\|_\infty\]</div>
<p>where <span class="math notranslate nohighlight">\(\gamma \in [0,1)\)</span> is the discount factor and <span class="math notranslate nohighlight">\(\|\cdot\|_\infty\)</span> is the supremum norm.</p>
<p>Let’s start by writing out the definition of <span class="math notranslate nohighlight">\(\mathrm{L}v\)</span> and <span class="math notranslate nohighlight">\(\mathrm{L}u\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}(\mathrm{L}v)(s) &amp;= \max_{a \in A} \left\{r(s,a) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a)V(s')\right\}\\
   (\mathrm{L}u)(s) &amp;= \max_{a \in A} \left\{r(s,a) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a)U(s')\right\}\end{align*}\end{split}\]</div>
<p>Now, for any state s, let <span class="math notranslate nohighlight">\(a_V\)</span> be the action that achieves the maximum for <span class="math notranslate nohighlight">\(\mathrm{L}\)</span>V, and <span class="math notranslate nohighlight">\(a_U\)</span> be the action that achieves the maximum for <span class="math notranslate nohighlight">\(\mathrm{L}u\)</span>. Then:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}(\mathrm{L}v)(s)&amp;= r(s,a_V) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a_V)V(s')\\
   (\mathrm{L}u)(s) &amp;= r(s,a_U) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a_U)U(s')\end{align*}\end{split}\]</div>
<p>By the definition of <span class="math notranslate nohighlight">\(a_V\)</span> and <span class="math notranslate nohighlight">\(a_U\)</span>, we know that:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}(\mathrm{L}v)(s) &amp;\geq r(s,a_U) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a_U)V(s')\\
   (\mathrm{L}u)(s) &amp;\geq r(s,a_V) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a_V)U(s')\end{align*}\end{split}\]</div>
<p>Subtracting these inequalities:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}(\mathrm{L}v)(s) - (\mathrm{L}u)(s) &amp;\leq \gamma \sum_{j \in \mathcal{S}} p(j|s,a_V)(V(s') - U(s'))\\
   (\mathrm{L}u)(s) - (\mathrm{L}v)(s) &amp;\leq \gamma \sum_{j \in \mathcal{S}} p(j|s,a_U)(U(s') - V(s'))\end{align*}\end{split}\]</div>
<p>Taking the absolute value of both sides and using the fact that <span class="math notranslate nohighlight">\(\sum_{j \in \mathcal{S}} p(j|s,a) = 1\)</span> for any s and a:</p>
<div class="math notranslate nohighlight">
\[|(\mathrm{L}v)(s) - (\mathrm{L}u)(s)| \leq \gamma \max_{j \in \mathcal{S}} |V(s') - U(s')| = \gamma \|V - U\|_\infty\]</div>
<p>Since this holds for all <span class="math notranslate nohighlight">\(s\)</span>, we can take the supremum over <span class="math notranslate nohighlight">\(s\)</span>:</p>
<div class="math notranslate nohighlight">
\[\|\mathrm{L}v - \mathrm{L}u\|_\infty \leq \gamma \|V - U\|_\infty\]</div>
<p>Thus, we have shown that <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> is indeed a contraction mapping with contraction factor <span class="math notranslate nohighlight">\(\gamma\)</span>.</p>
<p>Now, let’s focus on parts 3. and 4. Suppose our algorithm has just terminated, i.e., <span class="math notranslate nohighlight">\(\|v_{n+1} - v_n\| &lt; \frac{\varepsilon(1-\gamma)}{2\gamma}\)</span> for some <span class="math notranslate nohighlight">\(n\)</span>. We want to show that our current value function <span class="math notranslate nohighlight">\(v_{n+1}\)</span> and the policy <span class="math notranslate nohighlight">\(\pi_\varepsilon\)</span> derived from it are close to optimal.</p>
<p>We start with the following inequality:</p>
<div class="math notranslate nohighlight">
\[\|V^{\pi_\varepsilon}_\gamma - v^*_\gamma\| \leq \|V^{\pi_\varepsilon}_\gamma - v_{n+1}\| + \|v_{n+1} - v^*_\gamma\|\]</div>
<p>This inequality is derived using the triangle inequality:</p>
<div class="math notranslate nohighlight">
\[\|V^{\pi_\varepsilon}_\gamma - v^*_\gamma\| = \|(V^{\pi_\varepsilon}_\gamma - v_{n+1}) + (v_{n+1} - v^*_\gamma)\| \leq \|V^{\pi_\varepsilon}_\gamma - v_{n+1}\| + \|v_{n+1} - v^*_\gamma\|\]</div>
<p>Let’s focus on the first term, <span class="math notranslate nohighlight">\(\|V^{\pi_\varepsilon}_\gamma - v_{n+1}\|\)</span>. We can expand this:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\|V^{\pi_\varepsilon}_\gamma - v_{n+1}\| &amp;= \|\mathrm{L}_{\pi_\varepsilon}V^{\pi_\varepsilon}_\gamma - v_{n+1}\| \\
&amp;\leq \|\mathrm{L}_{\pi_\varepsilon}V^{\pi_\varepsilon}_\gamma - \mathrm{L}v_{n+1}\| + \|\mathrm{L}v_{n+1} - v_{n+1}\| \\
&amp;= \|\mathrm{L}_{\pi_\varepsilon}V^{\pi_\varepsilon}_\gamma - \mathrm{L}_{\pi_\varepsilon}v_{n+1}\| + \|\mathrm{L}v_{n+1} - \mathrm{L}v_n\| \\
&amp;\leq \gamma\|V^{\pi_\varepsilon}_\gamma - v_{n+1}\| + \gamma\|v_{n+1} - v_n\|
\end{aligned}
\end{split}\]</div>
<p>Here, we’ve used the fact that <span class="math notranslate nohighlight">\(V^{\pi_\varepsilon}_\gamma\)</span> is a fixed point of <span class="math notranslate nohighlight">\(\mathrm{L}_{\pi_\varepsilon}\)</span>, that <span class="math notranslate nohighlight">\(\mathrm{L}_{\pi_\varepsilon}v_{n+1} = \mathrm{L}v_{n+1}\)</span> (by definition of <span class="math notranslate nohighlight">\(\pi_\varepsilon\)</span>), and that both <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> and <span class="math notranslate nohighlight">\(\mathrm{L}_{\pi_\varepsilon}\)</span> are contractions with factor <span class="math notranslate nohighlight">\(\gamma\)</span>.</p>
<p>Rearranging this inequality, we get:</p>
<div class="math notranslate nohighlight">
\[\|V^{\pi_\varepsilon}_\gamma - v_{n+1}\| \leq \frac{\gamma}{1-\gamma}\|v_{n+1} - v_n\|\]</div>
<p>We can derive a similar bound for <span class="math notranslate nohighlight">\(\|v_{n+1} - v^*_\gamma\|\)</span>:</p>
<div class="math notranslate nohighlight">
\[\|v_{n+1} - v^*_\gamma\| \leq \frac{\gamma}{1-\gamma}\|v_{n+1} - v_n\|\]</div>
<p>Now, remember that our algorithm terminated when <span class="math notranslate nohighlight">\(\|v_{n+1} - v_n\| &lt; \frac{\varepsilon(1-\gamma)}{2\gamma}\)</span>. Plugging this into our bounds:</p>
<div class="math notranslate nohighlight">
\[\|V^{\pi_\varepsilon}_\gamma - v_{n+1}\| \leq \frac{\gamma}{1-\gamma} \cdot \frac{\varepsilon(1-\gamma)}{2\gamma} = \frac{\varepsilon}{2}\]</div>
<div class="math notranslate nohighlight">
\[\|v_{n+1} - v^*_\gamma\| \leq \frac{\gamma}{1-\gamma} \cdot \frac{\varepsilon(1-\gamma)}{2\gamma} = \frac{\varepsilon}{2}\]</div>
<p>Finally, combining these results with our initial inequality:</p>
<div class="math notranslate nohighlight">
\[\|V^{\pi_\varepsilon}_\gamma - v^*_\gamma\| \leq \|V^{\pi_\varepsilon}_\gamma - v_{n+1}\| + \|v_{n+1} - v^*_\gamma\| \leq \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon\]</div>
<p>This completes the proof. We’ve shown that when the algorithm terminates, our value function <span class="math notranslate nohighlight">\(v_{n+1}\)</span> is within <span class="math notranslate nohighlight">\(\varepsilon/2\)</span> of optimal (part 4.), and our policy <span class="math notranslate nohighlight">\(\pi_\varepsilon\)</span> is <span class="math notranslate nohighlight">\(\varepsilon\)</span>-optimal (part 3.).</p>
</div>
<section id="newton-kantorovich-applied-to-the-optimality-equations">
<h3><span class="section-number">12.5.1. </span>Newton-Kantorovich applied to the Optimality Equations<a class="headerlink" href="#newton-kantorovich-applied-to-the-optimality-equations" title="Link to this heading">#</a></h3>
<p>Another perspective on the optimality equations is that instead of looking for <span class="math notranslate nohighlight">\(v_\gamma^\star\)</span> as the fixed-point of <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> in the fixed-point problem find <span class="math notranslate nohighlight">\(v\)</span> such that <span class="math notranslate nohighlight">\(\mathrm{L} v = v\)</span>, we consider instead the related form in the nonlinear operator equation <span class="math notranslate nohighlight">\(\mathrm{L}v - v = 0\)</span>. Writing things in this way highlights the fact that we could also consider using Newton-Kantorovich iteration to solve this equation instead of the successive approximation method.</p>
<p>If we were to apply the blueprint of Newton-Kantorovich, we would get:</p>
<div class="proof algorithm admonition" id="nk-bellman">
<p class="admonition-title"><span class="caption-number">Algorithm 12.3 </span> (Newton-Kantorovich for Bellman Optimality Equation)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, initial guess <span class="math notranslate nohighlight">\(v_0\)</span>, tolerance <span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span></p>
<p><strong>Output:</strong> Approximate optimal value function <span class="math notranslate nohighlight">\(v^\star\)</span></p>
<ol class="arabic">
<li><p>Initialize <span class="math notranslate nohighlight">\(n = 0\)</span></p></li>
<li><p><strong>repeat</strong></p>
<ol class="arabic" start="3">
<li><p>Compute the Fréchet derivative of <span class="math notranslate nohighlight">\(\mathrm{B}(v) = \mathrm{L}v - v\)</span> at <span class="math notranslate nohighlight">\(v_n\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathrm{B}'(v_n) = \mathrm{L}' - \mathrm{I}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathrm{I}\)</span> is the identity operator</p>
</li>
<li><p>Solve the linear equation for <span class="math notranslate nohighlight">\(h_n\)</span>:</p>
<div class="math notranslate nohighlight">
\[(\mathrm{L}' - \mathrm{I})h_n = v_n - \mathrm{L}v_n\]</div>
</li>
<li><p>Update: <span class="math notranslate nohighlight">\(v_{n+1} = v_n - h_n\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n = n + 1\)</span></p></li>
</ol>
</li>
<li><p><strong>until</strong> <span class="math notranslate nohighlight">\(\|\mathrm{L}v_n - v_n\| &lt; \epsilon\)</span></p></li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(v_n\)</span></p></li>
</ol>
</section>
</div><p>The key difference in this application is the specific form of our operator <span class="math notranslate nohighlight">\(\mathrm{B}(v) = \mathrm{L}v - v\)</span>, where <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> is the Bellman optimality operator. The Fréchet derivative of this operator is <span class="math notranslate nohighlight">\(\mathrm{B}'(v) = \mathrm{L}' - \mathrm{I}\)</span>, where <span class="math notranslate nohighlight">\(\mathrm{L}'\)</span> is the Fréchet derivative of the Bellman optimality operator and <span class="math notranslate nohighlight">\(\mathrm{I}\)</span> is the identity operator.</p>
<p>Let’s examine the Fréchet derivative of <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> more closely. The Bellman optimality operator <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[(\mathrm{L}v)(s) \equiv \max_{a \in \mathcal{A}_s} \left\{r(s,a) + \gamma \sum_{j \in S} p(j|s,a)v(j)\right\}\]</div>
<p>For each state <span class="math notranslate nohighlight">\(s\)</span>, let <span class="math notranslate nohighlight">\(a^*(s)\)</span> be the action that achieves the maximum in <span class="math notranslate nohighlight">\((\mathrm{L}v)(s)\)</span>. Then, for any function <span class="math notranslate nohighlight">\(h\)</span>, the Fréchet derivative of <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> <strong>at</strong> v has the following effect when applied to any function <span class="math notranslate nohighlight">\(h\)</span>:</p>
<div class="math notranslate nohighlight">
\[(\mathrm{L}'(v)h)(s) \equiv \gamma \sum_{j \in S} p(j|s,a^*(s))h(j)\]</div>
<p>This means that the Fréchet derivative <span class="math notranslate nohighlight">\(\mathrm{L}'(v)\)</span> is a linear operator that, when applied to a function <span class="math notranslate nohighlight">\(h\)</span>, gives a new function whose value at each state <span class="math notranslate nohighlight">\(s\)</span> is a discounted expectation of <span class="math notranslate nohighlight">\(h\)</span> over the next states, using the transition probabilities corresponding to the optimal action <span class="math notranslate nohighlight">\(a^*(s)\)</span> at the current point <span class="math notranslate nohighlight">\(v\)</span>.</p>
<p>Now, let’s look more closely at what happens in each iteration of this Newton-Kantorovich method:</p>
<ol class="arabic">
<li><p>In step 3, when we compute <span class="math notranslate nohighlight">\(\mathrm{L}'(v_n)\)</span>, we’re essentially defining a policy based on the current value function estimate. This policy chooses the action <span class="math notranslate nohighlight">\(a^*(s)\)</span> for each state <span class="math notranslate nohighlight">\(s\)</span>.</p></li>
<li><p>In step 4, we solve the linear equation:</p>
<div class="math notranslate nohighlight">
\[(\mathrm{L}' - \mathrm{I})h_n = v_n - \mathrm{L}v_n\]</div>
<p>This can be rearranged to:</p>
<div class="math notranslate nohighlight">
\[\mathrm{L}'h_n = \mathrm{L}v_n\]</div>
<p>Solving this equation is equivalent to evaluating the policy defined by <span class="math notranslate nohighlight">\(a^*(s)\)</span>.</p>
</li>
<li><p>The update in step 5 can be rewritten as:</p>
<div class="math notranslate nohighlight">
\[v_{n+1} = \mathrm{L}v_n\]</div>
<p>This step improves our value function estimate based on the policy we just evaluated.</p>
</li>
</ol>
<p>Interestingly, this sequence of steps - deriving a policy from the current value function, evaluating that policy, and then improving the value function - is a well-known algorithm in dynamic programming called policy iteration. In fact, policy iteration can be viewed as simply the application of the Newton-Kantorovich method to the operator <span class="math notranslate nohighlight">\(\mathrm{B}(v) = \mathrm{L}v - v\)</span>.</p>
</section>
<section id="policy-iteration">
<h3><span class="section-number">12.5.2. </span>Policy Iteration<a class="headerlink" href="#policy-iteration" title="Link to this heading">#</a></h3>
<p>While we derived policy iteration-like steps from the Newton-Kantorovich method, it’s worth examining policy iteration as a standalone algorithm, as it has been traditionally presented in the field of dynamic programming.</p>
<p>The policy iteration algorithm for discounted Markov decision problems is as follows:</p>
<div class="proof algorithm admonition" id="policy-iteration">
<p class="admonition-title"><span class="caption-number">Algorithm 12.4 </span> (Policy Iteration)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>
<strong>Output:</strong> Optimal policy <span class="math notranslate nohighlight">\(\pi^*\)</span></p>
<ol class="arabic">
<li><p>Initialize: <span class="math notranslate nohighlight">\(n = 0\)</span>, select an arbitrary decision rule <span class="math notranslate nohighlight">\(d_0 \in D\)</span></p></li>
<li><p><strong>repeat</strong>
3. (Policy evaluation) Obtain <span class="math notranslate nohighlight">\(\mathbf{v}^n\)</span> by solving:</p>
<div class="math notranslate nohighlight">
\[(\mathbf{I}-\gamma \mathbf{P}_{d_n}) \mathbf{v} = \mathbf{r}_{d_n}\]</div>
<ol class="arabic" start="4">
<li><p>(Policy improvement) Choose <span class="math notranslate nohighlight">\(d_{n+1}\)</span> to satisfy:</p>
<div class="math notranslate nohighlight">
\[d_{n+1} \in \arg\max_{d \in D}\left\{\mathbf{r}_d+\gamma \mathbf{P}_d \mathbf{v}^n\right\}\]</div>
<p>Set <span class="math notranslate nohighlight">\(d_{n+1} = d_n\)</span> if possible.</p>
</li>
<li><p>If <span class="math notranslate nohighlight">\(d_{n+1} = d_n\)</span>, <strong>return</strong> <span class="math notranslate nohighlight">\(d^* = d_n\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n = n + 1\)</span></p></li>
</ol>
</li>
<li><p><strong>until</strong> convergence</p></li>
</ol>
</section>
</div><p>As opposed to value iteration, this algorithm produces a sequence of both deterministic Markovian decision rules <span class="math notranslate nohighlight">\(\{d_n\}\)</span> and value functions <span class="math notranslate nohighlight">\(\{\mathbf{v}^n\}\)</span>. We recognize in this algorithm the linearization step of the Newton-Kantorovich procedure, which takes place here in the policy evaluation step 3 where we solve the linear system <span class="math notranslate nohighlight">\((\mathbf{I}-\gamma \mathbf{P}_{d_n}) \mathbf{v} = \mathbf{r}_{d_n}\)</span>. In practice, this linear sytem could be solved either using direct methods (eg. Gaussian elimination), using  simple iterative methods such as the successive approximation method for policy evaluation, or more sophisticated methods such as GMRES.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="mpc.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">7. </span>Model Predictive Control</p>
      </div>
    </a>
    <a class="right-next"
       href="adp.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">13. </span>Approximate Dynamic Programming</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">11. Dynamic Programming</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#notation-in-operations-reseach">11.1. Notation in Operations Reseach</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-rules-and-policies">11.2. Decision Rules and Policies</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-an-optimal-policy">11.3. What is an Optimal Policy?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-sample-size-determination-in-pharmaceutical-development">11.4. Example: Sample Size Determination in Pharmaceutical Development</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#infinite-horizon-mdps">12. Infinite-Horizon MDPs</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-horizon-interpretation-of-discounting">12.1. Random Horizon Interpretation of Discounting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-representation-in-markov-decision-processes">12.2. Vector Representation in Markov Decision Processes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-operator-equations">12.3. Solving Operator Equations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#successive-approximation-method">12.3.1. Successive Approximation Method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#newton-kantorovich-method">12.3.2. Newton-Kantorovich Method</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimality-equations-for-infinite-horizon-mdps">12.4. Optimality Equations for Infinite-Horizon MDPs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#algorithms-for-solving-the-optimality-equations">12.5. Algorithms for Solving the Optimality Equations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#newton-kantorovich-applied-to-the-optimality-equations">12.5.1. Newton-Kantorovich applied to the Optimality Equations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-iteration">12.5.2. Policy Iteration</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Pierre-Luc Bacon
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>