
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Dynamic Programming &#8212; Practical Reinforcement Learning: From Algorithms to Applications</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=b4b7a797" />
    <link rel="stylesheet" type="text/css" href="_static/graphviz.css?v=fd3f3429" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"bm": ["{\\boldsymbol #1}", 1]}, "processEscapes": true}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'dp';</script>
    <script src="_static/iframe-modal.js?v=f72a1242"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Smooth Bellman Optimality Equations" href="regmdp.html" />
    <link rel="prev" title="Model Predictive Control" href="mpc.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Practical Reinforcement Learning: From Algorithms to Applications</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Why This Book?
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Modeling</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="modeling.html">Why Build a Model? For Whom?</a></li>

<li class="toctree-l1"><a class="reference internal" href="ssm.html">Dynamics Models for Decision Making</a></li>




<li class="toctree-l1"><a class="reference internal" href="simulation.html">Programs as Models</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Numerical Trajectory Optimization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ocp.html">Discrete-Time Trajectory Optimization</a></li>


<li class="toctree-l1"><a class="reference internal" href="cocp.html">Trajectory Optimization in Continuous Time</a></li>




</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">From Trajectories to Policies</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="mpc.html">Model Predictive Control</a></li>




<li class="toctree-l1 current active"><a class="current reference internal" href="#">Dynamic Programming</a></li>



</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Approximate Dynamic Programming</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="regmdp.html">Smooth Bellman Optimality Equations</a></li>
<li class="toctree-l1"><a class="reference internal" href="projdp.html">Projection Methods for Functional Equations</a></li>
<li class="toctree-l1"><a class="reference internal" href="simadp.html">Simulation-Based Approximate Dynamic Programming</a></li>



<li class="toctree-l1"><a class="reference internal" href="cadp.html">Policy Parametrization Methods</a></li>







</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="appendix_examples.html">Example COCPs</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix_ivps.html">Solving Initial Value Problems</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix_nlp.html">Nonlinear Programming</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/pierrelux/rlbook" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pierrelux/rlbook/edit/main/dp.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pierrelux/rlbook/issues/new?title=Issue%20on%20page%20%2Fdp.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/dp.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Dynamic Programming</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Dynamic Programming</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-recursion">Backward Recursion</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-optimal-harvest-in-resource-management">Example: Optimal Harvest in Resource Management</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#handling-continuous-spaces-with-interpolation">Handling Continuous Spaces with Interpolation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-recursion-with-interpolation">Backward Recursion with Interpolation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-optimal-harvest-with-linear-interpolation">Example: Optimal Harvest with Linear Interpolation</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-dynamic-programming-and-markov-decision-processes">Stochastic Dynamic Programming and Markov Decision Processes</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-rules-and-policies">Decision Rules and Policies</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-system-dynamics">Stochastic System Dynamics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimality-equations-in-the-stochastic-setting">Optimality Equations in the Stochastic Setting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-stochastic-optimal-harvest-in-resource-management">Example: Stochastic Optimal Harvest in Resource Management</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-quadratic-regulator-via-dynamic-programming">Linear Quadratic Regulator via Dynamic Programming</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-decision-process-formulation">Markov Decision Process Formulation</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#notation-in-operations-reseach">Notation in Operations Reseach</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-an-optimal-policy">What is an Optimal Policy?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-sample-size-determination-in-pharmaceutical-development">Example: Sample Size Determination in Pharmaceutical Development</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#infinite-horizon-mdps">Infinite-Horizon MDPs</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-horizon-interpretation-of-discounting">Random Horizon Interpretation of Discounting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-representation-in-markov-decision-processes">Vector Representation in Markov Decision Processes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-operator-equations">Solving Operator Equations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#successive-approximation-method">Successive Approximation Method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#newton-kantorovich-method">Newton-Kantorovich Method</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimality-equations-for-infinite-horizon-mdps">Optimality Equations for Infinite-Horizon MDPs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#algorithms-for-solving-the-optimality-equations">Algorithms for Solving the Optimality Equations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#newton-kantorovich-applied-to-bellman-optimality">Newton-Kantorovich Applied to Bellman Optimality</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#perspective-1-max-of-affine-maps">Perspective 1: Max of Affine Maps</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#perspective-2-envelope-theorem">Perspective 2: Envelope Theorem</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#perspective-3-the-implicit-function-theorem">Perspective 3: The Implicit Function Theorem</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-policy-iteration">Connection to Policy Iteration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-semismooth-newton-perspective">The Semismooth Newton Perspective</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-iteration">Policy Iteration</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="dynamic-programming">
<h1>Dynamic Programming<a class="headerlink" href="#dynamic-programming" title="Link to this heading">#</a></h1>
<p>Unlike the methods we’ve discussed so far, dynamic programming takes a step back and considers an entire family of related problems rather than a single optimization problem. This approach, while seemingly more complex at first glance, can often lead to efficient solutions.</p>
<p>Dynamic programming leverage the solution structure underlying many control problems that allows for a decomposition it into smaller, more manageable subproblems. Each subproblem is itself an optimization problem, embedded within the larger whole. This recursive structure is the foundation upon which dynamic programming constructs its solutions.</p>
<p>To ground our discussion, let us return to the domain of discrete-time optimal control problems (DOCPs). These problems frequently arise from the discretization of continuous-time optimal control problems. While the focus here will be on deterministic problems, it is worth noting that these concepts extend naturally to stochastic problems by taking the expectation over the random quantities.</p>
<p>Consider a typical DOCP of Bolza type:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\text{minimize} \quad &amp; J \triangleq c_\mathrm{T}(\mathbf{x}_T) + \sum_{t=1}^{T-1} c_t(\mathbf{x}_t, \mathbf{u}_t) \\
\text{subject to} \quad 
&amp; \mathbf{x}_{t+1} = \mathbf{f}_t(\mathbf{x}_t, \mathbf{u}_t), \quad t = 1, \ldots, T-1, \\
&amp; \mathbf{u}_{lb} \leq \mathbf{u}_t \leq \mathbf{u}_{ub}, \quad t = 1, \ldots, T, \\
&amp; \mathbf{x}_{lb} \leq \mathbf{x}_t \leq \mathbf{x}_{ub}, \quad t = 1, \ldots, T, \\
\text{given} \quad &amp; \mathbf{x}_1
\end{align*}
\end{split}\]</div>
<p>Rather than considering only the total cost from the initial time to the final time, dynamic programming introduces the concept of cost from an arbitrary point in time to the end. This leads to the definition of the “cost-to-go” or “value function” <span class="math notranslate nohighlight">\(J_k(\mathbf{x}_k)\)</span>:</p>
<div class="math notranslate nohighlight">
\[
J_k(\mathbf{x}_k) \triangleq c_\mathrm{T}(\mathbf{x}_T) + \sum_{t=k}^{T-1} c_t(\mathbf{x}_t, \mathbf{u}_t)
\]</div>
<p>This function represents the total cost incurred from stage <span class="math notranslate nohighlight">\(k\)</span> onwards to the end of the time horizon, given that the system is initialized in state <span class="math notranslate nohighlight">\(\mathbf{x}_k\)</span> at stage <span class="math notranslate nohighlight">\(k\)</span>. Suppose the problem has been solved from stage <span class="math notranslate nohighlight">\(k+1\)</span> to the end, yielding the optimal cost-to-go <span class="math notranslate nohighlight">\(J_{k+1}^\star(\mathbf{x}_{k+1})\)</span> for any state <span class="math notranslate nohighlight">\(\mathbf{x}_{k+1}\)</span> at stage <span class="math notranslate nohighlight">\(k+1\)</span>. The question then becomes: how does this information inform the decision at stage <span class="math notranslate nohighlight">\(k\)</span>?</p>
<p>Given knowledge of the optimal behavior from <span class="math notranslate nohighlight">\(k+1\)</span> onwards, the task reduces to determining the optimal action <span class="math notranslate nohighlight">\(\mathbf{u}_k\)</span> at stage <span class="math notranslate nohighlight">\(k\)</span>. This control should minimize the sum of the immediate cost <span class="math notranslate nohighlight">\(c_k(\mathbf{x}_k, \mathbf{u}_k)\)</span> and the optimal future cost <span class="math notranslate nohighlight">\(J_{k+1}^\star(\mathbf{x}_{k+1})\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{x}_{k+1}\)</span> is the resulting state after applying action <span class="math notranslate nohighlight">\(\mathbf{u}_k\)</span>. Mathematically, this is expressed as:</p>
<div class="math notranslate nohighlight">
\[
J_k^\star(\mathbf{x}_k) = \min_{\mathbf{u}_k} \left[ c_k(\mathbf{x}_k, \mathbf{u}_k) + J_{k+1}^\star(\mathbf{f}_k(\mathbf{x}_k, \mathbf{u}_k)) \right]
\]</div>
<p>This equation is known as Bellman’s equation, named after Richard Bellman, who formulated the principle of optimality:</p>
<blockquote>
<div><p>An optimal policy has the property that whatever the previous state and decision, the remaining decisions must constitute an optimal policy with regard to the state resulting from the previous decision.</p>
</div></blockquote>
<p>In other words, any sub-path of an optimal path, from any intermediate point to the end, must itself be optimal. This principle is the basis for the backward induction procedure which computes the optimal value function and provides closed-loop control capabilities without having to use an explicit NLP solver.</p>
<p>Dynamic programming can handle nonlinear systems and non-quadratic cost functions naturally. It provides a global optimal solution, when one exists, and can incorporate state and control constraints with relative ease. However, as the dimension of the state space increases, this approach suffers from what Bellman termed the “curse of dimensionality.” The computational complexity and memory requirements grow exponentially with the state dimension, rendering direct application of dynamic programming intractable for high-dimensional problems.</p>
<p>Fortunately, learning-based methods offer efficient tools to combat the curse of dimensionality on two fronts: by using function approximation (e.g., neural networks) to avoid explicit discretization, and by leveraging randomization through Monte Carlo methods inherent in the learning paradigm. Most of this course is dedicated to those ideas.</p>
<section id="backward-recursion">
<h2>Backward Recursion<a class="headerlink" href="#backward-recursion" title="Link to this heading">#</a></h2>
<p>The principle of optimality provides a methodology for solving optimal control problems. Beginning at the final time horizon and working backwards, at each stage the local optimization problem given by Bellman’s equation is solved. This process, termed backward recursion or backward induction, constructs the optimal value function stage by stage.</p>
<div class="proof algorithm admonition" id="backward-recursion">
<p class="admonition-title"><span class="caption-number">Algorithm 3 </span> (Backward Recursion for Dynamic Programming)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> Terminal cost function <span class="math notranslate nohighlight">\(c_\mathrm{T}(\cdot)\)</span>, stage cost functions <span class="math notranslate nohighlight">\(c_t(\cdot, \cdot)\)</span>, system dynamics <span class="math notranslate nohighlight">\(f_t(\cdot, \cdot)\)</span>, time horizon <span class="math notranslate nohighlight">\(\mathrm{T}\)</span></p>
<p><strong>Output:</strong> Optimal value functions <span class="math notranslate nohighlight">\(J_t^\star(\cdot)\)</span> and optimal control policies <span class="math notranslate nohighlight">\(\mu_t^\star(\cdot)\)</span> for <span class="math notranslate nohighlight">\(t = 1, \ldots, T\)</span></p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(J_T^\star(\mathbf{x}) = c_\mathrm{T}(\mathbf{x})\)</span> for all <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> in the state space</p></li>
<li><p>For <span class="math notranslate nohighlight">\(t = T-1, T-2, \ldots, 1\)</span>:</p>
<ol class="arabic simple">
<li><p>For each state <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> in the state space:</p>
<ol class="arabic simple">
<li><p>Compute <span class="math notranslate nohighlight">\(J_t^\star(\mathbf{x}) = \min_{\mathbf{u}} \left[ c_t(\mathbf{x}, \mathbf{u}) + J_{t+1}^\star(f_t(\mathbf{x}, \mathbf{u})) \right]\)</span></p></li>
<li><p>Compute <span class="math notranslate nohighlight">\(\mu_t^\star(\mathbf{x}) = \arg\min_{\mathbf{u}} \left[ c_t(\mathbf{x}, \mathbf{u}) + J_{t+1}^\star(f_t(\mathbf{x}, \mathbf{u})) \right]\)</span></p></li>
</ol>
</li>
<li><p>End For</p></li>
</ol>
</li>
<li><p>End For</p></li>
<li><p>Return <span class="math notranslate nohighlight">\(J_t^\star(\cdot)\)</span>, <span class="math notranslate nohighlight">\(\mu_t^\star(\cdot)\)</span> for <span class="math notranslate nohighlight">\(t = 1, \ldots, T\)</span></p></li>
</ol>
</section>
</div><p>Upon completion of this backward pass, we now have access to the optimal control to take at any stage and in any state. Furthermore, we can simulate optimal trajectories from any initial state and applying the optimal policy at each stage to generate the optimal trajectory.</p>
<div class="proof theorem admonition" id="thm-bolza-backward">
<p class="admonition-title"><span class="caption-number">Theorem 2 </span> (Backward induction solves deterministic Bolza DOCP)</p>
<section class="theorem-content" id="proof-content">
<p><strong>Setting.</strong> Let <span class="math notranslate nohighlight">\(\mathbf{x}_{t+1}=\mathbf{f}_t(\mathbf{x}_t,\mathbf{u}_t)\)</span> for <span class="math notranslate nohighlight">\(t=1,\dots,T-1\)</span>, with admissible action sets <span class="math notranslate nohighlight">\(\mathcal{U}_t(\mathbf{x})\neq\varnothing\)</span>. Let stage costs <span class="math notranslate nohighlight">\(c_t(\mathbf{x},\mathbf{u})\)</span> and terminal cost <span class="math notranslate nohighlight">\(c_\mathrm{T}(\mathbf{x})\)</span> be real-valued and bounded below. Assume for every <span class="math notranslate nohighlight">\((t,\mathbf{x})\)</span> the one-step problem</p>
<div class="math notranslate nohighlight">
\[
\min_{\mathbf{u}\in\mathcal{U}_t(\mathbf{x})}\big\{c_t(\mathbf{x},\mathbf{u})+J_{t+1}^\star(\mathbf{f}_t(\mathbf{x},\mathbf{u}))\big\}
\]</div>
<p>admits a minimizer (e.g., compact <span class="math notranslate nohighlight">\(\mathcal{U}_t(\mathbf{x})\)</span> and continuity suffice).</p>
<p>Define <span class="math notranslate nohighlight">\(J_T^\star(\mathbf{x}) \equiv c_\mathrm{T}(\mathbf{x})\)</span> and for <span class="math notranslate nohighlight">\(t=T-1,\dots,1\)</span></p>
<div class="math notranslate nohighlight">
\[
J_t^\star(\mathbf{x}) \;\triangleq\; \min_{\mathbf{u}\in\mathcal{U}_t(\mathbf{x})}
\Big[c_t(\mathbf{x},\mathbf{u})+J_{t+1}^\star\big(\mathbf{f}_t(\mathbf{x},\mathbf{u})\big)\Big],
\]</div>
<p>and select any minimizer <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_t^\star(\mathbf{x})\in\arg\min(\cdot)\)</span>.</p>
<p><strong>Claim.</strong> For every initial state <span class="math notranslate nohighlight">\(\mathbf{x}_1\)</span>, the control sequence
<span class="math notranslate nohighlight">\(\boldsymbol{\mu}_1^\star(\mathbf{x}_1),\dots,\boldsymbol{\mu}_{T-1}^\star(\mathbf{x}_{T-1})\)</span>
generated by these selectors is optimal for the Bolza problem, and
<span class="math notranslate nohighlight">\(J_1^\star(\mathbf{x}_1)\)</span> equals the optimal cost. Moreover, <span class="math notranslate nohighlight">\(J_t^\star(\cdot)\)</span> is the optimal cost-to-go from stage <span class="math notranslate nohighlight">\(t\)</span> for every state, i.e., backward induction recovers the entire value function.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. We give a direct proof by backward induction. The general idea is that any feasible sequence can be improved by replacing its tail with an optimal continuation, so optimal solutions can be built stage by stage. This is sometimes called a “cut-and-paste” argument.</p>
<p><strong>Step 1 (verification of the recursion at a fixed stage).</strong><br />
Fix <span class="math notranslate nohighlight">\(t\in\{1,\dots,T-1\}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x}\in\mathbb{X}\)</span>. Consider any admissible control sequence <span class="math notranslate nohighlight">\(\mathbf{u}_t,\dots,\mathbf{u}_{T-1}\)</span> starting from <span class="math notranslate nohighlight">\(\mathbf{x}_t=\mathbf{x}\)</span> and define the induced states <span class="math notranslate nohighlight">\(\mathbf{x}_{k+1}=\mathbf{f}_k(\mathbf{x}_k,\mathbf{u}_k)\)</span>. Its total cost from <span class="math notranslate nohighlight">\(t\)</span> is</p>
<div class="math notranslate nohighlight">
\[
c_t(\mathbf{x}_t,\mathbf{u}_t)+\sum_{k=t+1}^{T-1}c_k(\mathbf{x}_k,\mathbf{u}_k)+c_\mathrm{T}(\mathbf{x}_T).
\]</div>
<p>By definition of <span class="math notranslate nohighlight">\(J_{t+1}^\star\)</span>, the tail cost satisfies</p>
<div class="math notranslate nohighlight">
\[
\sum_{k=t+1}^{T-1}c_k(\mathbf{x}_k,\mathbf{u}_k)+c_\mathrm{T}(\mathbf{x}_T)
\;\ge\; J_{t+1}^\star(\mathbf{x}_{t+1})
\;=\; J_{t+1}^\star\big(\mathbf{f}_t(\mathbf{x},\mathbf{u}_t)\big).
\]</div>
<p>Hence the total cost is bounded below by</p>
<div class="math notranslate nohighlight">
\[
c_t(\mathbf{x},\mathbf{u}_t)+J_{t+1}^\star\big(\mathbf{f}_t(\mathbf{x},\mathbf{u}_t)\big).
\]</div>
<p>Taking the minimum over <span class="math notranslate nohighlight">\(\mathbf{u}_t\in\mathcal{U}_t(\mathbf{x})\)</span> yields</p>
<div class="math notranslate nohighlight">
\[
\text{(any admissible cost from $t$)}\;\ge\;J_t^\star(\mathbf{x}).
\tag{$\ast$}
\]</div>
<p><strong>Step 2 (existence of an optimal prefix at stage <span class="math notranslate nohighlight">\(t\)</span>).</strong><br />
By assumption, there exists <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_t^\star(\mathbf{x})\)</span> attaining the minimum in the definition of <span class="math notranslate nohighlight">\(J_t^\star(\mathbf{x})\)</span>. If we now <strong>paste</strong> to <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_t^\star(\mathbf{x})\)</span> an optimal tail policy from <span class="math notranslate nohighlight">\(t+1\)</span> (whose existence we will establish inductively), the resulting sequence attains cost exactly</p>
<div class="math notranslate nohighlight">
\[
c_t\big(\mathbf{x},\boldsymbol{\mu}_t^\star(\mathbf{x})\big)
+J_{t+1}^\star\!\Big(\mathbf{f}_t\big(\mathbf{x},\boldsymbol{\mu}_t^\star(\mathbf{x})\big)\Big)
=J_t^\star(\mathbf{x}),
\]</div>
<p>which matches the lower bound <span class="math notranslate nohighlight">\((\ast)\)</span>; hence it is optimal from <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p><strong>Step 3 (backward induction over time).</strong><br />
Base case <span class="math notranslate nohighlight">\(t=T\)</span>. The statement holds because <span class="math notranslate nohighlight">\(J_T^\star(\mathbf{x})=c_\mathrm{T}(\mathbf{x})\)</span> and there is no control to choose.</p>
<p>Inductive step. Assume the tail statement holds for <span class="math notranslate nohighlight">\(t+1\)</span>: from any state <span class="math notranslate nohighlight">\(\mathbf{x}_{t+1}\)</span> there exists an optimal control sequence realizing <span class="math notranslate nohighlight">\(J_{t+1}^\star(\mathbf{x}_{t+1})\)</span>. Then by Steps 1–2, selecting <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_t^\star(\mathbf{x}_t)\)</span> at stage <span class="math notranslate nohighlight">\(t\)</span> and concatenating the optimal tail from <span class="math notranslate nohighlight">\(t+1\)</span> yields an optimal sequence from <span class="math notranslate nohighlight">\(t\)</span> with value <span class="math notranslate nohighlight">\(J_t^\star(\mathbf{x}_t)\)</span>.</p>
<p>By backward induction, the claim holds for all <span class="math notranslate nohighlight">\(t\)</span>, in particular for <span class="math notranslate nohighlight">\(t=1\)</span> and any initial <span class="math notranslate nohighlight">\(\mathbf{x}_1\)</span>. Therefore the backward recursion both <strong>certifies</strong> optimality (verification) and <strong>constructs</strong> an optimal policy (synthesis), while recovering the full family <span class="math notranslate nohighlight">\(\{J_t^\star\}_{t=1}^T\)</span>.</p>
</div>
<div class="proof remark admonition" id="remark-2">
<p class="admonition-title"><span class="caption-number">Remark 1 </span> (No “big NLP” required)</p>
<section class="remark-content" id="proof-content">
<p>The Bolza DOCP over the whole horizon couples all controls through the dynamics and is typically posed as a single large nonlinear program. The proof shows you can solve <strong><span class="math notranslate nohighlight">\(T-1\)</span> sequences of one-step problems</strong> instead: at each <span class="math notranslate nohighlight">\((t,\mathbf{x})\)</span> minimize</p>
<div class="math notranslate nohighlight">
\[
\mathbf{u}\mapsto c_t(\mathbf{x},\mathbf{u}) + J_{t+1}^\star(\mathbf{f}_t(\mathbf{x},\mathbf{u})).
\]</div>
<p>In finite state–action spaces this becomes pure table lookup and argmin. In continuous spaces you still solve local one-step minimizations, but you avoid a monolithic horizon-coupled NLP.</p>
</section>
</div><div class="proof remark admonition" id="remark-3">
<p class="admonition-title"><span class="caption-number">Remark 2 </span> (Graph interpretation (optional intuition))</p>
<section class="remark-content" id="proof-content">
<p>Unroll time to form a DAG whose nodes are <span class="math notranslate nohighlight">\((t,\mathbf{x})\)</span> and whose edges correspond to feasible controls with edge weight <span class="math notranslate nohighlight">\(c_t(\mathbf{x},\mathbf{u})\)</span>. The terminal node cost is <span class="math notranslate nohighlight">\(c_\mathrm{T}(\cdot)\)</span>. The Bolza problem is a shortest-path problem on this DAG. The equation</p>
<div class="math notranslate nohighlight">
\[
J_t^\star(\mathbf{x})=\min_{\mathbf{u}}\{c_t(\mathbf{x},\mathbf{u})+J_{t+1}^\star(\mathbf{f}_t(\mathbf{x},\mathbf{u}))\}
\]</div>
<p>is exactly the dynamic programming recursion for shortest paths on acyclic graphs, hence backward induction is optimal.</p>
</section>
</div><!-- ```{prf:remark} If minimizers may not exist
Replace each “min” by “inf” in the definitions and state that for every $\varepsilon>0$ there exist $\varepsilon$-optimal selectors $\boldsymbol{\mu}_t^\varepsilon(\cdot)$ achieving cost within $\varepsilon$ of $J_t^\star(\cdot)$. The same cut-and-paste and induction go through.
``` -->
<section id="example-optimal-harvest-in-resource-management">
<h3>Example: Optimal Harvest in Resource Management<a class="headerlink" href="#example-optimal-harvest-in-resource-management" title="Link to this heading">#</a></h3>
<p>Dynamic programming is often used in resource management and conservation biology to devise policies to be implemented by decision makers and stakeholders : for eg. in fishereries, or timber harvesting. Per <span id="id1">[<a class="reference internal" href="bibliography.html#id22" title="Michael J. Conroy and James T. Peterson. Decision Making in Natural Resource Management: A Structured, Adaptive Approach: A Structured, Adaptive Approach. Wiley, January 2013. ISBN 9781118506196. URL: http://dx.doi.org/10.1002/9781118506196, doi:10.1002/9781118506196.">9</a>]</span>, we consider a population of a particular species, whose abundance we denote by <span class="math notranslate nohighlight">\(x_t\)</span>, where <span class="math notranslate nohighlight">\(t\)</span> represents discrete time steps. Our objective is to maximize the cumulative harvest over a finite time horizon, while also considering the long-term sustainability of the population. This optimization problem can be formulated as:</p>
<div class="math notranslate nohighlight">
\[
\text{maximize} \quad \sum_{t=t_0}^{t_f} F(x_t \cdot h_t) + F_\mathrm{T}(x_{t_f})
\]</div>
<p>Here, <span class="math notranslate nohighlight">\(F(\cdot)\)</span> represents the immediate reward function associated with harvesting, <span class="math notranslate nohighlight">\(h_t\)</span> is the harvest rate at time <span class="math notranslate nohighlight">\(t\)</span>, and <span class="math notranslate nohighlight">\(F_\mathrm{T}(\cdot)\)</span> denotes a terminal value function that could potentially assign value to the final population state. In this particular problem, we assign no terminal value to the final population state, setting <span class="math notranslate nohighlight">\(F_\mathrm{T}(x_{t_f}) = 0\)</span> and allowing us to focus solely on the cumulative harvest over the time horizon.</p>
<p>In our model population model, the abundance of a specicy <span class="math notranslate nohighlight">\(x\)</span> ranges from 1 to 100 individuals. The decision variable is the harvest rate <span class="math notranslate nohighlight">\(h\)</span>, which can take values from the set <span class="math notranslate nohighlight">\(D = \{0, 0.1, 0.2, 0.3, 0.4, 0.5\}\)</span>. The population dynamics are governed by a modified logistic growth model:</p>
<div class="math notranslate nohighlight">
\[
x_{t+1} = x_t + 0.3x_t(1 - x_t/125) - h_tx_t
\]</div>
<p>where the <span class="math notranslate nohighlight">\(0.3\)</span> represents the growth rate and <span class="math notranslate nohighlight">\(125\)</span> is the carrying capacity (the maximum population size given the available resources). The logistic growth model returns continuous values; however our DP formulation uses a discrete state space. Therefore, we also round the the outcomes to the nearest integer.</p>
<p>Applying the principle of optimality, we can express the optimal value function <span class="math notranslate nohighlight">\(J^\star(x_t,t)\)</span> recursively:</p>
<div class="math notranslate nohighlight">
\[
J^\star(x_t, t) = \max_{h_t \in D} (F(x, h, t) + J^*(x_{t+1}, t+1))
\]</div>
<p>with the boundary condition <span class="math notranslate nohighlight">\(J^*(x_{t_f}) = 0\)</span>.</p>
<p>It’s worth noting that while this example uses a relatively simple model, the same principles can be applied to more complex scenarios involving stochasticity, multiple species interactions, or spatial heterogeneity.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Parameters</span>
<span class="n">r_max</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">125</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c1"># Number of time steps</span>
<span class="n">N_max</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># Maximum population size to consider</span>
<span class="n">h_max</span> <span class="o">=</span> <span class="mf">0.5</span>  <span class="c1"># Maximum harvest rate</span>
<span class="n">h_step</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># Step size for harvest rate</span>

<span class="c1"># Create state and decision spaces</span>
<span class="n">N_space</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">N_max</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">h_space</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">h_max</span> <span class="o">+</span> <span class="n">h_step</span><span class="p">,</span> <span class="n">h_step</span><span class="p">)</span>

<span class="c1"># Initialize value function and policy</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">T</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">N_space</span><span class="p">)))</span>
<span class="n">policy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">T</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">N_space</span><span class="p">)))</span>

<span class="c1"># Terminal value function (F_T)</span>
<span class="k">def</span> <span class="nf">terminal_value</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">0</span>

<span class="c1"># State return function (F)</span>
<span class="k">def</span> <span class="nf">state_return</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">h</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">N</span> <span class="o">*</span> <span class="n">h</span>

<span class="c1"># State dynamics function</span>
<span class="k">def</span> <span class="nf">state_dynamics</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">h</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">N</span> <span class="o">+</span> <span class="n">r_max</span> <span class="o">*</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">N</span> <span class="o">/</span> <span class="n">K</span><span class="p">)</span> <span class="o">-</span> <span class="n">N</span> <span class="o">*</span> <span class="n">h</span>

<span class="c1"># Backward iteration</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">N</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">N_space</span><span class="p">):</span>
        <span class="n">max_value</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">)</span>
        <span class="n">best_h</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">h_space</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">h</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># Ensure harvest rate doesn&#39;t exceed 100%</span>
                <span class="k">continue</span>

            <span class="n">next_N</span> <span class="o">=</span> <span class="n">state_dynamics</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">next_N</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># Ensure population doesn&#39;t go extinct</span>
                <span class="k">continue</span>

            <span class="n">next_N_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">searchsorted</span><span class="p">(</span><span class="n">N_space</span><span class="p">,</span> <span class="n">next_N</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">next_N_index</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">N_space</span><span class="p">):</span>
                <span class="n">next_N_index</span> <span class="o">-=</span> <span class="mi">1</span>

            <span class="n">value</span> <span class="o">=</span> <span class="n">state_return</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span> <span class="o">+</span> <span class="n">V</span><span class="p">[</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">next_N_index</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">value</span> <span class="o">&gt;</span> <span class="n">max_value</span><span class="p">:</span>
                <span class="n">max_value</span> <span class="o">=</span> <span class="n">value</span>
                <span class="n">best_h</span> <span class="o">=</span> <span class="n">h</span>

        <span class="n">V</span><span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_value</span>
        <span class="n">policy</span><span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">best_h</span>

<span class="c1"># Function to simulate the optimal policy with conversion to Python floats</span>
<span class="k">def</span> <span class="nf">simulate_optimal_policy</span><span class="p">(</span><span class="n">initial_N</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
    <span class="n">trajectory</span> <span class="o">=</span> <span class="p">[</span><span class="nb">float</span><span class="p">(</span><span class="n">initial_N</span><span class="p">)]</span>  <span class="c1"># Ensure first value is a Python float</span>
    <span class="n">harvests</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
        <span class="n">N</span> <span class="o">=</span> <span class="n">trajectory</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">N_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">searchsorted</span><span class="p">(</span><span class="n">N_space</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">N_index</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">N_space</span><span class="p">):</span>
            <span class="n">N_index</span> <span class="o">-=</span> <span class="mi">1</span>

        <span class="n">h</span> <span class="o">=</span> <span class="n">policy</span><span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="n">N_index</span><span class="p">]</span>
        <span class="n">harvests</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">N</span> <span class="o">*</span> <span class="n">h</span><span class="p">))</span>  <span class="c1"># Ensure harvest is a Python float</span>

        <span class="n">next_N</span> <span class="o">=</span> <span class="n">state_dynamics</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>
        <span class="n">trajectory</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">next_N</span><span class="p">))</span>  <span class="c1"># Ensure next population value is a Python float</span>

    <span class="k">return</span> <span class="n">trajectory</span><span class="p">,</span> <span class="n">harvests</span>

<span class="c1"># Example usage</span>
<span class="n">initial_N</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">trajectory</span><span class="p">,</span> <span class="n">harvests</span> <span class="o">=</span> <span class="n">simulate_optimal_policy</span><span class="p">(</span><span class="n">initial_N</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Optimal policy:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">policy</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Population trajectory:&quot;</span><span class="p">,</span> <span class="n">trajectory</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Harvests:&quot;</span><span class="p">,</span> <span class="n">harvests</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Total harvest:&quot;</span><span class="p">,</span> <span class="nb">sum</span><span class="p">(</span><span class="n">harvests</span><span class="p">))</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimal policy:
[[0.2 0.2 0.2 ... 0.4 0.4 0.5]
 [0.2 0.2 0.2 ... 0.4 0.4 0.5]
 [0.2 0.2 0.2 ... 0.4 0.4 0.4]
 ...
 [0.2 0.2 0.2 ... 0.5 0.5 0.5]
 [0.2 0.5 0.5 ... 0.5 0.5 0.5]
 [0.2 0.5 0.5 ... 0.5 0.5 0.5]]

Population trajectory: [50.0, 54.0, 63.2016, 53.614938617856, 62.80047226002128, 65.89520835342945, 62.063500827311884, 65.23169346891407, 61.5424456170318, 64.7610004703774, 61.171531280797, 64.42514256278633, 60.90621923290014, 52.003257133909514, 61.11382126799714, 64.37282756165249, 60.86484408994034, 39.80100508132969, 28.038916051902078, 20.544298889444192, 15.422475391094192]
Harvests: [5.0, 0.0, 18.960480000000004, 0.0, 6.280047226002129, 13.17904167068589, 6.206350082731189, 13.046338693782815, 6.15424456170318, 12.95220009407548, 6.1171531280797, 12.885028512557268, 18.271865769870047, 0.0, 6.111382126799715, 12.874565512330499, 30.43242204497017, 19.900502540664846, 14.019458025951039, 10.272149444722096]
Total harvest: 212.66322943492605
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="handling-continuous-spaces-with-interpolation">
<h2>Handling Continuous Spaces with Interpolation<a class="headerlink" href="#handling-continuous-spaces-with-interpolation" title="Link to this heading">#</a></h2>
<p>In many real-world problems, such as our resource management example, the state space is inherently continuous. Dynamic programming, however, is usually defined on discrete state spaces. To reconcile this, we approximate the value function on a finite grid of points and use interpolation to estimate its value elsewhere.</p>
<p>In our earlier example, we acted as if population sizes could only be whole numbers: 1 fish, 2 fish, 3 fish. But real measurements don’t fit neatly. What do you do with a survey that reports 42.7 fish? Our reflex in the code example was to round to the nearest integer, effectively saying “let’s just call it 43.” This corresponds to <strong>nearest-neighbor interpolation</strong>, also known as discretization. It’s the zeroth-order case: you assume the value between grid points is constant and equal to the closest one. In practice, this amounts to overlaying a grid on the continuous landscape and forcing yourself to stand at the intersections. In our demo code, this step was carried out with <a class="reference external" href="https://numpy.org/doc/2.0/reference/generated/numpy.searchsorted.html"><code class="docutils literal notranslate"><span class="pre">numpy.searchsorted</span></code></a>.</p>
<p>While easy to implement, nearest-neighbor interpolation can introduce artifacts:</p>
<ol class="arabic simple">
<li><p>Decisions may change abruptly, even if the state only shifts slightly.</p></li>
<li><p>Precision is lost, especially in regimes where small variations matter.</p></li>
<li><p>The curse of dimensionality forces an impractically fine grid if many state variables are added.</p></li>
</ol>
<p>To address these issues, we can use <strong>higher-order interpolation</strong>. Instead of taking the nearest neighbor, we estimate the value at off-grid points by leveraging multiple nearby values.</p>
<section id="backward-recursion-with-interpolation">
<h3>Backward Recursion with Interpolation<a class="headerlink" href="#backward-recursion-with-interpolation" title="Link to this heading">#</a></h3>
<p>Suppose we have computed <span class="math notranslate nohighlight">\(J_{k+1}^\star(\mathbf{x})\)</span> only at grid points <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathcal{X}_\text{grid}\)</span>.
To evaluate Bellman’s equation at an arbitrary <span class="math notranslate nohighlight">\(\mathbf{x}_{k+1}\)</span>, we interpolate.
Formally, let <span class="math notranslate nohighlight">\(I_{k+1}(\mathbf{x})\)</span> be the interpolation operator that extends the value function from <span class="math notranslate nohighlight">\(\mathcal{X}_\text{grid}\)</span> to the continuous space. Then:</p>
<div class="math notranslate nohighlight">
\[
J_k^\star(\mathbf{x}_k) 
= \min_{\mathbf{u}_k} 
\Big[ c_k(\mathbf{x}_k, \mathbf{u}_k) 
+ I_{k+1}\big(\mathbf{f}_k(\mathbf{x}_k, \mathbf{u}_k)\big) \Big].
\]</div>
<p>For instance, in one dimension, linear interpolation gives:</p>
<div class="math notranslate nohighlight">
\[
I_{k+1}(x) = J_{k+1}^\star(x_l) + \frac{x - x_l}{x_u - x_l} \big(J_{k+1}^\star(x_u) - J_{k+1}^\star(x_l)\big),
\]</div>
<p>where <span class="math notranslate nohighlight">\(x_l\)</span> and <span class="math notranslate nohighlight">\(x_u\)</span> are the nearest grid points bracketing <span class="math notranslate nohighlight">\(x\)</span>. Linear interpolation is often sufficient, but higher-order methods (cubic splines, radial basis functions) can yield smoother and more accurate estimates. The choice of interpolation scheme and grid layout both affect accuracy and efficiency. A finer grid improves resolution but increases computational cost, motivating strategies like adaptive grid refinement or replacing interpolation altogether with parametric function approximation which we are going to see later in this book.</p>
<p>In higher-dimensional spaces, naive interpolation becomes prohibitively expensive due to the curse of dimensionality. Several approaches such as tensorized multilinear interpolation, radial basis functions, and machine learning models address this challenge by extending a common principle: they approximate the value function at unobserved points using information from a finite set of evaluations. However, as dimensionality continues to grow, even tensor methods face scalability limits, which is why flexible parametric models like neural networks have become essential tools for high-dimensional function approximation.</p>
<div class="proof algorithm admonition" id="backward-recursion-interpolation">
<p class="admonition-title"><span class="caption-number">Algorithm 4 </span> (Backward Recursion with Interpolation)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong></p>
<ul class="simple">
<li><p>Terminal cost <span class="math notranslate nohighlight">\(c_\mathrm{T}(\cdot)\)</span></p></li>
<li><p>Stage costs <span class="math notranslate nohighlight">\(c_t(\cdot,\cdot)\)</span></p></li>
<li><p>Dynamics <span class="math notranslate nohighlight">\(f_t(\cdot,\cdot)\)</span></p></li>
<li><p>Time horizon <span class="math notranslate nohighlight">\(T\)</span></p></li>
<li><p>State grid <span class="math notranslate nohighlight">\(\mathcal{X}_\text{grid}\)</span></p></li>
<li><p>Action set <span class="math notranslate nohighlight">\(\mathcal{U}\)</span></p></li>
<li><p>Interpolation method <span class="math notranslate nohighlight">\(\mathcal{I}(\cdot)\)</span> (e.g., linear, cubic spline, RBF, neural network)</p></li>
</ul>
<p><strong>Output:</strong> Value functions <span class="math notranslate nohighlight">\(J_t^\star(\cdot)\)</span> and policies <span class="math notranslate nohighlight">\(\mu_t^\star(\cdot)\)</span> for all <span class="math notranslate nohighlight">\(t\)</span></p>
<ol class="arabic">
<li><p><strong>Initialize terminal values:</strong></p>
<ul class="simple">
<li><p>Compute <span class="math notranslate nohighlight">\(J_T^\star(\mathbf{x}) = c_\mathrm{T}(\mathbf{x})\)</span> for all <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathcal{X}_\text{grid}\)</span></p></li>
<li><p>Fit interpolator: <span class="math notranslate nohighlight">\(I_T \leftarrow \mathcal{I}(\{\mathbf{x}, J_T^\star(\mathbf{x})\}_{\mathbf{x} \in \mathcal{X}_\text{grid}})\)</span></p></li>
</ul>
</li>
<li><p><strong>Backward recursion:</strong><br />
For <span class="math notranslate nohighlight">\(t = T-1, T-2, \dots, 0\)</span>:</p>
<p>a. <strong>Bellman update at grid points:</strong><br />
For each <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathcal{X}_\text{grid}\)</span>:</p>
<ul class="simple">
<li><p>For each <span class="math notranslate nohighlight">\(\mathbf{u} \in \mathcal{U}\)</span>:</p>
<ul>
<li><p>Compute next state: <span class="math notranslate nohighlight">\(\mathbf{x}_\text{next} = f_t(\mathbf{x}, \mathbf{u})\)</span></p></li>
<li><p><strong>Interpolate future cost:</strong> <span class="math notranslate nohighlight">\(\hat{J}_{t+1}(\mathbf{x}_\text{next}) = I_{t+1}(\mathbf{x}_\text{next})\)</span></p></li>
<li><p>Compute total cost: <span class="math notranslate nohighlight">\(J_t(\mathbf{x}, \mathbf{u}) = c_t(\mathbf{x}, \mathbf{u}) + \hat{J}_{t+1}(\mathbf{x}_\text{next})\)</span></p></li>
</ul>
</li>
<li><p><strong>Minimize over actions:</strong> <span class="math notranslate nohighlight">\(J_t^\star(\mathbf{x}) = \min_{\mathbf{u} \in \mathcal{U}} J_t(\mathbf{x}, \mathbf{u})\)</span></p></li>
<li><p>Store optimal action: <span class="math notranslate nohighlight">\(\mu_t^\star(\mathbf{x}) = \arg\min_{\mathbf{u} \in \mathcal{U}} J_t(\mathbf{x}, \mathbf{u})\)</span></p></li>
</ul>
<p>b. <strong>Fit interpolator for current stage:</strong><br />
<span class="math notranslate nohighlight">\(I_t \leftarrow \mathcal{I}(\{\mathbf{x}, J_t^\star(\mathbf{x})\}_{\mathbf{x} \in \mathcal{X}_\text{grid}})\)</span></p>
</li>
<li><p><strong>Return:</strong> Interpolated value functions <span class="math notranslate nohighlight">\(\{I_t\}_{t=0}^T\)</span> and policies <span class="math notranslate nohighlight">\(\{\mu_t^\star\}_{t=0}^{T-1}\)</span></p></li>
</ol>
</section>
</div><section id="example-optimal-harvest-with-linear-interpolation">
<h4>Example: Optimal Harvest with Linear Interpolation<a class="headerlink" href="#example-optimal-harvest-with-linear-interpolation" title="Link to this heading">#</a></h4>
<p>Here is a demonstration of the backward recursion procedure using linear interpolation.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Parameters</span>
<span class="n">r_max</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">125</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c1"># Number of time steps</span>
<span class="n">N_max</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># Maximum population size to consider</span>
<span class="n">h_max</span> <span class="o">=</span> <span class="mf">0.5</span>  <span class="c1"># Maximum harvest rate</span>
<span class="n">h_step</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># Step size for harvest rate</span>

<span class="c1"># Create state and decision spaces</span>
<span class="n">N_space</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">N_max</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">h_space</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">h_max</span> <span class="o">+</span> <span class="n">h_step</span><span class="p">,</span> <span class="n">h_step</span><span class="p">)</span>

<span class="c1"># Initialize value function and policy</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">T</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">N_space</span><span class="p">)))</span>
<span class="n">policy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">T</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">N_space</span><span class="p">)))</span>

<span class="c1"># Terminal value function (F_T)</span>
<span class="k">def</span> <span class="nf">terminal_value</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">0</span>

<span class="c1"># State return function (F)</span>
<span class="k">def</span> <span class="nf">state_return</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">h</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">N</span> <span class="o">*</span> <span class="n">h</span>

<span class="c1"># State dynamics function</span>
<span class="k">def</span> <span class="nf">state_dynamics</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">h</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">N</span> <span class="o">+</span> <span class="n">r_max</span> <span class="o">*</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">N</span> <span class="o">/</span> <span class="n">K</span><span class="p">)</span> <span class="o">-</span> <span class="n">N</span> <span class="o">*</span> <span class="n">h</span>

<span class="c1"># Function to linearly interpolate between grid points in N_space</span>
<span class="k">def</span> <span class="nf">interpolate_value_function</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">N_space</span><span class="p">,</span> <span class="n">next_N</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">next_N</span> <span class="o">&lt;=</span> <span class="n">N_space</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">V</span><span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># Below or at minimum population, return minimum value</span>
    <span class="k">if</span> <span class="n">next_N</span> <span class="o">&gt;=</span> <span class="n">N_space</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">V</span><span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># Above or at maximum population, return maximum value</span>
    
    <span class="c1"># Find indices to interpolate between</span>
    <span class="n">lower_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">searchsorted</span><span class="p">(</span><span class="n">N_space</span><span class="p">,</span> <span class="n">next_N</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">upper_idx</span> <span class="o">=</span> <span class="n">lower_idx</span> <span class="o">+</span> <span class="mi">1</span>
    
    <span class="c1"># Linear interpolation</span>
    <span class="n">N_lower</span> <span class="o">=</span> <span class="n">N_space</span><span class="p">[</span><span class="n">lower_idx</span><span class="p">]</span>
    <span class="n">N_upper</span> <span class="o">=</span> <span class="n">N_space</span><span class="p">[</span><span class="n">upper_idx</span><span class="p">]</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="p">(</span><span class="n">next_N</span> <span class="o">-</span> <span class="n">N_lower</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">N_upper</span> <span class="o">-</span> <span class="n">N_lower</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">weight</span><span class="p">)</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="n">lower_idx</span><span class="p">]</span> <span class="o">+</span> <span class="n">weight</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="n">upper_idx</span><span class="p">]</span>

<span class="c1"># Backward iteration with interpolation</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">N</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">N_space</span><span class="p">):</span>
        <span class="n">max_value</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">)</span>
        <span class="n">best_h</span> <span class="o">=</span> <span class="mi">0</span>
        
        <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">h_space</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">h</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># Ensure harvest rate doesn&#39;t exceed 100%</span>
                <span class="k">continue</span>
            
            <span class="n">next_N</span> <span class="o">=</span> <span class="n">state_dynamics</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">next_N</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># Ensure population doesn&#39;t go extinct</span>
                <span class="k">continue</span>
            
            <span class="c1"># Interpolate value for next_N</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">state_return</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span> <span class="o">+</span> <span class="n">interpolate_value_function</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">N_space</span><span class="p">,</span> <span class="n">next_N</span><span class="p">,</span> <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">value</span> <span class="o">&gt;</span> <span class="n">max_value</span><span class="p">:</span>
                <span class="n">max_value</span> <span class="o">=</span> <span class="n">value</span>
                <span class="n">best_h</span> <span class="o">=</span> <span class="n">h</span>
        
        <span class="n">V</span><span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_value</span>
        <span class="n">policy</span><span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">best_h</span>

<span class="c1"># Function to simulate the optimal policy using interpolation</span>
<span class="k">def</span> <span class="nf">simulate_optimal_policy</span><span class="p">(</span><span class="n">initial_N</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
    <span class="n">trajectory</span> <span class="o">=</span> <span class="p">[</span><span class="n">initial_N</span><span class="p">]</span>
    <span class="n">harvests</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
        <span class="n">N</span> <span class="o">=</span> <span class="n">trajectory</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        
        <span class="c1"># Interpolate optimal harvest rate</span>
        <span class="k">if</span> <span class="n">N</span> <span class="o">&lt;=</span> <span class="n">N_space</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="n">h</span> <span class="o">=</span> <span class="n">policy</span><span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">N</span> <span class="o">&gt;=</span> <span class="n">N_space</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">h</span> <span class="o">=</span> <span class="n">policy</span><span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">lower_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">searchsorted</span><span class="p">(</span><span class="n">N_space</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="n">upper_idx</span> <span class="o">=</span> <span class="n">lower_idx</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="p">(</span><span class="n">N</span> <span class="o">-</span> <span class="n">N_space</span><span class="p">[</span><span class="n">lower_idx</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="n">N_space</span><span class="p">[</span><span class="n">upper_idx</span><span class="p">]</span> <span class="o">-</span> <span class="n">N_space</span><span class="p">[</span><span class="n">lower_idx</span><span class="p">])</span>
            <span class="n">h</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">weight</span><span class="p">)</span> <span class="o">*</span> <span class="n">policy</span><span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="n">lower_idx</span><span class="p">]</span> <span class="o">+</span> <span class="n">weight</span> <span class="o">*</span> <span class="n">policy</span><span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="n">upper_idx</span><span class="p">]</span>
        
        <span class="n">harvests</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">N</span> <span class="o">*</span> <span class="n">h</span><span class="p">))</span>  <span class="c1"># Ensure harvest is a Python float</span>
        <span class="n">next_N</span> <span class="o">=</span> <span class="n">state_dynamics</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>
        <span class="n">trajectory</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">next_N</span><span class="p">))</span>  <span class="c1"># Ensure next population value is a Python float</span>

    <span class="k">return</span> <span class="n">trajectory</span><span class="p">,</span> <span class="n">harvests</span>

<span class="c1"># Example usage</span>
<span class="n">initial_N</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">trajectory</span><span class="p">,</span> <span class="n">harvests</span> <span class="o">=</span> <span class="n">simulate_optimal_policy</span><span class="p">(</span><span class="n">initial_N</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Optimal policy:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">policy</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Population trajectory:&quot;</span><span class="p">,</span> <span class="n">trajectory</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Harvests:&quot;</span><span class="p">,</span> <span class="n">harvests</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Total harvest:&quot;</span><span class="p">,</span> <span class="nb">sum</span><span class="p">(</span><span class="n">harvests</span><span class="p">))</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimal policy:
[[0.  0.  0.  ... 0.4 0.4 0.4]
 [0.  0.  0.  ... 0.4 0.4 0.4]
 [0.  0.  0.  ... 0.4 0.4 0.4]
 ...
 [0.  0.  0.3 ... 0.5 0.5 0.5]
 [0.2 0.5 0.5 ... 0.5 0.5 0.5]
 [0.2 0.5 0.5 ... 0.5 0.5 0.5]]

Population trajectory: [50, 59.0, 62.445600000000006, 62.793456961535966, 60.906514028106535, 64.1847685511936, 60.71600257278426, 64.0117639631371, 60.5789261378371, 63.88717626457206, 60.48012279248407, 63.79731874379539, 60.40881570882111, 63.73243881376377, 60.3573056779798, 63.685556376683536, 60.32007179593332, 39.523630889226936, 27.8698229545787, 20.431713488016012, 15.34347899187751]
Harvests: [0.0, 5.9, 9.027135936000038, 11.26173625265758, 6.0906514028106535, 12.83695371023872, 6.071600257278426, 12.80235279262742, 6.057892613783711, 12.777435252914414, 6.0480122792484075, 12.759463748759078, 6.040881570882111, 12.746487762752755, 6.03573056779798, 12.737111275336709, 30.16003589796666, 19.761815444613468, 13.93491147728935, 10.215856744008006]
Total harvest: 213.2660649869655
</pre></div>
</div>
</div>
</div>
<p>Due to pedagogical considerations, this example is using our own implementation of the linear interpolation procedure. However, a more general and practical approach would be to use a built-in interpolation procedure in Numpy. Because our state space has a single dimension, we can simply use <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.">scipy.interpolate.interp1d</a> which offers various interpolation methods through its <code class="docutils literal notranslate"><span class="pre">kind</span></code> argument, which can take values in ‘linear’, ‘nearest’, ‘nearest-up’, ‘zero’, ‘slinear’, ‘quadratic’, ‘cubic’, ‘previous’, or ‘next’. ‘zero’, ‘slinear’, ‘quadratic’ and ‘cubic’.</p>
<p>Here’s a more general implementation which here uses cubic interpolation through the <code class="docutils literal notranslate"><span class="pre">scipy.interpolate.interp1d</span></code> function:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.interpolate</span> <span class="kn">import</span> <span class="n">interp1d</span>

<span class="c1"># Parameters</span>
<span class="n">r_max</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">125</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c1"># Number of time steps</span>
<span class="n">N_max</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># Maximum population size to consider</span>
<span class="n">h_max</span> <span class="o">=</span> <span class="mf">0.5</span>  <span class="c1"># Maximum harvest rate</span>
<span class="n">h_step</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># Step size for harvest rate</span>

<span class="c1"># Create state and decision spaces</span>
<span class="n">N_space</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">N_max</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">h_space</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">h_max</span> <span class="o">+</span> <span class="n">h_step</span><span class="p">,</span> <span class="n">h_step</span><span class="p">)</span>

<span class="c1"># Initialize value function and policy</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">T</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">N_space</span><span class="p">)))</span>
<span class="n">policy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">T</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">N_space</span><span class="p">)))</span>

<span class="c1"># Terminal value function (F_T)</span>
<span class="k">def</span> <span class="nf">terminal_value</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">0</span>

<span class="c1"># State return function (F)</span>
<span class="k">def</span> <span class="nf">state_return</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">h</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">N</span> <span class="o">*</span> <span class="n">h</span>

<span class="c1"># State dynamics function</span>
<span class="k">def</span> <span class="nf">state_dynamics</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">h</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">N</span> <span class="o">+</span> <span class="n">r_max</span> <span class="o">*</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">N</span> <span class="o">/</span> <span class="n">K</span><span class="p">)</span> <span class="o">-</span> <span class="n">N</span> <span class="o">*</span> <span class="n">h</span>

<span class="c1"># Function to create interpolation function for a given time step</span>
<span class="k">def</span> <span class="nf">create_interpolator</span><span class="p">(</span><span class="n">V_t</span><span class="p">,</span> <span class="n">N_space</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">interp1d</span><span class="p">(</span><span class="n">N_space</span><span class="p">,</span> <span class="n">V_t</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;cubic&#39;</span><span class="p">,</span> <span class="n">bounds_error</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fill_value</span><span class="o">=</span><span class="p">(</span><span class="n">V_t</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">V_t</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

<span class="c1"># Backward iteration with interpolation</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">interpolator</span> <span class="o">=</span> <span class="n">create_interpolator</span><span class="p">(</span><span class="n">V</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">N_space</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">N</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">N_space</span><span class="p">):</span>
        <span class="n">max_value</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">)</span>
        <span class="n">best_h</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">h_space</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">h</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># Ensure harvest rate doesn&#39;t exceed 100%</span>
                <span class="k">continue</span>

            <span class="n">next_N</span> <span class="o">=</span> <span class="n">state_dynamics</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">next_N</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># Ensure population doesn&#39;t go extinct</span>
                <span class="k">continue</span>

            <span class="c1"># Use interpolation to get the value for next_N</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">state_return</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span> <span class="o">+</span> <span class="n">interpolator</span><span class="p">(</span><span class="n">next_N</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">value</span> <span class="o">&gt;</span> <span class="n">max_value</span><span class="p">:</span>
                <span class="n">max_value</span> <span class="o">=</span> <span class="n">value</span>
                <span class="n">best_h</span> <span class="o">=</span> <span class="n">h</span>

        <span class="n">V</span><span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_value</span>
        <span class="n">policy</span><span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">best_h</span>

<span class="c1"># Function to simulate the optimal policy using interpolation</span>
<span class="k">def</span> <span class="nf">simulate_optimal_policy</span><span class="p">(</span><span class="n">initial_N</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
    <span class="n">trajectory</span> <span class="o">=</span> <span class="p">[</span><span class="n">initial_N</span><span class="p">]</span>
    <span class="n">harvests</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
        <span class="n">N</span> <span class="o">=</span> <span class="n">trajectory</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        
        <span class="c1"># Create interpolator for the policy at time t</span>
        <span class="n">policy_interpolator</span> <span class="o">=</span> <span class="n">interp1d</span><span class="p">(</span><span class="n">N_space</span><span class="p">,</span> <span class="n">policy</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;cubic&#39;</span><span class="p">,</span> <span class="n">bounds_error</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fill_value</span><span class="o">=</span><span class="p">(</span><span class="n">policy</span><span class="p">[</span><span class="n">t</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">policy</span><span class="p">[</span><span class="n">t</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        
        <span class="n">h</span> <span class="o">=</span> <span class="n">policy_interpolator</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
        <span class="n">harvests</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">N</span> <span class="o">*</span> <span class="n">h</span><span class="p">))</span>  <span class="c1"># Ensure harvest is a Python float</span>

        <span class="n">next_N</span> <span class="o">=</span> <span class="n">state_dynamics</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>
        <span class="n">trajectory</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">next_N</span><span class="p">))</span>  <span class="c1"># Ensure next population value is a Python float</span>

    <span class="k">return</span> <span class="n">trajectory</span><span class="p">,</span> <span class="n">harvests</span>

<span class="c1"># Example usage</span>
<span class="n">initial_N</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">trajectory</span><span class="p">,</span> <span class="n">harvests</span> <span class="o">=</span> <span class="n">simulate_optimal_policy</span><span class="p">(</span><span class="n">initial_N</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Optimal policy (first few rows):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">policy</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Population trajectory:&quot;</span><span class="p">,</span> <span class="n">trajectory</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Harvests:&quot;</span><span class="p">,</span> <span class="n">harvests</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Total harvest:&quot;</span><span class="p">,</span> <span class="nb">sum</span><span class="p">(</span><span class="n">harvests</span><span class="p">))</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimal policy (first few rows):
[[0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.1 0.1 0.1 0.1 0.1 0.1 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.3
  0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.4 0.4 0.4 0.4 0.4 0.4 0.4
  0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4]
 [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.3
  0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4
  0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4]
 [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.1 0.1 0.1 0.1 0.1 0.1 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.3
  0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.4 0.4 0.4 0.4 0.4 0.4 0.4
  0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4]
 [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.3
  0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4
  0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4]
 [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.1 0.1 0.1 0.1 0.1 0.1 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.3
  0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.4 0.4 0.4 0.4 0.4 0.4 0.4
  0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4]]

Population trajectory: [50, 59.0, 62.445600000000006, 62.855816819468515, 66.38501069094303, 62.46338144008508, 66.19082983826176, 62.307060290079974, 65.86630883298251, 62.0275406161329, 65.08602250342238, 61.40579635061663, 65.16431296091169, 61.453283417050585, 65.25607512917725, 61.51516182245858, 65.3615391678991, 42.035857104726354, 29.387853805711547, 21.437532761435143, 16.047063462998082]
Harvests: [3.3073317494565994e-20, 5.8999999999999995, 8.96477607806749, 5.84550227506383, 13.260405311492967, 5.647548383617885, 13.22607620843378, 5.815662115341462, 13.186571332467969, 6.31598238982395, 13.039176123074048, 5.613609913801768, 13.068992991332264, 5.569578810421298, 13.097683026436256, 5.526294879593217, 32.68102988779014, 21.017928552363177, 14.693926902855772, 10.718766380713353]
Total harvest: 213.18951156269063
</pre></div>
</div>
</div>
</div>
<!-- ## Linear Quadratic Regulator via Dynamic Programming

Let us now consider a special case of our dynamic programming formulation: the discrete-time Linear Quadratic Regulator (LQR) problem. This example will illustrate how the structure of linear dynamics and quadratic costs leads to a particularly elegant form of the backward recursion.

Consider a linear time-invariant system with dynamics:

$$
\mathbf{x}_{t+1} = A\mathbf{x}_t + B\mathbf{u}_t
$$

where $\mathbf{x}_t \in \mathbb{R}^n$ is the state and $\mathbf{u}_t \in \mathbb{R}^m$ is the control input.  
The cost function to be minimized is quadratic:

$$
J = \frac{1}{2}\mathbf{x}_T^\top S_T \mathbf{x}_T + \frac{1}{2}\sum_{t=0}^{T-1} \left(\mathbf{x}_t^\top Q \mathbf{x}_t + \mathbf{u}_t^\top R \mathbf{u}_t\right)
$$

where $S_T \geq 0$, $Q \geq 0$, and $R > 0$ are symmetric matrices of appropriate dimensions.  
Our goal is to find the optimal control sequence $\mathbf{u}_t^*$ that minimizes $J$ over a fixed time horizon $[0, T]$, given an initial state $\mathbf{x}_0$.

Let's apply the principle of optimality to derive the backward recursion for this problem. We'll start at the final time step and work our way backward.

At $t = T$, the terminal cost is given by:

$$
J_T^*(\mathbf{x}_T) = \frac{1}{2}\mathbf{x}_T^\top S_T \mathbf{x}_T
$$

At $t = T-1$, the cost-to-go is:

$$
J_{T-1}(\mathbf{x}_{T-1}, \mathbf{u}_{T-1}) = \frac{1}{2}\mathbf{x}_{T-1}^\top Q \mathbf{x}_{T-1} + \frac{1}{2}\mathbf{u}_{T-1}^\top R \mathbf{u}_{T-1} + J_T^*(\mathbf{x}_T)
$$

Substituting the dynamics equation:

$$
J_{T-1} = \frac{1}{2}\mathbf{x}_{T-1}^\top Q \mathbf{x}_{T-1} + \frac{1}{2}\mathbf{u}_{T-1}^\top R \mathbf{u}_{T-1} + \frac{1}{2}(A\mathbf{x}_{T-1} + B\mathbf{u}_{T-1})^\top S_T (A\mathbf{x}_{T-1} + B\mathbf{u}_{T-1})
$$

To find the optimal control, we differentiate with respect to $\mathbf{u}_{T-1}$ and set it to zero:

$$
\frac{\partial J_{T-1}}{\partial \mathbf{u}_{T-1}} = R\mathbf{u}_{T-1} + B^\top S_T (A\mathbf{x}_{T-1} + B\mathbf{u}_{T-1}) = 0
$$

Solving for $\mathbf{u}_{T-1}^*$:

$$
\mathbf{u}_{T-1}^* = -(R + B^\top S_T B)^{-1}B^\top S_T A\mathbf{x}_{T-1}
$$

We can define the gain matrix:

$$
K_{T-1} = (R + B^\top S_T B)^{-1}B^\top S_T A
$$

So that $\mathbf{u}_{T-1}^* = -K_{T-1}\mathbf{x}_{T-1}$. The optimal cost-to-go at $T-1$ is then:

$$
J_{T-1}^*(\mathbf{x}_{T-1}) = \frac{1}{2}\mathbf{x}_{T-1}^\top S_{T-1} \mathbf{x}_{T-1}
$$

Where $S_{T-1}$ is given by:

$$
S_{T-1} = Q + A^\top S_T A - A^\top S_T B(R + B^\top S_T B)^{-1}B^\top S_T A
$$

Continuing this process backward in time, we find that for any $t$:

$$
\mathbf{u}_t^* = -K_t\mathbf{x}_t
$$

Where:

$$
K_t = (R + B^\top S_{t+1} B)^{-1}B^\top S_{t+1} A
$$

And the optimal cost-to-go is:

$$
J_t^*(\mathbf{x}_t) = \frac{1}{2}\mathbf{x}_t^\top S_t \mathbf{x}_t
$$

Where $S_t$ satisfies the so-called discrete-time Riccati equation:

$$
S_t = Q + A^\top S_{t+1} A - A^\top S_{t+1} B(R + B^\top S_{t+1} B)^{-1}B^\top S_{t+1} A
$$ -->
<!-- 
### Example: Linear Quadratic Regulation of a Liquid Tank 

We are dealing with a liquid-level control system for a storage tank. This system consists of a reservoir connected to a tank via valves. These valves are controlled by a gear train, which is driven by a DC motor. The motor, in turn, is controlled by an electronic amplifier. The goal is to maintain a constant liquid level in the tank, adjusting only when necessary.

The system is described by a third-order continuous-time model with the following state variables:
- $x_1(t)$: the height of the liquid in the tank
- $x_2(t)$: the angular position of the electric motor driving the valves
- $x_3(t)$: the angular velocity of the motor

The input to the system, $u(t)$, represents the signal sent to the electronic amplifier connected to the motor.
The system dynamics are described by the following differential equations:

$$
\begin{aligned}
& \dot{x}_1(t) = -2x_1(t) \\
& \dot{x}_2(t) = x_3(t) \\
& \dot{x}_3(t) = -10x_3(t) + 9000u(t)
\end{aligned}
$$

To pose this as a discrete-time LQR problem, we need to discretize the continuous-time system. Let's assume a sampling time of $T_s$ seconds. We can use the forward Euler method for a simple discretization:

$$
\begin{aligned}
& x_1(k+1) = x_1(k) + T_s(-2x_1(k)) \\
& x_2(k+1) = x_2(k) + T_sx_3(k) \\
& x_3(k+1) = x_3(k) + T_s(-10x_3(k) + 9000u(k))
\end{aligned}
$$

This can be written in the standard discrete-time state-space form:

$x(k+1) = Ax(k) + Bu(k)$

Where:

$x(k) = \begin{bmatrix} x_1(k) \\ x_2(k) \\ x_3(k) \end{bmatrix}$

$A = \begin{bmatrix} 
1-2T_s & 0 & 0 \\
0 & 1 & T_s \\
0 & 0 & 1-10T_s
\end{bmatrix}$

$B = \begin{bmatrix} 
0 \\
0 \\
9000T_s
\end{bmatrix}$

The goal of our LQR controller is to maintain the liquid level at a desired reference value while minimizing control effort. We can formulate this as a discrete-time LQR problem with the following cost function:

$J = \sum_{k=0}^{\infty} \left( (x_1(k) - x_{1,ref})^2 + ru^2(k) \right)$

Where $x_{1,ref}$ is the reference liquid level and $r$ is a positive weight on the control input.

To put this in standard discrete-time LQR form, we rewrite the cost function as:

$J = \sum_{k=0}^{\infty} \left( x^\mathrm{T}(k)Qx(k) + ru^2(k) \right)$

Where:

$Q = \begin{bmatrix}
1 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{bmatrix}$

The discrete-time LQR problem is now to find the optimal control law $u^*(k) = -Kx(k)$ that minimizes this cost function, subject to the discrete-time system dynamics $x(k+1) = Ax(k) + Bu(k)$.

The solution involves solving the discrete-time algebraic Riccati equation:

$P = A^TPA - A^TPB(B^TPB + R)^{-1}B^TPA + Q$

Where $R = r$ (a scalar in this case), to find the positive definite matrix $P$. Then, the optimal gain matrix $K$ is given by:

$K = (B^TPB + R)^{-1}B^TPA$

This formulation ensures that:
1. The liquid level ($x_1(k)$) is maintained close to the reference value.
2. The system acts primarily when there's a change in the liquid level, as only $x_1(k)$ is directly penalized in the cost function.
3. The control effort is minimized, ensuring smooth operation of the valves.

By tuning the weight $r$ and the sampling time $T_s$, we can balance the trade-off between maintaining the desired liquid level, the amount of control effort used, and the responsiveness of the system. -->
</section>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="stochastic-dynamic-programming-and-markov-decision-processes">
<h1>Stochastic Dynamic Programming and Markov Decision Processes<a class="headerlink" href="#stochastic-dynamic-programming-and-markov-decision-processes" title="Link to this heading">#</a></h1>
<p>While our previous discussion centered on deterministic systems, many real-world problems involve uncertainty. Stochastic Dynamic Programming (SDP) extends our framework to handle stochasticity in both the objective function and system dynamics. This extension naturally leads us to consider more general policy classes and to formalize when simpler policies suffice.</p>
<section id="decision-rules-and-policies">
<h2>Decision Rules and Policies<a class="headerlink" href="#decision-rules-and-policies" title="Link to this heading">#</a></h2>
<p>Before diving into stochastic systems, we need to establish terminology for the different types of strategies a decision maker might employ. In the deterministic setting, we implicitly used feedback controllers of the form <span class="math notranslate nohighlight">\(u(\mathbf{x}, t)\)</span>. In the stochastic setting, we must be more precise about what information policies can use and how they select actions.</p>
<p>A <strong>decision rule</strong> is a prescription for action selection in each state at a specified decision epoch. These rules can vary in their complexity based on two main criteria:</p>
<ol class="arabic simple">
<li><p><strong>Dependence on history</strong>: Markovian or History-dependent</p></li>
<li><p><strong>Action selection method</strong>: Deterministic or Randomized</p></li>
</ol>
<p><strong>Markovian decision rules</strong> depend only on the current state, while <strong>history-dependent rules</strong> consider the entire sequence of past states and actions. Formally, a history <span class="math notranslate nohighlight">\(h_t\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> is:</p>
<div class="math notranslate nohighlight">
\[h_t = (s_1, a_1, \ldots, s_{t-1}, a_{t-1}, s_t)\]</div>
<p>The set of all possible histories at time <span class="math notranslate nohighlight">\(t\)</span>, denoted <span class="math notranslate nohighlight">\(H_t\)</span>, grows exponentially with <span class="math notranslate nohighlight">\(t\)</span>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H_1 = \mathcal{S}\)</span> (just the initial state)</p></li>
<li><p><span class="math notranslate nohighlight">\(H_2 = \mathcal{S} \times \mathcal{A} \times \mathcal{S}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(H_t = \mathcal{S} \times (\mathcal{A} \times \mathcal{S})^{t-1}\)</span></p></li>
</ul>
<p><strong>Deterministic rules</strong> select an action with certainty, while <strong>randomized rules</strong> specify a probability distribution over the action space.</p>
<p>These classifications lead to four types of decision rules:</p>
<ol class="arabic simple">
<li><p><strong>Markovian Deterministic (MD)</strong>: <span class="math notranslate nohighlight">\(\pi_t: \mathcal{S} \rightarrow \mathcal{A}_s\)</span></p></li>
<li><p><strong>Markovian Randomized (MR)</strong>: <span class="math notranslate nohighlight">\(\pi_t: \mathcal{S} \rightarrow \mathcal{P}(\mathcal{A}_s)\)</span></p></li>
<li><p><strong>History-dependent Deterministic (HD)</strong>: <span class="math notranslate nohighlight">\(\pi_t: H_t \rightarrow \mathcal{A}_s\)</span></p></li>
<li><p><strong>History-dependent Randomized (HR)</strong>: <span class="math notranslate nohighlight">\(\pi_t: H_t \rightarrow \mathcal{P}(\mathcal{A}_s)\)</span></p></li>
</ol>
<p>where <span class="math notranslate nohighlight">\(\mathcal{P}(\mathcal{A}_s)\)</span> denotes the set of probability distributions over <span class="math notranslate nohighlight">\(\mathcal{A}_s\)</span>.</p>
<p>A <strong>policy</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> is a sequence of decision rules, one for each decision epoch:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\pi} = (\pi_1, \pi_2, ..., \pi_{N-1})\]</div>
<p>The set of all policies of class <span class="math notranslate nohighlight">\(K\)</span> (where <span class="math notranslate nohighlight">\(K \in \{HR, HD, MR, MD\}\)</span>) is denoted as <span class="math notranslate nohighlight">\(\Pi^K\)</span>. These policy classes form a hierarchy:</p>
<div class="math notranslate nohighlight">
\[\Pi^{MD} \subset \Pi^{MR} \subset \Pi^{HR}, \quad \Pi^{MD} \subset \Pi^{HD} \subset \Pi^{HR}\]</div>
<p>The largest set <span class="math notranslate nohighlight">\(\Pi^{HR}\)</span> contains all possible policies. We ask: under what conditions can we restrict attention to the much simpler set <span class="math notranslate nohighlight">\(\Pi^{MD}\)</span> without loss of optimality?</p>
<div class="tip admonition">
<p class="admonition-title">Notation: rules vs. policies</p>
<ul class="simple">
<li><p><strong>Decision rule (kernel).</strong> A map from information to action distributions:</p>
<ul>
<li><p>Markov, deterministic:  <span class="math notranslate nohighlight">\(\pi_t:\mathcal{S}\to\mathcal{A}_s\)</span></p></li>
<li><p>Markov, randomized:     <span class="math notranslate nohighlight">\(\pi_t(\cdot\mid s)\in\Delta(\mathcal{A}_s)\)</span></p></li>
<li><p>History-dependent:       <span class="math notranslate nohighlight">\(\pi_t(\cdot\mid h_t)\in\Delta(\mathcal{A}_{s_t})\)</span></p></li>
</ul>
</li>
<li><p><strong>Policy (sequence).</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\pi}=(\pi_1,\pi_2,\ldots)\)</span>.</p></li>
<li><p><strong>Stationary policy.</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\pi}=\mathrm{const}(\pi)\)</span> with <span class="math notranslate nohighlight">\(\pi_t\equiv\pi \ \forall t\)</span>.<br />
By convention, we identify <span class="math notranslate nohighlight">\(\pi\)</span> with its stationary policy <span class="math notranslate nohighlight">\(\mathrm{const}(\pi)\)</span> when no confusion arises.</p></li>
</ul>
</div>
</section>
<section id="stochastic-system-dynamics">
<h2>Stochastic System Dynamics<a class="headerlink" href="#stochastic-system-dynamics" title="Link to this heading">#</a></h2>
<p>In the stochastic setting, our system evolution takes the form:</p>
<div class="math notranslate nohighlight">
\[ \mathbf{x}_{t+1} = \mathbf{f}_t(\mathbf{x}_t, \mathbf{u}_t, \mathbf{w}_t) \]</div>
<p>Here, <span class="math notranslate nohighlight">\(\mathbf{w}_t\)</span> represents a random disturbance or noise term at time <span class="math notranslate nohighlight">\(t\)</span> due to the inherent uncertainty in the system’s behavior. The stage cost function may also incorporate stochastic influences:</p>
<div class="math notranslate nohighlight">
\[ c_t(\mathbf{x}_t, \mathbf{u}_t, \mathbf{w}_t) \]</div>
<p>In this context, our objective shifts from minimizing a deterministic cost to minimizing the expected total cost:</p>
<div class="math notranslate nohighlight">
\[ \mathbb{E}\left[c_\mathrm{T}(\mathbf{x}_T) + \sum_{t=1}^{T-1} c_t(\mathbf{x}_t, \mathbf{u}_t, \mathbf{w}_t)\right] \]</div>
<p>where the expectation is taken over the distributions of the random variables <span class="math notranslate nohighlight">\(\mathbf{w}_t\)</span>. The principle of optimality still holds in the stochastic case, but Bellman’s optimality equation now involves an expectation:</p>
<div class="math notranslate nohighlight">
\[ J_k^\star(\mathbf{x}_k) = \min_{\mathbf{u}_k} \mathbb{E}_{\mathbf{w}_k}\left[c_k(\mathbf{x}_k, \mathbf{u}_k, \mathbf{w}_k) + J_{k+1}^\star(\mathbf{f}_k(\mathbf{x}_k, \mathbf{u}_k, \mathbf{w}_k))\right] \]</div>
<p>In practice, this expectation is often computed by discretizing the distribution of <span class="math notranslate nohighlight">\(\mathbf{w}_k\)</span> when the set of possible disturbances is very large or even continuous. Let’s say we approximate the distribution with <span class="math notranslate nohighlight">\(K\)</span> discrete values <span class="math notranslate nohighlight">\(\mathbf{w}_k^i\)</span>, each occurring with probability <span class="math notranslate nohighlight">\(p_k^i\)</span>. Then our Bellman equation becomes:</p>
<div class="math notranslate nohighlight">
\[ J_k^\star(\mathbf{x}_k) = \min_{\mathbf{u}_k} \sum_{i=1}^K p_k^i \left(c_k(\mathbf{x}_k, \mathbf{u}_k, \mathbf{w}_k^i) + J_{k+1}^\star(\mathbf{f}_k(\mathbf{x}_k, \mathbf{u}_k, \mathbf{w}_k^i))\right) \]</div>
</section>
<section id="optimality-equations-in-the-stochastic-setting">
<h2>Optimality Equations in the Stochastic Setting<a class="headerlink" href="#optimality-equations-in-the-stochastic-setting" title="Link to this heading">#</a></h2>
<p>When dealing with stochastic systems, a central question arises: what information should our control policy use? In the most general case, a policy might use the entire history of observations and actions. However, as we’ll see, the Markovian structure of our problems allows for dramatic simplifications.</p>
<p>Let <span class="math notranslate nohighlight">\(h_t = (s_1, a_1, s_2, a_2, \ldots, s_{t-1}, a_{t-1}, s_t)\)</span> denote the complete history up to time <span class="math notranslate nohighlight">\(t\)</span>. In the stochastic setting, the history-based optimality equations become:</p>
<div class="math notranslate nohighlight">
\[
u_t(h_t) = \sup_{a\in A_{s_t}}\left\{ r_t(s_t,a) + \sum_{j\in S} p_t(j\mid s_t,a)\, u_{t+1}(h_t,a,j) \right\},\quad u_N(h_N)=r_N(s_N)
\]</div>
<p>where we now explicitly use the transition probabilities <span class="math notranslate nohighlight">\(p_t(j|s_t,a)\)</span> rather than a deterministic dynamics function.</p>
<div class="proof theorem admonition" id="stoch-principle-opt">
<p class="admonition-title"><span class="caption-number">Theorem 3 </span> (Principle of optimality for stochastic systems)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(u_t^*\)</span> be the optimal expected return from epoch <span class="math notranslate nohighlight">\(t\)</span> onward. Then:</p>
<p><strong>a.</strong> <span class="math notranslate nohighlight">\(u_t^*\)</span> satisfies the optimality equations:</p>
<div class="math notranslate nohighlight">
\[u_t^*(h_t) = \sup_{a \in A_{s_t}} \left\{ r_t(s_t, a) + \sum_{j \in S} p_t(j|s_t, a) u_{t+1}^*(h_t, a, j) \right\}\]</div>
<p>with boundary condition <span class="math notranslate nohighlight">\(u_N^*(h_N) = r_N(s_N)\)</span>.</p>
<p><strong>b.</strong> Any policy <span class="math notranslate nohighlight">\(\pi^*\)</span> that selects actions attaining the supremum (or maximum) in the above equation at each history is optimal.</p>
</section>
</div><p><strong>Intuition:</strong> This formalizes Bellman’s principle of optimality: “An optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision.” The recursive structure means that optimal local decisions (choosing the best action at each step) lead to global optimality, even with uncertainty captured by the transition probabilities.</p>
<p>A remarkable simplification occurs when we examine these history-based equations more closely. The Markov property of our system dynamics and rewards means that the optimal return actually depends on the history only through the current state:</p>
<div class="proof proposition admonition" id="stoch-state-suff">
<p class="admonition-title"><span class="caption-number">Proposition 2 </span> (State sufficiency for stochastic MDPs)</p>
<section class="proposition-content" id="proof-content">
<p>In finite-horizon stochastic MDPs with Markovian dynamics and rewards, the optimal return <span class="math notranslate nohighlight">\(u_t^*(h_t)\)</span> depends on the history only through the current state <span class="math notranslate nohighlight">\(s_t\)</span>. Thus we can write <span class="math notranslate nohighlight">\(u_t^*(h_t) = v_t^*(s_t)\)</span> for some function <span class="math notranslate nohighlight">\(v_t^*\)</span> that depends only on state and time.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Following <span id="id2">Puterman [<a class="reference internal" href="bibliography.html#id23" title="Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley &amp; Sons, New York, 1994. ISBN 978-0-471-61977-3. First published in 1994.">34</a>]</span> Theorem 4.4.2. We proceed by backward induction.</p>
<p><strong>Base case:</strong> At the terminal time <span class="math notranslate nohighlight">\(N\)</span>, we have <span class="math notranslate nohighlight">\(u_N^*(h_N) = r_N(s_N)\)</span> by the boundary condition. Since the terminal reward depends only on the final state <span class="math notranslate nohighlight">\(s_N\)</span> and not on how we arrived there, <span class="math notranslate nohighlight">\(u_N^*(h_N) = u_N^*(s_N)\)</span>.</p>
<p><strong>Inductive step:</strong> Assume <span class="math notranslate nohighlight">\(u_{t+1}^*(h_{t+1})\)</span> depends on <span class="math notranslate nohighlight">\(h_{t+1}\)</span> only through <span class="math notranslate nohighlight">\(s_{t+1}\)</span> for all <span class="math notranslate nohighlight">\(t+1, \ldots, N\)</span>. Then from the optimality equation:</p>
<div class="math notranslate nohighlight">
\[u_t^*(h_t) = \sup_{a \in A_{s_t}} \left\{ r_t(s_t, a) + \sum_{j \in S} p_t(j|s_t, a) u_{t+1}^*(h_t, a, j) \right\}\]</div>
<p>By the induction hypothesis, <span class="math notranslate nohighlight">\(u_{t+1}^*(h_t, a, j)\)</span> depends only on the next state <span class="math notranslate nohighlight">\(j\)</span>, so:</p>
<div class="math notranslate nohighlight">
\[u_t^*(h_t) = \sup_{a \in A_{s_t}} \left\{ r_t(s_t, a) + \sum_{j \in S} p_t(j|s_t, a) u_{t+1}^*(j) \right\}\]</div>
<p>Since the expression in brackets depends on <span class="math notranslate nohighlight">\(h_t\)</span> only through the current state <span class="math notranslate nohighlight">\(s_t\)</span> (the rewards and transition probabilities are Markovian), we conclude that <span class="math notranslate nohighlight">\(u_t^*(h_t) = u_t^*(s_t)\)</span>.</p>
</div>
<p><strong>Intuition:</strong> The Markov property means that the current state contains all information needed to predict future evolution. The past provides no additional value for decision-making. This powerful result allows us to work with value functions <span class="math notranslate nohighlight">\(v_t^*(s)\)</span> indexed only by state and time, dramatically simplifying both theory and computation.</p>
<p>This state-sufficiency result, combined with the fact that randomization never helps when maximizing expected returns, leads to a dramatic simplification of the policy space:</p>
<div class="proof theorem admonition" id="stoch-policy-reduction">
<p class="admonition-title"><span class="caption-number">Theorem 4 </span> (Policy reduction for stochastic MDPs)</p>
<section class="theorem-content" id="proof-content">
<p>For finite-horizon stochastic MDPs with finite state and action sets:</p>
<div class="math notranslate nohighlight">
\[
\sup_{\pi \in \Pi^{\mathrm{HR}}} v_\pi(s,t) = \max_{\pi \in \Pi^{\mathrm{MD}}} v_\pi(s,t)
\]</div>
<p>That is, there exists an optimal policy that is both deterministic and Markovian.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Sketch following <span id="id3">Puterman [<a class="reference internal" href="bibliography.html#id23" title="Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley &amp; Sons, New York, 1994. ISBN 978-0-471-61977-3. First published in 1994.">34</a>]</span> Lemma 4.3.1 and Theorem 4.4.2. First, Lemma 4.3.1 shows that for any function <span class="math notranslate nohighlight">\(w\)</span> and any distribution <span class="math notranslate nohighlight">\(q\)</span> over actions, <span class="math notranslate nohighlight">\(\sup_a w(a) \ge \sum_a q(a) w(a)\)</span>. Thus randomization cannot improve the expected value over choosing a single maximizing action. Second, by state sufficiency (Proposition <span class="xref std std-ref">stoch-state-suff</span> and <span id="id4">Puterman [<a class="reference internal" href="bibliography.html#id23" title="Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley &amp; Sons, New York, 1994. ISBN 978-0-471-61977-3. First published in 1994.">34</a>]</span> Thm. 4.4.2(a)), the optimal return depends on the history only through <span class="math notranslate nohighlight">\((s_t,t)\)</span>. Therefore, selecting at each <span class="math notranslate nohighlight">\((s_t,t)\)</span> an action that attains the maximum yields a deterministic Markov decision rule which is optimal whenever the maximum is attained. If only a supremum exists, <span class="math notranslate nohighlight">\(\varepsilon\)</span>-optimal selectors exist by choosing actions within <span class="math notranslate nohighlight">\(\varepsilon\)</span> of the supremum (see <span id="id5">Puterman [<a class="reference internal" href="bibliography.html#id23" title="Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley &amp; Sons, New York, 1994. ISBN 978-0-471-61977-3. First published in 1994.">34</a>]</span> Thm. 4.3.4).</p>
</div>
<p><strong>Intuition:</strong> Even in stochastic systems, randomization in the policy doesn’t help when maximizing expected returns: you should always choose the action with the highest expected value. Combined with state sufficiency, this means simple state-to-action mappings are optimal.</p>
<p>These results justify focusing on deterministic Markov policies and lead to the backward recursion algorithm for stochastic systems:</p>
<div class="proof algorithm admonition" id="stochastic-backward-recursion">
<p class="admonition-title"><span class="caption-number">Algorithm 5 </span> (Backward Recursion for Stochastic Dynamic Programming)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> Terminal cost function <span class="math notranslate nohighlight">\(c_\mathrm{T}(\cdot)\)</span>, stage cost functions <span class="math notranslate nohighlight">\(c_t(\cdot, \cdot, \cdot)\)</span>, system dynamics <span class="math notranslate nohighlight">\(\mathbf{f}_t(\cdot, \cdot, \cdot)\)</span>, time horizon <span class="math notranslate nohighlight">\(\mathrm{T}\)</span>, disturbance distributions</p>
<p><strong>Output:</strong> Optimal value functions <span class="math notranslate nohighlight">\(J_t^\star(\cdot)\)</span> and optimal control policies <span class="math notranslate nohighlight">\(\mu_t^\star(\cdot)\)</span> for <span class="math notranslate nohighlight">\(t = 1, \ldots, T\)</span></p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(J_T^\star(\mathbf{x}) = c_\mathrm{T}(\mathbf{x})\)</span> for all <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> in the state space</p></li>
<li><p>For <span class="math notranslate nohighlight">\(t = T-1, T-2, \ldots, 1\)</span>:</p>
<ol class="arabic simple">
<li><p>For each state <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> in the state space:</p>
<ol class="arabic simple">
<li><p>Compute <span class="math notranslate nohighlight">\(J_t^\star(\mathbf{x}) = \min_{\mathbf{u}} \mathbb{E}_{\mathbf{w}_t}\left[c_t(\mathbf{x}, \mathbf{u}, \mathbf{w}_t) + J_{t+1}^\star(\mathbf{f}_t(\mathbf{x}, \mathbf{u}, \mathbf{w}_t))\right]\)</span></p></li>
<li><p>Compute <span class="math notranslate nohighlight">\(\mu_t^\star(\mathbf{x}) = \arg\min_{\mathbf{u}} \mathbb{E}_{\mathbf{w}_t}\left[c_t(\mathbf{x}, \mathbf{u}, \mathbf{w}_t) + J_{t+1}^\star(\mathbf{f}_t(\mathbf{x}, \mathbf{u}, \mathbf{w}_t))\right]\)</span></p></li>
</ol>
</li>
<li><p>End For</p></li>
</ol>
</li>
<li><p>End For</p></li>
<li><p>Return <span class="math notranslate nohighlight">\(J_t^\star(\cdot)\)</span>, <span class="math notranslate nohighlight">\(\mu_t^\star(\cdot)\)</span> for <span class="math notranslate nohighlight">\(t = 1, \ldots, T\)</span></p></li>
</ol>
</section>
</div><p>While SDP provides us with a framework to for handling uncertainty, it makes the curse of dimensionality even more difficult to handle in practice. Not only does the state space need to be discretized, but now the disturbance space must be discretized as well. This can lead to a combinatorial explosion in the number of scenarios to be evaluated at each stage.</p>
<p>However, just as we tackled the challenges of continuous state spaces with discretization and interpolation, we can devise efficient methods to handle the additional complexity of evaluating expectations. This problem essentially becomes one of numerical integration. When the set of disturbances is continuous (as is often the case with continuous state spaces), we enter a domain where numerical quadrature methods could be applied. But these methods tend to scale poorly as the number of dimensions grows. This is where more efficient techniques, often rooted in Monte Carlo methods, come into play. The combination of two key ingredients emerges to tackle the curse of dimensionality:</p>
<ol class="arabic simple">
<li><p>Function approximation (through discretization, interpolation, neural networks, etc.)</p></li>
<li><p>Monte Carlo integration (simulation)</p></li>
</ol>
<p>These two elements essentially distill the key ingredients of machine learning, which is the direction we’ll be exploring in this course.</p>
</section>
<section id="example-stochastic-optimal-harvest-in-resource-management">
<h2>Example: Stochastic Optimal Harvest in Resource Management<a class="headerlink" href="#example-stochastic-optimal-harvest-in-resource-management" title="Link to this heading">#</a></h2>
<p>Building upon our previous deterministic model, we now introduce stochasticity to more accurately reflect the uncertainties inherent in real-world resource management scenarios <span id="id6">[<a class="reference internal" href="bibliography.html#id22" title="Michael J. Conroy and James T. Peterson. Decision Making in Natural Resource Management: A Structured, Adaptive Approach: A Structured, Adaptive Approach. Wiley, January 2013. ISBN 9781118506196. URL: http://dx.doi.org/10.1002/9781118506196, doi:10.1002/9781118506196.">9</a>]</span>. As before, we consider a population of a particular species, whose abundance we denote by <span class="math notranslate nohighlight">\(x_t\)</span>, where <span class="math notranslate nohighlight">\(t\)</span> represents discrete time steps. Our objective remains to maximize the cumulative harvest over a finite time horizon, while also considering the long-term sustainability of the population. However, we now account for two sources of stochasticity: partial controllability of harvest and environmental variability affecting growth rates.
The optimization problem can be formulated as:</p>
<div class="math notranslate nohighlight">
\[
\text{maximize} \quad \mathbb{E}\left[\sum_{t=t_0}^{t_f} F(x_t \cdot h_t)\right]
\]</div>
<p>Here, <span class="math notranslate nohighlight">\(F(\cdot)\)</span> represents the immediate reward function associated with harvesting, and <span class="math notranslate nohighlight">\(h_t\)</span> is the realized harvest rate at time <span class="math notranslate nohighlight">\(t\)</span>. The expectation <span class="math notranslate nohighlight">\(\mathbb{E}[\cdot]\)</span> over both harvest and growth rates, which we view as random variables.
In our stochastic model, the abundance <span class="math notranslate nohighlight">\(x\)</span> still ranges from 1 to 100 individuals. The decision variable is now the desired harvest rate <span class="math notranslate nohighlight">\(d_t\)</span>, which can take values from the set <span class="math notranslate nohighlight">\(D = {0, 0.1, 0.2, 0.3, 0.4, 0.5}\)</span>. However, the realized harvest rate <span class="math notranslate nohighlight">\(h_t\)</span> is stochastic and follows a discrete distribution:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
h_t = \begin{cases}
0.75d_t &amp; \text{with probability } 0.25 \\
d_t &amp; \text{with probability } 0.5 \\
1.25d_t &amp; \text{with probability } 0.25
\end{cases}
\end{split}\]</div>
<p>By expressing the harvest rate as a random variable, we mean to capture the fact that harvesting is a not completely under our control: we might obtain more or less what we had intended to. Furthermore, we generalize the population dynamics to the stochastic case via:</p>
<div class="math notranslate nohighlight">
\[x_{t+1} = x_t + r_tx_t(1 - x_t/K) - h_tx_t
\]</div>
<p>where <span class="math notranslate nohighlight">\(K = 125\)</span> is the carrying capacity. The growth rate <span class="math notranslate nohighlight">\(r_t\)</span> is now stochastic and follows a discrete distribution:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
r_t = \begin{cases}
0.85r_{\text{max}} &amp; \text{with probability } 0.25 \\
1.05r_{\text{max}} &amp; \text{with probability } 0.5 \\
1.15r_{\text{max}} &amp; \text{with probability } 0.25
\end{cases}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(r_{\text{max}} = 0.3\)</span> is the maximum growth rate.
Applying the principle of optimality, we can express the optimal value function <span class="math notranslate nohighlight">\(J^\star(x_t, t)\)</span> recursively:</p>
<div class="math notranslate nohighlight">
\[
J^\star(x_t, t) = \max_{d(t) \in D} \mathbb{E}\left[F(x_t \cdot h_t) + J^\star(x_{t+1}, t+1)\right]
\]</div>
<p>where the expectation is taken over the harvest and growth rate random variables. The boundary condition remains <span class="math notranslate nohighlight">\(J^*(x_{t_f}) = 0\)</span>. We can now adapt our previous code to account for the stochasticity in our model. One important difference is that simulating a solution in this context requires multiple realizations of our process. This is an important consideration when evaluating reinforcement learning methods in practice, as success cannot be claimed based on a single successful trajectory.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.interpolate</span> <span class="kn">import</span> <span class="n">interp1d</span>

<span class="c1"># Parameters</span>
<span class="n">r_max</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">125</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">30</span>  <span class="c1"># Number of time steps</span>
<span class="n">N_max</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># Maximum population size to consider</span>
<span class="n">h_max</span> <span class="o">=</span> <span class="mf">0.5</span>  <span class="c1"># Maximum harvest rate</span>
<span class="n">h_step</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># Step size for harvest rate</span>

<span class="c1"># Create state and decision spaces</span>
<span class="n">N_space</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">N_max</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>  <span class="c1"># Using more granular state space</span>
<span class="n">h_space</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">h_max</span> <span class="o">+</span> <span class="n">h_step</span><span class="p">,</span> <span class="n">h_step</span><span class="p">)</span>

<span class="c1"># Stochastic parameters</span>
<span class="n">h_outcomes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">])</span>
<span class="n">h_probs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">])</span>
<span class="n">r_outcomes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.85</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">,</span> <span class="mf">1.15</span><span class="p">])</span> <span class="o">*</span> <span class="n">r_max</span>
<span class="n">r_probs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">])</span>

<span class="c1"># Initialize value function and policy</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">T</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">N_space</span><span class="p">)))</span>
<span class="n">policy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">T</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">N_space</span><span class="p">)))</span>

<span class="c1"># State return function (F)</span>
<span class="k">def</span> <span class="nf">state_return</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">h</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">N</span> <span class="o">*</span> <span class="n">h</span>

<span class="c1"># State dynamics function (stochastic)</span>
<span class="k">def</span> <span class="nf">state_dynamics</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">r</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">N</span> <span class="o">+</span> <span class="n">r</span> <span class="o">*</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">N</span> <span class="o">/</span> <span class="n">K</span><span class="p">)</span> <span class="o">-</span> <span class="n">h</span> <span class="o">*</span> <span class="n">N</span>

<span class="c1"># Function to create interpolation function for a given time step</span>
<span class="k">def</span> <span class="nf">create_interpolator</span><span class="p">(</span><span class="n">V_t</span><span class="p">,</span> <span class="n">N_space</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">interp1d</span><span class="p">(</span><span class="n">N_space</span><span class="p">,</span> <span class="n">V_t</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">bounds_error</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fill_value</span><span class="o">=</span><span class="p">(</span><span class="n">V_t</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">V_t</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

<span class="c1"># Backward iteration with stochastic dynamics</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">interpolator</span> <span class="o">=</span> <span class="n">create_interpolator</span><span class="p">(</span><span class="n">V</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">N_space</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">N</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">N_space</span><span class="p">):</span>
        <span class="n">max_value</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">)</span>
        <span class="n">best_h</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">h_space</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">h</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># Ensure harvest rate doesn&#39;t exceed 100%</span>
                <span class="k">continue</span>

            <span class="n">expected_value</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">h_factor</span><span class="p">,</span> <span class="n">h_prob</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">h_outcomes</span><span class="p">,</span> <span class="n">h_probs</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">r_factor</span><span class="p">,</span> <span class="n">r_prob</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">r_outcomes</span><span class="p">,</span> <span class="n">r_probs</span><span class="p">):</span>
                    <span class="n">realized_h</span> <span class="o">=</span> <span class="n">h</span> <span class="o">*</span> <span class="n">h_factor</span>
                    <span class="n">realized_r</span> <span class="o">=</span> <span class="n">r_factor</span>

                    <span class="n">next_N</span> <span class="o">=</span> <span class="n">state_dynamics</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">realized_h</span><span class="p">,</span> <span class="n">realized_r</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">next_N</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># Ensure population doesn&#39;t go extinct</span>
                        <span class="k">continue</span>

                    <span class="c1"># Use interpolation to get the value for next_N</span>
                    <span class="n">value</span> <span class="o">=</span> <span class="n">state_return</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">realized_h</span><span class="p">)</span> <span class="o">+</span> <span class="n">interpolator</span><span class="p">(</span><span class="n">next_N</span><span class="p">)</span>
                    <span class="n">expected_value</span> <span class="o">+=</span> <span class="n">value</span> <span class="o">*</span> <span class="n">h_prob</span> <span class="o">*</span> <span class="n">r_prob</span>

            <span class="k">if</span> <span class="n">expected_value</span> <span class="o">&gt;</span> <span class="n">max_value</span><span class="p">:</span>
                <span class="n">max_value</span> <span class="o">=</span> <span class="n">expected_value</span>
                <span class="n">best_h</span> <span class="o">=</span> <span class="n">h</span>

        <span class="n">V</span><span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_value</span>
        <span class="n">policy</span><span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">best_h</span>

<span class="c1"># Function to simulate the optimal policy using interpolation (stochastic version)</span>
<span class="k">def</span> <span class="nf">simulate_optimal_policy</span><span class="p">(</span><span class="n">initial_N</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">num_simulations</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">all_trajectories</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">all_harvests</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_simulations</span><span class="p">):</span>
        <span class="n">trajectory</span> <span class="o">=</span> <span class="p">[</span><span class="n">initial_N</span><span class="p">]</span>
        <span class="n">harvests</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
            <span class="n">N</span> <span class="o">=</span> <span class="n">trajectory</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            
            <span class="c1"># Create interpolator for the policy at time t</span>
            <span class="n">policy_interpolator</span> <span class="o">=</span> <span class="n">interp1d</span><span class="p">(</span><span class="n">N_space</span><span class="p">,</span> <span class="n">policy</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">bounds_error</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fill_value</span><span class="o">=</span><span class="p">(</span><span class="n">policy</span><span class="p">[</span><span class="n">t</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">policy</span><span class="p">[</span><span class="n">t</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
            
            <span class="n">intended_h</span> <span class="o">=</span> <span class="n">policy_interpolator</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
            
            <span class="c1"># Apply stochasticity</span>
            <span class="n">h_factor</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">h_outcomes</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">h_probs</span><span class="p">)</span>
            <span class="n">r_factor</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">r_outcomes</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">r_probs</span><span class="p">)</span>
            
            <span class="n">realized_h</span> <span class="o">=</span> <span class="n">intended_h</span> <span class="o">*</span> <span class="n">h_factor</span>
            <span class="n">harvests</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">N</span> <span class="o">*</span> <span class="n">realized_h</span><span class="p">)</span>

            <span class="n">next_N</span> <span class="o">=</span> <span class="n">state_dynamics</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">realized_h</span><span class="p">,</span> <span class="n">r_factor</span><span class="p">)</span>
            <span class="n">trajectory</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">next_N</span><span class="p">)</span>

        <span class="n">all_trajectories</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">trajectory</span><span class="p">)</span>
        <span class="n">all_harvests</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">harvests</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">all_trajectories</span><span class="p">,</span> <span class="n">all_harvests</span>

<span class="c1"># Example usage</span>
<span class="n">initial_N</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">trajectories</span><span class="p">,</span> <span class="n">harvests</span> <span class="o">=</span> <span class="n">simulate_optimal_policy</span><span class="p">(</span><span class="n">initial_N</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>

<span class="c1"># Calculate average trajectory and total harvest</span>
<span class="n">avg_trajectory</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">trajectories</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">avg_total_harvest</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="nb">sum</span><span class="p">(</span><span class="n">h</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">harvests</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Optimal policy (first few rows):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">policy</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Average population trajectory:&quot;</span><span class="p">,</span> <span class="n">avg_trajectory</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Average total harvest:&quot;</span><span class="p">,</span> <span class="n">avg_total_harvest</span><span class="p">)</span>

<span class="c1"># Plot results</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="k">for</span> <span class="n">traj</span> <span class="ow">in</span> <span class="n">trajectories</span><span class="p">[:</span><span class="mi">20</span><span class="p">]:</span>  <span class="c1"># Plot first 20 trajectories</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">traj</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">avg_trajectory</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Population Trajectories&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Population&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">([</span><span class="nb">sum</span><span class="p">(</span><span class="n">h</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">harvests</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Distribution of Total Harvest&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Total Harvest&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Frequency&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimal policy (first few rows):
[[0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.3
  0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.4 0.4 0.4 0.4 0.4 0.4
  0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4]
 [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.3
  0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.4 0.4 0.4 0.4 0.4 0.4
  0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4]
 [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.3
  0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.4 0.4 0.4 0.4 0.4 0.4
  0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4]
 [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.3
  0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.4 0.4 0.4 0.4 0.4 0.4
  0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4]
 [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.3
  0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.4 0.4 0.4 0.4 0.4 0.4
  0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4]]

Average population trajectory: [50.         59.126      62.69619476 62.50888039 62.47469976 62.03661246
 62.53917782 62.49630981 62.52289919 62.39429827 62.6029934  62.3916502
 62.81830013 62.4175036  62.67827868 62.5822419  62.18842845 62.41458732
 62.58263582 62.98740346 62.17142674 62.18879702 62.0832301  62.62570881
 62.85043722 62.78845198 61.44489905 42.63269697 30.2230333  22.26564984
 16.40866945]
Average total harvest: 313.43165025164313
</pre></div>
</div>
<img alt="_images/4e4be39bb1c5af9167aa90f83815afb1b1be05d433aae03d5bf5e4b3d8053e17.png" src="_images/4e4be39bb1c5af9167aa90f83815afb1b1be05d433aae03d5bf5e4b3d8053e17.png" />
</div>
</div>
</section>
<section id="linear-quadratic-regulator-via-dynamic-programming">
<h2>Linear Quadratic Regulator via Dynamic Programming<a class="headerlink" href="#linear-quadratic-regulator-via-dynamic-programming" title="Link to this heading">#</a></h2>
<p>We now examine a special case where the backward recursion admits a remarkable closed-form solution. When the system dynamics are linear and the cost function is quadratic, the optimization at each stage can be solved analytically. Moreover, the value function itself maintains a quadratic structure throughout the recursion, and the optimal policy reduces to a simple linear feedback law. This elegant result eliminates the need for discretization, interpolation, or any function approximation. The infinite-dimensional problem collapses to tracking a finite set of matrices.</p>
<p>Consider a discrete-time linear system:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}_{t+1} = A_t\mathbf{x}_t + B_t\mathbf{u}_t
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{x}_t \in \mathbb{R}^n\)</span> is the state and <span class="math notranslate nohighlight">\(\mathbf{u}_t \in \mathbb{R}^m\)</span> is the control input. The matrices <span class="math notranslate nohighlight">\(A_t \in \mathbb{R}^{n \times n}\)</span> and <span class="math notranslate nohighlight">\(B_t \in \mathbb{R}^{n \times m}\)</span> describe the system dynamics at time <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p>The cost function to be minimized is quadratic:</p>
<div class="math notranslate nohighlight">
\[
J = \frac{1}{2}\mathbf{x}_T^\top Q_T \mathbf{x}_T + \frac{1}{2}\sum_{t=0}^{T-1} \left(\mathbf{x}_t^\top Q_t \mathbf{x}_t + \mathbf{u}_t^\top R_t \mathbf{u}_t\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(Q_T \succeq 0\)</span> (positive semidefinite), <span class="math notranslate nohighlight">\(Q_t \succeq 0\)</span>, and <span class="math notranslate nohighlight">\(R_t \succ 0\)</span> (positive definite) are symmetric matrices of appropriate dimensions. The positive definiteness of <span class="math notranslate nohighlight">\(R_t\)</span> ensures the minimization problem is well-posed.</p>
<p>What we now have to observe is that if the terminal cost is quadratic, then the value function at every earlier stage remains quadratic. This is not immediately obvious, but it follows from the structure of Bellman’s equation combined with the linearity of the dynamics.</p>
<p>We claim that the optimal cost-to-go from any stage <span class="math notranslate nohighlight">\(t\)</span> takes the form:</p>
<div class="math notranslate nohighlight">
\[
J_t^\star(\mathbf{x}_t) = \frac{1}{2}\mathbf{x}_t^\top P_t \mathbf{x}_t
\]</div>
<p>for some positive semidefinite matrix <span class="math notranslate nohighlight">\(P_t\)</span>. At the terminal time, this is true by definition: <span class="math notranslate nohighlight">\(P_T = Q_T\)</span>.</p>
<p>Let’s verify this structure and derive the recursion for <span class="math notranslate nohighlight">\(P_t\)</span> using backward induction. Suppose we’ve established that <span class="math notranslate nohighlight">\(J_{t+1}^\star(\mathbf{x}_{t+1}) = \frac{1}{2}\mathbf{x}_{t+1}^\top P_{t+1} \mathbf{x}_{t+1}\)</span>. Bellman’s equation at stage <span class="math notranslate nohighlight">\(t\)</span> states:</p>
<div class="math notranslate nohighlight">
\[
J_t^\star(\mathbf{x}_t) = \min_{\mathbf{u}_t} \left[ \frac{1}{2}\mathbf{x}_t^\top Q_t \mathbf{x}_t + \frac{1}{2}\mathbf{u}_t^\top R_t \mathbf{u}_t + J_{t+1}^\star(\mathbf{x}_{t+1}) \right]
\]</div>
<p>Substituting the dynamics <span class="math notranslate nohighlight">\(\mathbf{x}_{t+1} = A_t\mathbf{x}_t + B_t\mathbf{u}_t\)</span> and the quadratic form for <span class="math notranslate nohighlight">\(J_{t+1}^\star\)</span>:</p>
<div class="math notranslate nohighlight">
\[
J_t^\star(\mathbf{x}_t) = \min_{\mathbf{u}_t} \left[ \frac{1}{2}\mathbf{x}_t^\top Q_t \mathbf{x}_t + \frac{1}{2}\mathbf{u}_t^\top R_t \mathbf{u}_t + \frac{1}{2}(A_t\mathbf{x}_t + B_t\mathbf{u}_t)^\top P_{t+1} (A_t\mathbf{x}_t + B_t\mathbf{u}_t) \right]
\]</div>
<p>Expanding the last term:</p>
<div class="math notranslate nohighlight">
\[
(A_t\mathbf{x}_t + B_t\mathbf{u}_t)^\top P_{t+1} (A_t\mathbf{x}_t + B_t\mathbf{u}_t) = \mathbf{x}_t^\top A_t^\top P_{t+1} A_t \mathbf{x}_t + 2\mathbf{x}_t^\top A_t^\top P_{t+1} B_t \mathbf{u}_t + \mathbf{u}_t^\top B_t^\top P_{t+1} B_t \mathbf{u}_t
\]</div>
<p>The expression inside the minimization becomes:</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{2}\mathbf{x}_t^\top Q_t \mathbf{x}_t + \frac{1}{2}\mathbf{u}_t^\top R_t \mathbf{u}_t + \frac{1}{2}\mathbf{x}_t^\top A_t^\top P_{t+1} A_t \mathbf{x}_t + \mathbf{x}_t^\top A_t^\top P_{t+1} B_t \mathbf{u}_t + \frac{1}{2}\mathbf{u}_t^\top B_t^\top P_{t+1} B_t \mathbf{u}_t
\]</div>
<p>Collecting terms involving <span class="math notranslate nohighlight">\(\mathbf{u}_t\)</span>:</p>
<div class="math notranslate nohighlight">
\[
= \frac{1}{2}\mathbf{x}_t^\top (Q_t + A_t^\top P_{t+1} A_t) \mathbf{x}_t + \mathbf{x}_t^\top A_t^\top P_{t+1} B_t \mathbf{u}_t + \frac{1}{2}\mathbf{u}_t^\top (R_t + B_t^\top P_{t+1} B_t) \mathbf{u}_t
\]</div>
<p>This is a quadratic function of <span class="math notranslate nohighlight">\(\mathbf{u}_t\)</span>. To find the minimizer, we take the gradient with respect to <span class="math notranslate nohighlight">\(\mathbf{u}_t\)</span> and set it to zero:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial}{\partial \mathbf{u}_t} = (R_t + B_t^\top P_{t+1} B_t) \mathbf{u}_t + B_t^\top P_{t+1} A_t \mathbf{x}_t = 0
\]</div>
<p>Since <span class="math notranslate nohighlight">\(R_t + B_t^\top P_{t+1} B_t\)</span> is positive definite (both <span class="math notranslate nohighlight">\(R_t\)</span> and <span class="math notranslate nohighlight">\(P_{t+1}\)</span> are positive semidefinite with <span class="math notranslate nohighlight">\(R_t\)</span> strictly positive), we can solve for the optimal control:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{u}_t^\star = -(R_t + B_t^\top P_{t+1} B_t)^{-1} B_t^\top P_{t+1} A_t \mathbf{x}_t
\]</div>
<p>Define the gain matrix:</p>
<div class="math notranslate nohighlight">
\[
K_t = (R_t + B_t^\top P_{t+1} B_t)^{-1} B_t^\top P_{t+1} A_t
\]</div>
<p>so that <span class="math notranslate nohighlight">\(\mathbf{u}_t^\star = -K_t\mathbf{x}_t\)</span>. This is a <strong>linear feedback policy</strong>: the optimal control is simply a linear function of the current state.</p>
<p>Substituting <span class="math notranslate nohighlight">\(\mathbf{u}_t^\star\)</span> back into the cost-to-go expression and simplifying (by completing the square), we obtain:</p>
<div class="math notranslate nohighlight">
\[
J_t^\star(\mathbf{x}_t) = \frac{1}{2}\mathbf{x}_t^\top P_t \mathbf{x}_t
\]</div>
<p>where <span class="math notranslate nohighlight">\(P_t\)</span> satisfies the <strong>discrete-time Riccati equation</strong>:</p>
<div class="math notranslate nohighlight">
\[
P_t = Q_t + A_t^\top P_{t+1} A_t - A_t^\top P_{t+1} B_t (R_t + B_t^\top P_{t+1} B_t)^{-1} B_t^\top P_{t+1} A_t
\]</div>
<p>Putting everything together, the backward induction procedure under the LQR setting then becomes:</p>
<div class="proof algorithm admonition" id="lqr-backward-recursion">
<p class="admonition-title"><span class="caption-number">Algorithm 6 </span> (Backward Recursion for LQR)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> System matrices <span class="math notranslate nohighlight">\(A_t, B_t\)</span>, cost matrices <span class="math notranslate nohighlight">\(Q_t, R_t, Q_T\)</span>, time horizon <span class="math notranslate nohighlight">\(T\)</span></p>
<p><strong>Output:</strong> Cost matrices <span class="math notranslate nohighlight">\(P_t\)</span> and gain matrices <span class="math notranslate nohighlight">\(K_t\)</span> for <span class="math notranslate nohighlight">\(t = 0, \ldots, T-1\)</span></p>
<ol class="arabic">
<li><p><strong>Initialize:</strong> <span class="math notranslate nohighlight">\(P_T = Q_T\)</span></p></li>
<li><p><strong>For</strong> <span class="math notranslate nohighlight">\(t = T-1, T-2, \ldots, 0\)</span>:</p>
<ol class="arabic">
<li><p>Compute the gain matrix:</p>
<div class="math notranslate nohighlight">
\[K_t = (R_t + B_t^\top P_{t+1} B_t)^{-1} B_t^\top P_{t+1} A_t\]</div>
</li>
<li><p>Compute the cost matrix via the Riccati equation:</p>
<div class="math notranslate nohighlight">
\[P_t = Q_t + A_t^\top P_{t+1} A_t - A_t^\top P_{t+1} B_t (R_t + B_t^\top P_{t+1} B_t)^{-1} B_t^\top P_{t+1} A_t\]</div>
</li>
</ol>
</li>
<li><p><strong>End For</strong></p></li>
<li><p><strong>Return:</strong> <span class="math notranslate nohighlight">\(\{P_0, \ldots, P_T\}\)</span> and <span class="math notranslate nohighlight">\(\{K_0, \ldots, K_{T-1}\}\)</span></p></li>
</ol>
<p><strong>Optimal policy:</strong> <span class="math notranslate nohighlight">\(\mathbf{u}_t^\star = -K_t\mathbf{x}_t\)</span></p>
<p><strong>Optimal cost-to-go:</strong> <span class="math notranslate nohighlight">\(J_t^\star(\mathbf{x}_t) = \frac{1}{2}\mathbf{x}_t^\top P_t \mathbf{x}_t\)</span></p>
</section>
</div></section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="markov-decision-process-formulation">
<h1>Markov Decision Process Formulation<a class="headerlink" href="#markov-decision-process-formulation" title="Link to this heading">#</a></h1>
<p>Rather than expressing the stochasticity in our system through a disturbance term as a parameter to a deterministic difference equation, we often work with an alternative representation (more common in operations research) which uses the Markov Decision Process formulation. The idea is that when we model our system in this way with the disturbance term being drawn indepently of the previous stages, the induced trajectory are those of a Markov chain. Hence, we can re-cast our control problem in that language, leading to the so-called Markov Decision Process framework in which we express the system dynamics in terms of transition probabilities rather than explicit state equations. In this framework, we express the probability that the system is in a given state using the transition probability function:</p>
<div class="math notranslate nohighlight">
\[ p_t(\mathbf{x}_{t+1} | \mathbf{x}_t, \mathbf{u}_t) \]</div>
<p>This function gives the probability of transitioning to state <span class="math notranslate nohighlight">\(\mathbf{x}_{t+1}\)</span> at time <span class="math notranslate nohighlight">\(t+1\)</span>, given that the system is in state <span class="math notranslate nohighlight">\(\mathbf{x}_t\)</span> and action <span class="math notranslate nohighlight">\(\mathbf{u}_t\)</span> is taken at time <span class="math notranslate nohighlight">\(t\)</span>. Therefore, <span class="math notranslate nohighlight">\(p_t\)</span> specifies a conditional probability distribution over the next states: namely, the sum (for discrete state spaces) or integral over the next state should be 1.</p>
<p>Given the control theory formulation of our problem via a deterministic dynamics function and a noise term, we can derive the corresponding transition probability function through the following relationship:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
p_t(\mathbf{x}_{t+1} | \mathbf{x}_t, \mathbf{u}_t) &amp;= \mathbb{P}(\mathbf{W}_t \in \left\{\mathbf{w} \in \mathbf{W}: \mathbf{x}_{t+1} = f_t(\mathbf{x}_t, \mathbf{u}_t, \mathbf{w})\right\}) \\
&amp;= \sum_{\left\{\mathbf{w} \in \mathbf{W}: \mathbf{x}_{t+1} = f_t(\mathbf{x}_t, \mathbf{u}_t, \mathbf{w})\right\}} q_t(\mathbf{w})
\end{aligned}
\end{split}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(q_t(\mathbf{w})\)</span> represents the probability density or mass function of the disturbance <span class="math notranslate nohighlight">\(\mathbf{W}_t\)</span> (assuming discrete state spaces). When dealing with continuous spaces, the above expression simply contains an integral rather than a summation.</p>
<p>For a system with deterministic dynamics and no disturbance, the transition probabilities become much simpler and be expressed using the indicator function. Given a deterministic system with dynamics:</p>
<div class="math notranslate nohighlight">
\[ \mathbf{x}_{t+1} = f_t(\mathbf{x}_t, \mathbf{u}_t) \]</div>
<p>The transition probability function can be expressed as:</p>
<div class="math notranslate nohighlight">
\[\begin{split} p_t(\mathbf{x}_{t+1} | \mathbf{x}_t, \mathbf{u}_t) = \begin{cases}
1 &amp; \text{if } \mathbf{x}_{t+1} = f_t(\mathbf{x}_t, \mathbf{u}_t) \\
0 &amp; \text{otherwise}
\end{cases} \end{split}\]</div>
<p>With this transition probability function, we can recast our Bellman optimality equation:</p>
<div class="math notranslate nohighlight">
\[ J_t^\star(\mathbf{x}_t) = \max_{\mathbf{u}_t \in \mathbf{U}} \left\{ c_t(\mathbf{x}_t, \mathbf{u}_t) + \sum_{\mathbf{x}_{t+1}} p_t(\mathbf{x}_{t+1} | \mathbf{x}_t, \mathbf{u}_t) J_{t+1}^\star(\mathbf{x}_{t+1}) \right\} \]</div>
<p>Here, <span class="math notranslate nohighlight">\({c}(\mathbf{x}_t, \mathbf{u}_t)\)</span> represents the expected immediate reward (or negative cost) when in state <span class="math notranslate nohighlight">\(\mathbf{x}_t\)</span> and taking action <span class="math notranslate nohighlight">\(\mathbf{u}_t\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>. The summation term computes the expected optimal value for the future states, weighted by their transition probabilities.</p>
<p>This formulation offers several advantages:</p>
<ol class="arabic simple">
<li><p>It makes the Markovian nature of the problem explicit: the future state depends only on the current state and action, not on the history of states and actions.</p></li>
<li><p>For discrete-state problems, the entire system dynamics can be specified by a set of transition matrices, one for each possible action.</p></li>
<li><p>It allows us to bridge the gap with the wealth of methods in the field of probabilistic graphical models and statistical machine learning techniques for modelling and analysis.</p></li>
</ol>
<section id="notation-in-operations-reseach">
<h2>Notation in Operations Reseach<a class="headerlink" href="#notation-in-operations-reseach" title="Link to this heading">#</a></h2>
<p>The presentation above was intended to bridge the gap between the control-theoretic perspective and the world of closed-loop control through the idea of determining the value function of a parametric optimal control problem. We then saw how the backward induction procedure was applicable to both the deterministic and stochastic cases by taking the expectation over the disturbance variable. We then said that we can alternatively work with a representation of our system where instead of writing our model as a deterministic dynamics function taking a disturbance as an input, we would rather work directly via its transition probability function, which gives rise to the Markov chain interpretation of our system in simulation.</p>
<p>Now we should highlight that the notation used in control theory tends to differ from that found in operations research communities, in which the field of dynamic programming flourished. We summarize those (purely notational) differences in this section.</p>
<p>In operations research, the system state at each decision epoch is typically denoted by <span class="math notranslate nohighlight">\(s \in \mathcal{S}\)</span>, where <span class="math notranslate nohighlight">\(S\)</span> is the set of possible system states. When the system is in state <span class="math notranslate nohighlight">\(s\)</span>, the decision maker may choose an action <span class="math notranslate nohighlight">\(a\)</span> from the set of allowable actions <span class="math notranslate nohighlight">\(\mathcal{A}_s\)</span>. The union of all action sets is denoted as <span class="math notranslate nohighlight">\(\mathcal{A} = \bigcup_{s \in \mathcal{S}} \mathcal{A}_s\)</span>.</p>
<p>The dynamics of the system are described by a transition probability function <span class="math notranslate nohighlight">\(p_t(j | s, a)\)</span>, which represents the probability of transitioning to state <span class="math notranslate nohighlight">\(j \in \mathcal{S}\)</span> at time <span class="math notranslate nohighlight">\(t+1\)</span>, given that the system is in state <span class="math notranslate nohighlight">\(s\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> and action <span class="math notranslate nohighlight">\(a \in \mathcal{A}_s\)</span> is chosen. This transition probability function satisfies:</p>
<div class="math notranslate nohighlight">
\[\sum_{j \in \mathcal{S}} p_t(j | s, a) = 1\]</div>
<p>It’s worth noting that in operations research, we typically work with reward maximization rather than cost minimization, which is more common in control theory. However, we can easily switch between these perspectives by simply negating the quantity. That is, maximizing a reward function is equivalent to minimizing its negative, which we would then call a cost function.</p>
<p>The reward function is denoted by <span class="math notranslate nohighlight">\(r_t(s, a)\)</span>, representing the reward received at time <span class="math notranslate nohighlight">\(t\)</span> when the system is in state <span class="math notranslate nohighlight">\(s\)</span> and action <span class="math notranslate nohighlight">\(a\)</span> is taken. In some cases, the reward may also depend on the next state, in which case it is denoted as <span class="math notranslate nohighlight">\(r_t(s, a, j)\)</span>. The expected reward can then be computed as:</p>
<div class="math notranslate nohighlight">
\[r_t(s, a) = \sum_{j \in \mathcal{S}} r_t(s, a, j) p_t(j | s, a)\]</div>
<p>Combined together, these elemetns specify a Markov decision process, which is fully described by the tuple:</p>
<div class="math notranslate nohighlight">
\[\{T, S, \mathcal{A}_s, p_t(\cdot | s, a), r_t(s, a)\}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathrm{T}\)</span> represents the set of decision epochs (the horizon).</p>
</section>
<section id="what-is-an-optimal-policy">
<h2>What is an Optimal Policy?<a class="headerlink" href="#what-is-an-optimal-policy" title="Link to this heading">#</a></h2>
<p>Let’s go back to the starting point and define what it means for a policy to be optimal in a Markov Decision Problem. For this, we will be considering different possible search spaces (policy classes) and compare policies based on the ordering of their value from any possible start state. The value of a policy <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> (optimal or not) is defined as the expected total reward obtained by following that policy from a given starting state. Formally, for a finite-horizon MDP with <span class="math notranslate nohighlight">\(N\)</span> decision epochs, we define the value function <span class="math notranslate nohighlight">\(v^{\boldsymbol{\pi}}(s, t)\)</span> as:</p>
<div class="math notranslate nohighlight">
\[
v^{\boldsymbol{\pi}}(s, t) \triangleq \mathbb{E}\left[\sum_{k=t}^{N-1} r_t(S_k, A_k) + r_N(S_N) \mid S_t = s\right]
\]</div>
<p>where <span class="math notranslate nohighlight">\(S_t\)</span> is the state at time <span class="math notranslate nohighlight">\(t\)</span>, <span class="math notranslate nohighlight">\(A_t\)</span> is the action taken at time <span class="math notranslate nohighlight">\(t\)</span>, and <span class="math notranslate nohighlight">\(r_t\)</span> is the reward function. For simplicity, we write <span class="math notranslate nohighlight">\(v^{\boldsymbol{\pi}}(s)\)</span> to denote <span class="math notranslate nohighlight">\(v^{\boldsymbol{\pi}}(s, 1)\)</span>, the value of following policy <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> from state <span class="math notranslate nohighlight">\(s\)</span> at the first stage over the entire horizon <span class="math notranslate nohighlight">\(N\)</span>.</p>
<p>In finite-horizon MDPs, our goal is to identify an optimal policy, denoted by <span class="math notranslate nohighlight">\(\boldsymbol{\pi}^*\)</span>, that maximizes total expected reward over the horizon <span class="math notranslate nohighlight">\(N\)</span>. Specifically:</p>
<div class="math notranslate nohighlight">
\[
v^{\boldsymbol{\pi}^*}(s) \geq v^{\boldsymbol{\pi}}(s), \quad \forall s \in \mathcal{S}, \quad \forall \boldsymbol{\pi} \in \Pi^{\text{HR}}
\]</div>
<p>We call <span class="math notranslate nohighlight">\(\boldsymbol{\pi}^*\)</span> an <strong>optimal policy</strong> because it yields the highest possible value across all states and all policies within the policy class <span class="math notranslate nohighlight">\(\Pi^{\text{HR}}\)</span>. We denote by <span class="math notranslate nohighlight">\(v^*\)</span> the maximum value achievable by any policy:</p>
<div class="math notranslate nohighlight">
\[
v^*(s) = \max_{\boldsymbol{\pi} \in \Pi^{\text{HR}}} v^{\boldsymbol{\pi}}(s), \quad \forall s \in \mathcal{S}
\]</div>
<p>In reinforcement learning literature, <span class="math notranslate nohighlight">\(v^*\)</span> is typically referred to as the “optimal value function,” while in some operations research references, it might be called the “value of an MDP.” An optimal policy <span class="math notranslate nohighlight">\(\boldsymbol{\pi}^*\)</span> is one for which its value function equals the optimal value function:</p>
<div class="math notranslate nohighlight">
\[
v^{\boldsymbol{\pi}^*}(s) = v^*(s), \quad \forall s \in \mathcal{S}
\]</div>
<p>This notion of optimality applies to every state. Policies optimal in this sense are sometimes called “uniformly optimal policies.” A weaker notion of optimality, often encountered in reinforcement learning practice, is optimality with respect to an initial distribution of states. In this case, we seek a policy <span class="math notranslate nohighlight">\(\boldsymbol{\pi} \in \Pi^{\text{HR}}\)</span> that maximizes:</p>
<div class="math notranslate nohighlight">
\[
\sum_{s \in \mathcal{S}} v^{\boldsymbol{\pi}}(s) P_1(S_1 = s)
\]</div>
<p>where <span class="math notranslate nohighlight">\(P_1(S_1 = s)\)</span> is the probability of starting in state <span class="math notranslate nohighlight">\(s\)</span>.</p>
<p>The maximum value can be achieved by searching over the space of deterministic Markovian Policies. Consequently:</p>
<div class="math notranslate nohighlight">
\[ v^*(s) = \max_{\boldsymbol{\pi} \in \Pi^{\mathrm{HR}}} v^{\boldsymbol{\pi}}(s) = \max _{\boldsymbol{\pi} \in \Pi^{M D}} v^{\boldsymbol{\pi}}(s), \quad s \in S\]</div>
<p>This equality significantly simplifies the computational complexity of our algorithms, as the search problem can now be decomposed into <span class="math notranslate nohighlight">\(N\)</span> sub-problems in which we only have to search over the set of possible actions. This is the backward induction algorithm, which we present a second time, but departing this time from the control-theoretic notation and using the MDP formalism:</p>
<div class="proof algorithm admonition" id="backward-induction">
<p class="admonition-title"><span class="caption-number">Algorithm 7 </span> (Backward Induction)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> State space <span class="math notranslate nohighlight">\(S\)</span>, Action space <span class="math notranslate nohighlight">\(A\)</span>, Transition probabilities <span class="math notranslate nohighlight">\(p_t\)</span>, Reward function <span class="math notranslate nohighlight">\(r_t\)</span>, Time horizon <span class="math notranslate nohighlight">\(N\)</span></p>
<p><strong>Output:</strong> Optimal value functions <span class="math notranslate nohighlight">\(v^*\)</span></p>
<ol class="arabic">
<li><p>Initialize:</p>
<ul>
<li><p>Set <span class="math notranslate nohighlight">\(t = N\)</span></p></li>
<li><p>For all <span class="math notranslate nohighlight">\(s_N \in S\)</span>:</p>
<div class="math notranslate nohighlight">
\[v^*(s_N, N) = r_N(s_N)\]</div>
</li>
</ul>
</li>
<li><p>For <span class="math notranslate nohighlight">\(t = N-1\)</span> to <span class="math notranslate nohighlight">\(1\)</span>:</p>
<ul>
<li><p>For each <span class="math notranslate nohighlight">\(s_t \in S\)</span>:
a. Compute the optimal value function:</p>
<div class="math notranslate nohighlight">
\[v^*(s_t, t) = \max_{a \in A_{s_t}} \left\{r_t(s_t, a) + \sum_{j \in S} p_t(j | s_t, a) v^*(j, t+1)\right\}\]</div>
<p>b. Determine the set of optimal actions:</p>
<div class="math notranslate nohighlight">
\[A_{s_t,t}^* = \arg\max_{a \in A_{s_t}} \left\{r_t(s_t, a) + \sum_{j \in S} p_t(j | s_t, a) v^*(j, t+1)\right\}\]</div>
</li>
</ul>
</li>
<li><p>Return the optimal value functions <span class="math notranslate nohighlight">\(u_t^*\)</span> and optimal action sets <span class="math notranslate nohighlight">\(A_{s_t,t}^*\)</span> for all <span class="math notranslate nohighlight">\(t\)</span> and <span class="math notranslate nohighlight">\(s_t\)</span></p></li>
</ol>
</section>
</div><p>Note that the same procedure can also be used for finding the value of a policy with minor changes;</p>
<div class="proof algorithm admonition" id="backward-policy-evaluation">
<p class="admonition-title"><span class="caption-number">Algorithm 8 </span> (Policy Evaluation)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong></p>
<ul class="simple">
<li><p>State space <span class="math notranslate nohighlight">\(S\)</span></p></li>
<li><p>Action space <span class="math notranslate nohighlight">\(A\)</span></p></li>
<li><p>Transition probabilities <span class="math notranslate nohighlight">\(p_t\)</span></p></li>
<li><p>Reward function <span class="math notranslate nohighlight">\(r_t\)</span></p></li>
<li><p>Time horizon <span class="math notranslate nohighlight">\(N\)</span></p></li>
<li><p>A markovian deterministic policy <span class="math notranslate nohighlight">\(\boldsymbol{\pi} = (\pi_1, \ldots, \pi_{N-1})\)</span></p></li>
</ul>
<p><strong>Output:</strong> Value function <span class="math notranslate nohighlight">\(v^{\boldsymbol{\pi}}\)</span> for policy <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span></p>
<ol class="arabic">
<li><p>Initialize:</p>
<ul>
<li><p>Set <span class="math notranslate nohighlight">\(t = N\)</span></p></li>
<li><p>For all <span class="math notranslate nohighlight">\(s_N \in S\)</span>:</p>
<div class="math notranslate nohighlight">
\[v^{\boldsymbol{\pi}}(s_N, N) = r_N(s_N)\]</div>
</li>
</ul>
</li>
<li><p>For <span class="math notranslate nohighlight">\(t = N-1\)</span> to <span class="math notranslate nohighlight">\(1\)</span>:</p>
<ul>
<li><p>For each <span class="math notranslate nohighlight">\(s_t \in S\)</span>:
a. Compute the value function for the given policy:</p>
<div class="math notranslate nohighlight">
\[v^{\boldsymbol{\pi}}(s_t, t) = r_t(s_t, \pi_t(s_t)) + \sum_{j \in S} p_t(j | s_t, \pi_t(s_t)) v^{\boldsymbol{\pi}}(j, t+1)\]</div>
</li>
</ul>
</li>
<li><p>Return the value function <span class="math notranslate nohighlight">\(v^{\boldsymbol{\pi}}(s_t, t)\)</span> for all <span class="math notranslate nohighlight">\(t\)</span> and <span class="math notranslate nohighlight">\(s_t\)</span></p></li>
</ol>
</section>
</div><p>This code could also finally be adapted to support randomized policies using:</p>
<div class="math notranslate nohighlight">
\[v^{\boldsymbol{\pi}}(s_t, t) = \sum_{a_t \in \mathcal{A}_{s_t}} \pi_t(a_t \mid s_t) \left( r_t(s_t, a_t) + \sum_{j \in S} p_t(j | s_t, a_t) v^{\boldsymbol{\pi}}(j, t+1) \right)\]</div>
</section>
<section id="example-sample-size-determination-in-pharmaceutical-development">
<h2>Example: Sample Size Determination in Pharmaceutical Development<a class="headerlink" href="#example-sample-size-determination-in-pharmaceutical-development" title="Link to this heading">#</a></h2>
<p>Pharmaceutical development is the process of bringing a new drug from initial discovery to market availability. This process is lengthy, expensive, and risky, typically involving several stages:</p>
<ol class="arabic simple">
<li><p><strong>Drug Discovery</strong>: Identifying a compound that could potentially treat a disease.</p></li>
<li><p><strong>Preclinical Testing</strong>: Laboratory and animal testing to assess safety and efficacy.
. <strong>Clinical Trials</strong>: Testing the drug in humans, divided into phases:</p>
<ul class="simple">
<li><p>Phase I: Testing for safety in a small group of healthy volunteers.</p></li>
<li><p>Phase II: Testing for efficacy and side effects in a larger group with the target condition.</p></li>
<li><p>Phase III: Large-scale testing to confirm efficacy and monitor side effects.</p></li>
</ul>
</li>
<li><p><strong>Regulatory Review</strong>: Submitting a New Drug Application (NDA) for approval.</p></li>
<li><p><strong>Post-Market Safety Monitoring</strong>: Continuing to monitor the drug’s effects after market release.</p></li>
</ol>
<p>This process can take 10-15 years and cost over $1 billion <span id="id7">[<a class="reference internal" href="bibliography.html#id20" title="Christopher Paul Adams and Van Vu Brantner. Spending on new drug development1. Health Economics, 19(2):130–141, February 2009. URL: http://dx.doi.org/10.1002/hec.1454, doi:10.1002/hec.1454.">1</a>]</span>. The high costs and risks involved call for a principled approach to decision making. We’ll focus on the clinical trial phases and NDA approval, per the MDP model presented by <span id="id8">[<a class="reference internal" href="bibliography.html#id21" title="Mark Chang. Monte Carlo Simulation for the Pharmaceutical Industry: Concepts, Algorithms, and Case Studies. CRC Press, September 2010. ISBN 9780429152382. URL: http://dx.doi.org/10.1201/EBK1439835920, doi:10.1201/ebk1439835920.">6</a>]</span>:</p>
<ol class="arabic simple">
<li><p><strong>States</strong> (<span class="math notranslate nohighlight">\(S\)</span>): Our state space is <span class="math notranslate nohighlight">\(S = \{s_1, s_2, s_3, s_4\}\)</span>, where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(s_1\)</span>: Phase I clinical trial</p></li>
<li><p><span class="math notranslate nohighlight">\(s_2\)</span>: Phase II clinical trial</p></li>
<li><p><span class="math notranslate nohighlight">\(s_3\)</span>: Phase III clinical trial</p></li>
<li><p><span class="math notranslate nohighlight">\(s_4\)</span>: NDA approval</p></li>
</ul>
</li>
<li><p><strong>Actions</strong> (<span class="math notranslate nohighlight">\(A\)</span>): At each state, the action is choosing the sample size <span class="math notranslate nohighlight">\(n_i\)</span> for the corresponding clinical trial. The action space is <span class="math notranslate nohighlight">\(A = \{10, 11, ..., 1000\}\)</span>, representing possible sample sizes.</p></li>
<li><p><strong>Transition Probabilities</strong> (<span class="math notranslate nohighlight">\(P\)</span>): The probability of moving from one state to the next depends on the chosen sample size and the inherent properties of the drug.
We define:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(s_2|s_1, n_1) = p_{12}(n_1) = \sum_{i=0}^{\lfloor\eta_1 n_1\rfloor} \binom{n_1}{i} p_0^i (1-p_0)^{n_1-i}\)</span>
where <span class="math notranslate nohighlight">\(p_0\)</span> is the true toxicity rate and <span class="math notranslate nohighlight">\(\eta_1\)</span> is the toxicity threshold for Phase I.</p></li>
</ul>
</li>
</ol>
<ul>
<li><p>Of particular interest is the transition from Phase II to Phase III which we model as:</p>
<p><span class="math notranslate nohighlight">\(P(s_3|s_2, n_2) = p_{23}(n_2) = \Phi\left(\frac{\sqrt{n_2}}{2}\delta - z_{1-\eta_2}\right)\)</span></p>
<p>where <span class="math notranslate nohighlight">\(\Phi\)</span> is the cumulative distribution function (CDF) of the standard normal distribution:</p>
<p><span class="math notranslate nohighlight">\(\Phi(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^x e^{-t^2/2} dt\)</span></p>
<p>This is giving us the probability that we would observe a treatment effect this large or larger if the null hypothesis (no treatment effect) were true. A higher probability indicates stronger evidence of a treatment effect, making it more likely that the drug will progress to Phase III.</p>
<p>In this expression, <span class="math notranslate nohighlight">\(\delta\)</span> is called the “normalized treatment effect”. In clinical trials, we’re often interested in the difference between the treatment and control groups. The “normalized” part means we’ve adjusted this difference for the variability in the data. Specifically <span class="math notranslate nohighlight">\(\delta = \frac{\mu_t - \mu_c}{\sigma}\)</span> where <span class="math notranslate nohighlight">\(\mu_t\)</span> is the mean outcome in the treatment group, <span class="math notranslate nohighlight">\(\mu_c\)</span> is the mean outcome in the control group, and <span class="math notranslate nohighlight">\(\sigma\)</span> is the standard deviation of the outcome. A larger <span class="math notranslate nohighlight">\(\delta\)</span> indicates a stronger treatment effect.</p>
<p>Furthermore, the term <span class="math notranslate nohighlight">\(z_{1-\eta_2}\)</span> is the <span class="math notranslate nohighlight">\((1-\eta_2)\)</span>-quantile of the standard normal distribution. In other words, it’s the value where the probability of a standard normal random variable being greater than this value is <span class="math notranslate nohighlight">\(\eta_2\)</span>. For example, if <span class="math notranslate nohighlight">\(\eta_2 = 0.05\)</span>, then <span class="math notranslate nohighlight">\(z_{1-\eta_2} \approx 1.645\)</span>. A smaller <span class="math notranslate nohighlight">\(\eta_2\)</span> makes the trial more conservative, requiring stronger evidence to proceed to Phase III.</p>
<p>Finally, <span class="math notranslate nohighlight">\(n_2\)</span> is the sample size for Phase II. The <span class="math notranslate nohighlight">\(\sqrt{n_2}\)</span> term reflects that the precision of our estimate of the treatment effect improves with the square root of the sample size.</p>
</li>
<li><p><span class="math notranslate nohighlight">\(P(s_4|s_3, n_3) = p_{34}(n_3) = \Phi\left(\frac{\sqrt{n_3}}{2}\delta - z_{1-\eta_3}\right)\)</span>
where <span class="math notranslate nohighlight">\(\eta_3\)</span> is the significance level for Phase III.</p></li>
</ul>
<ol class="arabic simple" start="4">
<li><p><strong>Rewards</strong> (<span class="math notranslate nohighlight">\(R\)</span>): The reward function captures the costs of running trials and the potential profit from a successful drug:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(r(s_i, n_i) = -c_i(n_i)\)</span> for <span class="math notranslate nohighlight">\(i = 1, 2, 3\)</span>, where <span class="math notranslate nohighlight">\(c_i(n_i)\)</span> is the cost of running a trial with sample size <span class="math notranslate nohighlight">\(n_i\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(r(s_4) = g_4\)</span>, where <span class="math notranslate nohighlight">\(g_4\)</span> is the expected profit from a successful drug.</p></li>
</ul>
</li>
<li><p><strong>Discount Factor</strong> (<span class="math notranslate nohighlight">\(\gamma\)</span>): We use a discount factor <span class="math notranslate nohighlight">\(0 &lt; \gamma \leq 1\)</span> to account for the time value of money and risk preferences.</p></li>
</ol>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">binom</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="k">def</span> <span class="nf">binomial_pmf</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">transition_prob_phase1</span><span class="p">(</span><span class="n">n1</span><span class="p">,</span> <span class="n">eta1</span><span class="p">,</span> <span class="n">p0</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">([</span><span class="n">binomial_pmf</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">n1</span><span class="p">,</span> <span class="n">p0</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">eta1</span> <span class="o">*</span> <span class="n">n1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)])</span>

<span class="k">def</span> <span class="nf">transition_prob_phase2</span><span class="p">(</span><span class="n">n2</span><span class="p">,</span> <span class="n">eta2</span><span class="p">,</span> <span class="n">delta</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n2</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">delta</span> <span class="o">-</span> <span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">eta2</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">transition_prob_phase3</span><span class="p">(</span><span class="n">n3</span><span class="p">,</span> <span class="n">eta3</span><span class="p">,</span> <span class="n">delta</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n3</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">delta</span> <span class="o">-</span> <span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">eta3</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">immediate_reward</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">n</span>  <span class="c1"># Negative to represent cost</span>

<span class="k">def</span> <span class="nf">backward_induction</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">g4</span><span class="p">,</span> <span class="n">p0</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">eta1</span><span class="p">,</span> <span class="n">eta2</span><span class="p">,</span> <span class="n">eta3</span><span class="p">):</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">S</span><span class="p">))</span>
    <span class="n">V</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">g4</span>  <span class="c1"># Value for NDA approval state</span>
    <span class="n">optimal_n</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span>  <span class="c1"># Store optimal n for each phase</span>

    <span class="c1"># Backward induction</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>  <span class="c1"># Iterate backwards from Phase III to Phase I</span>
        <span class="n">max_value</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">A</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Phase I</span>
                <span class="n">p</span> <span class="o">=</span> <span class="n">transition_prob_phase1</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">eta1</span><span class="p">,</span> <span class="n">p0</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># Phase II</span>
                <span class="n">p</span> <span class="o">=</span> <span class="n">transition_prob_phase2</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">eta2</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>  <span class="c1"># Phase III</span>
                <span class="n">p</span> <span class="o">=</span> <span class="n">transition_prob_phase3</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">eta3</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">immediate_reward</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">p</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">value</span> <span class="o">&gt;</span> <span class="n">max_value</span><span class="p">:</span>
                <span class="n">max_value</span> <span class="o">=</span> <span class="n">value</span>
                <span class="n">optimal_n</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">n</span>
        <span class="n">V</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_value</span>

    <span class="k">return</span> <span class="n">V</span><span class="p">,</span> <span class="n">optimal_n</span>

<span class="c1"># Set up the problem parameters</span>
<span class="n">S</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Phase I&#39;</span><span class="p">,</span> <span class="s1">&#39;Phase II&#39;</span><span class="p">,</span> <span class="s1">&#39;Phase III&#39;</span><span class="p">,</span> <span class="s1">&#39;NDA approval&#39;</span><span class="p">]</span>
<span class="n">A</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1001</span><span class="p">)</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.95</span>
<span class="n">g4</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">p0</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># Example toxicity rate for Phase I</span>
<span class="n">delta</span> <span class="o">=</span> <span class="mf">0.5</span>  <span class="c1"># Example normalized treatment difference</span>
<span class="n">eta1</span><span class="p">,</span> <span class="n">eta2</span><span class="p">,</span> <span class="n">eta3</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.025</span>

<span class="c1"># Run the backward induction algorithm</span>
<span class="n">V</span><span class="p">,</span> <span class="n">optimal_n</span> <span class="o">=</span> <span class="n">backward_induction</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">g4</span><span class="p">,</span> <span class="n">p0</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">eta1</span><span class="p">,</span> <span class="n">eta2</span><span class="p">,</span> <span class="n">eta3</span><span class="p">)</span>

<span class="c1"># Print results</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">state</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">S</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Value for </span><span class="si">{</span><span class="n">state</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">V</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Optimal sample sizes: Phase I: </span><span class="si">{</span><span class="n">optimal_n</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">, Phase II: </span><span class="si">{</span><span class="n">optimal_n</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">, Phase III: </span><span class="si">{</span><span class="n">optimal_n</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Sanity checks</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Sanity checks:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;1. NDA approval value: </span><span class="si">{</span><span class="n">V</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;2. All values non-positive and &lt;= NDA value: </span><span class="si">{</span><span class="nb">all</span><span class="p">(</span><span class="n">v</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">V</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">v</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">V</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;3. Optimal sample sizes in range: </span><span class="si">{</span><span class="nb">all</span><span class="p">(</span><span class="mi">10</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="mi">1000</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">optimal_n</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="ow">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="kc">None</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Value for Phase I: 7869.92
Value for Phase II: 8385.83
Value for Phase III: 9123.40
Value for NDA approval: 10000.00
Optimal sample sizes: Phase I: 75, Phase II: 239, Phase III: 326

Sanity checks:
1. NDA approval value: 10000.0
2. All values non-positive and &lt;= NDA value: True
3. Optimal sample sizes in range: True
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="infinite-horizon-mdps">
<h1>Infinite-Horizon MDPs<a class="headerlink" href="#infinite-horizon-mdps" title="Link to this heading">#</a></h1>
<p>It often makes sense to model control problems over infinite horizons. We extend the previous setting and define the expected total reward of policy <span class="math notranslate nohighlight">\(\boldsymbol{\pi} \in \Pi^{\mathrm{HR}}\)</span>, <span class="math notranslate nohighlight">\(v^{\boldsymbol{\pi}}\)</span> as:</p>
<div class="math notranslate nohighlight">
\[
v^{\boldsymbol{\pi}}(s) = \mathbb{E}\left[\sum_{t=1}^{\infty} r(S_t, A_t)\right]
\]</div>
<p>One drawback of this model is that we could easily encounter values that are <span class="math notranslate nohighlight">\(+\infty\)</span> or <span class="math notranslate nohighlight">\(-\infty\)</span>, even in a setting as simple as a single-state MDP which loops back into itself and where the accrued reward is nonzero.</p>
<p>Therefore, it is often more convenient to work with an alternative formulation which guarantees the existence of a limit: the expected total discounted reward of policy <span class="math notranslate nohighlight">\(\boldsymbol{\pi} \in \Pi^{\mathrm{HR}}\)</span> is defined to be:</p>
<div class="math notranslate nohighlight">
\[
v_\gamma^{\boldsymbol{\pi}}(s) \equiv \lim_{N \rightarrow \infty} \mathbb{E}\left[\sum_{t=1}^N \gamma^{t-1} r(S_t, A_t)\right]
\]</div>
<p>for <span class="math notranslate nohighlight">\(0 \leq \gamma &lt; 1\)</span> and when <span class="math notranslate nohighlight">\(\max_{s \in \mathcal{S}} \max_{a \in \mathcal{A}_s}|r(s, a)| = R_{\max} &lt; \infty\)</span>, in which case, <span class="math notranslate nohighlight">\(|v_\gamma^{\boldsymbol{\pi}}(s)| \leq (1-\gamma)^{-1} R_{\max}\)</span>.</p>
<p>Finally, another possibility for the infinite-horizon setting is the so-called average reward or gain of policy <span class="math notranslate nohighlight">\(\boldsymbol{\pi} \in \Pi^{\mathrm{HR}}\)</span> defined as:</p>
<div class="math notranslate nohighlight">
\[
g^{\boldsymbol{\pi}}(s) \equiv \lim_{N \rightarrow \infty} \frac{1}{N} \mathbb{E}\left[\sum_{t=1}^N r(S_t, A_t)\right]
\]</div>
<p>We won’t be working with this formulation in this course due to its inherent practical and theoretical complexities.</p>
<p>Extending the previous notion of optimality from finite-horizon models, a policy <span class="math notranslate nohighlight">\(\boldsymbol{\pi}^*\)</span> is said to be discount optimal for a given <span class="math notranslate nohighlight">\(\gamma\)</span> if:</p>
<div class="math notranslate nohighlight">
\[
v_\gamma^{\boldsymbol{\pi}^*}(s) \geq v_\gamma^{\boldsymbol{\pi}}(s) \quad \text { for each } s \in S \text { and all } \boldsymbol{\pi} \in \Pi^{\mathrm{HR}}
\]</div>
<p>Furthermore, the value of a discounted MDP <span class="math notranslate nohighlight">\(v_\gamma^*(s)\)</span>, is defined by:</p>
<div class="math notranslate nohighlight">
\[
v_\gamma^*(s) \equiv \max _{\boldsymbol{\pi} \in \Pi^{\mathrm{HR}}} v_\gamma^{\boldsymbol{\pi}}(s)
\]</div>
<p>More often, we refer to <span class="math notranslate nohighlight">\(v_\gamma\)</span> by simply calling it the optimal value function.</p>
<p>As for the finite-horizon setting, the infinite horizon discounted model does not require history-dependent policies, since for any <span class="math notranslate nohighlight">\(\boldsymbol{\pi} \in \Pi^{HR}\)</span> there exists a <span class="math notranslate nohighlight">\(\boldsymbol{\pi}^{\prime} \in \Pi^{MR}\)</span> with identical total discounted reward:
$<span class="math notranslate nohighlight">\(
v_\gamma^*(s) \equiv \max_{\boldsymbol{\pi} \in \Pi^{HR}} v_\gamma^{\boldsymbol{\pi}}(s)=\max_{\boldsymbol{\pi} \in \Pi^{MR}} v_\gamma^{\boldsymbol{\pi}}(s) .
\)</span>$</p>
<section id="random-horizon-interpretation-of-discounting">
<h2>Random Horizon Interpretation of Discounting<a class="headerlink" href="#random-horizon-interpretation-of-discounting" title="Link to this heading">#</a></h2>
<p>The use of discounting can be motivated both from a modeling perspective and as a means to ensure that the total reward remains bounded. From the modeling perspective, we can view discounting as a way to weight more or less importance on the immediate rewards vs. the long-term consequences. There is also another interpretation which stems from that of a finite horizon model but with an uncertain end time. More precisely:</p>
<p>Let <span class="math notranslate nohighlight">\(v_\nu^{\boldsymbol{\pi}}(s)\)</span> denote the expected total reward obtained by using policy <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> when the horizon length <span class="math notranslate nohighlight">\(\nu\)</span> is random. We define it by:</p>
<div class="math notranslate nohighlight">
\[
v_\nu^{\boldsymbol{\pi}}(s) \equiv \mathbb{E}_s^{\boldsymbol{\pi}}\left[\mathbb{E}_\nu\left\{\sum_{t=1}^\nu r(S_t, A_t)\right\}\right]
\]</div>
<div class="proof theorem admonition" id="prop-5-3-1">
<p class="admonition-title"><span class="caption-number">Theorem 5 </span> (Random horizon interpretation of discounting)</p>
<section class="theorem-content" id="proof-content">
<p>Suppose that the horizon <span class="math notranslate nohighlight">\(\nu\)</span> follows a geometric distribution with parameter <span class="math notranslate nohighlight">\(\gamma\)</span>, <span class="math notranslate nohighlight">\(0 \leq \gamma &lt; 1\)</span>, independent of the policy such that
<span class="math notranslate nohighlight">\(P(\nu=n) = (1-\gamma) \gamma^{n-1}, \, n=1,2, \ldots\)</span>, then <span class="math notranslate nohighlight">\(v_\nu^{\boldsymbol{\pi}}(s) = v_\gamma^{\boldsymbol{\pi}}(s)\)</span> for all <span class="math notranslate nohighlight">\(s \in \mathcal{S}\)</span> .</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. See proposition 5.3.1 in <span id="id9">[<a class="reference internal" href="bibliography.html#id23" title="Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley &amp; Sons, New York, 1994. ISBN 978-0-471-61977-3. First published in 1994.">34</a>]</span>.</p>
<p>By definition of the finite-horizon value function and the law of total expectation:</p>
<div class="math notranslate nohighlight">
\[
v_\nu^{\boldsymbol{\pi}}(s) = \sum_{n=1}^{\infty} P(\nu=n) \cdot v_n^{\boldsymbol{\pi}}(s) = \sum_{n=1}^{\infty} (1-\gamma) \gamma^{n-1} \cdot E_s^{\boldsymbol{\pi}} \left\{\sum_{t=1}^n r(S_t, A_t)\right\}.
\]</div>
<p>Combining the expectation with the sum over <span class="math notranslate nohighlight">\(n\)</span>:</p>
<div class="math notranslate nohighlight">
\[
v_\nu^{\boldsymbol{\pi}}(s) = E_s^{\boldsymbol{\pi}} \left\{\sum_{n=1}^{\infty} (1-\gamma) \gamma^{n-1} \sum_{t=1}^n r(S_t, A_t)\right\}.
\]</div>
<p><strong>Reordering the summations:</strong> Under the bounded reward assumption <span class="math notranslate nohighlight">\(|r(s,a)| \leq R_{\max}\)</span> and <span class="math notranslate nohighlight">\(\gamma &lt; 1\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
E_s^{\boldsymbol{\pi}} \left\{\sum_{n=1}^{\infty} \sum_{t=1}^n |r(S_t, A_t)| \cdot (1-\gamma) \gamma^{n-1}\right\} \leq R_{\max} \sum_{n=1}^{\infty} n (1-\gamma) \gamma^{n-1} = \frac{R_{\max}}{1-\gamma} &lt; \infty,
\]</div>
<p>which justifies exchanging the order of summation by Fubini’s theorem.</p>
<p>To reverse the order, note that the pair <span class="math notranslate nohighlight">\((n,t)\)</span> with <span class="math notranslate nohighlight">\(1 \leq t \leq n\)</span> can be reindexed by fixing <span class="math notranslate nohighlight">\(t\)</span> first and letting <span class="math notranslate nohighlight">\(n\)</span> range from <span class="math notranslate nohighlight">\(t\)</span> to <span class="math notranslate nohighlight">\(\infty\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\sum_{n=1}^{\infty} \sum_{t=1}^n = \sum_{t=1}^{\infty} \sum_{n=t}^{\infty}.
\]</div>
<p>Therefore:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
v_\nu^{\boldsymbol{\pi}}(s) &amp;= E_s^{\boldsymbol{\pi}} \left\{\sum_{t=1}^{\infty} r(S_t, A_t) \sum_{n=t}^{\infty} (1-\gamma) \gamma^{n-1}\right\}.
\end{align*}\]</div>
<p><strong>Evaluating the inner sum:</strong> Using the substitution <span class="math notranslate nohighlight">\(m = n - t + 1\)</span> (so <span class="math notranslate nohighlight">\(n = m + t - 1\)</span>):</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\sum_{n=t}^{\infty} (1-\gamma) \gamma^{n-1} &amp;= \sum_{m=1}^{\infty} (1-\gamma) \gamma^{m+t-2} \\
&amp;= \gamma^{t-1} (1-\gamma) \sum_{m=1}^{\infty} \gamma^{m-1} \\
&amp;= \gamma^{t-1} (1-\gamma) \cdot \frac{1}{1-\gamma} = \gamma^{t-1}.
\end{align*}\]</div>
<p>Substituting back:</p>
<div class="math notranslate nohighlight">
\[
v_\nu^{\boldsymbol{\pi}}(s) = E_s^{\boldsymbol{\pi}} \left\{\sum_{t=1}^{\infty} \gamma^{t-1} r(S_t, A_t)\right\} = v_\gamma^{\boldsymbol{\pi}}(s).
\]</div>
</div>
</section>
<section id="vector-representation-in-markov-decision-processes">
<h2>Vector Representation in Markov Decision Processes<a class="headerlink" href="#vector-representation-in-markov-decision-processes" title="Link to this heading">#</a></h2>
<p>Let V be the set of bounded real-valued functions on a discrete state space S. This means any function <span class="math notranslate nohighlight">\( f \in V \)</span> satisfies the condition:</p>
<div class="math notranslate nohighlight">
\[
\|f\| = \max_{s \in S} |f(s)| &lt; \infty.
\]</div>
<p>where notation <span class="math notranslate nohighlight">\( \|f\| \)</span> represents the sup-norm (or <span class="math notranslate nohighlight">\( \ell_\infty \)</span>-norm) of the function <span class="math notranslate nohighlight">\( f \)</span>.</p>
<p>When working with discrete state spaces, we can interpret elements of V as vectors and linear operators on V as matrices, allowing us to leverage tools from linear algebra. The sup-norm (<span class="math notranslate nohighlight">\(\ell_\infty\)</span> norm) of matrix <span class="math notranslate nohighlight">\(\mathbf{H}\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[
\|\mathbf{H}\| \equiv \max_{s \in S} \sum_{j \in S} |\mathbf{H}_{s,j}|
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{H}_{s,j}\)</span> represents the <span class="math notranslate nohighlight">\((s, j)\)</span>-th component of the matrix <span class="math notranslate nohighlight">\(\mathbf{H}\)</span>.</p>
<p>For a Markovian decision rule <span class="math notranslate nohighlight">\(\pi \in \Pi^{MD}\)</span>, we define:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{r}_\pi(s) &amp;\equiv r(s, \pi(s)), \quad \mathbf{r}_\pi \in \mathbb{R}^{|S|}, \\
[\mathbf{P}_\pi]_{s,j} &amp;\equiv p(j \mid s, \pi(s)), \quad \mathbf{P}_\pi \in \mathbb{R}^{|S| \times |S|}.
\end{align*}\]</div>
<p>For a randomized decision rule <span class="math notranslate nohighlight">\(\pi \in \Pi^{MR}\)</span>, these definitions extend to:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{r}_\pi(s) &amp;\equiv \sum_{a \in A_s} \pi(a \mid s) \, r(s, a), \\
[\mathbf{P}_\pi]_{s,j} &amp;\equiv \sum_{a \in A_s} \pi(a \mid s) \, p(j \mid s, a).
\end{align*}\]</div>
<p>In both cases, <span class="math notranslate nohighlight">\(\mathbf{r}_\pi\)</span> denotes a reward vector in <span class="math notranslate nohighlight">\(\mathbb{R}^{|S|}\)</span>, with each component <span class="math notranslate nohighlight">\(\mathbf{r}_\pi(s)\)</span> representing the reward associated with state <span class="math notranslate nohighlight">\(s\)</span>. Similarly, <span class="math notranslate nohighlight">\(\mathbf{P}_\pi\)</span> is a transition probability matrix in <span class="math notranslate nohighlight">\(\mathbb{R}^{|S| \times |S|}\)</span>, capturing the transition probabilities under decision rule <span class="math notranslate nohighlight">\(\pi\)</span>.</p>
<p>For a nonstationary Markovian policy <span class="math notranslate nohighlight">\(\boldsymbol{\pi} = (\pi_1, \pi_2, \ldots) \in \Pi^{MR}\)</span>, the expected total discounted reward is given by:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{v}_\gamma^{\boldsymbol{\pi}}(s)=\mathbb{E}\left[\sum_{t=1}^{\infty} \gamma^{t-1} r\left(S_t, A_t\right) \,\middle|\, S_1 = s\right].
\]</div>
<p>Using vector notation, this can be expressed as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{v}_\gamma^{\boldsymbol{\pi}} &amp;= \sum_{t=1}^{\infty} \gamma^{t-1} \mathbf{P}_{\boldsymbol{\pi}}^{t-1} \mathbf{r}_{\pi_1} \\
&amp;= \mathbf{r}_{\pi_1} + \gamma \mathbf{P}_{\pi_1} \mathbf{r}_{\pi_2} + \gamma^2 \mathbf{P}_{\pi_1} \mathbf{P}_{\pi_2} \mathbf{r}_{\pi_3} + \cdots \\
&amp;= \mathbf{r}_{\pi_1} + \gamma \mathbf{P}_{\pi_1} \left( \mathbf{r}_{\pi_2} + \gamma \mathbf{P}_{\pi_2} \mathbf{r}_{\pi_3} + \gamma^2 \mathbf{P}_{\pi_2} \mathbf{P}_{\pi_3} \mathbf{r}_{\pi_4} + \cdots \right).
\end{aligned}
\end{split}\]</div>
<p>This formulation leads to a recursive relationship:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathbf{v}_\gamma^{\boldsymbol{\pi}} &amp;= \mathbf{r}_{\pi_1} + \gamma \mathbf{P}_{\pi_1} \mathbf{v}_\gamma^{\boldsymbol{\pi}^{\prime}}\\
&amp;=\sum_{t=1}^{\infty} \gamma^{t-1} \mathbf{P}_{\boldsymbol{\pi}}^{t-1} \mathbf{r}_{\pi_t}
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\pi}^{\prime} = (\pi_2, \pi_3, \ldots)\)</span>.</p>
<p>For a stationary policy <span class="math notranslate nohighlight">\(\boldsymbol{\pi} = \mathrm{const}(\pi)\)</span> with constant decision rule <span class="math notranslate nohighlight">\(\pi\)</span>, the total expected reward simplifies to:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathbf{v}_\gamma^{\pi} &amp;= \mathbf{r}_\pi+ \gamma \mathbf{P}_\pi \mathbf{v}_\gamma^{\pi} \\
&amp;=\sum_{t=1}^{\infty} \gamma^{t-1} \mathbf{P}_\pi^{t-1} \mathbf{r}_{\pi}
\end{align*}
\end{split}\]</div>
<p>This last expression is called a Neumann series expansion, and it’s guaranteed to exists under the assumptions of bounded reward and discount factor strictly less than one.</p>
<div class="proof theorem admonition" id="neumann-series">
<p class="admonition-title"><span class="caption-number">Theorem 6 </span> (Neumann Series and Invertibility)</p>
<section class="theorem-content" id="proof-content">
<p>The <strong>spectral radius</strong> of a matrix <span class="math notranslate nohighlight">\(\mathbf{H}\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[
\rho(\mathbf{H}) \equiv \max_{i} |\lambda_i(\mathbf{H})|
\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda_i(\mathbf{H})\)</span> are the eigenvalues of <span class="math notranslate nohighlight">\(\mathbf{H}\)</span>.</p>
<p><strong>Neumann Series Existence:</strong> For any matrix <span class="math notranslate nohighlight">\(\mathbf{H}\)</span>, the Neumann series</p>
<div class="math notranslate nohighlight">
\[
\sum_{t=0}^{\infty} \mathbf{H}^t = \mathbf{I} + \mathbf{H} + \mathbf{H}^2 + \cdots
\]</div>
<p>converges if and only if <span class="math notranslate nohighlight">\(\rho(\mathbf{H}) &lt; 1\)</span>. When this condition holds, the matrix <span class="math notranslate nohighlight">\((\mathbf{I} - \mathbf{H})\)</span> is invertible and</p>
<div class="math notranslate nohighlight">
\[
(\mathbf{I} - \mathbf{H})^{-1} = \sum_{t=0}^{\infty} \mathbf{H}^t.
\]</div>
</section>
</div><p>Note that for any induced matrix norm <span class="math notranslate nohighlight">\(\|\cdot\|\)</span> (i.e., a norm satisfying <span class="math notranslate nohighlight">\(\|\mathbf{H}\mathbf{v}\| \leq \|\mathbf{H}\| \cdot \|\mathbf{v}\|\)</span> for all vectors <span class="math notranslate nohighlight">\(\mathbf{v}\)</span>) and any matrix <span class="math notranslate nohighlight">\(\mathbf{H}\)</span>, the spectral radius is bounded by:</p>
<div class="math notranslate nohighlight">
\[
\rho(\mathbf{H}) \leq \|\mathbf{H}\|.
\]</div>
<p>This inequality provides a practical way to verify the convergence condition <span class="math notranslate nohighlight">\(\rho(\mathbf{H}) &lt; 1\)</span> by checking the simpler condition <span class="math notranslate nohighlight">\(\|\mathbf{H}\| &lt; 1\)</span> rather than trying to compute the eigenvalues directly.</p>
<p>We can now verify that <span class="math notranslate nohighlight">\((\mathbf{I} - \gamma \mathbf{P}_d)\)</span> is invertible and the Neumann series converges.</p>
<ol class="arabic">
<li><p><strong>Norm of the transition matrix:</strong> Since <span class="math notranslate nohighlight">\(\mathbf{P}_d\)</span> is a stochastic matrix (each row sums to 1 and all entries are non-negative), its <span class="math notranslate nohighlight">\(\ell_\infty\)</span>-norm is:</p>
<div class="math notranslate nohighlight">
\[
   \|\mathbf{P}_d\| = \max_{s \in S} \sum_{j \in S} [\mathbf{P}_d]_{s,j} = \max_{s \in S} 1 = 1.
   \]</div>
</li>
<li><p><strong>Norm of the scaled matrix:</strong> Using the homogeneity property of norms, we have:</p>
<div class="math notranslate nohighlight">
\[
   \|\gamma \mathbf{P}_d\| = |\gamma| \cdot \|\mathbf{P}_d\| = |\gamma| \cdot 1 = |\gamma|.
   \]</div>
</li>
<li><p><strong>Bounding the spectral radius:</strong> Since the spectral radius is bounded by the matrix norm:</p>
<div class="math notranslate nohighlight">
\[
   \rho(\gamma \mathbf{P}_d) \leq \|\gamma \mathbf{P}_d\| = |\gamma|.
   \]</div>
</li>
<li><p><strong>Verifying convergence:</strong> Since <span class="math notranslate nohighlight">\(0 \leq \gamma &lt; 1\)</span> by assumption, we have:</p>
<div class="math notranslate nohighlight">
\[
   \rho(\gamma \mathbf{P}_d) \leq |\gamma| &lt; 1.
   \]</div>
<p>This strict inequality guarantees that <span class="math notranslate nohighlight">\((\mathbf{I} - \gamma \mathbf{P}_d)\)</span> is invertible and the Neumann series converges.</p>
</li>
</ol>
<p>Therefore, the Neumann series expansion converges and yields:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{v}_\gamma^{d^\infty} = (\mathbf{I} - \gamma \mathbf{P}_d)^{-1} \mathbf{r}_d = \sum_{t=0}^{\infty} (\gamma \mathbf{P}_d)^t \mathbf{r}_d = \sum_{t=1}^{\infty} \gamma^{t-1} \mathbf{P}_d^{t-1} \mathbf{r}_d.
\]</div>
<p>Consequently, for a stationary policy, <span class="math notranslate nohighlight">\(\mathbf{v}_\gamma^{d^\infty}\)</span> can be determined as the solution to the linear equation:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{v} = \mathbf{r}_d+ \gamma \mathbf{P}_d\mathbf{v},
\]</div>
<p>which can be rearranged to:</p>
<div class="math notranslate nohighlight">
\[
(\mathbf{I} - \gamma \mathbf{P}_d) \mathbf{v} = \mathbf{r}_d.
\]</div>
<p>We can also characterize <span class="math notranslate nohighlight">\(\mathbf{v}_\gamma^{d^\infty}\)</span> as the solution to an operator equation. More specifically, define the transformation <span class="math notranslate nohighlight">\(\mathrm{L}_d\)</span> by</p>
<div class="math notranslate nohighlight">
\[
\mathrm{L}_d \mathbf{v} \equiv \mathbf{r}_d+\gamma \mathbf{P}_d\mathbf{v}
\]</div>
<p>for any <span class="math notranslate nohighlight">\(\mathbf{v} \in V\)</span>. Intuitively, <span class="math notranslate nohighlight">\(\mathrm{L}_d\)</span> takes a value function <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> as input and returns a new value function that combines immediate rewards (<span class="math notranslate nohighlight">\(\mathbf{r}_d\)</span>) with discounted future values (<span class="math notranslate nohighlight">\(\gamma \mathbf{P}_d\mathbf{v}\)</span>).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>While we often refer to <span class="math notranslate nohighlight">\(\mathrm{L}_d\)</span> as a “linear operator” in the RL literature, it is technically an <strong>affine operator</strong> (or affine transformation), not a linear operator in the strict sense. To see why, recall that a linear operator <span class="math notranslate nohighlight">\(\mathcal{T}\)</span> must satisfy:</p>
<ol class="arabic simple">
<li><p><strong>Additivity:</strong> <span class="math notranslate nohighlight">\(\mathcal{T}(\mathbf{v}_1 + \mathbf{v}_2) = \mathcal{T}(\mathbf{v}_1) + \mathcal{T}(\mathbf{v}_2)\)</span></p></li>
<li><p><strong>Homogeneity:</strong> <span class="math notranslate nohighlight">\(\mathcal{T}(\alpha \mathbf{v}) = \alpha \mathcal{T}(\mathbf{v})\)</span> for all scalars <span class="math notranslate nohighlight">\(\alpha\)</span></p></li>
</ol>
<p>However, <span class="math notranslate nohighlight">\(\mathrm{L}_d\)</span> fails the additivity test:</p>
<div class="math notranslate nohighlight">
\[
\mathrm{L}_d(\mathbf{v}_1 + \mathbf{v}_2) = \mathbf{r}_d + \gamma \mathbf{P}_d(\mathbf{v}_1 + \mathbf{v}_2) = \mathbf{r}_d + \gamma \mathbf{P}_d\mathbf{v}_1 + \gamma \mathbf{P}_d\mathbf{v}_2
\]</div>
<p>while</p>
<div class="math notranslate nohighlight">
\[
\mathrm{L}_d(\mathbf{v}_1) + \mathrm{L}_d(\mathbf{v}_2) = (\mathbf{r}_d + \gamma \mathbf{P}_d\mathbf{v}_1) + (\mathbf{r}_d + \gamma \mathbf{P}_d\mathbf{v}_2) = 2\mathbf{r}_d + \gamma \mathbf{P}_d\mathbf{v}_1 + \gamma \mathbf{P}_d\mathbf{v}_2.
\]</div>
<p>The presence of the constant term <span class="math notranslate nohighlight">\(\mathbf{r}_d\)</span> makes <span class="math notranslate nohighlight">\(\mathrm{L}_d\)</span> affine rather than linear. An affine operator has the form <span class="math notranslate nohighlight">\(\mathcal{A}(\mathbf{v}) = \mathbf{b} + \mathcal{T}(\mathbf{v})\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> is a constant vector and <span class="math notranslate nohighlight">\(\mathcal{T}\)</span> is a linear operator. In our case, <span class="math notranslate nohighlight">\(\mathbf{b} = \mathbf{r}_d\)</span> and <span class="math notranslate nohighlight">\(\mathcal{T}(\mathbf{v}) = \gamma \mathbf{P}_d\mathbf{v}\)</span>.</p>
<p>Despite this technical distinction, the term “linear operator” is commonly used in the reinforcement learning literature when referring to <span class="math notranslate nohighlight">\(\mathrm{L}_d\)</span>, following a slight abuse of terminology.</p>
</div>
<p>Therefore, we view <span class="math notranslate nohighlight">\(\mathrm{L}_d\)</span> as an operator mapping elements of <span class="math notranslate nohighlight">\(V\)</span> to <span class="math notranslate nohighlight">\(V\)</span>: i.e., <span class="math notranslate nohighlight">\(\mathrm{L}_d: V \rightarrow V\)</span>. The fact that the value function of a policy is the solution to a fixed-point equation can then be expressed with the statement:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{v}_\gamma^{d^\infty}=\mathrm{L}_d \mathbf{v}_\gamma^{d^\infty}.
\]</div>
<p>This is a <strong>fixed-point equation</strong>: the value function <span class="math notranslate nohighlight">\(\mathbf{v}_\gamma^{d^\infty}\)</span> is a fixed point of the operator <span class="math notranslate nohighlight">\(\mathrm{L}_d\)</span>.</p>
</section>
<section id="solving-operator-equations">
<h2>Solving Operator Equations<a class="headerlink" href="#solving-operator-equations" title="Link to this heading">#</a></h2>
<p>The operator equation we encountered in MDPs, <span class="math notranslate nohighlight">\(\mathbf{v}_\gamma^{d^\infty} = \mathrm{L}_d \mathbf{v}_\gamma^{d^\infty}\)</span>, is a specific instance of a more general class of problems known as operator equations. These equations appear in various fields of mathematics and applied sciences, ranging from differential equations to functional analysis.</p>
<p>Operator equations can take several forms, each with its own characteristics and solution methods:</p>
<ol class="arabic simple">
<li><p><strong>Fixed Point Form</strong>: <span class="math notranslate nohighlight">\(x = \mathrm{T}(x)\)</span>, where <span class="math notranslate nohighlight">\(\mathrm{T}: X \rightarrow X\)</span>.
Common in fixed-point problems, such as our MDP equation, we seek a fixed point <span class="math notranslate nohighlight">\(x^*\)</span> such that <span class="math notranslate nohighlight">\(x^* = \mathrm{T}(x^*)\)</span>.</p></li>
<li><p><strong>General Operator Equation</strong>: <span class="math notranslate nohighlight">\(\mathrm{T}(x) = y\)</span>, where <span class="math notranslate nohighlight">\(\mathrm{T}: X \rightarrow Y\)</span>.
Here, <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> can be different spaces. We seek an <span class="math notranslate nohighlight">\(x \in X\)</span> that satisfies the equation for a given <span class="math notranslate nohighlight">\(y \in Y\)</span>.</p></li>
<li><p><strong>Nonlinear Equation</strong>: <span class="math notranslate nohighlight">\(\mathrm{T}(x) = 0\)</span>, where <span class="math notranslate nohighlight">\(\mathrm{T}: X \rightarrow Y\)</span>.
A special case of the general operator equation where we seek roots or zeros of the operator.</p></li>
<li><p><strong>Variational Inequality</strong>: Find <span class="math notranslate nohighlight">\(x^* \in K\)</span> such that <span class="math notranslate nohighlight">\(\langle \mathrm{T}(x^*), x - x^* \rangle \geq 0\)</span> for all <span class="math notranslate nohighlight">\(x \in K\)</span>.
Here, <span class="math notranslate nohighlight">\(K\)</span> is a closed convex subset of <span class="math notranslate nohighlight">\(X\)</span>, and <span class="math notranslate nohighlight">\(\mathrm{T}: K \rightarrow X^*\)</span> (the dual space of <span class="math notranslate nohighlight">\(X\)</span>). These problems often arise in optimization, game theory, and partial differential equations.</p></li>
</ol>
<section id="successive-approximation-method">
<h3>Successive Approximation Method<a class="headerlink" href="#successive-approximation-method" title="Link to this heading">#</a></h3>
<p>For equations in fixed point form, a common numerical solution method is successive approximation, also known as fixed-point iteration:</p>
<div class="proof algorithm admonition" id="successive-approximation">
<p class="admonition-title"><span class="caption-number">Algorithm 9 </span> (Successive Approximation)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> An operator <span class="math notranslate nohighlight">\(\mathrm{T}: X \rightarrow X\)</span>, an initial guess <span class="math notranslate nohighlight">\(x_0 \in X\)</span>, and a tolerance <span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span><br />
<strong>Output:</strong> An approximate fixed point <span class="math notranslate nohighlight">\(x^*\)</span> such that <span class="math notranslate nohighlight">\(\|x^* - \mathrm{T}(x^*)\| &lt; \epsilon\)</span></p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(n = 0\)</span></p></li>
<li><p><strong>repeat</strong><br />
3. Compute <span class="math notranslate nohighlight">\(x_{n+1} = \mathrm{T}(x_n)\)</span><br />
4. If <span class="math notranslate nohighlight">\(\|x_{n+1} - x_n\| &lt; \epsilon\)</span>, <strong>return</strong> <span class="math notranslate nohighlight">\(x_{n+1}\)</span><br />
5. Set <span class="math notranslate nohighlight">\(n = n + 1\)</span></p></li>
<li><p><strong>until</strong> convergence or maximum iterations reached</p></li>
</ol>
</section>
</div><p>The convergence of successive approximation depends on the properties of the operator <span class="math notranslate nohighlight">\(\mathrm{T}\)</span>. In the simplest and most common setting, we assume <span class="math notranslate nohighlight">\(\mathrm{T}\)</span> is a contraction mapping. The Banach Fixed-Point Theorem then guarantees that <span class="math notranslate nohighlight">\(\mathrm{T}\)</span> has a unique fixed point, and the successive approximation method will converge to this fixed point from any starting point. Specifically, <span class="math notranslate nohighlight">\(\mathrm{T}\)</span> is a contraction if there exists a constant <span class="math notranslate nohighlight">\(q \in [0,1)\)</span> such that for all <span class="math notranslate nohighlight">\(x,y \in X\)</span>:</p>
<div class="math notranslate nohighlight">
\[
d(\mathrm{T}(x), \mathrm{T}(y)) \leq q \cdot d(x,y)
\]</div>
<p>where <span class="math notranslate nohighlight">\(d\)</span> is the metric on <span class="math notranslate nohighlight">\(X\)</span>. In this case, the rate of convergence is linear, with error bound:</p>
<div class="math notranslate nohighlight">
\[
d(x_n, x^*) \leq \frac{q^n}{1-q} d(x_1, x_0)
\]</div>
<p>However, the contraction mapping condition is not the only one that can lead to convergence. For instance, if <span class="math notranslate nohighlight">\(\mathrm{T}\)</span> is nonexpansive (i.e., Lipschitz continuous with Lipschitz constant 1) and <span class="math notranslate nohighlight">\(X\)</span> is a Banach space with certain geometrical properties (e.g., uniformly convex), then under additional conditions (e.g., <span class="math notranslate nohighlight">\(\mathrm{T}\)</span> has at least one fixed point), the successive approximation method can still converge, albeit potentially more slowly than in the contraction case.</p>
<p>In practice, when dealing with specific problems like MDPs or differential equations, the properties of the operator often naturally align with one of these convergence conditions. For example, in discounted MDPs, the Bellman operator is a contraction in the supremum norm, which guarantees the convergence of value iteration.</p>
</section>
<section id="newton-kantorovich-method">
<h3>Newton-Kantorovich Method<a class="headerlink" href="#newton-kantorovich-method" title="Link to this heading">#</a></h3>
<p>The Newton-Kantorovich method is a generalization of Newton’s method from finite dimensional vector spaces to infinite dimensional function spaces: rather than iterating in the space of vectors, we are iterating in the space of functions.</p>
<p>Newton’s method is often written as the familiar update:</p>
<div class="math notranslate nohighlight">
\[
x_{k+1} = x_k - [DF(x_k)]^{-1} F(x_k),
\]</div>
<p>which makes it look as though the essence of the method is “take a derivative and invert it.” But the real workhorse behind Newton’s method (both in finite and infinite dimensions) is <strong>linearization</strong>.</p>
<p>At each step, the idea is to replace the nonlinear operator <span class="math notranslate nohighlight">\(F:X \to Y\)</span> by a local surrogate model of the form</p>
<div class="math notranslate nohighlight">
\[
F(x+h) \approx F(x) + Lh,
\]</div>
<p>where <span class="math notranslate nohighlight">\(L\)</span> is a linear map capturing how small perturbations in the input propagate to changes in the output. This is a Taylor-like expansion in Banach spaces: the role of the derivative is precisely to provide the correct notion of such a linear operator.</p>
<p>To find a root of <span class="math notranslate nohighlight">\(F\)</span>, we impose the condition that the surrogate vanishes at the next iterate:</p>
<div class="math notranslate nohighlight">
\[
0 = F(x+h) \approx F(x) + Lh.
\]</div>
<p>Solving this linear equation gives the increment <span class="math notranslate nohighlight">\(h\)</span>. In finite dimensions, <span class="math notranslate nohighlight">\(L\)</span> is the Jacobian matrix; in Banach spaces, it must be the <strong>Fréchet derivative</strong>.</p>
<p>But what exactly is a Fréchet derivative in infinite dimensions? To understand this, we need to generalize the concept of derivative from finite-dimensional calculus. In infinite-dimensional spaces, there are several notions of differentiability, each with different strengths and requirements:</p>
<p><strong>1. Gâteaux (Directional) Derivative</strong></p>
<p>We say that the Gâteaux derivative of <span class="math notranslate nohighlight">\(F\)</span> at <span class="math notranslate nohighlight">\(x\)</span> in a specific direction <span class="math notranslate nohighlight">\(h\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
F'(x; h) = \lim_{t \to 0} \frac{F(x + th) - F(x)}{t}
\]</div>
<p>This quantity measures how the function <span class="math notranslate nohighlight">\(F\)</span> changes along the ray <span class="math notranslate nohighlight">\(x + th\)</span>. While this limit may exist for each direction <span class="math notranslate nohighlight">\(h\)</span> separately, it doesn’t guarantee that the derivative is linear in <span class="math notranslate nohighlight">\(h\)</span>. This is a key limitation: the Gâteaux derivative can exist in all directions but still fail to provide a good linear approximation.</p>
<p><strong>2. Hadamard Directional Derivative</strong></p>
<p>Rather than considering a single direction of perturbation, we now consider a bundle of perturbations around <span class="math notranslate nohighlight">\(h\)</span>. We ask how the function changes as we approach the target direction from nearby directions. We say that <span class="math notranslate nohighlight">\(F\)</span> has a Hadamard directional derivative if:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
F'(x; h) = \lim_{\substack{t \downarrow 0 \\ h' \to h}} \frac{F(x + t h') - F(x)}{t}
\end{split}\]</div>
<p>This is a stronger condition than Gâteaux differentiability because it requires the limit to be uniform over nearby directions. However, it still doesn’t guarantee linearity in <span class="math notranslate nohighlight">\(h\)</span>.</p>
<p><strong>3. Fréchet Derivative</strong></p>
<p>The strongest and most natural notion: <span class="math notranslate nohighlight">\(F\)</span> is Fréchet differentiable at <span class="math notranslate nohighlight">\(x\)</span> if there exists a bounded linear operator <span class="math notranslate nohighlight">\(L\)</span> such that:</p>
<div class="math notranslate nohighlight">
\[
\lim_{h \to 0} \frac{\|F(x + h) - F(x) - Lh\|}{\|h\|} = 0
\]</div>
<p>This definition directly addresses the inadequacy of the previous notions. Unlike Gâteaux and Hadamard derivatives, the Fréchet derivative explicitly requires the existence of a linear operator <span class="math notranslate nohighlight">\(L\)</span> that provides a good approximation. Key properties:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(L\)</span> must be <strong>linear</strong> in <span class="math notranslate nohighlight">\(h\)</span> (unlike the directional derivatives above)</p></li>
<li><p>The approximation error is <span class="math notranslate nohighlight">\(o(\|h\|)\)</span>, uniform in all directions</p></li>
<li><p>This is the “true” derivative: it generalizes the Jacobian matrix to infinite dimensions</p></li>
<li><p>Notation: <span class="math notranslate nohighlight">\(L = F'(x)\)</span> or <span class="math notranslate nohighlight">\(DF(x)\)</span></p></li>
</ul>
<p><strong>Relationship:</strong></p>
<div class="math notranslate nohighlight">
\[
\text{Fréchet differentiable} \Rightarrow \text{Hadamard directionally diff.} \Rightarrow \text{Gâteaux directionally diff.}
\]</div>
<p>In the context of the Newton-Kantorovich method, we work with an operator <span class="math notranslate nohighlight">\(F: X \to Y\)</span> where both <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are Banach spaces. The Fréchet derivative <span class="math notranslate nohighlight">\(F'(x)\)</span> is the best linear approximation of <span class="math notranslate nohighlight">\(F\)</span> near <span class="math notranslate nohighlight">\(x\)</span>, and it’s exactly this linear operator <span class="math notranslate nohighlight">\(L\)</span> that we use in our linearization <span class="math notranslate nohighlight">\(F(x+h) \approx F(x) + F'(x)h\)</span>.</p>
<p>Now apart from those mathematical technicalities, Newton-Kantorovich has in essence the same structure as that of the original Newton’s method. That is, it applies the following sequence of steps:</p>
<ol class="arabic">
<li><p><strong>Linearize the Operator</strong>:
Given an approximation <span class="math notranslate nohighlight">\( x_n \)</span>, we consider the Fréchet derivative of <span class="math notranslate nohighlight">\( F \)</span>, denoted by <span class="math notranslate nohighlight">\( F'(x_n) \)</span>. This derivative is a linear operator that provides a local approximation of <span class="math notranslate nohighlight">\( F \)</span> near <span class="math notranslate nohighlight">\( x_n \)</span>.</p></li>
<li><p><strong>Set Up the Newton Step</strong>:
The method then solves the linearized equation for a correction <span class="math notranslate nohighlight">\( h_n \)</span>:</p>
<div class="math notranslate nohighlight">
\[
   F'(x_n) h_n = -F(x_n).
   \]</div>
<p>This equation represents a linear system where <span class="math notranslate nohighlight">\( h_n \)</span> is chosen so that the linearized operator <span class="math notranslate nohighlight">\( F(x_n) + F'(x_n)h_n \)</span> equals zero.</p>
</li>
<li><p><strong>Update the Solution</strong>:
The new approximation <span class="math notranslate nohighlight">\( x_{n+1} \)</span> is then given by:</p>
<div class="math notranslate nohighlight">
\[
   x_{n+1} = x_n + h_n.
   \]</div>
<p>This correction step refines <span class="math notranslate nohighlight">\( x_n \)</span>, bringing it closer to the true solution.</p>
</li>
<li><p><strong>Repeat Until Convergence</strong>:
We repeat the linearization and update steps until the solution <span class="math notranslate nohighlight">\( x_n \)</span> converges to the desired tolerance, which can be verified by checking that <span class="math notranslate nohighlight">\( \|F(x_n)\| \)</span> is sufficiently small, or by monitoring the norm <span class="math notranslate nohighlight">\( \|x_{n+1} - x_n\| \)</span>.</p></li>
</ol>
<p>The convergence of Newton-Kantorovich does not hinge on <span class="math notranslate nohighlight">\( F \)</span> being a contraction over the entire domain (as it could be the case for successive approximation). The convergence properties of the Newton-Kantorovich method are as follows:</p>
<ol class="arabic">
<li><p><strong>Local Convergence</strong>: Under mild conditions (e.g., <span class="math notranslate nohighlight">\(F\)</span> is Fréchet differentiable and <span class="math notranslate nohighlight">\(F'(x)\)</span> is invertible near the solution), the method converges locally. This means that if the initial guess is sufficiently close to the true solution, the method will converge.</p></li>
<li><p><strong>Global Convergence</strong>: Global convergence is not guaranteed in general. However, under stronger conditions (e.g., <span class="math notranslate nohighlight">\(F\)</span> is analytic and satisfies certain bounds), the method can converge globally.</p></li>
<li><p><strong>Rate of Convergence</strong>: When the method converges, it typically exhibits quadratic convergence. This means that the error at each step is proportional to the square of the error at the previous step:</p>
<div class="math notranslate nohighlight">
\[
   \|x_{n+1} - x^*\| \leq C\|x_n - x^*\|^2
   \]</div>
<p>where <span class="math notranslate nohighlight">\(x^*\)</span> is the true solution and <span class="math notranslate nohighlight">\(C\)</span> is some constant. This quadratic convergence is significantly faster than the linear convergence typically seen in methods like successive approximation.</p>
</li>
</ol>
</section>
</section>
<section id="optimality-equations-for-infinite-horizon-mdps">
<h2>Optimality Equations for Infinite-Horizon MDPs<a class="headerlink" href="#optimality-equations-for-infinite-horizon-mdps" title="Link to this heading">#</a></h2>
<p>Recall that in the finite-horizon setting, the optimality equations are:</p>
<div class="math notranslate nohighlight">
\[
v_n(s) = \max_{a \in A_s} \left\{r(s, a) + \gamma \sum_{j \in S} p(j | s, a) v_{n+1}(j)\right\}
\]</div>
<p>where <span class="math notranslate nohighlight">\(v_n(s)\)</span> is the value function at time step <span class="math notranslate nohighlight">\(n\)</span> for state <span class="math notranslate nohighlight">\(s\)</span>, <span class="math notranslate nohighlight">\(A_s\)</span> is the set of actions available in state <span class="math notranslate nohighlight">\(s\)</span>, <span class="math notranslate nohighlight">\(r(s, a)\)</span> is the reward function, <span class="math notranslate nohighlight">\(\gamma\)</span> is the discount factor, and <span class="math notranslate nohighlight">\(p(j | s, a)\)</span> is the transition probability from state <span class="math notranslate nohighlight">\(s\)</span> to state <span class="math notranslate nohighlight">\(j\)</span> given action <span class="math notranslate nohighlight">\(a\)</span>.</p>
<p>Intuitively, we would expect that by taking the limit of <span class="math notranslate nohighlight">\(n\)</span> to infinity, we might get the nonlinear equations:</p>
<div class="math notranslate nohighlight">
\[
v(s) = \max_{a \in A_s} \left\{r(s, a) + \gamma \sum_{j \in S} p(j | s, a) v(j)\right\}
\]</div>
<p>which are called the optimality equations or Bellman equations for infinite-horizon MDPs.</p>
<p>We can adopt an operator-theoretic perspective by defining operators on the space <span class="math notranslate nohighlight">\(V\)</span> of bounded real-valued functions on the state space <span class="math notranslate nohighlight">\(S\)</span>. For a deterministic Markov rule <span class="math notranslate nohighlight">\(\pi \in \Pi^{MD}\)</span>, define the <strong>policy-evaluation operator</strong>:</p>
<div class="math notranslate nohighlight">
\[
(\mathrm{L}_\pi v)(s) = r(s,\pi(s)) + \gamma \sum_{j \in \mathcal{S}} p(j|s,\pi(s)) v(j)
\]</div>
<p>The <strong>Bellman optimality operator</strong> is then:</p>
<div class="math notranslate nohighlight">
\[
\mathrm{L} \mathbf{v} \equiv \max_{\pi \in \Pi^{MD}} \left\{\mathbf{r}_\pi + \gamma \mathbf{P}_\pi \mathbf{v}\right\}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\Pi^{MD}\)</span> is the set of Markov deterministic decision rules, <span class="math notranslate nohighlight">\(\mathbf{r}_\pi\)</span> is the reward vector under decision rule <span class="math notranslate nohighlight">\(\pi\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{P}_\pi\)</span> is the transition probability matrix under decision rule <span class="math notranslate nohighlight">\(\pi\)</span>.</p>
<p>Note that while we write <span class="math notranslate nohighlight">\(\max_{\pi \in \Pi^{MD}}\)</span>, we do not implement the above operator by enumerating all decision rules. Rather, the fact that we compare policies based on their value functions in a componentwise fashion means that maximizing over the space of Markovian deterministic rules reduces to the following update in component form:</p>
<div class="math notranslate nohighlight">
\[
(\mathrm{L} \mathbf{v})(s) = \max_{a \in \mathcal{A}_s} \left\{r(s,a) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a) v(j)\right\}
\]</div>
<p>For convenience, we define the <strong>greedy selector</strong> <span class="math notranslate nohighlight">\(\mathrm{Greedy}(v) \in \Pi^{MD}\)</span> that extracts an optimal decision rule from a value function:</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Greedy}(v)(s) \in \arg\max_{a \in \mathcal{A}_s} \left\{r(s,a) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a) v(j)\right\}
\]</div>
<p>In Puterman’s terminology, such a greedy selector is called <strong><span class="math notranslate nohighlight">\(v\)</span>-improving</strong> (or <strong>conserving</strong> when it achieves the maximum). This operator will be useful for expressing algorithms succinctly:</p>
<ul class="simple">
<li><p><strong>Value iteration:</strong> <span class="math notranslate nohighlight">\(v_{k+1} = \mathrm{L}v_k\)</span>, then extract <span class="math notranslate nohighlight">\(\pi = \mathrm{Greedy}(v^*)\)</span></p></li>
<li><p><strong>Policy iteration:</strong> <span class="math notranslate nohighlight">\(\pi_{k+1} = \mathrm{Greedy}(v^{\pi_k})\)</span> with <span class="math notranslate nohighlight">\(v^{\pi_k}\)</span> solving <span class="math notranslate nohighlight">\(v = \mathrm{L}_{\pi_k}v\)</span></p></li>
</ul>
<p>The equivalence between these two forms can be shown mathematically, as demonstrated in the following proposition and proof.</p>
<div class="proof proposition admonition" id="proposition-15">
<p class="admonition-title"><span class="caption-number">Proposition 3 </span></p>
<section class="proposition-content" id="proof-content">
<p>The operator <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> defined as a maximization over Markov deterministic decision rules:</p>
<div class="math notranslate nohighlight">
\[(\mathrm{L} \mathbf{v})(s) = \max_{\pi \in \Pi^{MD}} \left\{r(s,\pi(s)) + \gamma \sum_{j \in \mathcal{S}} p(j|s,\pi(s)) v(j)\right\}\]</div>
<p>is equivalent to the componentwise maximization over actions:</p>
<div class="math notranslate nohighlight">
\[(\mathrm{L} \mathbf{v})(s) = \max_{a \in \mathcal{A}_s} \left\{r(s,a) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a) v(j)\right\}\]</div>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Fix <span class="math notranslate nohighlight">\(s\)</span>. Let</p>
<div class="math notranslate nohighlight">
\[
Q_v(s,a) \triangleq r(s,a)+\gamma\sum_{j}p(j\mid s,a)\,v(j).
\]</div>
<p>For any rule <span class="math notranslate nohighlight">\(\pi \in \Pi^{MD}\)</span>, we have <span class="math notranslate nohighlight">\((\mathrm{L}_\pi v)(s)=Q_v(s,\pi(s))\le \max_{a\in\mathcal{A}_s}Q_v(s,a)\)</span>.</p>
<p>Taking the maximum over <span class="math notranslate nohighlight">\(\pi\)</span> gives</p>
<div class="math notranslate nohighlight">
\[
\max_{\pi\in\Pi^{MD}}(\mathrm{L}_\pi v)(s) \le \max_{a\in\mathcal{A}_s}Q_v(s,a).
\]</div>
<p>Conversely, choose a <strong>greedy selector</strong> <span class="math notranslate nohighlight">\(\pi^v\in\Pi^{MD}\)</span> such that for each <span class="math notranslate nohighlight">\(s\)</span>,</p>
<div class="math notranslate nohighlight">
\[\pi^v(s)\in\arg\max_{a\in\mathcal{A}_s}Q_v(s,a)\]</div>
<p>(possible since <span class="math notranslate nohighlight">\(\mathcal{A}_s\)</span> is finite; otherwise use a measurable <span class="math notranslate nohighlight">\(\varepsilon\)</span>-greedy selector). Then</p>
<div class="math notranslate nohighlight">
\[
(\mathrm{L}_{\pi^v}v)(s)=Q_v(s,\pi^v(s))=\max_{a\in\mathcal{A}_s}Q_v(s,a),
\]</div>
<p>so <span class="math notranslate nohighlight">\(\max_{\pi}(\mathrm{L}_\pi v)(s)\ge \max_{a}Q_v(s,a)\)</span>. Combining both inequalities yields equality.</p>
</div>
</section>
<section id="algorithms-for-solving-the-optimality-equations">
<h2>Algorithms for Solving the Optimality Equations<a class="headerlink" href="#algorithms-for-solving-the-optimality-equations" title="Link to this heading">#</a></h2>
<p>The optimality equations are operator equations. Therefore, we can apply general numerical methods to solve them. Applying the successive approximation method to the Bellman optimality equation yields a method known as “value iteration” in dynamic programming. A direct application of the blueprint for successive approximation yields the following algorithm:</p>
<div class="proof algorithm admonition" id="value-iteration">
<p class="admonition-title"><span class="caption-number">Algorithm 10 </span> (Value Iteration)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> Given an MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span> and tolerance <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span></p>
<p><strong>Output</strong> Compute an <span class="math notranslate nohighlight">\(\varepsilon\)</span>-optimal value function <span class="math notranslate nohighlight">\(v\)</span> and policy <span class="math notranslate nohighlight">\(\pi\)</span></p>
<ol class="arabic">
<li><p>Initialize <span class="math notranslate nohighlight">\(v_0(s) = 0\)</span> for all <span class="math notranslate nohighlight">\(s \in S\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow 0\)</span></p></li>
<li><p><strong>repeat</strong></p>
<ol class="arabic simple">
<li><p>For each <span class="math notranslate nohighlight">\(s \in S\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(v_{n+1}(s) \leftarrow (\mathrm{L}v_n)(s) = \max_{a \in A} \left\{r(s,a) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a)v_n(j)\right\}\)</span></p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(\delta \leftarrow \|v_{n+1} - v_n\|_\infty\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow n + 1\)</span></p></li>
</ol>
</li>
<li><p><strong>until</strong> <span class="math notranslate nohighlight">\(\delta &lt; \frac{\varepsilon(1-\gamma)}{2\gamma}\)</span></p></li>
<li><p>Extract greedy policy: <span class="math notranslate nohighlight">\(\pi \leftarrow \mathrm{Greedy}(v_n)\)</span> where</p>
<div class="math notranslate nohighlight">
\[\mathrm{Greedy}(v)(s) \in \arg\max_{a \in \mathcal{A}_s} \left\{r(s,a) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a)v(j)\right\}\]</div>
</li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(v_n, \pi\)</span></p></li>
</ol>
</section>
</div><p>The termination criterion in this algorithm is based on a specific bound that provides guarantees on the quality of the solution. This is in contrast to supervised learning, where we often use arbitrary termination criteria based on computational budget or early stopping when the learning curve flattens. This is because establishing implementable generalization bounds in supervised learning is challenging.</p>
<p>However, in the dynamic programming context, we can derive various bounds that can be implemented in practice. These bounds help us terminate our procedure with a guarantee on the precision of our value function and, correspondingly, on the optimality of the resulting policy.</p>
<div class="proof proposition admonition" id="value-iteration-convergence">
<p class="admonition-title"><span class="caption-number">Proposition 4 </span> (Convergence of Value Iteration)</p>
<section class="proposition-content" id="proof-content">
<p>(Adapted from <span id="id10">Puterman [<a class="reference internal" href="bibliography.html#id23" title="Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley &amp; Sons, New York, 1994. ISBN 978-0-471-61977-3. First published in 1994.">34</a>]</span> theorem 6.3.1)</p>
<p>Let <span class="math notranslate nohighlight">\(v_0\)</span> be any initial value function, <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span> a desired accuracy, and let <span class="math notranslate nohighlight">\(\{v_n\}\)</span> be the sequence of value functions generated by value iteration, i.e., <span class="math notranslate nohighlight">\(v_{n+1} = \mathrm{L}v_n\)</span> for <span class="math notranslate nohighlight">\(n \geq 0\)</span>, where <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> is the Bellman optimality operator. Then:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(v_n\)</span> converges to the optimal value function <span class="math notranslate nohighlight">\(v^*_\gamma\)</span>,</p></li>
<li><p>The algorithm terminates in finite time,</p></li>
<li><p>The resulting policy <span class="math notranslate nohighlight">\(\pi_\varepsilon\)</span> is <span class="math notranslate nohighlight">\(\varepsilon\)</span>-optimal, and</p></li>
<li><p>When the algorithm terminates, <span class="math notranslate nohighlight">\(v_{n+1}\)</span> is within <span class="math notranslate nohighlight">\(\varepsilon/2\)</span> of <span class="math notranslate nohighlight">\(v^*_\gamma\)</span>.</p></li>
</ol>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Parts 1 and 2 follow directly from the fact that <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> is a contraction mapping. Hence, by Banach’s fixed-point theorem, it has a unique fixed point (which is <span class="math notranslate nohighlight">\(v^*_\gamma\)</span>), and repeated application of <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> will converge to this fixed point. Moreover, this convergence happens at a geometric rate, which ensures that we reach the termination condition in finite time.</p>
<p>To show that the Bellman optimality operator <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> is a contraction mapping, we need to prove that for any two value functions <span class="math notranslate nohighlight">\(v\)</span> and <span class="math notranslate nohighlight">\(u\)</span>:</p>
<div class="math notranslate nohighlight">
\[\|\mathrm{L}v - \mathrm{L}u\|_\infty \leq \gamma \|v - u\|_\infty\]</div>
<p>where <span class="math notranslate nohighlight">\(\gamma \in [0,1)\)</span> is the discount factor and <span class="math notranslate nohighlight">\(\|\cdot\|_\infty\)</span> is the supremum norm.</p>
<p>Let’s start by writing out the definition of <span class="math notranslate nohighlight">\(\mathrm{L}v\)</span> and <span class="math notranslate nohighlight">\(\mathrm{L}u\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
(\mathrm{L}v)(s) &amp;= \max_{a \in A} \left\{r(s,a) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a)v(j)\right\}\\
(\mathrm{L}u)(s) &amp;= \max_{a \in A} \left\{r(s,a) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a)u(j)\right\}
\end{align*}\end{split}\]</div>
<p>For any state <span class="math notranslate nohighlight">\(s\)</span>, let <span class="math notranslate nohighlight">\(a_v\)</span> be the action that achieves the maximum for <span class="math notranslate nohighlight">\((\mathrm{L}v)(s)\)</span>, and <span class="math notranslate nohighlight">\(a_u\)</span> be the action that achieves the maximum for <span class="math notranslate nohighlight">\((\mathrm{L}u)(s)\)</span>. By the definition of these maximizers:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
(\mathrm{L}v)(s) &amp;\geq r(s,a_u) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a_u)v(j)\\
(\mathrm{L}u)(s) &amp;\geq r(s,a_v) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a_v)u(j)
\end{align*}\end{split}\]</div>
<p>Subtracting these inequalities:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
(\mathrm{L}v)(s) - (\mathrm{L}u)(s) &amp;\leq \gamma \sum_{j \in \mathcal{S}} p(j|s,a_v)(v(j) - u(j))\\
(\mathrm{L}u)(s) - (\mathrm{L}v)(s) &amp;\leq \gamma \sum_{j \in \mathcal{S}} p(j|s,a_u)(u(j) - v(j))
\end{align*}\end{split}\]</div>
<p>Taking the absolute value and using the fact that <span class="math notranslate nohighlight">\(\sum_{j \in \mathcal{S}} p(j|s,a) = 1\)</span>:</p>
<div class="math notranslate nohighlight">
\[|(\mathrm{L}v)(s) - (\mathrm{L}u)(s)| \leq \gamma \max_{j \in \mathcal{S}} |v(j) - u(j)| = \gamma \|v - u\|_\infty\]</div>
<p>Since this holds for all <span class="math notranslate nohighlight">\(s \in \mathcal{S}\)</span>, taking the supremum over <span class="math notranslate nohighlight">\(s\)</span> gives:</p>
<div class="math notranslate nohighlight">
\[\|\mathrm{L}v - \mathrm{L}u\|_\infty \leq \gamma \|v - u\|_\infty\]</div>
<p>Thus, <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> is a contraction mapping with contraction factor <span class="math notranslate nohighlight">\(\gamma\)</span>.</p>
<p>Now, let’s prove parts 3 and 4. Suppose the algorithm has just terminated, i.e., <span class="math notranslate nohighlight">\(\|v_{n+1} - v_n\|_\infty &lt; \frac{\varepsilon(1-\gamma)}{2\gamma}\)</span> for some <span class="math notranslate nohighlight">\(n\)</span>. We want to show that our current value function <span class="math notranslate nohighlight">\(v_{n+1}\)</span> and the policy <span class="math notranslate nohighlight">\(\pi_\varepsilon\)</span> derived from it are close to optimal.</p>
<p>By the triangle inequality:</p>
<div class="math notranslate nohighlight">
\[\|v^{\pi_\varepsilon}_\gamma - v^*_\gamma\|_\infty \leq \|v^{\pi_\varepsilon}_\gamma - v_{n+1}\|_\infty + \|v_{n+1} - v^*_\gamma\|_\infty\]</div>
<p>For the first term, since <span class="math notranslate nohighlight">\(v^{\pi_\varepsilon}_\gamma\)</span> is the fixed point of <span class="math notranslate nohighlight">\(\mathrm{L}_{\pi_\varepsilon}\)</span> and <span class="math notranslate nohighlight">\(\pi_\varepsilon\)</span> is greedy with respect to <span class="math notranslate nohighlight">\(v_{n+1}\)</span> (i.e., <span class="math notranslate nohighlight">\(\mathrm{L}_{\pi_\varepsilon}v_{n+1} = \mathrm{L}v_{n+1}\)</span>):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\|v^{\pi_\varepsilon}_\gamma - v_{n+1}\|_\infty &amp;= \|\mathrm{L}_{\pi_\varepsilon}v^{\pi_\varepsilon}_\gamma - v_{n+1}\|_\infty \\
&amp;\leq \|\mathrm{L}_{\pi_\varepsilon}v^{\pi_\varepsilon}_\gamma - \mathrm{L}_{\pi_\varepsilon}v_{n+1}\|_\infty + \|\mathrm{L}_{\pi_\varepsilon}v_{n+1} - v_{n+1}\|_\infty \\
&amp;= \|\mathrm{L}_{\pi_\varepsilon}v^{\pi_\varepsilon}_\gamma - \mathrm{L}_{\pi_\varepsilon}v_{n+1}\|_\infty + \|\mathrm{L}v_{n+1} - v_{n+1}\|_\infty \\
&amp;\leq \gamma\|v^{\pi_\varepsilon}_\gamma - v_{n+1}\|_\infty + \gamma\|v_{n+1} - v_n\|_\infty
\end{aligned}
\end{split}\]</div>
<p>where we used that both <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> and <span class="math notranslate nohighlight">\(\mathrm{L}_{\pi_\varepsilon}\)</span> are contractions with factor <span class="math notranslate nohighlight">\(\gamma\)</span>, and that <span class="math notranslate nohighlight">\(v_{n+1} = \mathrm{L}v_n\)</span>.</p>
<p>Rearranging:</p>
<div class="math notranslate nohighlight">
\[\|v^{\pi_\varepsilon}_\gamma - v_{n+1}\|_\infty \leq \frac{\gamma}{1-\gamma}\|v_{n+1} - v_n\|_\infty\]</div>
<p>Similarly, since <span class="math notranslate nohighlight">\(v^*_\gamma\)</span> is the fixed point of <span class="math notranslate nohighlight">\(\mathrm{L}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\|v_{n+1} - v^*_\gamma\|_\infty = \|\mathrm{L}v_n - \mathrm{L}v^*_\gamma\|_\infty \leq \gamma\|v_n - v^*_\gamma\|_\infty \leq \frac{\gamma}{1-\gamma}\|v_{n+1} - v_n\|_\infty\]</div>
<p>Since <span class="math notranslate nohighlight">\(\|v_{n+1} - v_n\|_\infty &lt; \frac{\varepsilon(1-\gamma)}{2\gamma}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\|v^{\pi_\varepsilon}_\gamma - v_{n+1}\|_\infty \leq \frac{\gamma}{1-\gamma} \cdot \frac{\varepsilon(1-\gamma)}{2\gamma} = \frac{\varepsilon}{2}\]</div>
<div class="math notranslate nohighlight">
\[\|v_{n+1} - v^*_\gamma\|_\infty \leq \frac{\gamma}{1-\gamma} \cdot \frac{\varepsilon(1-\gamma)}{2\gamma} = \frac{\varepsilon}{2}\]</div>
<p>Combining these:</p>
<div class="math notranslate nohighlight">
\[\|v^{\pi_\varepsilon}_\gamma - v^*_\gamma\|_\infty \leq \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon\]</div>
<p>This completes the proof, showing that <span class="math notranslate nohighlight">\(v_{n+1}\)</span> is within <span class="math notranslate nohighlight">\(\varepsilon/2\)</span> of <span class="math notranslate nohighlight">\(v^*_\gamma\)</span> (part 4) and <span class="math notranslate nohighlight">\(\pi_\varepsilon\)</span> is <span class="math notranslate nohighlight">\(\varepsilon\)</span>-optimal (part 3).</p>
</div>
<section id="newton-kantorovich-applied-to-bellman-optimality">
<h3>Newton-Kantorovich Applied to Bellman Optimality<a class="headerlink" href="#newton-kantorovich-applied-to-bellman-optimality" title="Link to this heading">#</a></h3>
<p>We now apply the Newton-Kantorovich framework to the Bellman optimality equation. Let</p>
<div class="math notranslate nohighlight">
\[
(\mathrm{L}v)(s) = \max_{a \in A(s)} \left\{ r(s,a) + \gamma \sum_{s'} p(s' \mid s,a) v(s') \right\}.
\]</div>
<p>The problem is to find <span class="math notranslate nohighlight">\(v\)</span> such that <span class="math notranslate nohighlight">\(\mathrm{L}v = v\)</span>, or equivalently <span class="math notranslate nohighlight">\(\mathrm{B}(v) := \mathrm{L}v - v = 0\)</span>. The operator <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> is piecewise affine, hence not globally differentiable, but it is directionally differentiable everywhere in the Hadamard sense and Fréchet differentiable at points where the maximizer is unique.</p>
<p>We consider three complementary perspectives for understanding and computing its derivative.</p>
<section id="perspective-1-max-of-affine-maps">
<h4>Perspective 1: Max of Affine Maps<a class="headerlink" href="#perspective-1-max-of-affine-maps" title="Link to this heading">#</a></h4>
<p>In tabular form, for finite state and action spaces, the Bellman operator can be written as a pointwise maximum of affine maps:</p>
<div class="math notranslate nohighlight">
\[
(\mathrm{L}v)(s) = \max_{a \in A(s)} \left\{ r(s,a) + \gamma (P_a v)(s) \right\},
\]</div>
<p>where <span class="math notranslate nohighlight">\(P_a \in \mathbb{R}^{|S| \times |S|}\)</span> is the transition matrix associated with action <span class="math notranslate nohighlight">\(a\)</span>. Each <span class="math notranslate nohighlight">\(Q_a v := r^a + \gamma P_a v\)</span> is affine in <span class="math notranslate nohighlight">\(v\)</span>. The operator <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> therefore computes the upper envelope of a finite set of affine functions at each state.</p>
<p>At any <span class="math notranslate nohighlight">\(v\)</span>, let the <strong>active set</strong> at state <span class="math notranslate nohighlight">\(s\)</span> be</p>
<div class="math notranslate nohighlight">
\[
\mathcal{A}^*(s; v) := \arg\max_{a \in A(s)} (Q_a v)(s).
\]</div>
<p>Then the Hadamard directional derivative exists and is given by</p>
<div class="math notranslate nohighlight">
\[
(\mathrm{L}'(v; h))(s) = \max_{a \in \mathcal{A}^*(s; v)} \gamma (P_a h)(s).
\]</div>
<p>If the active set is a singleton, this expression becomes linear in <span class="math notranslate nohighlight">\(h\)</span>, and <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> is Fréchet differentiable at <span class="math notranslate nohighlight">\(v\)</span>, with</p>
<div class="math notranslate nohighlight">
\[
\mathrm{L}'(v) = \gamma P_{\pi_v},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\pi_v(s) := a^*(s)\)</span> is the greedy policy at <span class="math notranslate nohighlight">\(v\)</span>.</p>
<!-- In the presence of ties, the derivative becomes set-valued: the Clarke subdifferential consists of stochastic matrices whose rows are convex combinations of the $\gamma P_a$ over $a \in \mathcal{A}^*(s; v)$. -->
</section>
<section id="perspective-2-envelope-theorem">
<h4>Perspective 2: Envelope Theorem<a class="headerlink" href="#perspective-2-envelope-theorem" title="Link to this heading">#</a></h4>
<p>Consider now a value function approximated as a linear combination of basis functions:</p>
<div class="math notranslate nohighlight">
\[
v_c(s) = \sum_j c_j \phi_j(s).
\]</div>
<p>At a node <span class="math notranslate nohighlight">\(s_i\)</span>, define the parametric maximization</p>
<div class="math notranslate nohighlight">
\[
v_i(c) := (\mathrm{L}v_c)(s_i) = \max_{a \in A(s_i)} \left\{ r(s_i,a) + \gamma \sum_j c_j \mathbb{E}_{s' \mid s_i, a}[\phi_j(s')] \right\}.
\]</div>
<p>Define</p>
<div class="math notranslate nohighlight">
\[
F_i(a, c) := r(s_i,a) + \gamma \sum_j c_j \mathbb{E}_{s' \mid s_i, a}[\phi_j(s')],
\]</div>
<p>so that <span class="math notranslate nohighlight">\(v_i(c) = \max_a F_i(a, c)\)</span>. Since <span class="math notranslate nohighlight">\(F_i\)</span> is linear in <span class="math notranslate nohighlight">\(c\)</span>, we can apply the <strong>envelope theorem</strong> (Danskin’s theorem): if the optimizer <span class="math notranslate nohighlight">\(a_i^*(c)\)</span> is unique or selected measurably, then</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial v_i}{\partial c_j}(c) = \gamma \mathbb{E}_{s' \mid s_i, a_i^*(c)}[\phi_j(s')].
\]</div>
<p>The key is that we do not need to differentiate the optimizer <span class="math notranslate nohighlight">\(a_i^*(c)\)</span> itself. The result extends to the subdifferential case when ties occur, where the Jacobian becomes set-valued.</p>
<p>This result is useful when solving the collocation equation <span class="math notranslate nohighlight">\(\Phi c = v(c)\)</span>. Newton’s method requires the Jacobian <span class="math notranslate nohighlight">\(v'(c)\)</span>, and this expression allows us to compute it without involving any derivatives of the optimal action.</p>
</section>
<section id="perspective-3-the-implicit-function-theorem">
<h4>Perspective 3: The Implicit Function Theorem<a class="headerlink" href="#perspective-3-the-implicit-function-theorem" title="Link to this heading">#</a></h4>
<p>The third perspective applies the implicit function theorem to understand when the Bellman operator is differentiable despite containing a max operator. The maximization problem defines an implicit relationship between the value function and the optimal action, and the implicit function theorem tells us when this relationship is smooth enough to differentiate through.</p>
<p>The Bellman operator is defined as</p>
<div class="math notranslate nohighlight">
\[
(\mathrm{L}v)(s) = \max_{a} \left\{ r(s,a) + \gamma \sum_j p(j \mid s,a) v(j) \right\}.
\]</div>
<p>The difficulty is that the max operator encodes a discrete selection: which action achieves the maximum. To apply the implicit function theorem, we reformulate this as follows. For each action <span class="math notranslate nohighlight">\(a\)</span>, define the <strong>action-value function</strong>:</p>
<div class="math notranslate nohighlight">
\[
Q_a(v, s) := r(s,a) + \gamma \sum_j p(j \mid s,a) v(j).
\]</div>
<p>The optimal action at <span class="math notranslate nohighlight">\(v\)</span> satisfies the <strong>optimality condition</strong>:</p>
<div class="math notranslate nohighlight">
\[
Q_{a^*(s)}(v, s) \geq Q_a(v, s) \quad \text{for all } a.
\]</div>
<p>Now suppose that at a particular <span class="math notranslate nohighlight">\(v\)</span>, action <span class="math notranslate nohighlight">\(a^*(s)\)</span> is a <strong>strict local maximizer</strong> in the sense that there exists <span class="math notranslate nohighlight">\(\delta &gt; 0\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
Q_{a^*(s)}(v, s) &gt; Q_a(v, s) + \delta \quad \text{for all } a \neq a^*(s).
\]</div>
<p>This strict inequality is the regularity condition needed for the implicit function theorem. It ensures that the optimal action is not only unique at <span class="math notranslate nohighlight">\(v\)</span>, but remains so in a neighborhood of <span class="math notranslate nohighlight">\(v\)</span>.</p>
<p>To see why, consider any perturbation <span class="math notranslate nohighlight">\(v + h\)</span> with <span class="math notranslate nohighlight">\(\|h\|\)</span> small. Since <span class="math notranslate nohighlight">\(Q_a\)</span> is linear in <span class="math notranslate nohighlight">\(v\)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[
Q_a(v+h, s) = Q_a(v, s) + \gamma \sum_j p(j \mid s,a) h(j).
\]</div>
<p>The perturbation term is bounded: <span class="math notranslate nohighlight">\(|\gamma \sum_j p(j \mid s,a) h(j)| \leq \gamma \|h\|\)</span>. Therefore, for <span class="math notranslate nohighlight">\(\|h\| &lt; \delta/\gamma\)</span>, the strict gap ensures that</p>
<div class="math notranslate nohighlight">
\[
Q_{a^*(s)}(v+h, s) &gt; Q_a(v+h, s) \quad \text{for all } a \neq a^*(s).
\]</div>
<p>Thus <span class="math notranslate nohighlight">\(a^*(s)\)</span> remains the unique maximizer throughout the neighborhood <span class="math notranslate nohighlight">\(\{v + h : \|h\| &lt; \delta/\gamma\}\)</span>.</p>
<p>The implicit function theorem now applies: in this neighborhood, the mapping <span class="math notranslate nohighlight">\(v \mapsto a^*(s; v)\)</span> is <strong>constant</strong> (and hence smooth), taking the value <span class="math notranslate nohighlight">\(a^*(s)\)</span>. This allows us to write</p>
<div class="math notranslate nohighlight">
\[
(\mathrm{L}v)(s) = Q_{a^*(s)}(v, s) = r(s,a^*(s)) + \gamma \sum_j p(j \mid s,a^*(s)) v(j)
\]</div>
<p>as an explicit formula that holds throughout the neighborhood. Since <span class="math notranslate nohighlight">\(Q_{a^*(s)}(\cdot, s)\)</span> is an affine (hence smooth) function of <span class="math notranslate nohighlight">\(v\)</span>, we can differentiate it:</p>
<div class="math notranslate nohighlight">
\[
\frac{d}{dv} (\mathrm{L}v)(s) = \gamma P_{a^*(s)}.
\]</div>
<p>More precisely, for any perturbation <span class="math notranslate nohighlight">\(h\)</span>:</p>
<div class="math notranslate nohighlight">
\[
(\mathrm{L}(v+h))(s) = (\mathrm{L}v)(s) + \gamma \sum_j p(j \mid s,a^*(s)) h(j) + o(\|h\|).
\]</div>
<p>This is the Fréchet derivative:</p>
<div class="math notranslate nohighlight">
\[
\mathrm{L}'(v) = \gamma P_{\pi_v},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\pi_v(s) = a^*(s)\)</span> is the greedy policy.</p>
<p><strong>The role of the implicit function theorem</strong>: It guarantees that when the maximizer is unique with a strict gap (the regularity condition), the argmax function <span class="math notranslate nohighlight">\(v \mapsto a^*(s; v)\)</span> is locally constant, which removes the non-differentiability of the max operator. Without this regularity condition (specifically, at points where multiple actions tie for optimality), the implicit function theorem does not apply, and the operator is not Fréchet differentiable. The active set perspective (Perspective 1) and the envelope theorem (Perspective 2) provide the tools to handle these non-smooth points.</p>
</section>
</section>
<section id="connection-to-policy-iteration">
<h3>Connection to Policy Iteration<a class="headerlink" href="#connection-to-policy-iteration" title="Link to this heading">#</a></h3>
<p>We return to the Newton-Kantorovich step:</p>
<div class="math notranslate nohighlight">
\[
(I - \mathrm{L}'(v_n)) h_n = v_n - \mathrm{L}v_n,
\quad
v_{n+1} = v_n - h_n.
\]</div>
<p>Suppose <span class="math notranslate nohighlight">\(\mathrm{L}'(v_n) = \gamma P_{\pi_{v_n}}\)</span> for the greedy policy <span class="math notranslate nohighlight">\(\pi_{v_n}\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
(I - \gamma P_{\pi_{v_n}}) v_{n+1} = r^{\pi_{v_n}},
\]</div>
<p>which is exactly policy evaluation for <span class="math notranslate nohighlight">\(\pi_{v_n}\)</span>. Recomputing the greedy policy from <span class="math notranslate nohighlight">\(v_{n+1}\)</span> yields the next iterate.</p>
<p>Thus, <strong>policy iteration is Newton-Kantorovich</strong> applied to the Bellman optimality equation. At points of nondifferentiability (when ties occur), the operator is still semismooth, and policy iteration corresponds to a semismooth Newton method. The envelope theorem is what justifies the simplification of the Jacobian to <span class="math notranslate nohighlight">\(\gamma P_{\pi_v}\)</span>, bypassing the need to differentiate through the optimizer. This completes the equivalence.</p>
</section>
<section id="the-semismooth-newton-perspective">
<h3>The Semismooth Newton Perspective<a class="headerlink" href="#the-semismooth-newton-perspective" title="Link to this heading">#</a></h3>
<p>The three perspectives we developed above (the active set view, the envelope theorem, and the implicit function theorem) all point toward a deeper framework for understanding Newton-type methods on non-smooth operators. This framework, known as semismooth Newton methods, was developed precisely to handle operators like the Bellman operator that are piecewise smooth but not globally differentiable. The connection between policy iteration and semismooth Newton methods has been rigorously developed in recent work <span id="id11">[<a class="reference internal" href="bibliography.html#id61" title="M. Gargiani, A. Zanelli, D. Liao-McPherson, T. H. Summers, and J. Lygeros. Dynamic programming through the lens of semismooth newton-type methods. IEEE Control Systems Letters, 6:2996-3001, 2022. doi:10.1109/LCSYS.2022.3181213.">15</a>]</span>.</p>
<p>The classical Newton-Kantorovich method assumes the operator is Fréchet differentiable everywhere. The derivative exists, is unique, and varies continuously with the base point. But the Bellman operator <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> violates this assumption at any value function where multiple actions tie for optimality at some state. At such points, the implicit function theorem fails, and there is no unique Fréchet derivative.</p>
<p>Semismooth Newton methods address this by replacing the notion of a single Jacobian with a generalized derivative that captures the behavior of the operator near non-smooth points. The most commonly used generalized derivative is the Clarke subdifferential, which we can think of as the convex hull of all possible “candidate Jacobians” that arise from limits approaching the non-smooth point from different directions.</p>
<p>For the Bellman residual <span class="math notranslate nohighlight">\(\mathrm{B}(v) = \mathrm{L}v - v\)</span>, the Clarke subdifferential at a point <span class="math notranslate nohighlight">\(v\)</span> can be characterized explicitly using our first perspective. Recall that at each state <span class="math notranslate nohighlight">\(s\)</span>, we defined the active set <span class="math notranslate nohighlight">\(\mathcal{A}^*(s; v) = \arg\max_a Q_a(v, s)\)</span>. When this set contains multiple actions, the operator is not Fréchet differentiable. However, it remains directionally differentiable in all directions, and the Clarke subdifferential consists of all matrices of the form</p>
<div class="math notranslate nohighlight">
\[
\partial \mathrm{B}(v) = \left\{ I - \gamma P_\pi : \pi(s) \in \mathcal{A}^*(s; v) \text{ for all } s \right\}.
\]</div>
<p>In words, the generalized Jacobian is the set of all matrices <span class="math notranslate nohighlight">\(I - \gamma P_\pi\)</span> where <span class="math notranslate nohighlight">\(\pi\)</span> is any policy that selects an action from the active set at each state. When the maximizer is unique everywhere, this set reduces to a singleton, and we recover the classical Fréchet derivative. When ties occur, the set has multiple elements: precisely the convex combinations mentioned in Perspective 1.</p>
<p>The semismooth Newton method for solving <span class="math notranslate nohighlight">\(\mathrm{B}(v) = 0\)</span> proceeds by selecting an element <span class="math notranslate nohighlight">\(J_k \in \partial \mathrm{B}(v_k)\)</span> at each iteration and solving</p>
<div class="math notranslate nohighlight">
\[
J_k h_k = -\mathrm{B}(v_k), \quad v_{k+1} = v_k + h_k.
\]</div>
<p>The main takeaway is that any choice from the Clarke subdifferential yields a valid Newton-like update. In the context of the Bellman equation, choosing <span class="math notranslate nohighlight">\(J_k = I - \gamma P_{\pi_k}\)</span> where <span class="math notranslate nohighlight">\(\pi_k\)</span> is any greedy policy corresponds exactly to the policy evaluation step in policy iteration. The freedom in selecting which action to choose when ties occur translates to the freedom in selecting which element of the subdifferential to use.</p>
<p>Under appropriate regularity conditions (specifically, when the residual function is BD-regular or CD-regular), the semismooth Newton method converges locally at a quadratic rate <span id="id12">[<a class="reference internal" href="bibliography.html#id61" title="M. Gargiani, A. Zanelli, D. Liao-McPherson, T. H. Summers, and J. Lygeros. Dynamic programming through the lens of semismooth newton-type methods. IEEE Control Systems Letters, 6:2996-3001, 2022. doi:10.1109/LCSYS.2022.3181213.">15</a>]</span>. This means that near the solution, the error decreases quadratically:
$<span class="math notranslate nohighlight">\(
\|v_{k+1} - v^*\| \leq C \|v_k - v^*\|^2.
\)</span>$</p>
<p>This theoretical result explains an empirical observation that has long been noted in practice: policy iteration typically converges in remarkably few iterations, often just a handful, even when the state and action spaces are enormous and the space of possible policies is exponentially large.</p>
<p>The semismooth Newton framework also suggests a spectrum of methods interpolating between value iteration and policy iteration. Value iteration can be interpreted as a Newton-like method where we choose <span class="math notranslate nohighlight">\(J_k = I\)</span> at every iteration, ignoring the dependence of <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> on <span class="math notranslate nohighlight">\(v\)</span> entirely. This choice guarantees global convergence through the contraction property but sacrifices the quadratic local convergence rate. Policy iteration, at the other extreme, uses the full generalized Jacobian <span class="math notranslate nohighlight">\(J_k = I - \gamma P_{\pi_k}\)</span>, achieving quadratic convergence but at the cost of solving a linear system at each iteration.</p>
<p>Between these extremes lie methods that use approximate Jacobians. One natural variant is to choose <span class="math notranslate nohighlight">\(J_k = \alpha I\)</span> for some scalar <span class="math notranslate nohighlight">\(\alpha &gt; 1\)</span>. This leads to the update</p>
<div class="math notranslate nohighlight">
\[
v_{k+1} = \frac{\alpha - 1}{\alpha} v_k + \frac{1}{\alpha} \mathrm{L}v_k.
\]</div>
<p>This is known as <span class="math notranslate nohighlight">\(\alpha\)</span>-value iteration or successive over-relaxation when <span class="math notranslate nohighlight">\(\alpha &gt; 1\)</span>. For appropriate choices of <span class="math notranslate nohighlight">\(\alpha\)</span>, this method retains global convergence while achieving better local rates than standard value iteration, and it requires only pointwise operations rather than solving a linear system. The Newton perspective thus not only unifies existing algorithms but also generates new ones by systematically exploring different approximations to the generalized Jacobian.</p>
<p>The connection to semismooth Newton methods places policy iteration within a broader mathematical framework that extends far beyond dynamic programming. Semismooth Newton methods are used in optimization (for complementarity problems and variational inequalities), in PDE-constrained optimization (for problems with control constraints), and in economics (for equilibrium problems). The Bellman equation, viewed through this lens, is simply one instance of a piecewise smooth equation, and the tools developed for such equations apply directly.</p>
</section>
<section id="policy-iteration">
<h3>Policy Iteration<a class="headerlink" href="#policy-iteration" title="Link to this heading">#</a></h3>
<p>While we derived policy iteration-like steps from the Newton-Kantorovich method, it’s worth examining policy iteration as a standalone algorithm, as it has been traditionally presented in the field of dynamic programming.</p>
<p>The policy iteration algorithm for discounted Markov decision problems is as follows:</p>
<div class="proof algorithm admonition" id="policy-iteration">
<p class="admonition-title"><span class="caption-number">Algorithm 11 </span> (Policy Iteration)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>
<strong>Output:</strong> Optimal policy <span class="math notranslate nohighlight">\(\pi^*\)</span></p>
<ol class="arabic">
<li><p>Initialize: <span class="math notranslate nohighlight">\(n = 0\)</span>, select an arbitrary decision rule <span class="math notranslate nohighlight">\(\pi_0 \in \Pi^{MD}\)</span></p></li>
<li><p><strong>repeat</strong>
3. (Policy evaluation) Obtain <span class="math notranslate nohighlight">\(\mathbf{v}^n\)</span> by solving:</p>
<div class="math notranslate nohighlight">
\[(\mathbf{I}-\gamma \mathbf{P}_{\pi_n}) \mathbf{v} = \mathbf{r}_{\pi_n}\]</div>
<ol class="arabic" start="4">
<li><p>(Policy improvement) Choose <span class="math notranslate nohighlight">\(\pi_{n+1} = \mathrm{Greedy}(\mathbf{v}^n)\)</span> where:</p>
<div class="math notranslate nohighlight">
\[\pi_{n+1} \in \arg\max_{\pi \in \Pi^{MD}}\left\{\mathbf{r}_\pi+\gamma \mathbf{P}_\pi \mathbf{v}^n\right\}\]</div>
<p>equivalently, for each <span class="math notranslate nohighlight">\(s\)</span>:</p>
<div class="math notranslate nohighlight">
\[\pi_{n+1}(s) \in \arg\max_{a \in \mathcal{A}_s}\left\{r(s,a)+\gamma \sum_j p(j|s,a) \mathbf{v}^n(j)\right\}\]</div>
<p>Set <span class="math notranslate nohighlight">\(\pi_{n+1} = \pi_n\)</span> if possible.</p>
</li>
<li><p>If <span class="math notranslate nohighlight">\(\pi_{n+1} = \pi_n\)</span>, <strong>return</strong> <span class="math notranslate nohighlight">\(\pi^* = \pi_n\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n = n + 1\)</span></p></li>
</ol>
</li>
<li><p><strong>until</strong> convergence</p></li>
</ol>
</section>
</div><p>As opposed to value iteration, this algorithm produces a sequence of both deterministic Markovian decision rules <span class="math notranslate nohighlight">\(\{\pi_n\}\)</span> and value functions <span class="math notranslate nohighlight">\(\{\mathbf{v}^n\}\)</span>. We recognize in this algorithm the linearization step of the Newton-Kantorovich procedure, which takes place here in the policy evaluation step 3 where we solve the linear system <span class="math notranslate nohighlight">\((\mathbf{I}-\gamma \mathbf{P}_{\pi_n}) \mathbf{v} = \mathbf{r}_{\pi_n}\)</span>. In practice, this linear system could be solved either using direct methods (eg. Gaussian elimination), using simple iterative methods such as the successive approximation method for policy evaluation, or more sophisticated methods such as GMRES.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="mpc.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Model Predictive Control</p>
      </div>
    </a>
    <a class="right-next"
       href="regmdp.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Smooth Bellman Optimality Equations</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Dynamic Programming</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-recursion">Backward Recursion</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-optimal-harvest-in-resource-management">Example: Optimal Harvest in Resource Management</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#handling-continuous-spaces-with-interpolation">Handling Continuous Spaces with Interpolation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-recursion-with-interpolation">Backward Recursion with Interpolation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-optimal-harvest-with-linear-interpolation">Example: Optimal Harvest with Linear Interpolation</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-dynamic-programming-and-markov-decision-processes">Stochastic Dynamic Programming and Markov Decision Processes</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-rules-and-policies">Decision Rules and Policies</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-system-dynamics">Stochastic System Dynamics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimality-equations-in-the-stochastic-setting">Optimality Equations in the Stochastic Setting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-stochastic-optimal-harvest-in-resource-management">Example: Stochastic Optimal Harvest in Resource Management</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-quadratic-regulator-via-dynamic-programming">Linear Quadratic Regulator via Dynamic Programming</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-decision-process-formulation">Markov Decision Process Formulation</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#notation-in-operations-reseach">Notation in Operations Reseach</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-an-optimal-policy">What is an Optimal Policy?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-sample-size-determination-in-pharmaceutical-development">Example: Sample Size Determination in Pharmaceutical Development</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#infinite-horizon-mdps">Infinite-Horizon MDPs</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-horizon-interpretation-of-discounting">Random Horizon Interpretation of Discounting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-representation-in-markov-decision-processes">Vector Representation in Markov Decision Processes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-operator-equations">Solving Operator Equations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#successive-approximation-method">Successive Approximation Method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#newton-kantorovich-method">Newton-Kantorovich Method</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimality-equations-for-infinite-horizon-mdps">Optimality Equations for Infinite-Horizon MDPs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#algorithms-for-solving-the-optimality-equations">Algorithms for Solving the Optimality Equations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#newton-kantorovich-applied-to-bellman-optimality">Newton-Kantorovich Applied to Bellman Optimality</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#perspective-1-max-of-affine-maps">Perspective 1: Max of Affine Maps</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#perspective-2-envelope-theorem">Perspective 2: Envelope Theorem</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#perspective-3-the-implicit-function-theorem">Perspective 3: The Implicit Function Theorem</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-policy-iteration">Connection to Policy Iteration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-semismooth-newton-perspective">The Semismooth Newton Perspective</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-iteration">Policy Iteration</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Pierre-Luc Bacon
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>