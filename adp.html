
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Approximate Dynamic Programming &#8212; Practical Reinforcement Learning: From Algorithms to Applications</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/graphviz.css?v=fd3f3429" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"bm": ["{\\boldsymbol #1}", 1]}, "processEscapes": true}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'adp';</script>
    <script src="_static/iframe-modal.js?v=f72a1242"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Policy Parametrization Methods" href="cadp.html" />
    <link rel="prev" title="Dynamic Programming" href="dp.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Practical Reinforcement Learning: From Algorithms to Applications</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Why This Book?
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Modeling</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="modeling.html">Why Build a Model? For Whom?</a></li>

<li class="toctree-l1"><a class="reference internal" href="ssm.html">Dynamics Models for Decision Making</a></li>




<li class="toctree-l1"><a class="reference internal" href="simulation.html">Programs as Models</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Numerical Trajectory Optimization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ocp.html">Discrete-Time Trajectory Optimization</a></li>


<li class="toctree-l1"><a class="reference internal" href="cocp.html">Trajectory Optimization in Continuous Time</a></li>




</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">From Trajectories to Policies</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="mpc.html">Model Predictive Control</a></li>


<li class="toctree-l1"><a class="reference internal" href="dp.html">Dynamic Programming</a></li>



</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Learning from Data</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Approximate Dynamic Programming</a></li>





<li class="toctree-l1"><a class="reference internal" href="cadp.html">Policy Parametrization Methods</a></li>







</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="appendix_examples.html">Example COCPs</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix_ivps.html">Solving Initial Value Problems</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix_nlp.html">Nonlinear Programming</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/pierrelux/rlbook" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pierrelux/rlbook/edit/main/adp.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pierrelux/rlbook/issues/new?title=Issue%20on%20page%20%2Fadp.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/adp.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Approximate Dynamic Programming</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Approximate Dynamic Programming</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#smooth-optimality-equations-for-infinite-horizon-mdps">Smooth Optimality Equations for Infinite-Horizon MDPs</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gumbel-noise-on-the-rewards">Gumbel Noise on the Rewards</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#control-as-inference-perspective">Control as Inference Perspective</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#message-passing">Message Passing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deriving-the-optimal-policy">Deriving the Optimal Policy</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regularized-markov-decision-processes">Regularized Markov Decision Processes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#legendre-fenchel-transform">Legendre-Fenchel Transform</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regularized-bellman-operators">Regularized Bellman Operators</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recovering-the-smooth-bellman-equations">Recovering the Smooth Bellman Equations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#approximating-the-bellman-operator-using-numerical-integration">Approximating the Bellman Operator using Numerical Integration</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#discretization-and-numerical-quadrature">Discretization and Numerical Quadrature</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#monte-carlo-integration">Monte Carlo Integration</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overestimation-bias-in-monte-carlo-value-iteration">Overestimation Bias in Monte Carlo Value Iteration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-keane-wolpin-bias-correction-algorithm">The Keane-Wolpin Bias Correction Algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decoupling-selection-and-evaluation">Decoupling Selection and Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#an-hvac-analogy">An HVAC analogy</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#consistency">Consistency</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#parametric-dynamic-programming">Parametric Dynamic Programming</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#partial-updates-in-the-tabular-case">Partial Updates in the Tabular Case</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#partial-updates-by-operator-fitting-parametric-value-iteration">Partial Updates by Operator Fitting: Parametric Value Iteration</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parametric-policy-iteration">Parametric Policy Iteration</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#q-factor-representation">Q-Factor Representation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#warmstarting-the-choice-of-initialization">Warmstarting: The Choice of Initialization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inner-optimization-fit-to-convergence-or-not">Inner Optimization: Fit to Convergence or Not?</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#example-methods">Example Methods</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-based-reinforcement-learning-2002">Kernel-Based Reinforcement Learning (2002)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ernst-s-fitted-q-iteration-2005">Ernst’s Fitted Q Iteration (2005)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-fitted-q-iteration-2005">Neural Fitted Q Iteration (2005)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-q-networks-2013">Deep Q Networks (2013)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warmstarting-and-partial-fitting">Warmstarting and Partial Fitting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#flattening-the-updates-with-target-swapping">Flattening the Updates with Target Swapping</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exponential-moving-average-targets">Exponential Moving Average Targets</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#online-data-collection-and-experience-replay">Online Data Collection and Experience Replay</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#double-q-network-variant">Double-Q Network Variant</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-q-networks-with-resets-2022">Deep Q Networks with Resets (2022)</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#does-parametric-dynamic-programming-converge">Does Parametric Dynamic Programming Converge?</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-search-for-nonexpansive-operators">The Search for Nonexpansive Operators</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gordon-s-averagers">Gordon’s Averagers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#which-function-approximators-interpolate-vs-extrapolate">Which Function Approximators Interpolate vs Extrapolate?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#k-nearest-neighbors-knn">K-nearest neighbors (KNN)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression">Linear Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#spline-interpolation">Spline Interpolation</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="approximate-dynamic-programming">
<h1>Approximate Dynamic Programming<a class="headerlink" href="#approximate-dynamic-programming" title="Link to this heading">#</a></h1>
<p>Dynamic programming methods suffer from the curse of dimensionality and can quickly become difficult to apply in practice. Not only this, we may also be dealing with large or continuous state or action spaces. We have seen so far that we could address this problem using discretization, or interpolation. These were already examples of approximate dynamic programming. In this chapter, we will see other forms of approximations meant to facilitate the optimization problem, either by approximating the optimality equations, the value function, or the policy itself.
Approximation theory is at the heart of learning methods, and fundamentally, this chapter will be about the application of learning ideas to solve complex decision-making problems.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="smooth-optimality-equations-for-infinite-horizon-mdps">
<h1>Smooth Optimality Equations for Infinite-Horizon MDPs<a class="headerlink" href="#smooth-optimality-equations-for-infinite-horizon-mdps" title="Link to this heading">#</a></h1>
<p>While the standard Bellman optimality equations use the max operator to determine the best action, an alternative formulation known as the smooth or soft Bellman optimality equations replaces this with a softmax operator. This approach originated from <span id="id1">[<a class="reference internal" href="bibliography.html#id24" title="John Rust. Optimal replacement of gmc bus engines: an empirical model of harold zurcher. Econometrica, 55(5):999-1033, 1987.">36</a>]</span> and was later rediscovered in the context of maximum entropy inverse reinforcement learning <span id="id2">[<a class="reference internal" href="bibliography.html#id25" title="Brian D. Ziebart, Andrew L. Maas, J. Andrew Bagnell, and Anind K. Dey. Maximum entropy inverse reinforcement learning. In Proceedings of the 23rd AAAI Conference on Artificial Intelligence, 1433-1438. 2008.">41</a>]</span>, which then led to soft Q-learning <span id="id3">[<a class="reference internal" href="bibliography.html#id26" title="Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. Proceedings of the 34th International Conference on Machine Learning, 70:1352-1361, 2017.">21</a>]</span> and soft actor-critic <span id="id4">[<a class="reference internal" href="bibliography.html#id30" title="Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the 35th International Conference on Machine Learning (ICML), 1861–1870. PMLR, 2018.">22</a>]</span>, a state-of-the-art deep reinforcement learning algorithm.</p>
<p>In the infinite-horizon setting, the smooth Bellman optimality equations take the form:</p>
<div class="math notranslate nohighlight">
\[ v_\gamma^\star(s) = \frac{1}{\beta} \log \sum_{a \in A_s} \exp\left(\beta\left(r(s, a) + \gamma \sum_{j \in S} p(j | s, a) v_\gamma^\star(j)\right)\right) \]</div>
<p>Adopting an operator-theoretic perspective, we can define a nonlinear operator <span class="math notranslate nohighlight">\(\mathrm{L}_\beta\)</span> such that the smooth value function of an MDP is then the solution to the following fixed-point equation:</p>
<div class="math notranslate nohighlight">
\[ (\mathrm{L}_\beta \mathbf{v})(s) = \frac{1}{\beta} \log \sum_{a \in \mathcal{A}_s} \exp\left(\beta\left(r(s,a) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a) v(j)\right)\right) \]</div>
<p>As <span class="math notranslate nohighlight">\(\beta \to \infty\)</span>, <span class="math notranslate nohighlight">\(\mathrm{L}_\beta\)</span> converges to the standard Bellman operator <span class="math notranslate nohighlight">\(\mathrm{L}\)</span>. Furthermore, it can be shown that the smooth Bellman operator is a contraction mapping in the supremum norm, and therefore has a unique fixed point. However, as opposed to the usual “hard” setting, the fixed point of <span class="math notranslate nohighlight">\(\mathrm{L}_\beta\)</span> is associated with the value function of an optimal stochastic policy defined by the softmax distribution:</p>
<div class="math notranslate nohighlight">
\[ d(a|s) = \frac{\exp\left(\beta\left(r(s,a) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a) v_\gamma^\star(j)\right)\right)}{\sum_{a' \in \mathcal{A}_s} \exp\left(\beta\left(r(s,a') + \gamma \sum_{j \in \mathcal{S}} p(j|s,a') v_\gamma^\star(j)\right)\right)} \]</div>
<p>Despite the confusing terminology, the above “softmax” policy is simply the smooth counterpart to the argmax operator in the original optimality equation: it acts as a soft-argmax.</p>
<p>This formulation is interesting for several reasons. First, smoothness is a desirable property from an optimization standpoint. Unlike <span class="math notranslate nohighlight">\(\gamma\)</span>, we view <span class="math notranslate nohighlight">\(\beta\)</span> as a hyperparameter of our algorithm, which we can control to achieve the desired level of accuracy.</p>
<p>Second, while presented from an intuitive standpoint where we replace the max by the log-sum-exp (a smooth maximum) and the argmax by the softmax (a smooth argmax), this formulation can also be obtained from various other perspectives, offering theoretical tools and solution methods. For example, <span id="id5">Rust [<a class="reference internal" href="bibliography.html#id24" title="John Rust. Optimal replacement of gmc bus engines: an empirical model of harold zurcher. Econometrica, 55(5):999-1033, 1987.">36</a>]</span> derived this algorithm by considering a setting in which the rewards are stochastic and perturbed by a Gumbel noise variable. When considering the corresponding augmented state space and integrating the noise, we obtain smooth equations. This interpretation is leveraged by Rust for modeling purposes.</p>
<p>There is also a way to obtain this equation by starting from the energy-based formulation often used in supervised learning, in which we convert an unnormalized probability distribution into a distribution using the softmax transformation. This is essentially what <span id="id6">Ziebart <em>et al.</em> [<a class="reference internal" href="bibliography.html#id25" title="Brian D. Ziebart, Andrew L. Maas, J. Andrew Bagnell, and Anind K. Dey. Maximum entropy inverse reinforcement learning. In Proceedings of the 23rd AAAI Conference on Artificial Intelligence, 1433-1438. 2008.">41</a>]</span> did in their paper. Furthermore, this perspective bridges with the literature on probabilistic graphical models, in which we can now cast the problem of finding an optimal smooth policy into one of maximum likelihood estimation (an inference problem). This is the idea of control as inference, which also admits the converse - that of inference as control - used nowadays for deriving fast samples and amortized inference techniques using reinforcement learning <span id="id7">[<a class="reference internal" href="bibliography.html#id27" title="Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Reinforcement learning as a framework for control: a survey. arXiv preprint arXiv:1806.04222, 2018.">27</a>]</span>.</p>
<p>Finally, it’s worth noting that we can also derive this form by considering an entropy-regularized formulation in which we penalize for the entropy of our policy in the reward function term. This formulation admits a solution that coincides with the smooth Bellman equations <span id="id8">[<a class="reference internal" href="bibliography.html#id26" title="Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. Proceedings of the 34th International Conference on Machine Learning, 70:1352-1361, 2017.">21</a>]</span>.</p>
<section id="gumbel-noise-on-the-rewards">
<h2>Gumbel Noise on the Rewards<a class="headerlink" href="#gumbel-noise-on-the-rewards" title="Link to this heading">#</a></h2>
<p>We can obtain the smooth Bellman equation by considering a setting in which we have Gumbel noise added to the reward function. More precisely, we define an MDP whose state space is now that of <span class="math notranslate nohighlight">\(\tilde{s} = (s, \epsilon)\)</span>, where the reward function is given by</p>
<div class="math notranslate nohighlight">
\[\tilde{r}(\tilde{s}, a) = r(s,a) + \epsilon(a)\]</div>
<p>and where the transition probability function is:</p>
<div class="math notranslate nohighlight">
\[ p(\tilde{s}' | \tilde{s}, a) = p(s' | s, a) \cdot p(\epsilon') \]</div>
<p>This expression stems from the conditional independence assumption that we make on the noise variable given the state.</p>
<p>Furthermore, we assume that <span class="math notranslate nohighlight">\(\epsilon(a)\)</span> is a random variable following a Gumbel distribution with location 0 and scale <span class="math notranslate nohighlight">\(1/\beta\)</span>. The Gumbel distribution is a continuous probability distribution used to model the maximum (or minimum) of a number of samples of various distributions. Its probability density function is:</p>
<div class="math notranslate nohighlight">
\[ f(x; \mu, \beta) = \frac{1}{\beta}\exp\left(-\left(\frac{x-\mu}{\beta}+\exp\left(-\frac{x-\mu}{\beta}\right)\right)\right) \]</div>
<p>where <span class="math notranslate nohighlight">\(\mu\)</span> is the location parameter and <span class="math notranslate nohighlight">\(\beta\)</span> is the scale parameter. To generate a Gumbel-distributed random variable, one can use the inverse transform sampling method: and set <span class="math notranslate nohighlight">\( X = \mu - \beta \ln(-\ln(U)) \)</span>
where <span class="math notranslate nohighlight">\(U\)</span> is a uniform random variable on the interval <span class="math notranslate nohighlight">\((0,1)\)</span>.</p>
<p>The Bellman equation in this augmented state space becomes:</p>
<div class="math notranslate nohighlight">
\[ v_\gamma^\star(\tilde{s}) = \max_{a \in \mathcal{A}_s} \left\{ \tilde{r}(\tilde{s},a) + \gamma \mathbb{E}_{}\left[v_\gamma^\star(\tilde{s}')\mid \tilde{s}, a\right] \right\} \]</div>
<p>Furthermore, since all we did is to define another MDP, we still have a contraction and an optimal stationary policy <span class="math notranslate nohighlight">\(d^\infty = (d, d, ...)\)</span> can be found via the following deterministic Markovian decision rule:</p>
<div class="math notranslate nohighlight">
\[
d(\tilde{s})  \in \operatorname{argmax}_{a \in \mathcal{A}_s} \left\{ \tilde{r}(\tilde{s},a) + \gamma \mathbb{E}_{}\left[v_\gamma^\star(\tilde{s}')\mid \tilde{s}, a\right] \right\}
\]</div>
<p>Note how the expectation is now over the next augmented state space and is therefore both over the next state in the original MDP and over the next perturbation. While in the general case there isn’t much that we can do to simplify the expression for the expectation over the next state in the MDP, we can however leverage a remarkable property of the Gumbel distribution which allows us to eliminate the <span class="math notranslate nohighlight">\(\epsilon\)</span> term in the above and recover the familiar smooth Bellman equation.</p>
<p>For a set of random variables <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span>, each following a Gumbel distribution with location parameters <span class="math notranslate nohighlight">\(\mu_1, \ldots, \mu_n\)</span> and scale parameter <span class="math notranslate nohighlight">\(1/\beta\)</span>, extreme value theory tells us that:</p>
<div class="math notranslate nohighlight">
\[ \mathbb{E}\left[\max_{i} X_i\right] = \frac{1}{\beta} \log \sum_{i=1}^n \exp(\beta\mu_i) \]</div>
<p>In our case, each <span class="math notranslate nohighlight">\(X_i\)</span> corresponds to <span class="math notranslate nohighlight">\(r(s,a_i) + \epsilon(a_i) + \gamma \mathbb{E}_{s'}[v(s')]\)</span> for a given action <span class="math notranslate nohighlight">\(a_i\)</span>. The location parameter <span class="math notranslate nohighlight">\(\mu_i\)</span> is <span class="math notranslate nohighlight">\(r(s,a_i) + \gamma \mathbb{E}_{s'}[v(s')]\)</span>, and the scale parameter is <span class="math notranslate nohighlight">\(1/\beta\)</span>.</p>
<p>Applying this result to our problem, and taking the expectation over the noise <span class="math notranslate nohighlight">\(\epsilon\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{align*}
v_\gamma^\star(s,\epsilon) &amp;= \max_{a \in \mathcal{A}_s} \left\{ r(s,a) + \epsilon(a) + \gamma \mathbb{E}_{s', \epsilon'}\left[v_\gamma^\star(s',\epsilon')\mid s, \epsilon, a\right] \right\} \\
\mathbb{E}_\epsilon[v_\gamma^\star(s,\epsilon)] &amp;= \mathbb{E}_\epsilon\left[\max_{a \in \mathcal{A}_s} \left\{ r(s,a) + \epsilon(a) + \gamma \mathbb{E}_{s', \epsilon'}\left[v_\gamma^\star(s',\epsilon')\mid s, \epsilon, a\right] \right\}\right] \\
&amp;= \frac{1}{\beta} \log \sum_{a \in \mathcal{A}_s} \exp\left(\beta\left(r(s,a) + \gamma \mathbb{E}_{s', \epsilon'}\left[v_\gamma^\star(s',\epsilon')\mid s, a\right]\right)\right) \\
&amp;= \frac{1}{\beta} \log \sum_{a \in \mathcal{A}_s} \exp\left(\beta\left(r(s,a) + \gamma \mathbb{E}_{s'}\left[\mathbb{E}_{\epsilon'}[v_\gamma^\star(s',\epsilon')]\mid s, a\right]\right)\right)
\end{align*}
\end{split}\]</div>
<p>If we define <span class="math notranslate nohighlight">\(v_\gamma^\star(s) = \mathbb{E}_\epsilon[v_\gamma^\star(s,\epsilon)]\)</span>, we obtain the smooth Bellman equation:</p>
<div class="math notranslate nohighlight">
\[ v_\gamma^\star(s) = \frac{1}{\beta} \log \sum_{a \in \mathcal{A}_s} \exp\left(\beta\left(r(s,a) + \gamma \mathbb{E}_{s'}\left[v_\gamma^\star(s')\mid s, a\right]\right)\right) \]</div>
<p>This final equation is the smooth Bellman equation, which we derived by introducing Gumbel noise to the reward function and leveraging properties of the Gumbel distribution and extreme value theory.</p>
<p>Now, in the same way that we have been able to simplify and specialize the form of the value function under Gumbel noise, we can also derive an expression for the corresponding optimal policy. To see this, we apply similar steps and start with the optimal decision rule for the augmented MDP:</p>
<div class="math notranslate nohighlight">
\[
d(\tilde{s}) \in \operatorname{argmax}_{a \in \mathcal{A}_s} \left\{ \tilde{r}(\tilde{s},a) + \gamma \mathbb{E}_{}\left[v_\gamma^\star(\tilde{s}') \mid \tilde{s}, a\right] \right\}
\]</div>
<p>In order to simplify this expression by taking the expectation over the noise variable, we define an indicator function for the event that action <span class="math notranslate nohighlight">\(a\)</span> is in the set of optimal actions:</p>
<div class="math notranslate nohighlight">
\[\begin{split} I_a(\epsilon) = \begin{cases} 
   1 &amp; \text{if } a \in \operatorname{argmax}_{a' \in \mathcal{A}_s} \left\{ r(s,a') + \epsilon(a') + \gamma \mathbb{E}_{s', \epsilon'}\left[v_\gamma^\star(s',\epsilon')\mid s, a'\right] \right\} \\
   0 &amp; \text{otherwise}
   \end{cases} \end{split}\]</div>
<p>Note that this definition allows us to recover the original expression since:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
d(s,\epsilon) &amp;= \left\{a \in \mathcal{A}_s : I_a(\epsilon) = 1\right\} \\
&amp;= \left\{a \in \mathcal{A}_s : a \in \operatorname{argmax}_{a' \in \mathcal{A}_s} \left\{ r(s,a') + \epsilon(a') + \gamma \mathbb{E}_{s', \epsilon'}\left[v_\gamma^\star(s',\epsilon')\mid s, \epsilon,  a'\right] \right\}\right\}
\end{align*}
\end{split}\]</div>
<p>This set-valued – but determinisic – function <span class="math notranslate nohighlight">\(d(s,\epsilon)\)</span> gives us the set of optimal actions for a given state <span class="math notranslate nohighlight">\(s\)</span> and noise realization <span class="math notranslate nohighlight">\(\epsilon\)</span>. For simplicity, consider the case where the optimal set of actions at <span class="math notranslate nohighlight">\(s\)</span> is a singleton such that taking the expection over the noise variable gives us:
$<span class="math notranslate nohighlight">\( \begin{align*}
\mathbb{E}_\epsilon[I_a(\epsilon)] = \mathbb{P}\left(a \in \operatorname{argmax}_{a' \in \mathcal{A}_s} \left\{ r(s,a') + \epsilon(a') + \gamma \mathbb{E}_{s', \epsilon'}\left[v_\gamma^\star(s',\epsilon')\mid s, \epsilon, a'\right] \right\}\right)
\end{align*} \)</span>$</p>
<p>Now, we can leverage a key property of the Gumbel distribution. For a set of random variables <span class="math notranslate nohighlight">\(\{X_i = \mu_i + \epsilon_i\}\)</span> where <span class="math notranslate nohighlight">\(\epsilon_i\)</span> are i.i.d. Gumbel(0, 1/β) random variables, we have:</p>
<div class="math notranslate nohighlight">
\[ P(X_i \geq X_j, \forall j \neq i) = \frac{\exp(\beta\mu_i)}{\sum_j \exp(\beta\mu_j)} \]</div>
<p>In our case, <span class="math notranslate nohighlight">\(X_a = r(s,a) + \epsilon(a) + \gamma \mathbb{E}_{s', \epsilon'}\left[v_\gamma^\star(s',\epsilon')\mid s, a\right]\)</span> for each action <span class="math notranslate nohighlight">\(a\)</span>, with <span class="math notranslate nohighlight">\(\mu_a = r(s,a) + \gamma \mathbb{E}_{s', \epsilon'}\left[v_\gamma^\star(s',\epsilon')\mid s, a\right]\)</span>.</p>
<p>Applying this property and using the definition <span class="math notranslate nohighlight">\(v_\gamma^\star(s) = \mathbb{E}_\epsilon[v_\gamma^\star(s,\epsilon)]\)</span>, we get:</p>
<div class="math notranslate nohighlight">
\[ d(a|s) = \frac{\exp\left(\beta\left(r(s,a) + \gamma \mathbb{E}_{s'}\left[v_\gamma^\star(s')\mid s, a\right]\right)\right)}{\sum_{a' \in \mathcal{A}_s} \exp\left(\beta\left(r(s,a') + \gamma \mathbb{E}_{s'}\left[v_\gamma^\star(s')\mid s, a'\right]\right)\right)} \]</div>
<p>This gives us the optimal stochastic policy for the smooth MDP. Note that as <span class="math notranslate nohighlight">\(\beta \to \infty\)</span>, this policy approaches the deterministic policy of the original MDP, while for finite <span class="math notranslate nohighlight">\(\beta\)</span>, it gives a stochastic policy.</p>
</section>
<section id="control-as-inference-perspective">
<h2>Control as Inference Perspective<a class="headerlink" href="#control-as-inference-perspective" title="Link to this heading">#</a></h2>
<p>The smooth Bellman optimality equations can also be derived from probabilistic inference perspective. To see this, let’s go back to the idea from the previous section in which we introduced an indicator function <span class="math notranslate nohighlight">\(I_a(\epsilon)\)</span> to represent whether an action <span class="math notranslate nohighlight">\(a\)</span> is optimal given a particular realization of the noise <span class="math notranslate nohighlight">\(\epsilon\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split} I_a(\epsilon) = \begin{cases} 
   1 &amp; \text{if } a \in \operatorname{argmax}_{a' \in \mathcal{A}_s} \left\{ r(s,a') + \epsilon(a') + \gamma \mathbb{E}_{s', \epsilon'}\left[v_\gamma^\star(s',\epsilon')\mid s, a'\right] \right\} \\
   0 &amp; \text{otherwise}
   \end{cases} \end{split}\]</div>
<p>When we took the expectation over the noise <span class="math notranslate nohighlight">\(\epsilon\)</span>, we obtained a soft version of this indicator:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{align*}
\mathbb{E}_\epsilon[I_a(\epsilon)] &amp;= \mathbb{P}\left(a \in \operatorname{argmax}_{a' \in \mathcal{A}_s} \left\{ r(s,a') + \epsilon(a') + \gamma \mathbb{E}_{s', \epsilon'}\left[v_\gamma^\star(s',\epsilon')\mid s, \epsilon, a'\right] \right\}\right) \\
&amp;= \frac{\exp\left(\beta\left(r(s,a) + \gamma \mathbb{E}_{s'}\left[v_\gamma^\star(s')\mid s, a\right]\right)\right)}{\sum_{a' \in \mathcal{A}_s} \exp\left(\beta\left(r(s,a') + \gamma \mathbb{E}_{s'}\left[v_\gamma^\star(s')\mid s, a'\right]\right)\right)}
\end{align*} \end{split}\]</div>
<p>Given this indicator function, we can “infer” the optimal action in any state. This is the intuition and starting point behind the control as inference perspective in which we directly define a continuous-valued “optimality” variable <span class="math notranslate nohighlight">\(O_t\)</span> at each time step <span class="math notranslate nohighlight">\(t\)</span>. We define the probability of optimality given a state-action pair as:</p>
<div class="math notranslate nohighlight">
\[ p(O_t = 1 | s_t, a_t) = \exp(\beta r(s_t, a_t)) \]</div>
<p>Building on this notion of soft optimality, we can formulate the MDP as a probabilistic graphical model. We define the following probabilities:</p>
<ol class="arabic simple">
<li><p>State transition probability: <span class="math notranslate nohighlight">\(p(s_{t+1} | s_t, a_t)\)</span> (given by the MDP dynamics)</p></li>
<li><p>Prior policy: <span class="math notranslate nohighlight">\(p(a_t | s_t)\)</span> (which we’ll assume to be uniform for simplicity)</p></li>
<li><p>Optimality probability: <span class="math notranslate nohighlight">\(p(O_t = 1 | s_t, a_t) = \exp(\beta r(s_t, a_t))\)</span></p></li>
</ol>
<p>This formulation encodes the idea that more rewarding state-action pairs are more likely to be “optimal,” which directly parallels the soft assignment of optimality we obtained by taking the expectation over the Gumbel noise.</p>
<p>The control problem can now be framed as an inference problem: we want to find the posterior distribution over actions given that all time steps are optimal:</p>
<div class="math notranslate nohighlight">
\[ p(a_t | s_t, O_{1:T} = 1) \]</div>
<p>where <span class="math notranslate nohighlight">\(O_{1:T} = 1\)</span> means <span class="math notranslate nohighlight">\(O_t = 1\)</span> for all <span class="math notranslate nohighlight">\(t\)</span> from 1 to T.</p>
<section id="message-passing">
<h3>Message Passing<a class="headerlink" href="#message-passing" title="Link to this heading">#</a></h3>
<p>To solve this inference problem, we can use a technique from probabilistic graphical models called message passing, specifically the belief propagation algorithm. Message passing is a way to efficiently compute marginal distributions in a graphical model by passing local messages between nodes. Messages are passed between nodes in both forward and backward directions. Each message represents a belief about the distribution of a variable, based on the information available to the sending node. After messages have been passed, each node updates its belief about its associated variable by combining all incoming messages.</p>
<p>In our specific case, we’re particularly interested in the backward messages, which propagate information about future optimality backwards in time. Let’s define the backward message <span class="math notranslate nohighlight">\(\beta_t(s_t)\)</span> as:</p>
<div class="math notranslate nohighlight">
\[ \beta_t(s_t) = p(O_{t:T} = 1 | s_t) \]</div>
<p>This represents the probability of optimality for all future time steps given the current state. We can compute this recursively:</p>
<div class="math notranslate nohighlight">
\[ \beta_t(s_t) = \sum_{a_t} p(a_t | s_t) p(O_t = 1 | s_t, a_t) \sum_{s_{t+1}} p(s_{t+1} | s_t, a_t) \beta_{t+1}(s_{t+1}) \]</div>
<p>Taking the log and assuming a uniform prior over actions, we get:</p>
<div class="math notranslate nohighlight">
\[ \log \beta_t(s_t) = \log \sum_{a_t} \sum_{s_{t+1}} p(s_{t+1} | s_t, a_t) \exp(\beta (r(s_t, a_t) + \gamma v(_{t+1}) + \frac{1}{\beta} \log \beta_{t+1}(s_{t+1}))) \]</div>
<p>If we define the soft value function as <span class="math notranslate nohighlight">\(V_t(s_t) = \frac{1}{\beta} \log \beta_t(s_t)\)</span>, we can rewrite the above equation as:</p>
<div class="math notranslate nohighlight">
\[ V_t(s_t) = \frac{1}{\beta} \log \sum_{a_t} \sum_{s_{t+1}} p(s_{t+1} | s_t, a_t) \exp(\beta (r(s_t, a_t) + \gamma V_{t+1}(s_{t+1}))) \]</div>
<p>This is exactly the smooth Bellman equation we derived earlier, but now interpreted as the result of probabilistic inference in a graphical model.</p>
</section>
<section id="deriving-the-optimal-policy">
<h3>Deriving the Optimal Policy<a class="headerlink" href="#deriving-the-optimal-policy" title="Link to this heading">#</a></h3>
<p>The backward message recursion we derived earlier assumes a uniform prior policy <span class="math notranslate nohighlight">\(p(a_t | s_t)\)</span>. However, our goal is to find not just any policy, but an optimal one. We can extract this optimal policy efficiently by computing the posterior distribution over actions given our backward messages.</p>
<p>Starting from the definition of conditional probability and applying Bayes’ rule, we can write:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{align}
p(a_t | s_t, O_{1:T} = 1) &amp;= \frac{p(O_{1:T} = 1 | s_t, a_t) p(a_t | s_t)}{p(O_{1:T} = 1 | s_t)} \\
&amp;\propto p(a_t | s_t) p(O_t = 1 | s_t, a_t) p(O_{t+1:T} = 1 | s_t, a_t) \\
&amp;= p(a_t | s_t) p(O_t = 1 | s_t, a_t) \sum_{s_{t+1}} p(s_{t+1} | s_t, a_t) \beta_{t+1}(s_{t+1})
\end{align} \end{split}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\beta_{t+1}(s_{t+1}) = p(O_{t+1:T} = 1 | s_{t+1})\)</span> is our backward message.</p>
<p>Now, let’s substitute our definitions for the optimality probability and the soft value function:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{align}
p(a_t | s_t, O_{1:T} = 1) &amp;\propto p(a_t | s_t) \exp(\beta r(s_t, a_t)) \sum_{s_{t+1}} p(s_{t+1} | s_t, a_t) \exp(\beta \gamma V_{t+1}(s_{t+1})) \\
&amp;= p(a_t | s_t) \exp(\beta (r(s_t, a_t) + \gamma \sum_{s_{t+1}} p(s_{t+1} | s_t, a_t) V_{t+1}(s_{t+1})))
\end{align} \end{split}\]</div>
<p>After normalization, and assuming a uniform prior <span class="math notranslate nohighlight">\(p(a_t | s_t)\)</span>, we obtain the randomized decision rule:</p>
<div class="math notranslate nohighlight">
\[ d(a_t | s_t) = \frac{\exp(\beta (r(s_t, a_t) + \gamma \sum_{s_{t+1}} p(s_{t+1} | s_t, a_t) V_{t+1}(s_{t+1})))}{\sum_{a'_t} \exp(\beta (r(s_t, a'_t) + \gamma \sum_{s_{t+1}} p(s_{t+1} | s_t, a'_t) V_{t+1}(s_{t+1})))} \]</div>
</section>
</section>
<section id="regularized-markov-decision-processes">
<h2>Regularized Markov Decision Processes<a class="headerlink" href="#regularized-markov-decision-processes" title="Link to this heading">#</a></h2>
<p>Regularized MDPs <span id="id9">[<a class="reference internal" href="bibliography.html#id28" title="Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized Markov decision processes. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, 2160–2169. PMLR, 09–15 Jun 2019. URL: https://proceedings.mlr.press/v97/geist19a.html.">15</a>]</span> provide another perspective on how the smooth Bellman equations come to be. This framework offers a more general approach in which we seek to find optimal policies under the infinite horizon criterion while also accounting for a regularizer that influences the kind of policies we try to obtain.</p>
<p>Let’s set up some necessary notation. First, recall that the policy evaluation operator for a stationary policy with decision rule <span class="math notranslate nohighlight">\(d\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[ L_d v = r_d + \gamma P_d v \]</div>
<p>where <span class="math notranslate nohighlight">\(r_d\)</span> is the expected reward under policy <span class="math notranslate nohighlight">\(d\)</span>, <span class="math notranslate nohighlight">\(\gamma\)</span> is the discount factor, and <span class="math notranslate nohighlight">\(P_d\)</span> is the state transition probability matrix under <span class="math notranslate nohighlight">\(d\)</span>. A complementary object to the value function is the q-function (or Q-factor) representation:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{align*}
q_\gamma^{d^\infty}(s, a) &amp;= r(s, a) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a) v_\gamma^{d^\infty}(j) \\
v_\gamma^{d^\infty}(s) &amp;= \sum_{a \in \mathcal{A}_s} d(a | s) q_\gamma^{d^\infty}(s, a) 
\end{align*} \end{split}\]</div>
<p>The policy evaluation operator can then be written in terms of the q-function as:</p>
<div class="math notranslate nohighlight">
\[ [L_d v](s) = \langle d(\cdot | s), q(s, \cdot) \rangle \]</div>
<section id="legendre-fenchel-transform">
<h3>Legendre-Fenchel Transform<a class="headerlink" href="#legendre-fenchel-transform" title="Link to this heading">#</a></h3>
<p>A key concept in the theory of regularized MDPs is the Legendre-Fenchel transform, also known as the convex conjugate. For a strongly convex function <span class="math notranslate nohighlight">\(\Omega: \Delta_{\mathcal{A}} \rightarrow \mathbb{R}\)</span>, its Legendre-Fenchel transform <span class="math notranslate nohighlight">\(\Omega^*: \mathbb{R}^{\mathcal{A}} \rightarrow \mathbb{R}\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[ \Omega^*(q(s, \cdot)) = \max_{d(\cdot|s) \in \Delta_{\mathcal{A}}} \langle d(\cdot | s), q(s, \cdot) \rangle - \Omega(d(\cdot | s)) \]</div>
<p>An important property of this transform is that it has a unique maximizing argument, given by the gradient of <span class="math notranslate nohighlight">\(\Omega^*\)</span>. This gradient is Lipschitz and satisfies:</p>
<div class="math notranslate nohighlight">
\[ \nabla \Omega^*(q(s, \cdot)) = \arg\max_d \langle d(\cdot | s), q(s, \cdot) \rangle - \Omega(d(\cdot | s)) \]</div>
<p>An important example of a regularizer is the negative entropy, which gives rise to the smooth Bellman equations as we are about to see.</p>
</section>
</section>
<section id="regularized-bellman-operators">
<h2>Regularized Bellman Operators<a class="headerlink" href="#regularized-bellman-operators" title="Link to this heading">#</a></h2>
<p>With these concepts in place, we can now define the regularized Bellman operators:</p>
<ol class="arabic">
<li><p><strong>Regularized Policy Evaluation Operator</strong> <span class="math notranslate nohighlight">\((L_{d,\Omega})\)</span>:</p>
<div class="math notranslate nohighlight">
\[ [L_{d,\Omega} v](s) = \langle q(s,\cdot), d(\cdot | s) \rangle - \Omega(d(\cdot | s)) \]</div>
</li>
<li><p><strong>Regularized Bellman Optimality Operator</strong> <span class="math notranslate nohighlight">\((L_\Omega)\)</span>:</p>
<div class="math notranslate nohighlight">
\[ [L_\Omega v](s) = [\max_d L_{d,\Omega} v ](s) = \Omega^*(q(s, \cdot)) \]</div>
</li>
</ol>
<p>It can be shown that the addition of a regularizer in these regularized operators still preserves the contraction properties, and therefore the existence of a solution to the optimality equations and the convergence of successive approximation.</p>
<p>The regularized value function of a stationary policy with decision rule <span class="math notranslate nohighlight">\(d\)</span>, denoted by <span class="math notranslate nohighlight">\(v_{d,\Omega}\)</span>, is the unique fixed point of the operator equation:</p>
<div class="math notranslate nohighlight">
\[\text{find $v$ such that } \enspace v = L_{d,\Omega} v\]</div>
<p>Under the usual assumptions on the discount factor and the boundedness of the reward, the value of a policy can also be found in closed form by solving for <span class="math notranslate nohighlight">\(v\)</span> in the linear system of equations:</p>
<div class="math notranslate nohighlight">
\[ (I - \gamma P_d) v =  (r_d - \Omega(d)) \]</div>
<p>The associated state-action value function <span class="math notranslate nohighlight">\(q_{d,\Omega}\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
q_{d,\Omega}(s, a) &amp;= r(s, a) + \sum_{j \in \mathcal{S}} \gamma p(j|s,a) v_{d,\Omega}(j) \\
v_{d,\Omega}(s) &amp;= \sum_{a \in \mathcal{A}_s} d(a | s) q_{d,\Omega}(s, a) - \Omega(d(\cdot | s))
\end{align*} \end{split}\]</div>
<p>The regularized optimal value function <span class="math notranslate nohighlight">\(v^*_\Omega\)</span> is then the unique fixed point of <span class="math notranslate nohighlight">\(L_\Omega\)</span> in the fixed point equation:</p>
<div class="math notranslate nohighlight">
\[\text{find $v$ such that } v = L_\Omega v\]</div>
<p>The associated state-action value function <span class="math notranslate nohighlight">\(q^*_\Omega\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{align*}
q^*_\Omega(s, a) &amp;= r(s, a) + \sum_{j \in \mathcal{S}} \gamma p(j|s,a) v^*_\Omega(j) \\
v^*_\Omega(s) &amp;= \Omega^*(q^*_\Omega(s, \cdot))\end{align*} \end{split}\]</div>
<p>An important result in the theory of regularized MDPs is that there exists a unique optimal regularized policy. Specifically, if <span class="math notranslate nohighlight">\(d^*_\Omega\)</span> is a conserving decision rule (i.e., <span class="math notranslate nohighlight">\(d^*_\Omega = \arg\max_d L_{d,\Omega} v^*_\Omega\)</span>), then the randomized stationary policy <span class="math notranslate nohighlight">\((d^*_\Omega)^\infty\)</span> is the unique optimal regularized policy.</p>
<p>In practice, once we have found <span class="math notranslate nohighlight">\(v^*_\Omega\)</span>, we can derive the optimal decision rule by taking the gradient of the convex conjugate evaluated at the optimal action-value function:</p>
<div class="math notranslate nohighlight">
\[ d^*(\cdot | s) = \nabla \Omega^*(q^*_\Omega(s, \cdot)) \]</div>
<section id="recovering-the-smooth-bellman-equations">
<h3>Recovering the Smooth Bellman Equations<a class="headerlink" href="#recovering-the-smooth-bellman-equations" title="Link to this heading">#</a></h3>
<p>Under this framework, we can recover the smooth Bellman equations by choosing <span class="math notranslate nohighlight">\(\Omega\)</span> to be the negative entropy, and obtain the softmax policy as the gradient of the convex conjugate. Let’s show this explicitly:</p>
<ol class="arabic">
<li><p>Using the negative entropy regularizer:</p>
<div class="math notranslate nohighlight">
\[ \Omega(d(\cdot|s)) = \sum_{a \in \mathcal{A}_s} d(a|s) \ln d(a|s) \]</div>
</li>
<li><p>The convex conjugate:</p>
<div class="math notranslate nohighlight">
\[ \Omega^*(q(s, \cdot)) = \ln \sum_{a \in \mathcal{A}_s} \exp q(s,a) \]</div>
</li>
<li><p>Now, let’s write out the regularized Bellman optimality equation:</p>
<div class="math notranslate nohighlight">
\[ v^*_\Omega(s) = \Omega^*(q^*_\Omega(s, \cdot)) \]</div>
</li>
<li><p>Substituting the expressions for <span class="math notranslate nohighlight">\(\Omega^*\)</span> and <span class="math notranslate nohighlight">\(q^*_\Omega\)</span>:</p>
<div class="math notranslate nohighlight">
\[ v^*_\Omega(s) = \ln \sum_{a \in \mathcal{A}_s} \exp \left(r(s, a) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a) v^*_\Omega(j)\right) \]</div>
</li>
</ol>
<p>This is precisely the form of the smooth Bellman equation we derived earlier, with the log-sum-exp operation replacing the max operation of the standard Bellman equation.</p>
<p>Furthermore, the optimal policy is given by the gradient of <span class="math notranslate nohighlight">\(\Omega^*\)</span>:</p>
<div class="math notranslate nohighlight">
\[ d^*(a|s) = \nabla \Omega^*(q^*_\Omega(s, \cdot)) = \frac{\exp(q^*_\Omega(s,a))}{\sum_{a' \in \mathcal{A}_s} \exp(q^*_\Omega(s,a'))} \]</div>
<p>This is the familiar softmax policy we encountered in the smooth MDP setting.</p>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="approximating-the-bellman-operator-using-numerical-integration">
<h1>Approximating the Bellman Operator using Numerical Integration<a class="headerlink" href="#approximating-the-bellman-operator-using-numerical-integration" title="Link to this heading">#</a></h1>
<p>For both the “hard” and smooth Bellman operators, evaluating the expectation over next states can be challenging when the state space is very large or continuous. In scenarios where we have access to an explicit representation of the transition probability function—the so-called “model-based” setting we’ve worked with throughout this course—this problem can be addressed using numerical integration methods. But what if we lack these exact probabilities and instead only have access to samples of next states for given state-action pairs? In this case, we must turn to Monte Carlo integration methods, which brings us fully into what we would recognize as a learning setting.</p>
<section id="discretization-and-numerical-quadrature">
<h2>Discretization and Numerical Quadrature<a class="headerlink" href="#discretization-and-numerical-quadrature" title="Link to this heading">#</a></h2>
<p>Recall that the Bellman operator for continuous state spaces takes the form:</p>
<div class="math notranslate nohighlight">
\[ (\mathrm{L} v)(s) \equiv \max_{a \in \mathcal{A}_s} \left\{r(s,a) + \gamma \int v(s')p(ds'|s,a)\right\}, \, \forall s \in \mathcal{S} \]</div>
<p>To make this computationally tractable for continuous state spaces, we can discretize the state space and approximate the integral over this discretized space. While this allows us to evaluate the Bellman operator componentwise, we must first decide how to represent value functions in this discretized setting.</p>
<p>When working with discretized representations, we partition the state space into <span class="math notranslate nohighlight">\(N_s\)</span> cells with centers at grid points <span class="math notranslate nohighlight">\(\{s_i\}_{i=1}^{N_s}\)</span>. We then work with value functions that are piecewise constant on each cell: ie. for any <span class="math notranslate nohighlight">\(s \in S\)</span>: <span class="math notranslate nohighlight">\(v(s) = v(s_{k(s)})\)</span> where <span class="math notranslate nohighlight">\(k(s)\)</span> is the index of the cell containing <span class="math notranslate nohighlight">\(s\)</span>. We denote the discretized reward function by
<span class="math notranslate nohighlight">\( r_h(s, a) \equiv r(s_{k(s)}, a) \)</span>.</p>
<p>For transition probabilities, we need to be more careful. While we similarly map any state-action pair to its corresponding cell, we must ensure that integrating over the discretized transition function yields a valid probability distribution. We achieve this by normalizing:</p>
<div class="math notranslate nohighlight">
\[ p_h(s'|s, a) \equiv \frac{p(s_{k(s')}|s_{k(s)}, a)}{\int p(s_{k(s')}|s_{k(s)}, a) ds'} \]</div>
<p>After defining our discretized reward and transition probability functions, we can write down our discretized Bellman operator.
We start with the Bellman operator using our discretized functions <span class="math notranslate nohighlight">\(r_h\)</span> and <span class="math notranslate nohighlight">\(p_h\)</span>. While these functions map to grid points, they’re still defined over continuous spaces - we haven’t yet dealt with the computational challenge of the integral. With this discretization approach, the value function is piecewise constant over cells. This lets us express the integral as a sum over cells, where each cell’s contribution is the probability of transitioning to that cell multiplied by the value at that cell’s grid point:
$<span class="math notranslate nohighlight">\( \begin{aligned}
(\widehat{\mathrm{L}}_h v)(s) &amp;= \max_{k=1,\ldots,N_a} \left\{r_h(s, a_k) + \gamma \int v(s')p_h(s'|s, a_k)ds'\right\} \\
&amp;= \max_{k=1,\ldots,N_a} \left\{r_h(s, a_k) + \gamma \int v(s_{k(s')})p_h(s'|s, a_k)ds'\right\} \\
&amp;= \max_{k=1,\ldots,N_a} \left\{r_h(s, a_k) + \gamma \sum_{i=1}^{N_s} v(s_i) \int_{cell_i} p_h(s'|s, a_k)ds'\right\} \\
&amp;= \max_{k=1,\ldots,N_a} \left\{r_h(s, a_k) + \gamma \sum_{i=1}^{N_s} v(s_i)p_h(s_i|s_{k(s)}, a_k)\right\}
\end{aligned} \)</span>$</p>
<p>This form makes clear how discretization converts our continuous-space problem into a finite computation: we’ve replaced integration over continuous space with summation over grid points. The price we pay is that the number of terms in our sum grows exponentially with the dimension of our state space - the familiar curse of dimensionality.</p>
</section>
<section id="monte-carlo-integration">
<h2>Monte Carlo Integration<a class="headerlink" href="#monte-carlo-integration" title="Link to this heading">#</a></h2>
<p>Numerical quadrature methods scale poorly with increasing dimension. Specifically, for a fixed error tolerance <span class="math notranslate nohighlight">\(\epsilon\)</span>, the number of required quadrature points grows exponentially with dimension <span class="math notranslate nohighlight">\(d\)</span> as <span class="math notranslate nohighlight">\(O\left(\left(\frac{1}{\epsilon}\right)^d\right)\)</span>. Furthermore, quadrature methods require explicit evaluation of the transition probability function <span class="math notranslate nohighlight">\(p(s'|s,a)\)</span> at specified points—a luxury we don’t have in the “model-free” setting where we only have access to samples from the MDP.</p>
<p>Let <span class="math notranslate nohighlight">\(\mathcal{B} = \{s_1, \ldots, s_M\}\)</span> be our set of base points where we will evaluate the operator. At each base point <span class="math notranslate nohighlight">\(s_k \in \mathcal{B}\)</span>, Monte Carlo integration approximates the expectation using <span class="math notranslate nohighlight">\(N\)</span> samples:</p>
<div class="math notranslate nohighlight">
\[
\int v_n(s')p(ds'|s_k,a) \approx \frac{1}{N} \sum_{i=1}^N v_n(s'_{k,i}), \quad s'_{k,i} \sim p(\cdot|s_k,a)
\]</div>
<p>where <span class="math notranslate nohighlight">\(s'_{k,i}\)</span> denotes the <span class="math notranslate nohighlight">\(i\)</span>-th sample drawn from <span class="math notranslate nohighlight">\(p(\cdot|s_k,a)\)</span> for base point <span class="math notranslate nohighlight">\(s_k\)</span>. This approach has two remarkable properties making it particularly attractive for high-dimensional problems and model-free settings:</p>
<ol class="arabic simple">
<li><p>The convergence rate is <span class="math notranslate nohighlight">\(O\left(\frac{1}{\sqrt{N}}\right)\)</span> regardless of the number of dimensions</p></li>
<li><p>It only requires samples from <span class="math notranslate nohighlight">\(p(\cdot|s_k,a)\)</span>, not explicit probability values</p></li>
</ol>
<p>Using this approximation of the expected value over the next state, we can define a new “empirical” Bellman optimality operator:</p>
<div class="math notranslate nohighlight">
\[
(\widehat{\mathrm{L}}_N v)(s_k) \equiv \max_{a \in \mathcal{A}_{s_k}} \left\{r(s_k,a) + \frac{\gamma}{N} \sum_{i=1}^N v(s'_{k,i})\right\}, \quad s'_{k,i} \sim p(\cdot|s_k,a)
\]</div>
<p>for each <span class="math notranslate nohighlight">\(s_k \in \mathcal{B}\)</span>. A direct adaptation of the successive approximation method for this empirical operator leads to:</p>
<div class="proof algorithm admonition" id="mc-value-iteration">
<p class="admonition-title"><span class="caption-number">Algorithm 12 </span> (Monte Carlo Value Iteration)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, , number of samples <span class="math notranslate nohighlight">\(N\)</span>, tolerance <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span>, maximum iterations <span class="math notranslate nohighlight">\(K\)</span>
<strong>Output:</strong> Value function <span class="math notranslate nohighlight">\(v\)</span></p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(v_0(s) = 0\)</span> for all <span class="math notranslate nohighlight">\(s \in S\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(i \leftarrow 0\)</span></p></li>
<li><p><strong>repeat</strong></p>
<ol class="arabic simple">
<li><p>For each <span class="math notranslate nohighlight">\(s \in \mathcal{B}\)</span>:</p>
<ol class="arabic simple">
<li><p>Draw <span class="math notranslate nohighlight">\(s'_{j} \sim p(\cdot|s,a)\)</span> for <span class="math notranslate nohighlight">\(j = 1,\ldots,N\)</span></p></li>
<li><p>Set <span class="math notranslate nohighlight">\(v_{i+1}(s) \leftarrow \max_{a \in A} \left\{r(s,a) + \frac{\gamma}{N} \sum_{j=1}^N v_i(s'_{j})\right\}\)</span></p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(\delta \leftarrow \|v_{i+1} - v_i\|_{\infty}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(i \leftarrow i + 1\)</span></p></li>
</ol>
</li>
<li><p><strong>until</strong> <span class="math notranslate nohighlight">\(\delta &lt; \varepsilon\)</span> or <span class="math notranslate nohighlight">\(i \geq K\)</span></p></li>
<li><p><strong>return</strong> final <span class="math notranslate nohighlight">\(v_{i+1}\)</span></p></li>
</ol>
</section>
</div><p>Note that the original error bound derived as a termination criterion for value iteration need not hold in this approximate setting. Hence, we use a generic termination criterion based on computational budget and desired tolerance. While this aspect could be improved, we’ll focus on a more pressing matter: the algorithm’s tendency to produce upwardly biased values. In other words, this algorithm “thinks” the world is more rosy than it actually is - it overestimates values.</p>
<section id="overestimation-bias-in-monte-carlo-value-iteration">
<h3>Overestimation Bias in Monte Carlo Value Iteration<a class="headerlink" href="#overestimation-bias-in-monte-carlo-value-iteration" title="Link to this heading">#</a></h3>
<p>In statistics, bias refers to a systematic error where an estimator consistently deviates from the true parameter value. For an estimator <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> of a parameter <span class="math notranslate nohighlight">\(\theta\)</span>, we define bias as: <span class="math notranslate nohighlight">\(\text{Bias}(\hat{\theta}) = \mathbb{E}[\hat{\theta}] - \theta\)</span>. While bias isn’t always problematic — sometimes we deliberately introduce bias to reduce variance, as in ridge regression — uncontrolled bias can lead to significantly distorted results. In the context of value iteration, this distortion gets amplified even more due to the recursive nature of the algorithm.</p>
<p>Consider how the Bellman operator works in value iteration. At iteration n, we have a value function estimate <span class="math notranslate nohighlight">\(v_i(s)\)</span> and aim to improve it by applying the Bellman operator <span class="math notranslate nohighlight">\(\mathrm{L}\)</span>. The ideal update would be:</p>
<div class="math notranslate nohighlight">
\[(\mathrm{{L}}v_i)(s) = \max_{a \in \mathcal{A}(s)} \left\{ r(s,a) + \gamma \int v_i(s') p(ds'|s,a) \right\}\]</div>
<p>However, we can’t compute this integral exactly and use Monte Carlo integration instead, drawing <span class="math notranslate nohighlight">\(N\)</span> next-state samples for each state and action pair. The bias emerges when we take the maximum over actions:</p>
<div class="math notranslate nohighlight">
\[(\widehat{\mathrm{L}}v_i)(s) = \max_{a \in \mathcal{A}(s)} \hat{q}_i(s,a), \enspace \text{where} \enspace \hat{q}_i(s,a) \equiv r(s,a) + \frac{\gamma}{N} \sum_{j=1}^N v_i(s'_j), \quad s'_j \sim p(\cdot|s,a)\]</div>
<p>White the Monte Carlo estimate <span class="math notranslate nohighlight">\(\hat{q}_n(s,a)\)</span> is unbiased for any individual action, the empirical Bellman operator is biased upward due to Jensen’s inequality, which states that for any convex function <span class="math notranslate nohighlight">\(f\)</span>, we have <span class="math notranslate nohighlight">\(\mathbb{E}[f(X)] \geq f(\mathbb{E}[X])\)</span>. Since the maximum operator is convex, this implies:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[(\widehat{\mathrm{L}}v_i)(s)] = \mathbb{E}\left[\max_{a \in \mathcal{A}(s)} \hat{q}_i(s,a)\right] \geq \max_{a \in \mathcal{A}(s)} \mathbb{E}[\hat{q}_i(s,a)] = (\mathrm{L}v_i)(s)\]</div>
<p>This means that our Monte Carlo approximation of the Bellman operator is biased upward:</p>
<div class="math notranslate nohighlight">
\[b_i(s) = \mathbb{E}[(\widehat{\mathrm{L}}v_i)(s)] - (\mathrm{L}v_i)(s) \geq 0\]</div>
<p>Even worse, this bias compounds through iterations as each new value function estimate <span class="math notranslate nohighlight">\(v_{n+1}\)</span> is based on targets generated by the biased operator <span class="math notranslate nohighlight">\(\widehat{\mathrm{L}}\)</span>, creating a nested structure of bias accumulation.
This bias remains nonnegative at every step, and each application of the Bellman operator potentially adds more upward bias. As a result, instead of converging to the true value function <span class="math notranslate nohighlight">\(v^*\)</span>, the algorithm typically stabilizes at a biased approximation that systematically overestimates true values.</p>
</section>
<section id="the-keane-wolpin-bias-correction-algorithm">
<h3>The Keane-Wolpin Bias Correction Algorithm<a class="headerlink" href="#the-keane-wolpin-bias-correction-algorithm" title="Link to this heading">#</a></h3>
<p>Keane and Wolpin proposed to de-bias such estimators by essentially “learning” the bias, then subtracting it when computing the empirical Bellman operator. If we knew this bias function, we could subtract it from our empirical estimate to get an unbiased estimate of the true Bellman operator:</p>
<div class="math notranslate nohighlight">
\[
(\widehat{\mathrm{L}}v_n)(s) - \text{bias}(s) = (\widehat{\mathrm{L}}v_n)(s) - (\mathbb{E}[(\widehat{\mathrm{L}}v_n)(s)] - (\mathrm{L}v_n)(s)) \approx (\mathrm{L}v_n)(s)
\]</div>
<p>This equality holds in expectation, though any individual estimate would still have variance around the true value.</p>
<p>So how can we estimate the bias function? The Keane-Wolpin manages this using an important fact from extreme value theory: for normal random variables, the difference between the expected maximum and maximum of expectations scales with the standard deviation:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}\left[\max_{a \in \mathcal{A}} \hat{q}_i(s,a)\right] - \max_{a \in \mathcal{A}} \mathbb{E}[\hat{q}_i(s,a)] \approx c \cdot \sqrt{\max_{a \in \mathcal{A}} \text{Var}_i(s,a)}
\]</div>
<p>The variance term <span class="math notranslate nohighlight">\(\max_{a \in \mathcal{A}} \text{Var}_i(s,a)\)</span> will typically be dominated by the action with the largest value – the greedy action <span class="math notranslate nohighlight">\(a^*_i(s)\)</span>. Rather than deriving the constant <span class="math notranslate nohighlight">\(c\)</span> theoretically, Keane-Wolpin proposed learning the relationship between variance and bias empirically through these steps:</p>
<ol class="arabic">
<li><p>Select a small set of “benchmark” states (typically 20-50) that span the state space</p></li>
<li><p>For these states, compute more accurate value estimates using many more Monte Carlo samples (10-100x more than usual)</p></li>
<li><p>Compute the empirical bias at each benchmark state <span class="math notranslate nohighlight">\(s\)</span>:</p>
<div class="math notranslate nohighlight">
\[\hat{b}_i(s) = (\hat{\mathrm{L}}v_i)(s) - (\hat{\mathrm{L}}_{\text{accurate}}v_i)(s)\]</div>
</li>
<li><p>Fit a linear relationship between this bias and the variance at the greedy action:</p>
<div class="math notranslate nohighlight">
\[\hat{b}_i(s) = \alpha_i \cdot \text{Var}_i(s,a^*_i(s)) + \epsilon\]</div>
</li>
</ol>
<p>This creates a dataset of pairs <span class="math notranslate nohighlight">\((\text{Var}_i(s,a^*_i(s)), \hat{b}_i(s))\)</span> that can be used to estimate <span class="math notranslate nohighlight">\(\alpha_i\)</span> through ordinary least squares regression. Once we have learned this bias function <span class="math notranslate nohighlight">\(\hat{b}\)</span>, we can define the bias-corrected Bellman operator:</p>
<div class="math notranslate nohighlight">
\[ 
(\widetilde{\mathrm{L}}v_i)(s) \triangleq (\hat{\mathrm{L}}v_i)(s) - \hat{b}(s)
\]</div>
<p>Interestingly, while this bias correction approach has been influential in econometrics, it hasn’t gained much traction in the machine learning community. A major drawback is the need for accurate operator estimation at benchmark states, which requires allocating substantially more samples to these states. In the next section, we’ll explore an alternative strategy that, while requiring the maintenance of two sets of value estimates, achieves bias correction without demanding additional samples.</p>
</section>
<section id="decoupling-selection-and-evaluation">
<h3>Decoupling Selection and Evaluation<a class="headerlink" href="#decoupling-selection-and-evaluation" title="Link to this heading">#</a></h3>
<p>A simpler approach to addressing the upward bias is to maintain two separate q-function estimates - one for action selection and another for evaluation. Let’s first start by looking at the corresponding Monte Carlo value iteration algorithm and then convince ourselves that this is good idea using math. Assume a monte carlo integration setup over Q factors:</p>
<div class="proof algorithm admonition" id="double-mc-value-iteration">
<p class="admonition-title"><span class="caption-number">Algorithm 13 </span> (Double Monte Carlo Q-Value Iteration)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong>: MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, number of samples <span class="math notranslate nohighlight">\(N\)</span>, tolerance <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span>, maximum iterations <span class="math notranslate nohighlight">\(K\)</span>
<strong>Output</strong>: Q-functions <span class="math notranslate nohighlight">\(q^A, q^B\)</span></p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(q^A_0(s,a) = q^B_0(s,a) = 0\)</span> for all <span class="math notranslate nohighlight">\(s \in S, a \in A\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(i \leftarrow 0\)</span></p></li>
<li><p>repeat</p>
<ol class="arabic simple">
<li><p>For each <span class="math notranslate nohighlight">\(s \in S, a \in A\)</span>:</p>
<ol class="arabic simple">
<li><p>Draw <span class="math notranslate nohighlight">\(s'_j \sim p(\cdot|s,a)\)</span> for <span class="math notranslate nohighlight">\(j = 1,\ldots,N\)</span></p></li>
<li><p>For network A:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(a^*_i \leftarrow \arg\max_{a'} q^A_i(s'_j,a')\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(q^A_{i+1}(s,a) \leftarrow r(s,a) + \frac{\gamma}{N} \sum_{j=1}^N q^B_i(s'_j,a^*_i)\)</span></p></li>
</ol>
</li>
<li><p>For network B:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(b^*_i \leftarrow \arg\max_{a'} q^B_i(s'_j,a')\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(q^B_{i+1}(s,a) \leftarrow r(s,a) + \frac{\gamma}{N} \sum_{j=1}^N q^A_i(s'_j,b^*_i)\)</span></p></li>
</ol>
</li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(\delta \leftarrow \max(\|q^A_{i+1} - q^A_i\|_{\infty}, \|q^B_{i+1} - q^B_i\|_{\infty})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(i \leftarrow i + 1\)</span></p></li>
</ol>
</li>
<li><p>until <span class="math notranslate nohighlight">\(\delta &lt; \varepsilon\)</span> or <span class="math notranslate nohighlight">\(i \geq K\)</span></p></li>
<li><p>return final <span class="math notranslate nohighlight">\(q^A_{i+1}, q^B_{i+1}\)</span></p></li>
</ol>
</section>
</div><p>In this algorithm, we maintain two separate Q-functions (<span class="math notranslate nohighlight">\(q^A\)</span> and <span class="math notranslate nohighlight">\(q^B\)</span>) and use them asymmetrically: when updating <span class="math notranslate nohighlight">\(q^A\)</span>, we use network A to select the best action (<span class="math notranslate nohighlight">\(a^*_i = \arg\max_{a'} q^A_i(s'_j,a')\)</span>) but then evaluate that action using network B’s estimates (<span class="math notranslate nohighlight">\(q^B_i(s'_j,a^*_i)\)</span>). We do the opposite for updating <span class="math notranslate nohighlight">\(q^B\)</span>. You can see this separation clearly in steps 3.2.2 and 3.2.3 of the algorithm, where for each network update, we first use one network to pick the action and then plug that chosen action into the other network for evaluation. We will see that this decomposition helps mitigate the positive bias that occurs due to Jensen’s inequality.</p>
<section id="an-hvac-analogy">
<h4>An HVAC analogy<a class="headerlink" href="#an-hvac-analogy" title="Link to this heading">#</a></h4>
<p>Consider a building where each HVAC unit <span class="math notranslate nohighlight">\(i\)</span> has some true maximum power draw <span class="math notranslate nohighlight">\(\mu_i\)</span> under worst-case conditions. Let’s pretend that we don’t have access to manufacturer datasheets, so we need to estimate these maxima from actual measurements. Now the challenge is that power draw fluctuates with environmental conditions. If we use a single day’s measurements and look at the highest power draw, we systematically overestimate the true maximum draw across all units.</p>
<p>To see this, let <span class="math notranslate nohighlight">\(X_A^i\)</span> be unit i’s power draw on day A and <span class="math notranslate nohighlight">\(X_B^i\)</span> be unit i’s power draw on day. Wile both measurements are unbiased <span class="math notranslate nohighlight">\(\mathbb{E}[X_A^i] = \mathbb{E}[X_B^i] = \mu_i\)</span>, their maximum is not due to Jensen’s inequality:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[\max_i X_A^i] \geq \max_i \mathbb{E}[X_A^i] = \max_i \mu_i
\]</div>
<p>Intuitively, this problem occurs because reading tends to come from units that experienced particularly demanding conditions (e.g., direct sunlight, full occupancy, peak humidity) rather than just those with high true maximum draw. To estimate the true maximum power draw more accurately, we use the following measurement protocol:</p>
<ol class="arabic simple">
<li><p>Use day A measurements to <strong>select</strong> which unit hit the highest peak</p></li>
<li><p>Use day B measurements to <strong>evaluate</strong> that unit’s power consumption</p></li>
</ol>
<p>This yields the estimator:</p>
<div class="math notranslate nohighlight">
\[
Y = X_B^{\arg\max_i X_A^i}
\]</div>
<p>We can show that by decoupling selection and evaluation in this fashion, our estimator <span class="math notranslate nohighlight">\(Y\)</span> will no longer systematically overestimate the true maximum draw. First, observe that <span class="math notranslate nohighlight">\(\arg\max_i X_A^i\)</span> is a random variable (call it <span class="math notranslate nohighlight">\(J\)</span>) - it tells us which unit had highest power draw on day A. It has some probability distribution based on day A’s conditions:
<span class="math notranslate nohighlight">\(P(J = j) = P(\arg\max_i X_A^i = j)\)</span>.
Using the law of total expectation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathbb{E}[Y] = \mathbb{E}[X_B^J] &amp;= \mathbb{E}[\mathbb{E}[X_B^J \mid J]] \text{ (by tower property)} \\
&amp;= \sum_{j=1}^n \mathbb{E}[X_B^j \mid J = j] P(J = j) \\
&amp;= \sum_{j=1}^n \mathbb{E}[X_B^j \mid \arg\max_i X_A^i = j] P(\arg\max_i X_A^i = j)
\end{align*}
\end{split}\]</div>
<p>Now we need to make an imporant observation: Unit j’s power draw on day B (<span class="math notranslate nohighlight">\(X_B^j\)</span>) is independent of whether it had the highest reading on day A (<span class="math notranslate nohighlight">\(\{\arg\max_i X_A^i = j\}\)</span>). An extreme cold event on day A shouldn’t affect day B’s readings(especially in Quebec where the wheather tend to vary widely from day to day). Therefore:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[X_B^j \mid \arg\max_i X_A^i = j] = \mathbb{E}[X_B^j] = \mu_j
\]</div>
<p>This tells us that the two-day estimator is now an average of the true underlying power consumptions:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[Y] = \sum_{j=1}^n \mu_j P(\arg\max_i X_A^i = j)
\]</div>
<p>To analyze <span class="math notranslate nohighlight">\( \mathbb{E}[Y] \)</span> more closely, let’s use a general result: if we have a real-valued function <span class="math notranslate nohighlight">\( f \)</span> defined on a discrete set of units <span class="math notranslate nohighlight">\( \{1, \dots, n\} \)</span> and a probability distribution <span class="math notranslate nohighlight">\( q(\cdot) \)</span> over these units, then the maximum value of <span class="math notranslate nohighlight">\( f \)</span> across all units is at least as large as the weighted sum of <span class="math notranslate nohighlight">\( f \)</span> values with weights <span class="math notranslate nohighlight">\( q \)</span>. Formally,</p>
<div class="math notranslate nohighlight">
\[
\max_{j \in \{1, \dots, n\}} f(j) \geq \sum_{j=1}^n q(j) f(j).
\]</div>
<p>Applying this to our setting, we set <span class="math notranslate nohighlight">\( f(j) = \mu_j \)</span> (the true maximum power draw for unit <span class="math notranslate nohighlight">\( j \)</span>) and <span class="math notranslate nohighlight">\( q(j) = P(J = j) \)</span> (the probability that unit <span class="math notranslate nohighlight">\( j \)</span> achieves the maximum reading on day A). This gives us:</p>
<div class="math notranslate nohighlight">
\[
\max_{j \in \{1, \dots, n\}} \mu_j \geq \sum_{j=1}^n P(J = j) \mu_j = \mathbb{E}[Y].
\]</div>
<p>Therefore, the expected value of <span class="math notranslate nohighlight">\( Y \)</span> (our estimator) will always be less than or equal to the true maximum value <span class="math notranslate nohighlight">\( \max_j \mu_j \)</span>. In other words, <span class="math notranslate nohighlight">\( Y \)</span> provides a <strong>conservative estimate</strong> of the true maximum: it tends not to overestimate <span class="math notranslate nohighlight">\( \max_j \mu_j \)</span> but instead approximates it as closely as possible without systematic upward bias.</p>
</section>
<section id="consistency">
<h4>Consistency<a class="headerlink" href="#consistency" title="Link to this heading">#</a></h4>
<p>Even though <span class="math notranslate nohighlight">\( Y \)</span> is not an unbiased estimator of <span class="math notranslate nohighlight">\( \max_j \mu_j \)</span> (since <span class="math notranslate nohighlight">\( \mathbb{E}[Y] \leq \max_j \mu_j \)</span>), it is <strong>consistent</strong>. As more independent days (or measurements) are observed, the selection-evaluation procedure becomes more effective at isolating the intrinsic maximum, reducing the influence of day-specific environmental fluctuations. Over time, this approach yields a stable and increasingly accurate approximation of <span class="math notranslate nohighlight">\( \max_j \mu_j \)</span>.</p>
<p>To show that <span class="math notranslate nohighlight">\( Y \)</span> is a consistent estimator of <span class="math notranslate nohighlight">\( \max_i \mu_i \)</span>, we want to demonstrate that as the number of independent measurements (days, in this case) increases, <span class="math notranslate nohighlight">\( Y \)</span> converges in probability to <span class="math notranslate nohighlight">\( \max_i \mu_i \)</span>. Let’s suppose we have <span class="math notranslate nohighlight">\( m \)</span> independent days of measurements for each unit. Denote:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( X_A^{(k),i} \)</span> as the power draw for unit <span class="math notranslate nohighlight">\( i \)</span> on day <span class="math notranslate nohighlight">\( A_k \)</span>, where <span class="math notranslate nohighlight">\( k \in \{1, \dots, m\} \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( J_m = \arg\max_i \left( \frac{1}{m} \sum_{k=1}^m X_A^{(k),i} \right) \)</span>, which identifies the unit with the highest average power draw over <span class="math notranslate nohighlight">\( m \)</span> days.</p></li>
</ul>
<p>The estimator we construct is:
<span class="math notranslate nohighlight">\(
Y_m = X_B^{(J_m)},
\)</span>
where <span class="math notranslate nohighlight">\( X_B^{(J_m)} \)</span> is the power draw of the selected unit <span class="math notranslate nohighlight">\( J_m \)</span> on an independent evaluation day <span class="math notranslate nohighlight">\( B \)</span>.
We will now show that <span class="math notranslate nohighlight">\( Y_m \)</span> converges to <span class="math notranslate nohighlight">\( \max_i \mu_i \)</span> as <span class="math notranslate nohighlight">\( m \to \infty \)</span>. This involves two main steps:</p>
<ol class="arabic simple">
<li><p><strong>Consistency of the Selection Step <span class="math notranslate nohighlight">\( J_m \)</span></strong>: As <span class="math notranslate nohighlight">\( m \to \infty \)</span>, the unit selected by <span class="math notranslate nohighlight">\( J_m \)</span> will tend to be the one with the true maximum power draw <span class="math notranslate nohighlight">\( \max_i \mu_i \)</span>.</p></li>
<li><p><strong>Convergence of <span class="math notranslate nohighlight">\( Y_m \)</span> to <span class="math notranslate nohighlight">\( \mu_{J_m} \)</span></strong>: Since the evaluation day <span class="math notranslate nohighlight">\( B \)</span> measurement <span class="math notranslate nohighlight">\( X_B^{(J_m)} \)</span> is unbiased with expectation <span class="math notranslate nohighlight">\( \mu_{J_m} \)</span>, as <span class="math notranslate nohighlight">\( m \to \infty \)</span>, <span class="math notranslate nohighlight">\( Y_m \)</span> will converge to <span class="math notranslate nohighlight">\( \mu_{J_m} \)</span>, which in turn converges to <span class="math notranslate nohighlight">\( \max_i \mu_i \)</span>.</p></li>
</ol>
<p>The average power draw over <span class="math notranslate nohighlight">\( m \)</span> days for each unit <span class="math notranslate nohighlight">\( i \)</span> is:</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{m} \sum_{k=1}^m X_A^{(k),i}.
\]</div>
<p>By the law of large numbers, as <span class="math notranslate nohighlight">\( m \to \infty \)</span>, this sample average converges to the true expected power draw <span class="math notranslate nohighlight">\( \mu_i \)</span> for each unit <span class="math notranslate nohighlight">\( i \)</span>:</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{m} \sum_{k=1}^m X_A^{(k),i} \xrightarrow{m \to \infty} \mu_i.
\]</div>
<p>Since <span class="math notranslate nohighlight">\( J_m \)</span> selects the unit with the highest sample average, in the limit, <span class="math notranslate nohighlight">\( J_m \)</span> will almost surely select the unit with the highest true mean, <span class="math notranslate nohighlight">\( \max_i \mu_i \)</span>. Thus, as <span class="math notranslate nohighlight">\( m \to \infty \)</span>,
$<span class="math notranslate nohighlight">\(
\mu_{J_m} \to \max_i \mu_i.
\)</span>$</p>
<p>Given that <span class="math notranslate nohighlight">\( J_m \)</span> identifies the unit with the maximum true mean power draw in the limit, we now look at <span class="math notranslate nohighlight">\( Y_m = X_B^{(J_m)} \)</span>, which is the power draw of unit <span class="math notranslate nohighlight">\( J_m \)</span> on the independent evaluation day <span class="math notranslate nohighlight">\( B \)</span>.</p>
<p>Since <span class="math notranslate nohighlight">\( X_B^{(J_m)} \)</span> is an unbiased estimator of <span class="math notranslate nohighlight">\( \mu_{J_m} \)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[Y_m \mid J_m] = \mu_{J_m}.
\]</div>
<p>As <span class="math notranslate nohighlight">\( m \to \infty \)</span>, <span class="math notranslate nohighlight">\( \mu_{J_m} \)</span> converges to <span class="math notranslate nohighlight">\( \max_i \mu_i \)</span>. Thus, <span class="math notranslate nohighlight">\( Y_m \)</span> will also converge in probability to <span class="math notranslate nohighlight">\( \max_i \mu_i \)</span> because <span class="math notranslate nohighlight">\( Y_m \)</span> is centered around <span class="math notranslate nohighlight">\( \mu_{J_m} \)</span> and <span class="math notranslate nohighlight">\( J_m \)</span> converges to the index of the unit with <span class="math notranslate nohighlight">\( \max_i \mu_i \)</span>.</p>
<p>Combining these two steps, we conclude that:</p>
<div class="math notranslate nohighlight">
\[
Y_m \xrightarrow{m \to \infty} \max_i \mu_i \text{ in probability}.
\]</div>
<p>This establishes the <strong>consistency</strong> of <span class="math notranslate nohighlight">\( Y \)</span> as an estimator for <span class="math notranslate nohighlight">\( \max_i \mu_i \)</span>: as the number of independent measurements grows, <span class="math notranslate nohighlight">\( Y_m \)</span> converges to the true maximum power draw <span class="math notranslate nohighlight">\( \max_i \mu_i \)</span>.</p>
</section>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="parametric-dynamic-programming">
<h1>Parametric Dynamic Programming<a class="headerlink" href="#parametric-dynamic-programming" title="Link to this heading">#</a></h1>
<p>We have so far considered a specific kind of approximation: that of the Bellman operator itself. We explored a modified version of the operator with the desirable property of smoothness, which we deemed beneficial for optimization purposes and due to its rich multifaceted interpretations. We now turn our attention to another form of approximation, complementary to the previous kind, which seeks to address the challenge of applying the operator across the entire state space.</p>
<p>To be precise, suppose we can compute the Bellman operator <span class="math notranslate nohighlight">\(\mathrm{L}v\)</span> at some state <span class="math notranslate nohighlight">\(s\)</span>, producing a new function <span class="math notranslate nohighlight">\(U\)</span> whose value at state <span class="math notranslate nohighlight">\(s\)</span> is <span class="math notranslate nohighlight">\(u(s) = (\mathrm{L}v)(s)\)</span>. Then, putting aside the problem of pointwise evaluation, we want to carry out this update across the entire domain of <span class="math notranslate nohighlight">\(v\)</span>. When working with small state spaces, this is not an issue, and we can afford to carry out the update across the entirety of the state space. However, for larger or infinite state spaces, this becomes a major challenge.</p>
<p>So what can we do? Our approach will be to compute the operator at chosen “grid points,” then “fill in the blanks” for the states where we haven’t carried out the update by “fitting” the resulting output function on a dataset of input-output pairs. The intuition is that for sufficiently well-behaved functions and sufficiently expressive function approximators, we hope to generalize well enough. Our community calls this “learning,” while others would call it “function approximation” — a field of its own in mathematics. To truly have a “learning algorithm,” we’ll need to add one more piece of machinery: the use of samples — of simulation — to pick the grid points and perform numerical integration. But this is for the next section…</p>
<section id="partial-updates-in-the-tabular-case">
<h2>Partial Updates in the Tabular Case<a class="headerlink" href="#partial-updates-in-the-tabular-case" title="Link to this heading">#</a></h2>
<p>The ideas presented in this section apply more broadly to the successive approximation method applied to a fixed-point problem. Consider again the problem of finding the optimal value function <span class="math notranslate nohighlight">\(v_\gamma^\star\)</span> as the solution to the Bellman optimality operator <span class="math notranslate nohighlight">\(\mathrm{L}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathrm{L} \mathbf{v} \equiv \max_{d \in D^{MD}} \left\{\mathbf{r}_d + \gamma \mathbf{P}_d \mathbf{v}\right\}
\]</div>
<p>Value iteration — the name for the method of successive approximation applied to <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> — computes a sequence of iterates <span class="math notranslate nohighlight">\(v_{n+1} = \mathrm{L}v_n\)</span> from some arbitrary <span class="math notranslate nohighlight">\(v_0\)</span>. Let’s pause to consider what the equality sign in this expression means: it represents an assignment (perhaps better denoted as <span class="math notranslate nohighlight">\(:=\)</span>) across the entire domain. This becomes clearer when writing the update in component form:</p>
<div class="math notranslate nohighlight">
\[
v_{n+1}(s) := (\mathrm{L} v_n)(s) \equiv \max_{a \in \mathcal{A}_s} \left\{r(s,a) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a) v_n(j)\right\}, \, \forall s \in \mathcal{S}
\]</div>
<p>Pay particular attention to the <span class="math notranslate nohighlight">\(\forall s \in \mathcal{S}\)</span> notation: what happens when we can’t afford to update all components in each step of value iteration? A potential solution is to use Gauss-Seidel Value Iteration, which updates states sequentially, immediately using fresh values for subsequent updates.</p>
<div class="proof algorithm admonition" id="alg-gsvi">
<p class="admonition-title"><span class="caption-number">Algorithm 14 </span> (Gauss-Seidel Value Iteration)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, convergence threshold <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span><br />
<strong>Output:</strong> Value function <span class="math notranslate nohighlight">\(v\)</span> and policy <span class="math notranslate nohighlight">\(d\)</span></p>
<ol class="arabic">
<li><p><strong>Initialization:</strong></p>
<ul class="simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(v^0(s)\)</span> for all <span class="math notranslate nohighlight">\(s \in S\)</span></p></li>
<li><p>Set iteration counter <span class="math notranslate nohighlight">\(n = 0\)</span></p></li>
</ul>
</li>
<li><p><strong>Main Loop:</strong></p>
<ul class="simple">
<li><p>Set state index <span class="math notranslate nohighlight">\(j = 1\)</span></p></li>
</ul>
<p>a) <strong>State Update:</strong> Compute <span class="math notranslate nohighlight">\(v^{n+1}(s_j)\)</span> as:</p>
<div class="math notranslate nohighlight">
\[
      v^{n+1}(s_j) = \max_{a \in A_j} \left\{r(s_j, a) + \gamma \left[\sum_{i&lt;j} p(s_i|s_j,a)v^{n+1}(s_i) + \sum_{i \geq j} p(s_i|s_j,a)v^n(s_i)\right]\right\}
      \]</div>
<p>b) If <span class="math notranslate nohighlight">\(j = |S|\)</span>, proceed to step 3
Otherwise, increment <span class="math notranslate nohighlight">\(j\)</span> and return to step 2(a)</p>
</li>
<li><p><strong>Convergence Check:</strong></p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(\|v^{n+1} - v^n\| &lt; \varepsilon(1-\gamma)/(2\gamma)\)</span>, proceed to step 4</p></li>
<li><p>Otherwise, increment <span class="math notranslate nohighlight">\(n\)</span> and return to step 2</p></li>
</ul>
</li>
<li><p><strong>Policy Extraction:</strong>
For each <span class="math notranslate nohighlight">\(s \in S\)</span>, compute optimal policy:</p>
<div class="math notranslate nohighlight">
\[
   d(s) \in \operatorname{argmax}_{a \in A_s} \left\{r(s,a) + \gamma\sum_{j \in S} p(j|s,a)v^{n+1}(j)\right\}
   \]</div>
</li>
</ol>
<p><strong>Note:</strong> The algorithm differs from standard value iteration in that it immediately uses updated values within each iteration. This is reflected in the first sum of step 2(a), where <span class="math notranslate nohighlight">\(v^{n+1}\)</span> is used for already-updated states.</p>
</section>
</div><p>The Gauss-Seidel value iteration approach offers several advantages over standard value iteration: it can be more memory-efficient and often leads to faster convergence. This idea generalizes further (see for example <span id="id10">Bertsekas [<a class="reference internal" href="bibliography.html#id29" title="Dimitri P. Bertsekas. Distributed asynchronous computation of fixed points. Mathematical Programming, 27(1):107–120, September 1983. URL: http://dx.doi.org/10.1007/BF02591967, doi:10.1007/bf02591967.">5</a>]</span>) to accommodate fully asynchronous updates in any order. However, these methods, while more flexible in their update patterns, still fundamentally rely on a tabular representation—that is, they require storing and eventually updating a separate value for each state in memory. Even if we update states one at a time or in blocks, we must maintain this complete table of values, and our convergence guarantee assumes that every entry in this table will eventually be revised.</p>
<p>But what if maintaining such a table is impossible? This challenge arises naturally when dealing with continuous state spaces, where we cannot feasibly store values for every possible state, let alone update them. This is where function approximation comes into play.</p>
</section>
<section id="partial-updates-by-operator-fitting-parametric-value-iteration">
<h2>Partial Updates by Operator Fitting: Parametric Value Iteration<a class="headerlink" href="#partial-updates-by-operator-fitting-parametric-value-iteration" title="Link to this heading">#</a></h2>
<p>In the parametric approach to dynamic programming, instead of maintaining an explicit table of values, we represent the value function using a parametric function approximator <span class="math notranslate nohighlight">\(v(s; \boldsymbol{\theta})\)</span>, where <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> are parameters that get adjusted across iterations rather than the entries of a tabular representation. This idea traces back to the inception of dynamic programming and was described as early as 1963 by Bellman himself, who considered polynomial approximations. For a value function <span class="math notranslate nohighlight">\(v(s)\)</span>, we can write its polynomial approximation as:</p>
<div class="math notranslate nohighlight">
\[
v(s) \approx \sum_{i=0}^{n} \theta_i \phi_i(s)
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\{\phi_i(s)\}\)</span> is the set of basis functions</p></li>
<li><p><span class="math notranslate nohighlight">\(\theta_i\)</span> are the coefficients (our parameters)</p></li>
<li><p><span class="math notranslate nohighlight">\(n\)</span> is the degree of approximation</p></li>
</ul>
<p>As we discussed earlier in the context of trajectory optimization, we can choose from different polynomial bases beyond the usual monomial basis <span class="math notranslate nohighlight">\(\phi_i(s) = s^i\)</span>, such as Legendre or Chebyshev polynomials. While polynomials offer attractive mathematical properties, they become challenging to work with in higher dimensions due to the curse of dimensionality. This limitation motivates our later turn to neural network parameterizations, which scale better with dimensionality.</p>
<p>Given a parameterization, our value iteration procedure must now update the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> rather than tabular values directly. At each iteration, we aim to find parameters that best approximate the Bellman operator’s output at chosen base points. More precisely, we collect a dataset:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{D}_n = \{(s_i, (\mathrm{L}v)(s_i; \boldsymbol{\theta}_n)) \mid s_i \in B\}
\]</div>
<p>and fit a regressor <span class="math notranslate nohighlight">\(v(\cdot; \boldsymbol{\theta}_{n+1})\)</span> to this data.</p>
<p>This process differs from standard supervised learning in a specific way: rather than working with a fixed dataset, we iteratively generate our training targets using the previous value function approximation. During this process, the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span> remain “frozen”, entering only through dataset creation. This naturally leads to maintaining two sets of parameters:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span>: parameters of the target model used for generating training targets</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{n+1}\)</span>: parameters being optimized in the current iteration</p></li>
</ul>
<p>This target model framework emerges naturally from the structure of parametric value iteration — an insight that provides theoretical grounding for modern deep reinforcement learning algorithms where we commonly hear about the importance of the “target network trick” .</p>
<p>Parametric successive approximation, known in reinforcement learning literature as Fitted Value Iteration, offers a flexible template for deriving new algorithms by varying the choice of function approximator. Various instantiations of this approach have emerged across different fields:</p>
<ul class="simple">
<li><p>Using polynomial basis functions with linear regression yields Kortum’s method <span id="id11">[<a class="reference internal" href="bibliography.html#id31" title="Samuel Kortum. Value function approximation in an estimation routine. 1992. Manuscript, Boston University.">26</a>]</span>, known to econometricians. In reinforcement learning terms, this corresponds to value iteration with projected Bellman equations <span id="id12">[<a class="reference internal" href="bibliography.html#id32" title="John Rust. Chapter 14 Numerical dynamic programming in economics, pages 619–729. Elsevier, 1996. URL: http://dx.doi.org/10.1016/S1574-0021(96)01016-7, doi:10.1016/s1574-0021(96)01016-7.">37</a>]</span>.</p></li>
<li><p>Employing extremely randomized trees (via <code class="docutils literal notranslate"><span class="pre">ExtraTreesRegressor</span></code>) leads to the tree-based fitted value iteration of Ernst et al. <span id="id13">[<a class="reference internal" href="bibliography.html#id33" title="Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. Journal of Machine Learning Research, 6:503–556, 2005.">11</a>]</span>.</p></li>
<li><p>Neural network approximation (via <code class="docutils literal notranslate"><span class="pre">MLPRegressor</span></code>) gives rise to Neural Fitted Q-Iteration as developed by Riedmiller <span id="id14">[<a class="reference internal" href="bibliography.html#id34" title="Martin Riedmiller. Neural fitted q iteration – first experiences with a data efficient neural reinforcement learning method. In Proceedings of the 16th European Conference on Machine Learning (ECML), 317–328. Berlin, Heidelberg, 2005. Springer.">34</a>]</span>.</p></li>
</ul>
<p>The \texttt{fit} function in our algorithm represents this supervised learning step and can be implemented using any standard regression tool that follows the scikit-learn interface. This flexibility in choice of function approximator allows practitioners to leverage the extensive ecosystem of modern machine learning tools while maintaining the core dynamic programming structure.</p>
<div class="proof algorithm admonition" id="parametric-value-iteration">
<p class="admonition-title"><span class="caption-number">Algorithm 15 </span> (Parametric Value Iteration)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> Given an MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, base points <span class="math notranslate nohighlight">\(B \subset S\)</span>, function approximator class <span class="math notranslate nohighlight">\(v(s; \boldsymbol{\theta})\)</span>, maximum iterations <span class="math notranslate nohighlight">\(N\)</span>, tolerance <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span></p>
<p><strong>Output</strong> Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> for value function approximation</p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span> (e.g., for zero initialization)</p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow 0\)</span></p></li>
<li><p><strong>repeat</strong></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{D} \leftarrow \emptyset\)</span></p></li>
<li><p>For each <span class="math notranslate nohighlight">\(s \in B\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(y_s \leftarrow \max_{a \in A} \left\{r(s,a) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a)v(j; \boldsymbol{\theta}_n)\right\}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D} \leftarrow \mathcal{D} \cup \{(s, y_s)\}\)</span></p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{n+1} \leftarrow \texttt{fit}(\mathcal{D})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\delta \leftarrow \frac{1}{|B|}\sum_{s \in B} (v(s; \boldsymbol{\theta}_{n+1}) - v(s; \boldsymbol{\theta}_n))^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow n + 1\)</span></p></li>
</ol>
</li>
<li><p><strong>until</strong> (<span class="math notranslate nohighlight">\(\delta &lt; \varepsilon\)</span> or <span class="math notranslate nohighlight">\(n \geq N\)</span>)</p></li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span></p></li>
</ol>
</section>
</div><p>The structure of the above algorithm mirrors value iteration in its core idea of iteratively applying the Bellman operator. However, several key modifications distinguish this fitted variant:</p>
<p>First, rather than applying updates across the entire state space, we compute the operator only at selected base points <span class="math notranslate nohighlight">\(B\)</span>. The resulting values are then stored implicitly through the parameter vector <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> via the fitting step, rather than explicitly as in the tabular case.</p>
<p>The fitting procedure itself may introduce an “inner optimization loop.” For instance, when using neural networks, this involves an iterative gradient descent procedure to optimize the parameters. This creates an interesting parallel with modified policy iteration: just as we might truncate policy evaluation steps there, we can consider variants where this inner loop runs for a fixed number of iterations rather than to convergence.</p>
<p>Finally, the termination criterion from standard value iteration may no longer hold. The classical criterion relied on the sup-norm contractivity property of the Bellman operator — a property that isn’t generally preserved under function approximation. While certain function approximation schemes can maintain this sup-norm contraction property (as we’ll see later), this is the exception rather than the rule.</p>
<section id="parametric-policy-iteration">
<h3>Parametric Policy Iteration<a class="headerlink" href="#parametric-policy-iteration" title="Link to this heading">#</a></h3>
<p>We can extend this idea of fitting partial operator updates to the policy iteration setting. Remember, policy iteration involves iterating in the space of policies rather than in the space of value functions. Given an initial guess on a deterministic decision rule <span class="math notranslate nohighlight">\(d_0\)</span>, we iteratively:</p>
<ol class="arabic simple">
<li><p>Compute the value function for the current policy (policy evaluation)</p></li>
<li><p>Derive a new improved policy (policy improvement)</p></li>
</ol>
<p>When computationally feasible and under the model-based setting, we can solve the policy evaluation step directly as a linear system equation. Alternatively, we could carry out policy evaluation by applying successive approximation to the operator <span class="math notranslate nohighlight">\(L_{d_n}\)</span> until convergence, or as in modified policy iteration, for just a few steps.</p>
<p>To apply the idea of fitting partial updates, we start at the level of the policy evaluation operator <span class="math notranslate nohighlight">\(L_{d_n}\)</span>. For a given decision rule <span class="math notranslate nohighlight">\(d_n\)</span>, this operator in component form is:</p>
<div class="math notranslate nohighlight">
\[
(L_{d_n}v)(s) = r(s,d_n(s)) + \gamma \int v(s')p(ds'|s,d_n(s))
\]</div>
<p>For a set of base points <span class="math notranslate nohighlight">\(B = \{s_1, ..., s_M\}\)</span>, we form our dataset:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{D}_n = \{(s_k, y_k) : s_k \in B\}
\]</div>
<p>where:</p>
<div class="math notranslate nohighlight">
\[
y_k = r(s_k,d_n(s_k)) + \gamma \int v_n(s')p(ds'|s_k,d_n(s_k))
\]</div>
<p>This gives us a way to perform approximate policy evaluation through function fitting. However, we now face the question of how to perform policy improvement in this parametric setting. A key insight comes from the the fact that in the exact form of policy iteration, we don’t need to improve the policy everywhere to guarantee progress. In fact, improving the policy at even a single state is sufficient for convergence.</p>
<p>This suggests a natural approach: rather than trying to approximate an improved policy over the entire state space, we can simply:</p>
<ol class="arabic simple">
<li><p>Compute improved actions at our base points:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
d_{n+1}(s_k) = \arg\max_{a \in \mathcal{A}} \left\{r(s_k,a) + \gamma \int v_n(s')p(ds'|s_k,a)\right\}, \quad \forall s_k \in B
\]</div>
<ol class="arabic simple" start="2">
<li><p>Let the function approximation of the value function implicitly generalize these improvements to other states during the next policy evaluation phase.</p></li>
</ol>
<p>This leads to the following algorithm:</p>
<div class="proof algorithm admonition" id="parametric-policy-iteration">
<p class="admonition-title"><span class="caption-number">Algorithm 16 </span> (Parametric Policy Iteration)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> Given an MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, base points <span class="math notranslate nohighlight">\(B \subset S\)</span>, function approximator class <span class="math notranslate nohighlight">\(v(s; \boldsymbol{\theta})\)</span>, maximum iterations <span class="math notranslate nohighlight">\(N\)</span>, tolerance <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span></p>
<p><strong>Output</strong> Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> for value function approximation</p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span>, decision rules <span class="math notranslate nohighlight">\(\{d_0(s_k)\}_{s_k \in B}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow 0\)</span></p></li>
<li><p><strong>repeat</strong></p>
<ol class="arabic simple">
<li><p>// Policy Evaluation</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D} \leftarrow \emptyset\)</span></p></li>
<li><p>For each <span class="math notranslate nohighlight">\(s_k \in B\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(y_k \leftarrow r(s_k,d_n(s_k)) + \gamma \int v_n(s')p(ds'|s_k,d_n(s_k))\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D} \leftarrow \mathcal{D} \cup \{(s_k, y_k)\}\)</span></p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{n+1} \leftarrow \texttt{fit}(\mathcal{D})\)</span></p></li>
<li><p>// Policy Improvement at Base Points</p></li>
<li><p>For each <span class="math notranslate nohighlight">\(s_k \in B\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(d_{n+1}(s_k) \leftarrow \arg\max_{a \in A} \{r(s_k,a) + \gamma \int v_n(s')p(ds'|s_k,a)\}\)</span></p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow n + 1\)</span></p></li>
</ol>
</li>
<li><p><strong>until</strong> (<span class="math notranslate nohighlight">\(n \geq N\)</span> or convergence criterion met)</p></li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span></p></li>
</ol>
</section>
</div><p>As opposed to exact policy iteration, the iterates of parametric policy iteration need not converge monotonically to the optimal value function. Intuitively, this is because we use function approximation to generalize  from base points to the entire state space which can lead to Value estimates improving at base points but degrading at other states or can cause interference between updates at different states due to the shared parametric representation</p>
</section>
</section>
<section id="q-factor-representation">
<h2>Q-Factor Representation<a class="headerlink" href="#q-factor-representation" title="Link to this heading">#</a></h2>
<p>As we discussed above, Monte Carlo integration is the method of choice when it comes to approximating the effect of the Bellman operator. This is due to both its computational advantages in higher dimensions and its compatibility with the model-free assumption. However, there is an additional important detail that we have neglected to properly cover: extracting actions from values in a model-free fashion. While we can obtain a value function using the Monte Carlo approach described above, we still face the challenge of extracting an optimal policy from this value function.</p>
<p>More precisely, recall that an optimal decision rule takes the form:</p>
<div class="math notranslate nohighlight">
\[
d(s) = \arg\max_{a \in \mathcal{A}} \left\{r(s,a) + \gamma \int v(s')p(ds'|s,a)\right\}
\]</div>
<p>Therefore, even given an optimal value function <span class="math notranslate nohighlight">\(v\)</span>, deriving an optimal policy would still require Monte Carlo integration every time we query the decision rule/policy at a state.</p>
<p>An important idea in dynamic programming is that rather than approximating a state-value function, we can instead approximate a state-action value function. These two functions are related: the value function is the expectation of the Q-function (called Q-factors by some authors in the operations research literature) over the conditional distribution of actions given the current state:</p>
<div class="math notranslate nohighlight">
\[
v(s) = \mathbb{E}[q(s,a)|s]
\]</div>
<p>If <span class="math notranslate nohighlight">\(q^*\)</span> is an optimal state-action value function, then <span class="math notranslate nohighlight">\(v^*(s) = \max_a q^*(s,a)\)</span>. Just as we had a Bellman operator for value functions, we can also define an optimality operator for Q-functions. In component form:</p>
<div class="math notranslate nohighlight">
\[
(\mathrm{L}q)(s,a) = r(s,a) + \gamma \int p(ds'|s,a)\max_{a' \in \mathcal{A}(s')} q(s', a')
\]</div>
<p>Furthermore, this operator for Q-functions is also a contraction in the sup-norm and therefore has a unique fixed point <span class="math notranslate nohighlight">\(q^*\)</span>.</p>
<p>The advantage of iterating over Q-functions rather than value functions is that we can immediately extract optimal actions without having to represent the reward function or transition dynamics directly, nor perform numerical integration. Indeed, an optimal decision rule at state <span class="math notranslate nohighlight">\(s\)</span> is obtained as:</p>
<div class="math notranslate nohighlight">
\[
d(s) = \arg\max_{a \in \mathcal{A}(s)} q(s,a)
\]</div>
<p>With this insight, we can adapt our parametric value iteration algorithm to work with Q-functions:</p>
<div class="proof algorithm admonition" id="parametric-q-value-iteration">
<p class="admonition-title"><span class="caption-number">Algorithm 17 </span> (Parametric Q-Value Iteration)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> Given an MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, base points <span class="math notranslate nohighlight">\(\mathcal{B} \subset S\)</span>, function approximator class <span class="math notranslate nohighlight">\(q(s,a; \boldsymbol{\theta})\)</span>, maximum iterations <span class="math notranslate nohighlight">\(N\)</span>, tolerance <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span></p>
<p><strong>Output</strong> Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> for Q-function approximation</p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span> (e.g., for zero initialization)</p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow 0\)</span></p></li>
<li><p><strong>repeat</strong></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{D} \leftarrow \emptyset\)</span></p></li>
<li><p>For each <span class="math notranslate nohighlight">\((s,a) \in \mathcal{B} \times A\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(y_{s,a} \leftarrow r(s,a) + \gamma \int p(ds'|s,a)\max_{a' \in A} q(s',a'; \boldsymbol{\theta}_n)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D} \leftarrow \mathcal{D} \cup \{((s,a), y_{s,a})\}\)</span></p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{n+1} \leftarrow \texttt{fit}(\mathcal{D})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\delta \leftarrow \frac{1}{|\mathcal{D}||A|}\sum_{(s,a) \in \mathcal{D} \times A} (q(s,a; \boldsymbol{\theta}_{n+1}) - q(s,a; \boldsymbol{\theta}_n))^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow n + 1\)</span></p></li>
</ol>
</li>
<li><p><strong>until</strong> (<span class="math notranslate nohighlight">\(\delta &lt; \varepsilon\)</span> or <span class="math notranslate nohighlight">\(n \geq N\)</span>)</p></li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span></p></li>
</ol>
</section>
</div></section>
<section id="warmstarting-the-choice-of-initialization">
<h2>Warmstarting: The Choice of Initialization<a class="headerlink" href="#warmstarting-the-choice-of-initialization" title="Link to this heading">#</a></h2>
<p>Parametric dynamic programming involves solving a sequence of related optimization problems, one for each fitting procedure at each iteration. While we’ve presented these as independent fitting problems, in practice we can leverage the relationship between successive iterations through careful initialization. This “warmstarting” strategy can significantly impact both computational efficiency and solution quality.</p>
<p>The basic idea is simple: rather than starting each fitting procedure from scratch, we initialize the function approximator with parameters from the previous iteration. This can speed up convergence since successive Q-functions tend to be similar. However, recent work suggests that persistent warmstarting might sometimes be detrimental, potentially leading to a form of overfitting. Alternative “reset” strategies that occasionally reinitialize parameters have shown promise in mitigating this issue.</p>
<p>Here’s how warmstarting can be incorporated into parametric Q-learning with one-step Monte Carlo integration:</p>
<div class="proof algorithm admonition" id="warmstarted-q-learning">
<p class="admonition-title"><span class="caption-number">Algorithm 18 </span> (Warmstarted Parametric Q-Learning with N=1 Monte Carlo Integration)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> Given dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> with transitions <span class="math notranslate nohighlight">\((s, a, r, s')\)</span>, function approximator class <span class="math notranslate nohighlight">\(q(s,a; \boldsymbol{\theta})\)</span>, maximum iterations <span class="math notranslate nohighlight">\(N\)</span>, tolerance <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span>, warmstart frequency <span class="math notranslate nohighlight">\(k\)</span></p>
<p><strong>Output</strong> Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> for Q-function approximation</p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span> randomly</p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow 0\)</span></p></li>
<li><p><strong>repeat</strong></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{D} \leftarrow \emptyset\)</span></p></li>
<li><p>For each <span class="math notranslate nohighlight">\((s,a,r,s') \in \mathcal{D}\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(y_{s,a} \leftarrow r + \gamma \max_{a'} q(s',a'; \boldsymbol{\theta}_n)\)</span>  // One-step Monte Carlo estimate</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D} \leftarrow \mathcal{D} \cup \{((s,a), y_{s,a})\}\)</span></p></li>
</ol>
</li>
<li><p><strong>if</strong> <span class="math notranslate nohighlight">\(n \bmod k = 0\)</span>:  // Reset parameters periodically</p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{temp}\)</span> randomly</p></li>
</ol>
</li>
<li><p><strong>else</strong>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{temp} \leftarrow \boldsymbol{\theta}_n\)</span>  // Warmstart from previous iteration</p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{n+1} \leftarrow \texttt{fit}(\mathcal{D}, \boldsymbol{\theta}_{temp})\)</span>  // Initialize optimizer with <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{temp}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\delta \leftarrow \frac{1}{|\mathcal{D}|}\sum_{(s,a) \in \mathcal{D}} (q(s,a; \boldsymbol{\theta}_{n+1}) - q(s,a; \boldsymbol{\theta}_n))^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow n + 1\)</span></p></li>
</ol>
</li>
<li><p><strong>until</strong> (<span class="math notranslate nohighlight">\(\delta &lt; \varepsilon\)</span> or <span class="math notranslate nohighlight">\(n \geq N\)</span>)</p></li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span></p></li>
</ol>
</section>
</div><p>The main addition here is the periodic reset of parameters (controlled by frequency <span class="math notranslate nohighlight">\(k\)</span>) which helps balance the benefits of warmstarting with the need to avoid potential overfitting. When <span class="math notranslate nohighlight">\(k=\infty\)</span>, we get traditional persistent warmstarting, while <span class="math notranslate nohighlight">\(k=1\)</span> corresponds to training from scratch each iteration.</p>
</section>
<section id="inner-optimization-fit-to-convergence-or-not">
<h2>Inner Optimization: Fit to Convergence or Not?<a class="headerlink" href="#inner-optimization-fit-to-convergence-or-not" title="Link to this heading">#</a></h2>
<p>Beyond the choice of initialization and whether to chain optimization problems through warmstarting, we can also control how we terminate the inner optimization procedure. In the templates presented above, we implicitly assumed that <span class="math notranslate nohighlight">\(\texttt{fit}\)</span> is run to convergence. However, this need not be the case, and different implementations handle this differently.</p>
<p>For example, scikit-learn’s MLPRegressor terminates based on several criteria: when the improvement in loss falls below a tolerance (default <code class="docutils literal notranslate"><span class="pre">tol=1e-4</span></code>), when it reaches the maximum number of iterations (default <code class="docutils literal notranslate"><span class="pre">max_iter=200</span></code>), or when the loss fails to improve for <code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> consecutive epochs. In contrast, ExtraTreesRegressor builds trees deterministically to completion based on its splitting criteria, with termination controlled by parameters like <code class="docutils literal notranslate"><span class="pre">min_samples_split</span></code> and <code class="docutils literal notranslate"><span class="pre">max_depth</span></code>.</p>
<p>The intuition for using early stopping in the inner optimization mirrors that of modified policy iteration in exact dynamic programming. Just as modified policy iteration truncates the Neumann series during policy evaluation rather than solving to convergence, we might only partially optimize our function approximator at each iteration. While this complicates the theoretical analysis, it often works well in practice and can be computationally more efficient.</p>
<p>This perspective helps us understand modern deep reinforcement learning algorithms. For instance, DQN can be viewed as an instance of fitted Q-iteration where the inner optimization is intentionally limited. Here’s how we can formalize this approach:</p>
<div class="proof algorithm admonition" id="early-stopping-fqi">
<p class="admonition-title"><span class="caption-number">Algorithm 19 </span> (Early-Stopping Fitted Q-Iteration)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> Given dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> with transitions <span class="math notranslate nohighlight">\((s, a, r, s')\)</span>, function approximator <span class="math notranslate nohighlight">\(q(s,a; \boldsymbol{\theta})\)</span>, maximum outer iterations <span class="math notranslate nohighlight">\(N_{outer}\)</span>, maximum inner iterations <span class="math notranslate nohighlight">\(N_{inner}\)</span>, outer tolerance <span class="math notranslate nohighlight">\(\varepsilon_{outer}\)</span>, inner tolerance <span class="math notranslate nohighlight">\(\varepsilon_{inner}\)</span></p>
<p><strong>Output</strong> Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> for Q-function approximation</p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span> randomly</p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow 0\)</span></p></li>
<li><p><strong>repeat</strong></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{P} \leftarrow \emptyset\)</span></p></li>
<li><p>For each <span class="math notranslate nohighlight">\((s,a,r,s') \in \mathcal{D}\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(y_{s,a} \leftarrow r + \gamma \max_{a'} q(s',a'; \boldsymbol{\theta}_n)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{P} \leftarrow \mathcal{P} \cup \{((s,a), y_{s,a})\}\)</span></p></li>
</ol>
</li>
<li><p>// Inner optimization loop with early stopping</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{temp} \leftarrow \boldsymbol{\theta}_n\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(k \leftarrow 0\)</span></p></li>
<li><p><strong>repeat</strong></p>
<ol class="arabic simple">
<li><p>Update <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{temp}\)</span> using one step of optimizer on <span class="math notranslate nohighlight">\(\mathcal{P}\)</span></p></li>
<li><p>Compute inner loop loss <span class="math notranslate nohighlight">\(\delta_{inner}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(k \leftarrow k + 1\)</span></p></li>
</ol>
</li>
<li><p><strong>until</strong> (<span class="math notranslate nohighlight">\(\delta_{inner} &lt; \varepsilon_{inner}\)</span> or <span class="math notranslate nohighlight">\(k \geq N_{inner}\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{n+1} \leftarrow \boldsymbol{\theta}_{temp}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\delta_{outer} \leftarrow \frac{1}{|\mathcal{D}|}\sum_{(s,a) \in \mathcal{D}} (q(s,a; \boldsymbol{\theta}_{n+1}) - q(s,a; \boldsymbol{\theta}_n))^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow n + 1\)</span></p></li>
</ol>
</li>
<li><p><strong>until</strong> (<span class="math notranslate nohighlight">\(\delta_{outer} &lt; \varepsilon_{outer}\)</span> or <span class="math notranslate nohighlight">\(n \geq N_{outer}\)</span>)</p></li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span></p></li>
</ol>
</section>
</div><p>This formulation makes explicit the two-level optimization structure and allows us to control the trade-off between inner loop optimization accuracy and overall computational efficiency. When <span class="math notranslate nohighlight">\(N_{inner}=1\)</span>, we recover something closer to DQN’s update rule, while larger values of <span class="math notranslate nohighlight">\(N_{inner}\)</span> bring us closer to the full fitted Q-iteration approach.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="example-methods">
<h1>Example Methods<a class="headerlink" href="#example-methods" title="Link to this heading">#</a></h1>
<p>There are several moving parts we can swap in and out when working with parametric dynamic programming - from the function approximator we choose, to how we warm start things, to the specific methods we use for numerical integration and inner optimization. In this section, we’ll look at some concrete examples and see how they fit into this general framework.</p>
<section id="kernel-based-reinforcement-learning-2002">
<h2>Kernel-Based Reinforcement Learning (2002)<a class="headerlink" href="#kernel-based-reinforcement-learning-2002" title="Link to this heading">#</a></h2>
<p>Ormoneit and Sen’s Kernel-Based Reinforcement Learning (KBRL) <span id="id15">[<a class="reference internal" href="bibliography.html#id35" title="Dirk Ormoneit and Śaunak Sen. Kernel-based reinforcement learning. Machine Learning, 49(2/3):161–178, 2002. URL: http://dx.doi.org/10.1023/A:1017928328829, doi:10.1023/a:1017928328829.">31</a>]</span> helped establish the general paradigm of batch reinforcement learning later advocated by <span id="id16">[<a class="reference internal" href="bibliography.html#id36" title="Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. J. Mach. Learn. Res., 6:503–556, 2005. URL: https://jmlr.org/papers/v6/ernst05a.html.">12</a>]</span>. KBRL is a purely offline method that first collects a fixed set of transitions and then uses kernel regression to solve the optimal control problem through value iteration on this dataset. While the dominant approaches at the time were online methods like temporal difference, KBRL showed that another path to developping reinforcement learning algorithm was possible: one that capable of leveraging advances in supervised learning to provide both theoretical and practical benefits.</p>
<p>As the name suggests, KBRL uses kernel based regression within the general framework of outlined above.</p>
<div class="proof algorithm admonition" id="kernel-based-q-iteration">
<p class="admonition-title"><span class="caption-number">Algorithm 20 </span> (Kernel-Based Q-Value Iteration)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> Given an MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> with observed transitions <span class="math notranslate nohighlight">\((s, a, r, s')\)</span>, kernel bandwidth <span class="math notranslate nohighlight">\(b\)</span>, maximum iterations <span class="math notranslate nohighlight">\(N\)</span>, tolerance <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span></p>
<p><strong>Output</strong> Kernel-based Q-function approximation</p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\hat{Q}_0\)</span> to zero everywhere</p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow 0\)</span></p></li>
<li><p><strong>repeat</strong></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{D} \leftarrow \emptyset\)</span></p></li>
<li><p>For each <span class="math notranslate nohighlight">\((s, a, r, s') \in \mathcal{D}\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(y_{s,a} \leftarrow r + \gamma \max_{a' \in A} \hat{Q}_n(s', a')\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D} \leftarrow \mathcal{D} \cup \{((s,a), y_{s,a})\}\)</span></p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(\hat{Q}_{n+1}(s,a) \leftarrow \sum_{(s_i,a_i,r_i,s_i') \in \mathcal{D}} k_b(s_i, s)\mathbb{1}[a_i=a] y_{s_i,a_i} / \sum_{(s_i,a_i,r_i,s_i') \in \mathcal{D}} k_b(s_i, s)\mathbb{1}[a_i=a]\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\delta \leftarrow \frac{1}{|\mathcal{D}|}\sum_{(s,a,r,s') \in \mathcal{D}} (\hat{Q}_{n+1}(s,a) - \hat{Q}_n(s,a))^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow n + 1\)</span></p></li>
</ol>
</li>
<li><p><strong>until</strong> (<span class="math notranslate nohighlight">\(\delta &lt; \varepsilon\)</span> or <span class="math notranslate nohighlight">\(n \geq N\)</span>)</p></li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(\hat{Q}_n\)</span></p></li>
</ol>
</section>
</div><p>Step 3 is where KBRL uses kernel regression with a normalized weighting kernel:</p>
<div class="math notranslate nohighlight">
\[k_b(x^l_t, x) = \frac{\phi(\|x^l_t - x\|/b)}{\sum_{l'} \phi(\|x^l_{t'} - x\|/b)}\]</div>
<p>where <span class="math notranslate nohighlight">\(\phi\)</span> is a kernel function (often Gaussian) and <span class="math notranslate nohighlight">\(b\)</span> is the bandwidth parameter. Each iteration reuses the entire fixed dataset to re-estimate Q-values through this kernel regression.</p>
<p>An important theoretical contribution of KBRL is showing that this kernel-based approach ensures convergence of the Q-function sequence. The authors prove that, with appropriate choice of kernel bandwidth decreasing with sample size, the method is consistent - the estimated Q-function converges to the true Q-function as the number of samples grows.</p>
<p>The main practical limitation of KBRL is computational - being a batch method, it requires storing and using all transitions at each iteration, leading to quadratic complexity in the number of samples. The authors acknowledge this limitation for online settings, suggesting that modifications like discarding old samples or summarizing data clusters would be needed for online applications. Ernst’s later work with tree-based methods would help address this limitation while maintaining many of the theoretical advantages of the batch approach.</p>
</section>
<section id="ernst-s-fitted-q-iteration-2005">
<h2>Ernst’s Fitted Q Iteration (2005)<a class="headerlink" href="#ernst-s-fitted-q-iteration-2005" title="Link to this heading">#</a></h2>
<p>Ernst’s <span id="id17">[<a class="reference internal" href="bibliography.html#id36" title="Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. J. Mach. Learn. Res., 6:503–556, 2005. URL: https://jmlr.org/papers/v6/ernst05a.html.">12</a>]</span> specific instantiation of parametric q-value iteration uses extremely randomized trees, an extension to random forests proposed by  <span id="id18">Geurts <em>et al.</em> [<a class="reference internal" href="bibliography.html#id37" title="Pierre Geurts, Damien Ernst, and Louis Wehenkel. Extremely randomized trees. Machine Learning, 63(1):3–42, March 2006. URL: http://dx.doi.org/10.1007/s10994-006-6226-1, doi:10.1007/s10994-006-6226-1.">16</a>]</span>. This algorithm became particularly well-known, partly because it was one of the first to demonstrate the advantages of offline reinforcement learning in practice on several challenging benchmarks at the time.</p>
<p>Random Forests and Extra-Trees differ primarily in how they construct individual trees. Random Forests creates diversity in two ways: it resamples the training data (bootstrap) for each tree, and at each node it randomly selects a subset of features but then searches exhaustively for the best cut-point within each selected feature. In contrast, Extra-Trees uses the full training set for each tree and injects randomization differently: at each node, it not only randomly selects features but also randomly selects the cut-points without searching for the optimal one. It then picks the best among these completely random splits according to a variance reduction criterion. This double randomization - in both feature and cut-point selection - combined with using the full dataset makes Extra-Trees faster than Random Forests while maintaining similar predictive accuracy.</p>
<p>An important implementation detail concerns how tree structures can be reused across iterations of fitted Q iteration. With parametric methods like neural networks, warmstarting is straightforward - you simply initialize the weights with values from the previous iteration. For decision trees, the situation is more subtle because the model structure is determined by how splits are chosen at each node. When the number of candidate splits per node is <span class="math notranslate nohighlight">\(K=1\)</span> (totally randomized trees), the algorithm selects both the splitting variable and threshold purely at random, without looking at the target values (the Q-values we’re trying to predict) to evaluate the quality of the split. This means the tree structure only depends on the input variables and random choices, not on what we’re predicting. As a result, we can build the trees once in the first iteration and reuse their structure throughout all iterations, only updating the prediction values at the leaves.</p>
<p>Standard Extra-Trees (<span class="math notranslate nohighlight">\(K&gt;1\)</span>), however, uses target values to choose the best among K random splits by calculating which split best reduces the variance of the predictions. Since these target values change in each iteration of fitted Q iteration (as our estimate of Q evolves), we must rebuild the trees completely. While this is computationally more expensive, it allows the trees to better adapt their structure to capture the evolving Q-function.</p>
<p>The complete algorithm can be formalized as follows:</p>
<div class="proof algorithm admonition" id="extra-trees-fqi">
<p class="admonition-title"><span class="caption-number">Algorithm 21 </span> (Extra-Trees Fitted Q Iteration)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> Given an MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> with observed transitions <span class="math notranslate nohighlight">\((s, a, r, s')\)</span>, Extra-Trees parameters <span class="math notranslate nohighlight">\((K, n_{min}, M)\)</span>, maximum iterations <span class="math notranslate nohighlight">\(N\)</span>, tolerance <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span></p>
<p><strong>Output</strong> Extra-Trees model for Q-function approximation</p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\hat{Q}_0\)</span> to zero everywhere</p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow 0\)</span></p></li>
<li><p><strong>repeat</strong></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{D} \leftarrow \emptyset\)</span></p></li>
<li><p>For each <span class="math notranslate nohighlight">\((s, a, r, s') \in \mathcal{D}\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(y_{s,a} \leftarrow r + \gamma \max_{a' \in A} \hat{Q}_n(s', a')\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D} \leftarrow \mathcal{D} \cup \{((s,a), y_{s,a})\}\)</span></p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(\hat{Q}_{n+1} \leftarrow \text{BuildExtraTrees}(\mathcal{D}, K, n_{min}, M)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\delta \leftarrow \frac{1}{|\mathcal{D}|}\sum_{(s,a,r,s') \in \mathcal{D}} (\hat{Q}_{n+1}(s,a) - \hat{Q}_n(s,a))^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow n + 1\)</span></p></li>
</ol>
</li>
<li><p><strong>until</strong> (<span class="math notranslate nohighlight">\(\delta &lt; \varepsilon\)</span> or <span class="math notranslate nohighlight">\(n \geq N\)</span>)</p></li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(\hat{Q}_n\)</span></p></li>
</ol>
</section>
</div></section>
<section id="neural-fitted-q-iteration-2005">
<h2>Neural Fitted Q Iteration (2005)<a class="headerlink" href="#neural-fitted-q-iteration-2005" title="Link to this heading">#</a></h2>
<p>Riedmiller’s Neural Fitted Q Iteration (NFQI) <span id="id19">[<a class="reference internal" href="bibliography.html#id38" title="Martin A. Riedmiller. Neural fitted Q iteration - first experiences with a data efficient neural reinforcement learning method. In João Gama, Rui Camacho, Pavel Brazdil, Al\'ıpio Jorge, and Lu\'ıs Torgo, editors, Machine Learning: ECML 2005, 16th European Conference on Machine Learning, Porto, Portugal, October 3-7, 2005, Proceedings, volume 3720 of Lecture Notes in Computer Science, 317–328. Springer, 2005. URL: https://doi.org/10.1007/11564096\_32, doi:10.1007/11564096\_32.">35</a>]</span> is a natural instantiation of parametric Q-value iteration where:</p>
<ol class="arabic simple">
<li><p>The function approximator <span class="math notranslate nohighlight">\(q(s,a; \boldsymbol{\theta})\)</span> is a multi-layer perceptron</p></li>
<li><p>The <span class="math notranslate nohighlight">\(\texttt{fit}\)</span> function uses Rprop optimization trained to convergence on each iteration’s pattern set</p></li>
<li><p>The expected next-state values are estimated through Monte Carlo integration with <span class="math notranslate nohighlight">\(N=1\)</span>, using the observed next states from transitions</p></li>
</ol>
<p>Specifically, rather than using numerical quadrature which would require known transition probabilities, NFQ approximates the expected future value using observed transitions:</p>
<div class="math notranslate nohighlight">
\[
\int q_n(s',a')p(ds'|s,a) \approx q_n(s'_{observed},a')
\]</div>
<p>where <span class="math notranslate nohighlight">\(s'_{observed}\)</span> is the actual next state that was observed after taking action <span class="math notranslate nohighlight">\(a\)</span> in state <span class="math notranslate nohighlight">\(s\)</span>. This is equivalent to Monte Carlo integration with a single sample, making the algorithm fully model-free.</p>
<p>The algorithm follows from the parametric Q-value iteration template:</p>
<div class="proof algorithm admonition" id="neural-fitted-q-iteration">
<p class="admonition-title"><span class="caption-number">Algorithm 22 </span> (Neural Fitted Q Iteration)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> Given an MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> with observed transitions <span class="math notranslate nohighlight">\((s, a, r, s')\)</span>, MLP architecture <span class="math notranslate nohighlight">\(q(s,a; \boldsymbol{\theta})\)</span>, maximum iterations <span class="math notranslate nohighlight">\(N\)</span>, tolerance <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span></p>
<p><strong>Output</strong> Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> for Q-function approximation</p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span> randomly</p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow 0\)</span></p></li>
<li><p><strong>repeat</strong></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{D} \leftarrow \emptyset\)</span></p></li>
<li><p>For each <span class="math notranslate nohighlight">\((s,a,r,s') \in \mathcal{D}\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(y_{s,a} \leftarrow r + \gamma \max_{a'} q(s',a'; \boldsymbol{\theta}_n)\)</span>  // Monte Carlo estimate with one sample</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D} \leftarrow \mathcal{D} \cup \{((s,a), y_{s,a})\}\)</span></p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{n+1} \leftarrow \text{Rprop}(\mathcal{D})\)</span> // Train MLP to convergence</p></li>
<li><p><span class="math notranslate nohighlight">\(\delta \leftarrow \frac{1}{|\mathcal{D}||A|}\sum_{(s,a) \in \mathcal{D} \times A} (q(s,a; \boldsymbol{\theta}_{n+1}) - q(s,a; \boldsymbol{\theta}_n))^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow n + 1\)</span></p></li>
</ol>
</li>
<li><p><strong>until</strong> (<span class="math notranslate nohighlight">\(\delta &lt; \varepsilon\)</span> or <span class="math notranslate nohighlight">\(n \geq N\)</span>)</p></li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span></p></li>
</ol>
</section>
</div><p>While NFQI was originally introduced as an offline method with base points collected a priori, the authors also present a variant where base points are collected incrementally. In this online variant, new transitions are gathered using the current policy (greedy with respect to <span class="math notranslate nohighlight">\(Q_k\)</span>) and added to the experience set. This approach proves particularly useful when random exploration cannot efficiently collect representative experiences.</p>
</section>
<section id="deep-q-networks-2013">
<h2>Deep Q Networks (2013)<a class="headerlink" href="#deep-q-networks-2013" title="Link to this heading">#</a></h2>
<p>DQN <span id="id20">[<a class="reference internal" href="bibliography.html#id41" title="Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, and others. Playing atari with deep reinforcement learning. In NIPS Deep Learning Workshop. 2013.">30</a>]</span> is a close relative of NFQI - in fact, Riedmiller, the author of NFQI, was also an author on the DQN paper. What at first glance might look like a different algorithm can actually be understood as a special case of parametric dynamic programming with practical adaptations. Let’s build this connection step by step.</p>
<p>First, let’s start with basic parametric Q-value iteration using a neural network:</p>
<div class="proof algorithm admonition" id="basic-q-iteration">
<p class="admonition-title"><span class="caption-number">Algorithm 23 </span> (Basic Offline Neural Fitted Q-Value Iteration)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> Given an MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, dataset of transitions <span class="math notranslate nohighlight">\(\mathcal{T}\)</span>, neural network <span class="math notranslate nohighlight">\(q(s,a; \boldsymbol{\theta})\)</span>, maximum iterations <span class="math notranslate nohighlight">\(N\)</span>, tolerance <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span>, initialization <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span></p>
<p><strong>Output</strong> Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> for Q-function approximation</p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span> randomly</p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow 0\)</span></p></li>
<li><p><strong>repeat</strong></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}_n \leftarrow \emptyset\)</span>  // Regression dataset</p></li>
<li><p>For each <span class="math notranslate nohighlight">\((s,a,r,s') \in \mathcal{T}\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(y_{s,a} \leftarrow r + \gamma \max_{a' \in A} q(s',a'; \boldsymbol{\theta}_n)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}_n \leftarrow \mathcal{D}_n \cup \{((s,a), y_{s,a})\}\)</span></p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{n+1} \leftarrow \texttt{fit}(\mathcal{D}_n, \boldsymbol{\theta}_0)\)</span> // Fit neural network using built-in convergence criterion</p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow n + 1\)</span></p></li>
</ol>
</li>
<li><p><strong>until</strong> training complete</p></li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span></p></li>
</ol>
</section>
</div><p>Next, let’s open up the <code class="docutils literal notranslate"><span class="pre">fit</span></code> procedure to show the inner optimization loop using gradient descent:</p>
<div class="proof algorithm admonition" id="q-iteration-inner-loop">
<p class="admonition-title"><span class="caption-number">Algorithm 24 </span> (Fitted Q-Value Iteration with Explicit Inner Loop)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> Given MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, dataset of transitions <span class="math notranslate nohighlight">\(\mathcal{T}\)</span>, neural network <span class="math notranslate nohighlight">\(q(s,a; \boldsymbol{\theta})\)</span>, learning rate <span class="math notranslate nohighlight">\(\alpha\)</span>, convergence test <span class="math notranslate nohighlight">\(\texttt{has_converged}(\cdot)\)</span>, initialization <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span>, regression loss function <span class="math notranslate nohighlight">\(\mathcal{L}\)</span></p>
<p><strong>Output</strong> Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> for Q-function approximation</p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span> randomly</p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow 0\)</span>  // Outer iteration index</p></li>
<li><p><strong>repeat</strong></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}_n \leftarrow \emptyset\)</span>  // Regression dataset</p></li>
<li><p>For each <span class="math notranslate nohighlight">\((s,a,r,s') \in \mathcal{T}\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(y_{s,a} \leftarrow r + \gamma \max_{a' \in A} q(s',a'; \boldsymbol{\theta}_n)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}_n \leftarrow \mathcal{D}_n \cup \{((s,a), y_{s,a})\}\)</span></p></li>
</ol>
</li>
<li><p>// Inner optimization loop</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(0)} \leftarrow \boldsymbol{\theta}_0\)</span>  // Start from initial parameters</p></li>
<li><p><span class="math notranslate nohighlight">\(k \leftarrow 0\)</span>  // Inner iteration index</p></li>
<li><p><strong>repeat</strong></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(k+1)} \leftarrow \boldsymbol{\theta}^{(k)} - \alpha \nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta}^{(k)}; \mathcal{D}_n)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(k \leftarrow k + 1\)</span></p></li>
</ol>
</li>
<li><p><strong>until</strong> <span class="math notranslate nohighlight">\(\texttt{has_converged}(\boldsymbol{\theta}^{(0)}, ..., \boldsymbol{\theta}^{(k)}, \mathcal{D}_n)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{n+1} \leftarrow \boldsymbol{\theta}^{(k)}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow n + 1\)</span></p></li>
</ol>
</li>
<li><p><strong>until</strong> training complete</p></li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span></p></li>
</ol>
</section>
</div><section id="warmstarting-and-partial-fitting">
<h3>Warmstarting and Partial Fitting<a class="headerlink" href="#warmstarting-and-partial-fitting" title="Link to this heading">#</a></h3>
<p>A natural modification is to initialize the inner optimization loop with the previous iteration’s parameters - a strategy known as warmstarting - rather than starting from <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span> each time. Additionally, similar to how modified policy iteration performs partial policy evaluation rather than solving to convergence, we can limit ourselves to a fixed number of optimization steps. These pragmatic changes, when combined, yield:</p>
<div class="proof algorithm admonition" id="nfqi-warmstart-partial">
<p class="admonition-title"><span class="caption-number">Algorithm 25 </span> (Neural Fitted Q-Iteration with Warmstarting and Partial Optimization)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> Given MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, dataset of transitions <span class="math notranslate nohighlight">\(\mathcal{T}\)</span>, neural network <span class="math notranslate nohighlight">\(q(s,a; \boldsymbol{\theta})\)</span>, learning rate <span class="math notranslate nohighlight">\(\alpha\)</span>, number of steps <span class="math notranslate nohighlight">\(K\)</span></p>
<p><strong>Output</strong> Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> for Q-function approximation</p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span> randomly</p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow 0\)</span>  // Outer iteration index</p></li>
<li><p><strong>repeat</strong></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}_n \leftarrow \emptyset\)</span>  // Regression dataset</p></li>
<li><p>For each <span class="math notranslate nohighlight">\((s,a,r,s') \in \mathcal{T}\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(y_{s,a} \leftarrow r + \gamma \max_{a' \in A} q(s',a'; \boldsymbol{\theta}_n)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}_n \leftarrow \mathcal{D}_n \cup \{((s,a), y_{s,a})\}\)</span></p></li>
</ol>
</li>
<li><p>// Inner optimization loop with warmstart and fixed steps</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(0)} \leftarrow \boldsymbol{\theta}_n\)</span>  // Warmstart from previous iteration</p></li>
<li><p>For <span class="math notranslate nohighlight">\(k = 0\)</span> to <span class="math notranslate nohighlight">\(K-1\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(k+1)} \leftarrow \boldsymbol{\theta}^{(k)} - \alpha \nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta}^{(k)}; \mathcal{D}_n)\)</span></p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{n+1} \leftarrow \boldsymbol{\theta}^{(K)}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow n + 1\)</span></p></li>
</ol>
</li>
<li><p><strong>until</strong> training complete</p></li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span></p></li>
</ol>
</section>
</div></section>
<section id="flattening-the-updates-with-target-swapping">
<h3>Flattening the Updates with Target Swapping<a class="headerlink" href="#flattening-the-updates-with-target-swapping" title="Link to this heading">#</a></h3>
<p>Now rather than maintaining two sets of indices for the outer and inner levels, we could also “flatten” this algorithm  under a single loop structure using modulo arithmetics. Here’s how we could rewrite it:</p>
<div class="proof algorithm admonition" id="nfqi-flattened-swap">
<p class="admonition-title"><span class="caption-number">Algorithm 26 </span> (Flattened Neural Fitted Q-Iteration)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> Given MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, dataset of transitions <span class="math notranslate nohighlight">\(\mathcal{T}\)</span>, neural network <span class="math notranslate nohighlight">\(q(s,a; \boldsymbol{\theta})\)</span>, learning rate <span class="math notranslate nohighlight">\(\alpha\)</span>, target update frequency <span class="math notranslate nohighlight">\(K\)</span></p>
<p><strong>Output</strong> Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> for Q-function approximation</p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span> randomly</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{target} \leftarrow \boldsymbol{\theta}_0\)</span>  // Initialize target parameters</p></li>
<li><p><span class="math notranslate nohighlight">\(t \leftarrow 0\)</span>  // Single iteration counter</p></li>
<li><p><strong>while</strong> training:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}_t \leftarrow \emptyset\)</span>  // Regression dataset</p></li>
<li><p>For each <span class="math notranslate nohighlight">\((s,a,r,s') \in \mathcal{T}\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(y_{s,a} \leftarrow r + \gamma \max_{a' \in A} q(s',a'; \boldsymbol{\theta}_{target})\)</span>  // Use target parameters</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}_t \leftarrow \mathcal{D}_t \cup \{((s,a), y_{s,a})\}\)</span></p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{t+1} \leftarrow \boldsymbol{\theta}_t - \alpha \nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta}_t; \mathcal{D}_t)\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(t \bmod K = 0\)</span>:  // Every K steps</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{target} \leftarrow \boldsymbol{\theta}_t\)</span>  // Update target parameters</p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(t \leftarrow t + 1\)</span></p></li>
</ol>
</li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_t\)</span></p></li>
</ol>
</section>
</div><p>The flattened version with target parameters achieves exactly the same effect as our previous nested-loop structure with warmstarting and K gradient steps. In the nested version, we would create a dataset using parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span>, then perform K gradient steps to obtain <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{n+1}\)</span>. In our flattened version, we maintain a separate <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{target}\)</span> that gets updated every K steps, ensuring that the dataset <span class="math notranslate nohighlight">\(\mathcal{D}_n\)</span> is created using the same parameters for K consecutive iterations - just as it would be in the nested version. The only difference is that we’ve restructured the algorithm to avoid explicitly nesting the loops, making it more suitable for continuous online training which we are about to introduce. The periodic synchronization of <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{target}\)</span> with the current parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span> effectively marks the boundary of what would have been the outer loop in our previous version.</p>
</section>
<section id="exponential-moving-average-targets">
<h3>Exponential Moving Average Targets<a class="headerlink" href="#exponential-moving-average-targets" title="Link to this heading">#</a></h3>
<p>An alternative to this periodic swap of parameters is to use an exponential moving average (EMA) of the parameters:</p>
<div class="proof algorithm admonition" id="nfqi-flattened-ema">
<p class="admonition-title"><span class="caption-number">Algorithm 27 </span> (Flattened Neural Fitted Q-Iteration with EMA)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> Given MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, dataset of transitions <span class="math notranslate nohighlight">\(\mathcal{T}\)</span>, neural network <span class="math notranslate nohighlight">\(q(s,a; \boldsymbol{\theta})\)</span>, learning rate <span class="math notranslate nohighlight">\(\alpha\)</span>, EMA rate <span class="math notranslate nohighlight">\(\tau\)</span></p>
<p><strong>Output</strong> Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> for Q-function approximation</p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span> randomly</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{target} \leftarrow \boldsymbol{\theta}_0\)</span>  // Initialize target parameters</p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow 0\)</span>  // Single iteration counter</p></li>
<li><p><strong>while</strong> training:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}_n \leftarrow \emptyset\)</span>  // Regression dataset</p></li>
<li><p>For each <span class="math notranslate nohighlight">\((s,a,r,s') \in \mathcal{T}\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(y_{s,a} \leftarrow r + \gamma \max_{a' \in A} q(s',a'; \boldsymbol{\theta}_{target})\)</span>  // Use target parameters</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}_n \leftarrow \mathcal{D}_n \cup \{((s,a), y_{s,a})\}\)</span></p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{n+1} \leftarrow \boldsymbol{\theta}_n - \alpha \nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta}_n; \mathcal{D}_n)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{target} \leftarrow \tau\boldsymbol{\theta}_{n+1} + (1-\tau)\boldsymbol{\theta}_{target}\)</span>  // Smooth update of target parameters</p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow n + 1\)</span></p></li>
</ol>
</li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span></p></li>
</ol>
</section>
</div><p>Note that the original DQN used the periodic swap of parameters rather than EMA targets.
EMA targets (also called “Polyak averaging”) started becoming popular in deep RL with DDPG <span id="id21">[<a class="reference internal" href="bibliography.html#id42" title="Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.">28</a>]</span> where they used a “soft” target update: <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{target} \leftarrow \tau\boldsymbol{\theta} + (1-\tau)\boldsymbol{\theta}_{target}\)</span> with a small <span class="math notranslate nohighlight">\(\tau\)</span> (like 0.001). This has since become a common choice in many algorithms like TD3 <span id="id22">[<a class="reference internal" href="bibliography.html#id43" title="Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In International Conference on Machine Learning (ICML), 1587–1596. 2018.">14</a>]</span> and SAC <span id="id23">[<a class="reference internal" href="bibliography.html#id30" title="Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the 35th International Conference on Machine Learning (ICML), 1861–1870. PMLR, 2018.">22</a>]</span>.</p>
</section>
<section id="online-data-collection-and-experience-replay">
<h3>Online Data Collection and Experience Replay<a class="headerlink" href="#online-data-collection-and-experience-replay" title="Link to this heading">#</a></h3>
<p>Rather than using offline data, we now consider a modification where we incrementally gather samples under our current policy. A common exploration strategy is <span class="math notranslate nohighlight">\(\varepsilon\)</span>-greedy: with probability <span class="math notranslate nohighlight">\(\varepsilon\)</span> we select a random action, and with probability <span class="math notranslate nohighlight">\(1-\varepsilon\)</span> we select the greedy action <span class="math notranslate nohighlight">\(\arg\max_a q(s,a;\boldsymbol{\theta}_n)\)</span>. This ensures we maintain some exploration even as our Q-function estimates improve. Typically <span class="math notranslate nohighlight">\(\varepsilon\)</span> is annealed over time, starting with a high value (e.g., 1.0) to encourage early exploration and gradually decreasing to a small final value (e.g., 0.01) to maintain a minimal level of exploration while mostly exploiting our learned policy.</p>
<div class="proof algorithm admonition" id="online-nfqi-flattened">
<p class="admonition-title"><span class="caption-number">Algorithm 28 </span> (Flattened Online Neural Fitted Q-Iteration)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> Given MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, neural network <span class="math notranslate nohighlight">\(q(s,a; \boldsymbol{\theta})\)</span>, learning rate <span class="math notranslate nohighlight">\(\alpha\)</span>, target update frequency <span class="math notranslate nohighlight">\(K\)</span></p>
<p><strong>Output</strong> Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> for Q-function approximation</p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span> randomly</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{target} \leftarrow \boldsymbol{\theta}_0\)</span>  // Initialize target parameters</p></li>
<li><p>Initialize <span class="math notranslate nohighlight">\(\mathcal{T} \leftarrow \emptyset\)</span>  // Initialize transition dataset</p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow 0\)</span>  // Single iteration counter</p></li>
<li><p><strong>while</strong> training:</p>
<ol class="arabic simple">
<li><p>Observe current state <span class="math notranslate nohighlight">\(s\)</span></p></li>
<li><p>Select action <span class="math notranslate nohighlight">\(a\)</span> using policy derived from <span class="math notranslate nohighlight">\(q(s,\cdot;\boldsymbol{\theta}_n)\)</span> (e.g., ε-greedy)</p></li>
<li><p>Execute <span class="math notranslate nohighlight">\(a\)</span>, observe reward <span class="math notranslate nohighlight">\(r\)</span> and next state <span class="math notranslate nohighlight">\(s'\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{T}_n \leftarrow \mathcal{T}_n \cup \{(s,a,r,s')\}\)</span>  // Add transition to dataset</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}_n \leftarrow \emptyset\)</span>  // Regression dataset</p></li>
<li><p>For each <span class="math notranslate nohighlight">\((s,a,r,s') \in \mathcal{T}_n\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(y_{s,a} \leftarrow r + \gamma \max_{a' \in A} q(s',a'; \boldsymbol{\theta}_{target})\)</span>  // Use target parameters</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}_n \leftarrow \mathcal{D}_n \cup \{((s,a), y_{s,a})\}\)</span></p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{n+1} \leftarrow \boldsymbol{\theta}_n - \alpha \nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta}_n; \mathcal{D}_n)\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(n \bmod K = 0\)</span>:  // Every K steps</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{target} \leftarrow \boldsymbol{\theta}_n\)</span>  // Update target parameters</p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow n + 1\)</span></p></li>
</ol>
</li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span></p></li>
</ol>
</section>
</div><p>This version faces two practical challenges. First, the transition dataset <span class="math notranslate nohighlight">\(\mathcal{T}_n\)</span> grows unbounded over time, creating memory issues. Second, computing gradients over the entire dataset becomes increasingly expensive. These are common challenges in online learning settings, and the standard solutions from supervised learning apply here:</p>
<ol class="arabic simple">
<li><p>Use a fixed-size circular buffer (often called replay buffer, in reference to “experience replay” by <span id="id24">[<a class="reference internal" href="bibliography.html#id46" title="Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning, and teaching. PhD thesis, Carnegie Mellon University, Pittsburgh, PA, USA, 1992. Technical Report, CMU-CS-92-170.">29</a>]</span>) to limit memory usage</p></li>
<li><p>Compute gradients on mini-batches rather than the full dataset</p></li>
</ol>
<p>Here’s how we can modify our algorithm to incorporate these ideas:</p>
<div class="proof algorithm admonition" id="online-nfqi-replay">
<p class="admonition-title"><span class="caption-number">Algorithm 29 </span> (Deep-Q Network)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> Given MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, neural network <span class="math notranslate nohighlight">\(q(s,a; \boldsymbol{\theta})\)</span>, learning rate <span class="math notranslate nohighlight">\(\alpha\)</span>, target update frequency <span class="math notranslate nohighlight">\(K\)</span>, replay buffer size <span class="math notranslate nohighlight">\(B\)</span>, mini-batch size <span class="math notranslate nohighlight">\(b\)</span></p>
<p><strong>Output</strong> Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> for Q-function approximation</p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span> randomly</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{target} \leftarrow \boldsymbol{\theta}_0\)</span>  // Initialize target parameters</p></li>
<li><p>Initialize replay buffer <span class="math notranslate nohighlight">\(\mathcal{R}\)</span> with capacity <span class="math notranslate nohighlight">\(B\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow 0\)</span>  // Single iteration counter</p></li>
<li><p><strong>while</strong> training:</p>
<ol class="arabic simple">
<li><p>Observe current state <span class="math notranslate nohighlight">\(s\)</span></p></li>
<li><p>Select action <span class="math notranslate nohighlight">\(a\)</span> using policy derived from <span class="math notranslate nohighlight">\(q(s,\cdot;\boldsymbol{\theta}_n)\)</span> (e.g., ε-greedy)</p></li>
<li><p>Execute <span class="math notranslate nohighlight">\(a\)</span>, observe reward <span class="math notranslate nohighlight">\(r\)</span> and next state <span class="math notranslate nohighlight">\(s'\)</span></p></li>
<li><p>Store <span class="math notranslate nohighlight">\((s,a,r,s')\)</span> in <span class="math notranslate nohighlight">\(\mathcal{R}\)</span>, replacing oldest if full  // Circular buffer update</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}_n \leftarrow \emptyset\)</span>  // Regression dataset</p></li>
<li><p>Sample mini-batch of <span class="math notranslate nohighlight">\(b\)</span> transitions <span class="math notranslate nohighlight">\((s_i,a_i,r_i,s_i')\)</span> from <span class="math notranslate nohighlight">\(\mathcal{R}\)</span></p></li>
<li><p>For each sampled <span class="math notranslate nohighlight">\((s_i,a_i,r_i,s_i')\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(y_i \leftarrow r_i + \gamma \max_{a' \in A} q(s_i',a'; \boldsymbol{\theta}_{target})\)</span>  // Use target parameters</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}_n \leftarrow \mathcal{D}_n \cup \{((s_i,a_i), y_i)\}\)</span></p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{n+1} \leftarrow \boldsymbol{\theta}_n - \alpha \nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta}_n; \mathcal{D}_n)\)</span> // Replace by RMSProp to obtain DQN</p></li>
<li><p>If <span class="math notranslate nohighlight">\(n \bmod K = 0\)</span>:  // Every K steps</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{target} \leftarrow \boldsymbol{\theta}_n\)</span>  // Update target parameters</p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow n + 1\)</span></p></li>
</ol>
</li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span></p></li>
</ol>
</section>
</div><p>This formulation naturally leads to an important concept in deep reinforcement learning: the replay ratio (or data reuse ratio). In our algorithm, for each new transition we collect, we sample a mini-batch of size b from our replay buffer and perform one update. This means we’re reusing past experiences at a ratio of b:1 - for every new piece of data, we’re learning from b experiences. This ratio can be tuned as a hyperparameter. Higher ratios mean more computation per environment step but better data efficiency, as we’re extracting more learning from each collected transition. This highlights one of the key benefits of experience replay: it allows us to decouple the rate of data collection from the rate of learning updates. Some modern algorithms like SAC or TD3 explicitly tune this ratio, sometimes using multiple gradient steps per environment step to achieve higher data efficiency.</p>
<p>I’ll write a subsection that naturally follows from the previous material and introduces double Q-learning in the context of DQN.</p>
</section>
<section id="double-q-network-variant">
<h3>Double-Q Network Variant<a class="headerlink" href="#double-q-network-variant" title="Link to this heading">#</a></h3>
<p>As we saw earlier, the max operator in the target computation can lead to overestimation of Q-values. This happens because we use the same network to both select and evaluate actions in the target computation: <span class="math notranslate nohighlight">\(y_i \leftarrow r_i + \gamma \max_{a' \in A} q(s_i',a'; \boldsymbol{\theta}_{target})\)</span>. The max operator means we’re both choosing the action that looks best under our current estimates and then using that same set of estimates to evaluate how good that action is, potentially compounding any optimization bias.</p>
<p>Double DQN <span id="id25">Van Hasselt <em>et al.</em> [<a class="reference internal" href="bibliography.html#id47" title="Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. Proceedings of the AAAI Conference on Artificial Intelligence, 2016.">40</a>]</span> addresses this by using the current network parameters to select actions but the target network parameters to evaluate them. This leads to a simple modification of the DQN algorithm:</p>
<div class="proof algorithm admonition" id="double-dqn">
<p class="admonition-title"><span class="caption-number">Algorithm 30 </span> (Double Deep-Q Network)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> Given MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, neural network <span class="math notranslate nohighlight">\(q(s,a; \boldsymbol{\theta})\)</span>, learning rate <span class="math notranslate nohighlight">\(\alpha\)</span>, target update frequency <span class="math notranslate nohighlight">\(K\)</span>, replay buffer size <span class="math notranslate nohighlight">\(B\)</span>, mini-batch size <span class="math notranslate nohighlight">\(b\)</span></p>
<p><strong>Output</strong> Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> for Q-function approximation</p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span> randomly</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{target} \leftarrow \boldsymbol{\theta}_0\)</span>  // Initialize target parameters</p></li>
<li><p>Initialize replay buffer <span class="math notranslate nohighlight">\(\mathcal{R}\)</span> with capacity <span class="math notranslate nohighlight">\(B\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow 0\)</span>  // Single iteration counter</p></li>
<li><p><strong>while</strong> training:</p>
<ol class="arabic simple">
<li><p>Observe current state <span class="math notranslate nohighlight">\(s\)</span></p></li>
<li><p>Select action <span class="math notranslate nohighlight">\(a\)</span> using policy derived from <span class="math notranslate nohighlight">\(q(s,\cdot;\boldsymbol{\theta}_n)\)</span> (e.g., ε-greedy)</p></li>
<li><p>Execute <span class="math notranslate nohighlight">\(a\)</span>, observe reward <span class="math notranslate nohighlight">\(r\)</span> and next state <span class="math notranslate nohighlight">\(s'\)</span></p></li>
<li><p>Store <span class="math notranslate nohighlight">\((s,a,r,s')\)</span> in <span class="math notranslate nohighlight">\(\mathcal{R}\)</span>, replacing oldest if full</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}_n \leftarrow \emptyset\)</span>  // Regression dataset</p></li>
<li><p>Sample mini-batch of <span class="math notranslate nohighlight">\(b\)</span> transitions <span class="math notranslate nohighlight">\((s_i,a_i,r_i,s_i')\)</span> from <span class="math notranslate nohighlight">\(\mathcal{R}\)</span></p></li>
<li><p>For each sampled <span class="math notranslate nohighlight">\((s_i,a_i,r_i,s_i')\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(a^*_i \leftarrow \arg\max_{a' \in A} q(s_i',a'; \boldsymbol{\theta}_n)\)</span>  // Select action using current network</p></li>
<li><p><span class="math notranslate nohighlight">\(y_i \leftarrow r_i + \gamma q(s_i',a^*_i; \boldsymbol{\theta}_{target})\)</span>  // Evaluate using target network</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}_n \leftarrow \mathcal{D}_n \cup \{((s_i,a_i), y_i)\}\)</span></p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{n+1} \leftarrow \boldsymbol{\theta}_n - \alpha \nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta}_n; \mathcal{D}_n)\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(n \bmod K = 0\)</span>:  // Every K steps</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{target} \leftarrow \boldsymbol{\theta}_n\)</span>  // Update target parameters</p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow n + 1\)</span></p></li>
</ol>
</li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span></p></li>
</ol>
</section>
</div><p>The main difference from the original DQN is in step 7, where we now separate action selection from action evaluation. Rather than directly taking the max over the target network’s Q-values, we first select the action using our current network (<span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span>) and then evaluate that specific action using the target network (<span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{target}\)</span>). This simple change has been shown to lead to more stable learning and better final performance across a range of tasks.</p>
</section>
</section>
<section id="deep-q-networks-with-resets-2022">
<h2>Deep Q Networks with Resets (2022)<a class="headerlink" href="#deep-q-networks-with-resets-2022" title="Link to this heading">#</a></h2>
<p>In flattening neural fitted Q-iteration, our field had perhaps lost sight of an important structural element: the choice of inner-loop initializer inherent in the original FQI algorithm. The traditional structure explicitly separated outer iterations (computing targets) from inner optimization (fitting to those targets), with each inner optimization starting fresh from parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span>.</p>
<p>The flattened version with persistent warmstarting seemed like a natural optimization - why throw away learned parameters? However, recent work <span id="id26">[<a class="reference internal" href="bibliography.html#id45" title="Pierluca D'Oro, Max Schwarzer, Evgenii Nikishin, Pierre-Luc Bacon, Marc G. Bellemare, and Aaron C. Courville. Sample-efficient reinforcement learning by breaking the replay ratio barrier. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023.">10</a>]</span> has shown that persistent warmstarting can actually be detrimental to learning. Neural networks tend to lose their ability to learn and generalize over the course of training, suggesting that occasionally starting fresh from <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span> might be beneficial. Here’s how this looks algorithmically in the context of DQN:</p>
<div class="proof algorithm admonition" id="dqn-hard-resets">
<p class="admonition-title"><span class="caption-number">Algorithm 31 </span> (DQN with Hard Resets)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> Given MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, neural network <span class="math notranslate nohighlight">\(q(s,a; \boldsymbol{\theta})\)</span>, learning rate <span class="math notranslate nohighlight">\(\alpha\)</span>, reset interval <span class="math notranslate nohighlight">\(K\)</span>, replay buffer size <span class="math notranslate nohighlight">\(B\)</span>, mini-batch size <span class="math notranslate nohighlight">\(b\)</span></p>
<p><strong>Output</strong> Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> for Q-function approximation</p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span> randomly</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{target} \leftarrow \boldsymbol{\theta}_0\)</span>  // Initialize target parameters</p></li>
<li><p>Initialize replay buffer <span class="math notranslate nohighlight">\(\mathcal{R}\)</span> with capacity <span class="math notranslate nohighlight">\(B\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow 0\)</span>  // Single iteration counter</p></li>
<li><p><strong>while</strong> training:</p>
<ol class="arabic simple">
<li><p>Observe current state <span class="math notranslate nohighlight">\(s\)</span></p></li>
<li><p>Select action <span class="math notranslate nohighlight">\(a\)</span> using policy derived from <span class="math notranslate nohighlight">\(q(s,\cdot;\boldsymbol{\theta}_n)\)</span> (e.g., ε-greedy)</p></li>
<li><p>Execute <span class="math notranslate nohighlight">\(a\)</span>, observe reward <span class="math notranslate nohighlight">\(r\)</span> and next state <span class="math notranslate nohighlight">\(s'\)</span></p></li>
<li><p>Store <span class="math notranslate nohighlight">\((s,a,r,s')\)</span> in <span class="math notranslate nohighlight">\(\mathcal{R}\)</span>, replacing oldest if full</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}_n \leftarrow \emptyset\)</span>  // Regression dataset</p></li>
<li><p>Sample mini-batch of <span class="math notranslate nohighlight">\(b\)</span> transitions <span class="math notranslate nohighlight">\((s_i,a_i,r_i,s_i')\)</span> from <span class="math notranslate nohighlight">\(\mathcal{R}\)</span></p></li>
<li><p>For each sampled <span class="math notranslate nohighlight">\((s_i,a_i,r_i,s_i')\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(y_i \leftarrow r_i + \gamma \max_{a' \in A} q(s_i',a'; \boldsymbol{\theta}_{target})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}_n \leftarrow \mathcal{D}_n \cup \{((s_i,a_i), y_i)\}\)</span></p></li>
</ol>
</li>
<li><p>If <span class="math notranslate nohighlight">\(n \bmod K = 0\)</span>:  // Periodic reset</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{n+1} \leftarrow \boldsymbol{\theta}_0\)</span>  // Reset to initial parameters</p></li>
</ol>
</li>
<li><p>Else:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{n+1} \leftarrow \boldsymbol{\theta}_n - \alpha \nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta}_n; \mathcal{D}_n)\)</span></p></li>
</ol>
</li>
<li><p>If <span class="math notranslate nohighlight">\(n \bmod K = 0\)</span>:  // Every K steps</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{target} \leftarrow \boldsymbol{\theta}_n\)</span>  // Update target parameters</p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow n + 1\)</span></p></li>
</ol>
</li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span></p></li>
</ol>
</section>
</div><p>This algorithm change allows us to push the limits of our update ratio - the number of gradient steps we perform per environment interaction. Without resets, increasing this ratio leads to diminishing returns as the network’s ability to learn degrades. However, by periodically resetting the parameters while maintaining our dataset of transitions, we can perform many more updates per interaction, effectively making our algorithm more “offline” and thus more sample efficient.</p>
<p>The hard reset strategy, while effective, might be too aggressive in some settings as it completely discards learned parameters. An alternative approach is to use a softer form of reset, adapting the “Shrink and Perturb” technique originally introduced by <span id="id27">Ash and Adams [<a class="reference internal" href="bibliography.html#id44" title="Jordan T Ash and Ryan P Adams. Warm-starting and amortization in continual learning. In International Conference on Learning Representations (ICLR). 2020.">4</a>]</span> in the context of continual learning. In their work, they found that neural networks that had been trained on one task could better adapt to new tasks if their parameters were partially reset - interpolated with a fresh initialization - rather than either kept intact or completely reset.</p>
<p>We can adapt this idea to our setting. Instead of completely resetting to <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span>, we can perform a soft reset by interpolating between our current parameters and a fresh random initialization:</p>
<div class="proof algorithm admonition" id="dqn-soft-resets">
<p class="admonition-title"><span class="caption-number">Algorithm 32 </span> (DQN with Shrink and Perturb)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> Given MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, neural network <span class="math notranslate nohighlight">\(q(s,a; \boldsymbol{\theta})\)</span>, learning rate <span class="math notranslate nohighlight">\(\alpha\)</span>, reset interval <span class="math notranslate nohighlight">\(K\)</span>, replay buffer size <span class="math notranslate nohighlight">\(B\)</span>, mini-batch size <span class="math notranslate nohighlight">\(b\)</span>, interpolation coefficient <span class="math notranslate nohighlight">\(\beta\)</span></p>
<p><strong>Output</strong> Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> for Q-function approximation</p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span> randomly</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{target} \leftarrow \boldsymbol{\theta}_0\)</span>  // Initialize target parameters</p></li>
<li><p>Initialize replay buffer <span class="math notranslate nohighlight">\(\mathcal{R}\)</span> with capacity <span class="math notranslate nohighlight">\(B\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow 0\)</span>  // Single iteration counter</p></li>
<li><p><strong>while</strong> training:</p>
<ol class="arabic simple">
<li><p>Observe current state <span class="math notranslate nohighlight">\(s\)</span></p></li>
<li><p>Select action <span class="math notranslate nohighlight">\(a\)</span> using policy derived from <span class="math notranslate nohighlight">\(q(s,\cdot;\boldsymbol{\theta}_n)\)</span> (e.g., ε-greedy)</p></li>
<li><p>Execute <span class="math notranslate nohighlight">\(a\)</span>, observe reward <span class="math notranslate nohighlight">\(r\)</span> and next state <span class="math notranslate nohighlight">\(s'\)</span></p></li>
<li><p>Store <span class="math notranslate nohighlight">\((s,a,r,s')\)</span> in <span class="math notranslate nohighlight">\(\mathcal{R}\)</span>, replacing oldest if full</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}_n \leftarrow \emptyset\)</span>  // Regression dataset</p></li>
<li><p>Sample mini-batch of <span class="math notranslate nohighlight">\(b\)</span> transitions <span class="math notranslate nohighlight">\((s_i,a_i,r_i,s_i')\)</span> from <span class="math notranslate nohighlight">\(\mathcal{R}\)</span></p></li>
<li><p>For each sampled <span class="math notranslate nohighlight">\((s_i,a_i,r_i,s_i')\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(y_i \leftarrow r_i + \gamma \max_{a' \in A} q(s_i',a'; \boldsymbol{\theta}_{target})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}_n \leftarrow \mathcal{D}_n \cup \{((s_i,a_i), y_i)\}\)</span></p></li>
</ol>
</li>
<li><p>If <span class="math notranslate nohighlight">\(n \bmod K = 0\)</span>:  // Periodic soft reset</p>
<ol class="arabic simple">
<li><p>Sample <span class="math notranslate nohighlight">\(\boldsymbol{\phi} \sim\)</span> initializer  // Fresh random parameters</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{n+1} \leftarrow \beta\boldsymbol{\theta}_n + (1-\beta)\boldsymbol{\phi}\)</span></p></li>
</ol>
</li>
<li><p>Else:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{n+1} \leftarrow \boldsymbol{\theta}_n - \alpha \nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta}_n; \mathcal{D}_n)\)</span></p></li>
</ol>
</li>
<li><p>If <span class="math notranslate nohighlight">\(n \bmod K = 0\)</span>:  // Every K steps</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{target} \leftarrow \boldsymbol{\theta}_n\)</span>  // Update target parameters</p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow n + 1\)</span></p></li>
</ol>
</li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span></p></li>
</ol>
</section>
</div><p>The interpolation coefficient <span class="math notranslate nohighlight">\(\beta\)</span> controls how much of the learned parameters we retain, with <span class="math notranslate nohighlight">\(\beta = 0\)</span> recovering the hard reset case and <span class="math notranslate nohighlight">\(\beta = 1\)</span> corresponding to no reset at all. This provides a more flexible approach to restoring learning capability while potentially preserving useful features that have been learned. Like hard resets, this softer variant still enables high update ratios by preventing the degradation of learning capability, but does so in a more gradual way.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="does-parametric-dynamic-programming-converge">
<h1>Does Parametric Dynamic Programming Converge?<a class="headerlink" href="#does-parametric-dynamic-programming-converge" title="Link to this heading">#</a></h1>
<p>So far we have avoided the discussion of convergence and focused on intuitive algorithm development, showing how we can extend successive approximation by computing only a few operator evaluations which then get generalized over the entire domain at each step of the value iteration procedure. Now we turn our attention to understanding the conditions under which this general idea can be shown to converge.</p>
<p>A crucial question to ask is whether our algorithm maintains the contraction property that made value iteration so appealing in the first place - the property that allowed us to show convergence to a unique fixed point. We must be careful here because the contraction mapping theorem is specific to a given norm. In the case of value iteration, we showed the Bellman optimality operator is a contraction in the sup-norm, which aligns naturally with how we compare policies based on their value functions.</p>
<p>The situation becomes more complicated with fitted methods because we are not dealing with just a single operator. At each iteration, we perform exact, unbiased pointwise evaluations of the Bellman operator, but instead of obtaining the next function exactly, we get the closest representable one under our chosen function approximation scheme. A key insight from <span id="id28">Gordon [<a class="reference internal" href="bibliography.html#id39" title="Geoffrey J. Gordon. Stable function approximation in dynamic programming. In Proceedings of the Twelfth International Conference on International Conference on Machine Learning, ICML'95, 261–268. San Francisco, CA, USA, 1995. Morgan Kaufmann Publishers Inc.">18</a>]</span> is that the fitting step can be conceptualized as an additional operator that gets applied on top of the exact Bellman operator to produce the next function parameters. This leads to viewing fitted value methods - which for simplicity we describe only for the value case, though the Q-value setting follows similarly - as the composition of two operators:</p>
<div class="math notranslate nohighlight">
\[v_{n+1} = \Gamma(\mathrm{L}(v_n))\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> is the Bellman operator and <span class="math notranslate nohighlight">\(\Gamma\)</span> represents the function approximation mapping.</p>
<p>Now we arrive at the central question: if <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> was a sup-norm contraction, is <span class="math notranslate nohighlight">\(\Gamma\)</span> composed with <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> still a sup-norm contraction? What conditions must hold for this to be true? This question is fundamental because if we can establish that the composition of these two operators maintains the contraction property in the sup-norm, we get directly that our resulting successive approximation method will converge.</p>
<section id="the-search-for-nonexpansive-operators">
<h2>The Search for Nonexpansive Operators<a class="headerlink" href="#the-search-for-nonexpansive-operators" title="Link to this heading">#</a></h2>
<p>Consider what happens in the fitting step: we have two value functions <span class="math notranslate nohighlight">\(v\)</span> and <span class="math notranslate nohighlight">\(w\)</span>, and after applying the Bellman operator <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> to each, we get new target values that differ by at most <span class="math notranslate nohighlight">\(\gamma\)</span> times their original difference in sup-norm (due to <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> being a <span class="math notranslate nohighlight">\(\gamma\)</span>-contraction in the sup norm). But what happens when we fit to these target values? If the function approximator can exaggerate differences between its target values, even a small difference in the targets could lead to a larger difference in the fitted functions. This would be disastrous - even though the Bellman operator shrinks differences between value functions by a factor of <span class="math notranslate nohighlight">\(\gamma\)</span>, the fitting step could amplify them back up, potentially breaking the contraction property of the composite operator.</p>
<p>In order to ensure that the composite operator is contractive, we need conditions on <span class="math notranslate nohighlight">\(\Gamma\)</span> such that if <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> is a sup-norm contraction then the composition also is. A natural property to consider is when <span class="math notranslate nohighlight">\(\Gamma\)</span> is a non-expansion. By definition, this means that for any functions <span class="math notranslate nohighlight">\(v\)</span> and <span class="math notranslate nohighlight">\(w\)</span>:</p>
<div class="math notranslate nohighlight">
\[\|\Gamma(v) - \Gamma(w)\|_\infty \leq \|v - w\|_\infty\]</div>
<p>This turns out to be exactly what we need, since if <span class="math notranslate nohighlight">\(\Gamma\)</span> is a non-expansion, then for any functions <span class="math notranslate nohighlight">\(v\)</span> and <span class="math notranslate nohighlight">\(w\)</span>:</p>
<div class="math notranslate nohighlight">
\[\|\Gamma(\mathrm{L}(v)) - \Gamma(\mathrm{L}(w))\|_\infty \leq \|L(v) - L(w)\|_\infty \leq \gamma\|v - w\|_\infty\]</div>
<p>The first inequality uses the non-expansion property of <span class="math notranslate nohighlight">\(\Gamma\)</span>, while the second uses the fact that <span class="math notranslate nohighlight">\(\mathrm{L}\)</span> is a <span class="math notranslate nohighlight">\(\gamma\)</span>-contraction. Together they show that the composite operator <span class="math notranslate nohighlight">\(\Gamma \circ L\)</span> remains a <span class="math notranslate nohighlight">\(\gamma\)</span>-contraction.</p>
</section>
<section id="gordon-s-averagers">
<h2>Gordon’s Averagers<a class="headerlink" href="#gordon-s-averagers" title="Link to this heading">#</a></h2>
<p>But which function approximators satisfy this non-expansion property? Gordon shows that “averagers” - approximators that compute their outputs as weighted averages of their training values - are always non-expansions in sup-norm. This includes many common approximation schemes like k-nearest neighbors, linear interpolation, and kernel smoothing with normalized weights. The intuition is that if you’re taking weighted averages with weights that sum to one, you can never extrapolate beyond the range of your training values – these methods “interpolate”.  This theoretical framework explains why simple interpolation methods like k-nearest neighbors have proven remarkably stable in practice, while more sophisticated approximators can fail catastrophically. It suggests a clear design principle: to guarantee convergence, we should either use averagers directly or modify other approximators to ensure they never extrapolate beyond their training targets.</p>
<p>More precisely, a function approximator <span class="math notranslate nohighlight">\(\Gamma\)</span> is an averager if for any state <span class="math notranslate nohighlight">\(s\)</span> and any target function <span class="math notranslate nohighlight">\(v\)</span>, the fitted value can be written as:</p>
<div class="math notranslate nohighlight">
\[\Gamma(v)(s) = \sum_{i=1}^n w_i(s) v(s_i)\]</div>
<p>where the weights <span class="math notranslate nohighlight">\(w_i(s)\)</span> satisfy:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(w_i(s) \geq 0\)</span> for all <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(s\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\sum_{i=1}^n w_i(s) = 1\)</span> for all <span class="math notranslate nohighlight">\(s\)</span></p></li>
<li><p>The weights <span class="math notranslate nohighlight">\(w_i(s)\)</span> depend only on <span class="math notranslate nohighlight">\(s\)</span> and the training points <span class="math notranslate nohighlight">\(\{s_i\}\)</span>, not on the values <span class="math notranslate nohighlight">\(v(s_i)\)</span></p></li>
</ol>
<p>Let <span class="math notranslate nohighlight">\(m = \min_i v(s_i)\)</span> and <span class="math notranslate nohighlight">\(M = \max_i v(s_i)\)</span>. Then:</p>
<div class="math notranslate nohighlight">
\[m = m\sum_i w_i(s) \leq \sum_i w_i(s)v(s_i) \leq M\sum_i w_i(s) = M\]</div>
<p>So <span class="math notranslate nohighlight">\(\Gamma(v)(s) \in [m,M]\)</span> for all <span class="math notranslate nohighlight">\(s\)</span>, meaning the fitted function cannot take values outside the range of its training values. This property is what makes averagers “interpolate” rather than “extrapolate” and is directly related to why they preserve the contraction property when composed with the Bellman operator. To see why averagers are non-expansions, consider two functions <span class="math notranslate nohighlight">\(v\)</span> and <span class="math notranslate nohighlight">\(w\)</span>. At any state <span class="math notranslate nohighlight">\(s\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
|\Gamma(v)(s) - \Gamma(w)(s)| &amp;= \left|\sum_{i=1}^n w_i(s)v(s_i) - \sum_{i=1}^n w_i(s)w(s_i)\right| \\
&amp;= \left|\sum_{i=1}^n w_i(s)(v(s_i) - w(s_i))\right| \\
&amp;\leq \sum_{i=1}^n w_i(s)|v(s_i) - w(s_i)| \\
&amp;\leq \|v - w\|_\infty \sum_{i=1}^n w_i(s) \\
&amp;= \|v - w\|_\infty
\end{align*}\end{split}\]</div>
<p>Since this holds for all <span class="math notranslate nohighlight">\(s\)</span>, we have <span class="math notranslate nohighlight">\(\|\Gamma(v) - \Gamma(w)\|_\infty \leq \|v - w\|_\infty\)</span>, proving that <span class="math notranslate nohighlight">\(\Gamma\)</span> is a non-expansion.</p>
</section>
<section id="which-function-approximators-interpolate-vs-extrapolate">
<h2>Which Function Approximators Interpolate vs Extrapolate?<a class="headerlink" href="#which-function-approximators-interpolate-vs-extrapolate" title="Link to this heading">#</a></h2>
<section id="k-nearest-neighbors-knn">
<h3>K-nearest neighbors (KNN)<a class="headerlink" href="#k-nearest-neighbors-knn" title="Link to this heading">#</a></h3>
<p>Let’s look at specific examples, starting with k-nearest neighbors. For any state <span class="math notranslate nohighlight">\(s\)</span>, let <span class="math notranslate nohighlight">\(s_{(1)}, ..., s_{(k)}\)</span> denote the k nearest training points to <span class="math notranslate nohighlight">\(s\)</span>. Then:</p>
<div class="math notranslate nohighlight">
\[\Gamma(v)(s) = \frac{1}{k}\sum_{i=1}^k v(s_{(i)})\]</div>
<p>This is clearly an averager with weights <span class="math notranslate nohighlight">\(w_i(s) = \frac{1}{k}\)</span> for the k nearest neighbors and 0 for all other points.</p>
<p>For kernel smoothing with a kernel function <span class="math notranslate nohighlight">\(K\)</span>, the fitted value is:</p>
<div class="math notranslate nohighlight">
\[\Gamma(v)(s) = \frac{\sum_{i=1}^n K(s - s_i)v(s_i)}{\sum_{i=1}^n K(s - s_i)}\]</div>
<p>The denominator normalizes the weights to sum to 1, making this an averager with weights <span class="math notranslate nohighlight">\(w_i(s) = \frac{K(s - s_i)}{\sum_{j=1}^n K(s - s_j)}\)</span>.</p>
</section>
<section id="linear-regression">
<h3>Linear Regression<a class="headerlink" href="#linear-regression" title="Link to this heading">#</a></h3>
<p>In contrast, methods like linear regression and neural networks can and often do extrapolate beyond their training targets. More precisely, given a dataset of state-value pairs <span class="math notranslate nohighlight">\(\{(s_i, v(s_i))\}_{i=1}^n\)</span>, these methods fit parameters to minimize some error criterion, and the resulting function <span class="math notranslate nohighlight">\(\Gamma(v)(s)\)</span> may take values outside the interval <span class="math notranslate nohighlight">\([\min_i v(s_i), \max_i v(s_i)]\)</span> even when evaluated at a new state <span class="math notranslate nohighlight">\(s\)</span>. For instance, linear regression finds parameters by minimizing squared error:</p>
<div class="math notranslate nohighlight">
\[\min_{\theta} \sum_{i=1}^n (v(s_i) - \theta^T\phi(s_i))^2\]</div>
<p>The resulting fitted function is:</p>
<div class="math notranslate nohighlight">
\[\Gamma(v)(s) = \phi(s)^T(\Phi^T\Phi)^{-1}\Phi^T v\]</div>
<p>where <span class="math notranslate nohighlight">\(\Phi\)</span> is the feature matrix with rows <span class="math notranslate nohighlight">\(\phi(s_i)^T\)</span>. This cannot be written as a weighted average with weights independent of <span class="math notranslate nohighlight">\(v\)</span>. Indeed, we can construct examples where the fitted value at a point lies outside the range of training values. For example, consider two sets of target values defined on just three points <span class="math notranslate nohighlight">\(s_1 = 0\)</span>, <span class="math notranslate nohighlight">\(s_2 = 1\)</span>, and <span class="math notranslate nohighlight">\(s_3 = 2\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}v = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}, \quad w = \begin{bmatrix} 0 \\ 1 \\ 1 \end{bmatrix}\end{split}\]</div>
<p>Using a single feature <span class="math notranslate nohighlight">\(\phi(s) = s\)</span>, our feature matrix is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\Phi = \begin{bmatrix} 0 \\ 1 \\ 2 \end{bmatrix}\end{split}\]</div>
<p>For function <span class="math notranslate nohighlight">\(v\)</span>, the fitted parameters are:</p>
<div class="math notranslate nohighlight">
\[\theta_v = (\Phi^T\Phi)^{-1}\Phi^T v = \frac{1}{14}(2)\]</div>
<p>And for function <span class="math notranslate nohighlight">\(w\)</span>:</p>
<div class="math notranslate nohighlight">
\[\theta_w = (\Phi^T\Phi)^{-1}\Phi^T w = \frac{1}{14}(8)\]</div>
<p>Now if we evaluate these fitted functions at <span class="math notranslate nohighlight">\(s = 3\)</span> (outside our training points):</p>
<div class="math notranslate nohighlight">
\[\Gamma(v)(3) = 3\theta_v = \frac{6}{14} \approx 0.43\]</div>
<div class="math notranslate nohighlight">
\[\Gamma(w)(3) = 3\theta_w = \frac{24}{14} \approx 1.71\]</div>
<p>Therefore:</p>
<div class="math notranslate nohighlight">
\[|\Gamma(v)(3) - \Gamma(w)(3)| = \frac{18}{14} &gt; 1 = \|v - w\|_\infty\]</div>
</section>
<section id="spline-interpolation">
<h3>Spline Interpolation<a class="headerlink" href="#spline-interpolation" title="Link to this heading">#</a></h3>
<p>Linear interpolation between points – the technique used earlier in this chapter – is an averager since for any point <span class="math notranslate nohighlight">\(s\)</span> between knots <span class="math notranslate nohighlight">\(s_i\)</span> and <span class="math notranslate nohighlight">\(s_{i+1}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\Gamma(v)(s) = \left(\frac{s_{i+1}-s}{s_{i+1}-s_i}\right)v(s_i) + \left(\frac{s-s_i}{s_{i+1}-s_i}\right)v(s_{i+1})\]</div>
<p>The weights sum to 1 and are non-negative. However, cubic splines, despite their smoothness advantages, can violate the non-expansion property. To see this, consider fitting a natural cubic spline to three points:</p>
<div class="math notranslate nohighlight">
\[s_1 = 0,\; s_2 = 1,\; s_3 = 2\]</div>
<p>with two different sets of values:</p>
<div class="math notranslate nohighlight">
\[\begin{split}v = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, \quad w = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}\end{split}\]</div>
<p>The natural cubic spline for <span class="math notranslate nohighlight">\(v\)</span> will overshoot at <span class="math notranslate nohighlight">\(s \approx 0.5\)</span> and undershoot at <span class="math notranslate nohighlight">\(s \approx 1.5\)</span> due to its attempt to minimize curvature, giving values outside the range <span class="math notranslate nohighlight">\([0,1]\)</span>. Meanwhile, <span class="math notranslate nohighlight">\(w\)</span> fits a flat line at 0. Therefore:</p>
<div class="math notranslate nohighlight">
\[\|v - w\|_\infty = 1\]</div>
<p>but</p>
<div class="math notranslate nohighlight">
\[\|\Gamma(v) - \Gamma(w)\|_\infty &gt; 1\]</div>
<p>This illustrates a general principle: methods that try to create smooth functions by minimizing some global criterion (like curvature in splines) often sacrifice the non-expansion property to achieve their smoothness goals.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="dp.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Dynamic Programming</p>
      </div>
    </a>
    <a class="right-next"
       href="cadp.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Policy Parametrization Methods</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Approximate Dynamic Programming</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#smooth-optimality-equations-for-infinite-horizon-mdps">Smooth Optimality Equations for Infinite-Horizon MDPs</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gumbel-noise-on-the-rewards">Gumbel Noise on the Rewards</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#control-as-inference-perspective">Control as Inference Perspective</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#message-passing">Message Passing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deriving-the-optimal-policy">Deriving the Optimal Policy</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regularized-markov-decision-processes">Regularized Markov Decision Processes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#legendre-fenchel-transform">Legendre-Fenchel Transform</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regularized-bellman-operators">Regularized Bellman Operators</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recovering-the-smooth-bellman-equations">Recovering the Smooth Bellman Equations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#approximating-the-bellman-operator-using-numerical-integration">Approximating the Bellman Operator using Numerical Integration</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#discretization-and-numerical-quadrature">Discretization and Numerical Quadrature</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#monte-carlo-integration">Monte Carlo Integration</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overestimation-bias-in-monte-carlo-value-iteration">Overestimation Bias in Monte Carlo Value Iteration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-keane-wolpin-bias-correction-algorithm">The Keane-Wolpin Bias Correction Algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decoupling-selection-and-evaluation">Decoupling Selection and Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#an-hvac-analogy">An HVAC analogy</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#consistency">Consistency</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#parametric-dynamic-programming">Parametric Dynamic Programming</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#partial-updates-in-the-tabular-case">Partial Updates in the Tabular Case</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#partial-updates-by-operator-fitting-parametric-value-iteration">Partial Updates by Operator Fitting: Parametric Value Iteration</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parametric-policy-iteration">Parametric Policy Iteration</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#q-factor-representation">Q-Factor Representation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#warmstarting-the-choice-of-initialization">Warmstarting: The Choice of Initialization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inner-optimization-fit-to-convergence-or-not">Inner Optimization: Fit to Convergence or Not?</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#example-methods">Example Methods</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-based-reinforcement-learning-2002">Kernel-Based Reinforcement Learning (2002)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ernst-s-fitted-q-iteration-2005">Ernst’s Fitted Q Iteration (2005)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-fitted-q-iteration-2005">Neural Fitted Q Iteration (2005)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-q-networks-2013">Deep Q Networks (2013)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warmstarting-and-partial-fitting">Warmstarting and Partial Fitting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#flattening-the-updates-with-target-swapping">Flattening the Updates with Target Swapping</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exponential-moving-average-targets">Exponential Moving Average Targets</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#online-data-collection-and-experience-replay">Online Data Collection and Experience Replay</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#double-q-network-variant">Double-Q Network Variant</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-q-networks-with-resets-2022">Deep Q Networks with Resets (2022)</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#does-parametric-dynamic-programming-converge">Does Parametric Dynamic Programming Converge?</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-search-for-nonexpansive-operators">The Search for Nonexpansive Operators</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gordon-s-averagers">Gordon’s Averagers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#which-function-approximators-interpolate-vs-extrapolate">Which Function Approximators Interpolate vs Extrapolate?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#k-nearest-neighbors-knn">K-nearest neighbors (KNN)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression">Linear Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#spline-interpolation">Spline Interpolation</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Pierre-Luc Bacon
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>