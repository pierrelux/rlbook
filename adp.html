
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>13. Approximate Dynamic Programming &#8212; Pragmatic Reinforcement Learning: Algorithms and Applications</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="_static/graphviz.css?v=fd3f3429" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=87e54e7c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"bm": ["{\\boldsymbol #1}", 1]}, "processEscapes": true}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'adp';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="17. Bibliography" href="bibliography.html" />
    <link rel="prev" title="11. Dynamic Programming" href="dp.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Pragmatic Reinforcement Learning: Algorithms and Applications</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ocp.html">1. Discrete-Time Trajectory Optimization</a></li>



<li class="toctree-l1"><a class="reference internal" href="cocp.html">5. Continuous-Time Trajectory Optimization</a></li>

<li class="toctree-l1"><a class="reference internal" href="mpc.html">7. Model Predictive Control</a></li>



<li class="toctree-l1"><a class="reference internal" href="dp.html">11. Dynamic Programming</a></li>

<li class="toctree-l1 current active"><a class="current reference internal" href="#">13. Approximate Dynamic Programming</a></li>



<li class="toctree-l1"><a class="reference internal" href="bibliography.html">17. Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/pierrelux/rlbook" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pierrelux/rlbook/edit/main/adp.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pierrelux/rlbook/issues/new?title=Issue%20on%20page%20%2Fadp.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/adp.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Approximate Dynamic Programming</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">13. Approximate Dynamic Programming</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#smooth-optimality-equations-for-infinite-horizon-mdps">14. Smooth Optimality Equations for Infinite-Horizon MDPs</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gumbel-noise-on-the-rewards">14.1. Gumbel Noise on the Rewards</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#control-as-inference-perspective">14.2. Control as Inference Perspective</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#message-passing">14.2.1. Message Passing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deriving-the-optimal-policy">14.2.2. Deriving the Optimal Policy</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regularized-markov-decision-processes">14.3. Regularized Markov Decision Processes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#legendre-fenchel-transform">14.3.1. Legendre-Fenchel Transform</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regularized-bellman-operators">14.4. Regularized Bellman Operators</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recovering-the-smooth-bellman-equations">14.4.1. Recovering the Smooth Bellman Equations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#parametric-dynamic-programming">15. Parametric Dynamic Programming</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#partial-updates-in-the-tabular-case">15.1. Partial Updates in the Tabular Case</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#partial-updates-by-operator-fitting-parametric-value-iteration">15.2. Partial Updates by Operator Fitting: Parametric Value Iteration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parametric-policy-iteration">15.3. Parametric Policy Iteration</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#approximate-component-wise-evaluation-of-the-bellman-operator">16. Approximate Component-wise Evaluation of the Bellman Operator</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-quadrature-methods">16.1. Numerical Quadrature Methods</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#monte-carlo-integration">16.2. Monte Carlo Integration</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="approximate-dynamic-programming">
<h1><span class="section-number">13. </span>Approximate Dynamic Programming<a class="headerlink" href="#approximate-dynamic-programming" title="Link to this heading">#</a></h1>
<p>Dynamic programming methods suffer from the curse of dimensionality and can quickly become difficult to apply in practice. Not only this, we may also be dealing with large or continuous state or action spaces. We have seen so far that we could address this problem using discretization, or interpolation. These were already examples of approximate dynamic programming. In this chapter, we will see other forms of approximations meant to facilitate the optimization problem, either by approximating the optimality equations, the value function, or the policy itself.
Approximation theory is at the heart of learning methods, and fundamentally, this chapter will be about the application of learning ideas to solve complex decision-making problems.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="smooth-optimality-equations-for-infinite-horizon-mdps">
<h1><span class="section-number">14. </span>Smooth Optimality Equations for Infinite-Horizon MDPs<a class="headerlink" href="#smooth-optimality-equations-for-infinite-horizon-mdps" title="Link to this heading">#</a></h1>
<p>While the standard Bellman optimality equations use the max operator to determine the best action, an alternative formulation known as the smooth or soft Bellman optimality equations replaces this with a softmax operator. This approach originated from <span id="id1">[<a class="reference internal" href="bibliography.html#id24" title="John Rust. Optimal replacement of gmc bus engines: an empirical model of harold zurcher. Econometrica, 55(5):999-1033, 1987.">28</a>]</span> and was later rediscovered in the context of maximum entropy inverse reinforcement learning <span id="id2">[<a class="reference internal" href="bibliography.html#id25" title="Brian D. Ziebart, Andrew L. Maas, J. Andrew Bagnell, and Anind K. Dey. Maximum entropy inverse reinforcement learning. In Proceedings of the 23rd AAAI Conference on Artificial Intelligence, 1433-1438. 2008.">31</a>]</span>, which then led to soft Q-learning <span id="id3">[<a class="reference internal" href="bibliography.html#id26" title="Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. Proceedings of the 34th International Conference on Machine Learning, 70:1352-1361, 2017.">13</a>]</span> and soft actor-critic <span id="id4">[<a class="reference internal" href="bibliography.html#id30" title="Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the 35th International Conference on Machine Learning (ICML), 1861–1870. PMLR, 2018.">14</a>]</span>, a state-of-the-art deep reinforcement learning algorithm.</p>
<p>In the infinite-horizon setting, the smooth Bellman optimality equations take the form:</p>
<div class="math notranslate nohighlight">
\[ v_\gamma^\star(s) = \frac{1}{\beta} \log \sum_{a \in A_s} \exp\left(\beta\left(r(s, a) + \gamma \sum_{j \in S} p(j | s, a) v_\gamma^\star(j)\right)\right) \]</div>
<p>Adopting an operator-theoretic perspective, we can define a nonlinear operator <span class="math notranslate nohighlight">\(\mathrm{L}_\beta\)</span> such that the smooth value function of an MDP is then the solution to the following fixed-point equation:</p>
<div class="math notranslate nohighlight">
\[ (\mathrm{L}_\beta \mathbf{v})(s) = \frac{1}{\beta} \log \sum_{a \in \mathcal{A}_s} \exp\left(\beta\left(r(s,a) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a) v(j)\right)\right) \]</div>
<p>As <span class="math notranslate nohighlight">\(\beta \to \infty\)</span>, <span class="math notranslate nohighlight">\(\mathrm{L}_\beta\)</span> converges to the standard Bellman operator <span class="math notranslate nohighlight">\(\mathrm{L}\)</span>. Furthermore, it can be shown that the smooth Bellman operator is a contraction mapping in the supremum norm, and therefore has a unique fixed point. However, as opposed to the usual “hard” setting, the fixed point of <span class="math notranslate nohighlight">\(\mathrm{L}_\beta\)</span> is associated with the value function of an optimal stochastic policy defined by the softmax distribution:</p>
<div class="math notranslate nohighlight">
\[ d(a|s) = \frac{\exp\left(\beta\left(r(s,a) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a) v_\gamma^\star(j)\right)\right)}{\sum_{a' \in \mathcal{A}_s} \exp\left(\beta\left(r(s,a') + \gamma \sum_{j \in \mathcal{S}} p(j|s,a') v_\gamma^\star(j)\right)\right)} \]</div>
<p>Despite the confusing terminology, the above “softmax” policy is simply the smooth counterpart to the argmax operator in the original optimality equation: it acts as a soft-argmax.</p>
<p>This formulation is interesting for several reasons. First, smoothness is a desirable property from an optimization standpoint. Unlike <span class="math notranslate nohighlight">\(\gamma\)</span>, we view <span class="math notranslate nohighlight">\(\beta\)</span> as a hyperparameter of our algorithm, which we can control to achieve the desired level of accuracy.</p>
<p>Second, while presented from an intuitive standpoint where we replace the max by the log-sum-exp (a smooth maximum) and the argmax by the softmax (a smooth argmax), this formulation can also be obtained from various other perspectives, offering theoretical tools and solution methods. For example, <span id="id5">Rust [<a class="reference internal" href="bibliography.html#id24" title="John Rust. Optimal replacement of gmc bus engines: an empirical model of harold zurcher. Econometrica, 55(5):999-1033, 1987.">28</a>]</span> derived this algorithm by considering a setting in which the rewards are stochastic and perturbed by a Gumbel noise variable. When considering the corresponding augmented state space and integrating the noise, we obtain smooth equations. This interpretation is leveraged by Rust for modeling purposes.</p>
<p>There is also a way to obtain this equation by starting from the energy-based formulation often used in supervised learning, in which we convert an unnormalized probability distribution into a distribution using the softmax transformation. This is essentially what <span id="id6">Ziebart <em>et al.</em> [<a class="reference internal" href="bibliography.html#id25" title="Brian D. Ziebart, Andrew L. Maas, J. Andrew Bagnell, and Anind K. Dey. Maximum entropy inverse reinforcement learning. In Proceedings of the 23rd AAAI Conference on Artificial Intelligence, 1433-1438. 2008.">31</a>]</span> did in their paper. Furthermore, this perspective bridges with the literature on probabilistic graphical models, in which we can now cast the problem of finding an optimal smooth policy into one of maximum likelihood estimation (an inference problem). This is the idea of control as inference, which also admits the converse - that of inference as control - used nowadays for deriving fast samples and amortized inference techniques using reinforcement learning <span id="id7">[<a class="reference internal" href="bibliography.html#id27" title="Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Reinforcement learning as a framework for control: a survey. arXiv preprint arXiv:1806.04222, 2018.">22</a>]</span>.</p>
<p>Finally, it’s worth noting that we can also derive this form by considering an entropy-regularized formulation in which we penalize for the entropy of our policy in the reward function term. This formulation admits a solution that coincides with the smooth Bellman equations <span id="id8">[<a class="reference internal" href="bibliography.html#id26" title="Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. Proceedings of the 34th International Conference on Machine Learning, 70:1352-1361, 2017.">13</a>]</span>.</p>
<section id="gumbel-noise-on-the-rewards">
<h2><span class="section-number">14.1. </span>Gumbel Noise on the Rewards<a class="headerlink" href="#gumbel-noise-on-the-rewards" title="Link to this heading">#</a></h2>
<p>We can obtain the smooth Bellman equation by considering a setting in which we have Gumbel noise added to the reward function. More precisely, we define an MDP whose state space is now that of <span class="math notranslate nohighlight">\(\tilde{s} = (s, \epsilon)\)</span>, where the reward function is given by</p>
<div class="math notranslate nohighlight">
\[\tilde{r}(\tilde{s}, a) = r(s,a) + \epsilon(a)\]</div>
<p>and where the transition probability function is:</p>
<div class="math notranslate nohighlight">
\[ p(\tilde{s}' | \tilde{s}, a) = p(s' | s, a) \cdot p(\epsilon') \]</div>
<p>This expression stems from the conditional independence assumption that we make on the noise variable given the state.</p>
<p>Furthermore, we assume that <span class="math notranslate nohighlight">\(\epsilon(a)\)</span> is a random variable following a Gumbel distribution with location 0 and scale <span class="math notranslate nohighlight">\(1/\beta\)</span>. The Gumbel distribution is a continuous probability distribution used to model the maximum (or minimum) of a number of samples of various distributions. Its probability density function is:</p>
<div class="math notranslate nohighlight">
\[ f(x; \mu, \beta) = \frac{1}{\beta}\exp\left(-\left(\frac{x-\mu}{\beta}+\exp\left(-\frac{x-\mu}{\beta}\right)\right)\right) \]</div>
<p>where <span class="math notranslate nohighlight">\(\mu\)</span> is the location parameter and <span class="math notranslate nohighlight">\(\beta\)</span> is the scale parameter. To generate a Gumbel-distributed random variable, one can use the inverse transform sampling method: and set <span class="math notranslate nohighlight">\( X = \mu - \beta \ln(-\ln(U)) \)</span>
where <span class="math notranslate nohighlight">\(U\)</span> is a uniform random variable on the interval <span class="math notranslate nohighlight">\((0,1)\)</span>.</p>
<p>The Bellman equation in this augmented state space becomes:</p>
<div class="math notranslate nohighlight">
\[ v_\gamma^\star(\tilde{s}) = \max_{a \in \mathcal{A}_s} \left\{ \tilde{r}(\tilde{s},a) + \gamma \mathbb{E}_{}\left[v_\gamma^\star(\tilde{s}')\mid \tilde{s}, a\right] \right\} \]</div>
<p>Furthermore, since all we did is to define another MDP, we still have a contraction and an optimal stationary policy <span class="math notranslate nohighlight">\(d^\infty = (d, d, ...)\)</span> can be found via the following deterministic Markovian decision rule:</p>
<div class="math notranslate nohighlight">
\[
d(\tilde{s})  \in \operatorname{argmax}_{a \in \mathcal{A}_s} \left\{ \tilde{r}(\tilde{s},a) + \gamma \mathbb{E}_{}\left[v_\gamma^\star(\tilde{s}')\mid \tilde{s}, a\right] \right\}
\]</div>
<p>Note how the expectation is now over the next augmented state space and is therefore both over the next state in the original MDP and over the next perturbation. While in the general case there isn’t much that we can do to simplify the expression for the expectation over the next state in the MDP, we can however leverage a remarkable property of the Gumbel distribution which allows us to eliminate the <span class="math notranslate nohighlight">\(\epsilon\)</span> term in the above and recover the familiar smooth Bellman equation.</p>
<p>For a set of random variables <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span>, each following a Gumbel distribution with location parameters <span class="math notranslate nohighlight">\(\mu_1, \ldots, \mu_n\)</span> and scale parameter <span class="math notranslate nohighlight">\(1/\beta\)</span>, extreme value theory tells us that:</p>
<div class="math notranslate nohighlight">
\[ \mathbb{E}\left[\max_{i} X_i\right] = \frac{1}{\beta} \log \sum_{i=1}^n \exp(\beta\mu_i) \]</div>
<p>In our case, each <span class="math notranslate nohighlight">\(X_i\)</span> corresponds to <span class="math notranslate nohighlight">\(r(s,a_i) + \epsilon(a_i) + \gamma \mathbb{E}_{s'}[v(s')]\)</span> for a given action <span class="math notranslate nohighlight">\(a_i\)</span>. The location parameter <span class="math notranslate nohighlight">\(\mu_i\)</span> is <span class="math notranslate nohighlight">\(r(s,a_i) + \gamma \mathbb{E}_{s'}[v(s')]\)</span>, and the scale parameter is <span class="math notranslate nohighlight">\(1/\beta\)</span>.</p>
<p>Applying this result to our problem, and taking the expectation over the noise <span class="math notranslate nohighlight">\(\epsilon\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{align*}
v_\gamma^\star(s,\epsilon) &amp;= \max_{a \in \mathcal{A}_s} \left\{ r(s,a) + \epsilon(a) + \gamma \mathbb{E}_{s', \epsilon'}\left[v_\gamma^\star(s',\epsilon')\mid s, \epsilon, a\right] \right\} \\
\mathbb{E}_\epsilon[v_\gamma^\star(s,\epsilon)] &amp;= \mathbb{E}_\epsilon\left[\max_{a \in \mathcal{A}_s} \left\{ r(s,a) + \epsilon(a) + \gamma \mathbb{E}_{s', \epsilon'}\left[v_\gamma^\star(s',\epsilon')\mid s, \epsilon, a\right] \right\}\right] \\
&amp;= \frac{1}{\beta} \log \sum_{a \in \mathcal{A}_s} \exp\left(\beta\left(r(s,a) + \gamma \mathbb{E}_{s', \epsilon'}\left[v_\gamma^\star(s',\epsilon')\mid s, a\right]\right)\right) \\
&amp;= \frac{1}{\beta} \log \sum_{a \in \mathcal{A}_s} \exp\left(\beta\left(r(s,a) + \gamma \mathbb{E}_{s'}\left[\mathbb{E}_{\epsilon'}[v_\gamma^\star(s',\epsilon')]\mid s, a\right]\right)\right)
\end{align*}
\end{split}\]</div>
<p>If we define <span class="math notranslate nohighlight">\(v_\gamma^\star(s) = \mathbb{E}_\epsilon[v_\gamma^\star(s,\epsilon)]\)</span>, we obtain the smooth Bellman equation:</p>
<div class="math notranslate nohighlight">
\[ v_\gamma^\star(s) = \frac{1}{\beta} \log \sum_{a \in \mathcal{A}_s} \exp\left(\beta\left(r(s,a) + \gamma \mathbb{E}_{s'}\left[v_\gamma^\star(s')\mid s, a\right]\right)\right) \]</div>
<p>This final equation is the smooth Bellman equation, which we derived by introducing Gumbel noise to the reward function and leveraging properties of the Gumbel distribution and extreme value theory.</p>
<p>Now, in the same way that we have been able to simplify and specialize the form of the value function under Gumbel noise, we can also derive an expression for the corresponding optimal policy. To see this, we apply similar steps and start with the optimal decision rule for the augmented MDP:</p>
<div class="math notranslate nohighlight">
\[
d(\tilde{s}) \in \operatorname{argmax}_{a \in \mathcal{A}_s} \left\{ \tilde{r}(\tilde{s},a) + \gamma \mathbb{E}_{}\left[v_\gamma^\star(\tilde{s}') \mid \tilde{s}, a\right] \right\}
\]</div>
<p>In order to simplify this expression by taking the expectation over the noise variable, we define an indicator function for the event that action <span class="math notranslate nohighlight">\(a\)</span> is in the set of optimal actions:</p>
<div class="math notranslate nohighlight">
\[\begin{split} I_a(\epsilon) = \begin{cases} 
   1 &amp; \text{if } a \in \operatorname{argmax}_{a' \in \mathcal{A}_s} \left\{ r(s,a') + \epsilon(a') + \gamma \mathbb{E}_{s', \epsilon'}\left[v_\gamma^\star(s',\epsilon')\mid s, a'\right] \right\} \\
   0 &amp; \text{otherwise}
   \end{cases} \end{split}\]</div>
<p>Note that this definition allows us to recover the original expression since:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
d(s,\epsilon) &amp;= \left\{a \in \mathcal{A}_s : I_a(\epsilon) = 1\right\} \\
&amp;= \left\{a \in \mathcal{A}_s : a \in \operatorname{argmax}_{a' \in \mathcal{A}_s} \left\{ r(s,a') + \epsilon(a') + \gamma \mathbb{E}_{s', \epsilon'}\left[v_\gamma^\star(s',\epsilon')\mid s, \epsilon,  a'\right] \right\}\right\}
\end{align*}
\end{split}\]</div>
<p>This set-valued – but determinisic – function <span class="math notranslate nohighlight">\(d(s,\epsilon)\)</span> gives us the set of optimal actions for a given state <span class="math notranslate nohighlight">\(s\)</span> and noise realization <span class="math notranslate nohighlight">\(\epsilon\)</span>. For simplicity, consider the case where the optimal set of actions at <span class="math notranslate nohighlight">\(s\)</span> is a singleton such that taking the expection over the noise variable gives us:
$<span class="math notranslate nohighlight">\( \begin{align*}
\mathbb{E}_\epsilon[I_a(\epsilon)] = \mathbb{P}\left(a \in \operatorname{argmax}_{a' \in \mathcal{A}_s} \left\{ r(s,a') + \epsilon(a') + \gamma \mathbb{E}_{s', \epsilon'}\left[v_\gamma^\star(s',\epsilon')\mid s, \epsilon, a'\right] \right\}\right)
\end{align*} \)</span>$</p>
<p>Now, we can leverage a key property of the Gumbel distribution. For a set of random variables <span class="math notranslate nohighlight">\(\{X_i = \mu_i + \epsilon_i\}\)</span> where <span class="math notranslate nohighlight">\(\epsilon_i\)</span> are i.i.d. Gumbel(0, 1/β) random variables, we have:</p>
<div class="math notranslate nohighlight">
\[ P(X_i \geq X_j, \forall j \neq i) = \frac{\exp(\beta\mu_i)}{\sum_j \exp(\beta\mu_j)} \]</div>
<p>In our case, <span class="math notranslate nohighlight">\(X_a = r(s,a) + \epsilon(a) + \gamma \mathbb{E}_{s', \epsilon'}\left[v_\gamma^\star(s',\epsilon')\mid s, a\right]\)</span> for each action <span class="math notranslate nohighlight">\(a\)</span>, with <span class="math notranslate nohighlight">\(\mu_a = r(s,a) + \gamma \mathbb{E}_{s', \epsilon'}\left[v_\gamma^\star(s',\epsilon')\mid s, a\right]\)</span>.</p>
<p>Applying this property and using the definition <span class="math notranslate nohighlight">\(v_\gamma^\star(s) = \mathbb{E}_\epsilon[v_\gamma^\star(s,\epsilon)]\)</span>, we get:</p>
<div class="math notranslate nohighlight">
\[ d(a|s) = \frac{\exp\left(\beta\left(r(s,a) + \gamma \mathbb{E}_{s'}\left[v_\gamma^\star(s')\mid s, a\right]\right)\right)}{\sum_{a' \in \mathcal{A}_s} \exp\left(\beta\left(r(s,a') + \gamma \mathbb{E}_{s'}\left[v_\gamma^\star(s')\mid s, a'\right]\right)\right)} \]</div>
<p>This gives us the optimal stochastic policy for the smooth MDP. Note that as <span class="math notranslate nohighlight">\(\beta \to \infty\)</span>, this policy approaches the deterministic policy of the original MDP, while for finite <span class="math notranslate nohighlight">\(\beta\)</span>, it gives a stochastic policy.</p>
</section>
<section id="control-as-inference-perspective">
<h2><span class="section-number">14.2. </span>Control as Inference Perspective<a class="headerlink" href="#control-as-inference-perspective" title="Link to this heading">#</a></h2>
<p>The smooth Bellman optimality equations can also be derived from probabilistic inference perspective. To see this, let’s go back to the idea from the previous section in which we introduced an indicator function <span class="math notranslate nohighlight">\(I_a(\epsilon)\)</span> to represent whether an action <span class="math notranslate nohighlight">\(a\)</span> is optimal given a particular realization of the noise <span class="math notranslate nohighlight">\(\epsilon\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split} I_a(\epsilon) = \begin{cases} 
   1 &amp; \text{if } a \in \operatorname{argmax}_{a' \in \mathcal{A}_s} \left\{ r(s,a') + \epsilon(a') + \gamma \mathbb{E}_{s', \epsilon'}\left[v_\gamma^\star(s',\epsilon')\mid s, a'\right] \right\} \\
   0 &amp; \text{otherwise}
   \end{cases} \end{split}\]</div>
<p>When we took the expectation over the noise <span class="math notranslate nohighlight">\(\epsilon\)</span>, we obtained a soft version of this indicator:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{align*}
\mathbb{E}_\epsilon[I_a(\epsilon)] &amp;= \mathbb{P}\left(a \in \operatorname{argmax}_{a' \in \mathcal{A}_s} \left\{ r(s,a') + \epsilon(a') + \gamma \mathbb{E}_{s', \epsilon'}\left[v_\gamma^\star(s',\epsilon')\mid s, \epsilon, a'\right] \right\}\right) \\
&amp;= \frac{\exp\left(\beta\left(r(s,a) + \gamma \mathbb{E}_{s'}\left[v_\gamma^\star(s')\mid s, a\right]\right)\right)}{\sum_{a' \in \mathcal{A}_s} \exp\left(\beta\left(r(s,a') + \gamma \mathbb{E}_{s'}\left[v_\gamma^\star(s')\mid s, a'\right]\right)\right)}
\end{align*} \end{split}\]</div>
<p>Given this indicator function, we can “infer” the optimal action in any state. This is the intuition and starting point behind the control as inference perspective in which we directly define a continuous-valued “optimality” variable <span class="math notranslate nohighlight">\(O_t\)</span> at each time step <span class="math notranslate nohighlight">\(t\)</span>. We define the probability of optimality given a state-action pair as:</p>
<div class="math notranslate nohighlight">
\[ p(O_t = 1 | s_t, a_t) = \exp(\beta r(s_t, a_t)) \]</div>
<p>Building on this notion of soft optimality, we can formulate the MDP as a probabilistic graphical model. We define the following probabilities:</p>
<ol class="arabic simple">
<li><p>State transition probability: <span class="math notranslate nohighlight">\(p(s_{t+1} | s_t, a_t)\)</span> (given by the MDP dynamics)</p></li>
<li><p>Prior policy: <span class="math notranslate nohighlight">\(p(a_t | s_t)\)</span> (which we’ll assume to be uniform for simplicity)</p></li>
<li><p>Optimality probability: <span class="math notranslate nohighlight">\(p(O_t = 1 | s_t, a_t) = \exp(\beta r(s_t, a_t))\)</span></p></li>
</ol>
<p>This formulation encodes the idea that more rewarding state-action pairs are more likely to be “optimal,” which directly parallels the soft assignment of optimality we obtained by taking the expectation over the Gumbel noise.</p>
<p>The control problem can now be framed as an inference problem: we want to find the posterior distribution over actions given that all time steps are optimal:</p>
<div class="math notranslate nohighlight">
\[ p(a_t | s_t, O_{1:T} = 1) \]</div>
<p>where <span class="math notranslate nohighlight">\(O_{1:T} = 1\)</span> means <span class="math notranslate nohighlight">\(O_t = 1\)</span> for all <span class="math notranslate nohighlight">\(t\)</span> from 1 to T.</p>
<section id="message-passing">
<h3><span class="section-number">14.2.1. </span>Message Passing<a class="headerlink" href="#message-passing" title="Link to this heading">#</a></h3>
<p>To solve this inference problem, we can use a technique from probabilistic graphical models called message passing, specifically the belief propagation algorithm. Message passing is a way to efficiently compute marginal distributions in a graphical model by passing local messages between nodes. Messages are passed between nodes in both forward and backward directions. Each message represents a belief about the distribution of a variable, based on the information available to the sending node. After messages have been passed, each node updates its belief about its associated variable by combining all incoming messages.</p>
<p>In our specific case, we’re particularly interested in the backward messages, which propagate information about future optimality backwards in time. Let’s define the backward message <span class="math notranslate nohighlight">\(\beta_t(s_t)\)</span> as:</p>
<div class="math notranslate nohighlight">
\[ \beta_t(s_t) = p(O_{t:T} = 1 | s_t) \]</div>
<p>This represents the probability of optimality for all future time steps given the current state. We can compute this recursively:</p>
<div class="math notranslate nohighlight">
\[ \beta_t(s_t) = \sum_{a_t} p(a_t | s_t) p(O_t = 1 | s_t, a_t) \sum_{s_{t+1}} p(s_{t+1} | s_t, a_t) \beta_{t+1}(s_{t+1}) \]</div>
<p>Taking the log and assuming a uniform prior over actions, we get:</p>
<div class="math notranslate nohighlight">
\[ \log \beta_t(s_t) = \log \sum_{a_t} \sum_{s_{t+1}} p(s_{t+1} | s_t, a_t) \exp(\beta (r(s_t, a_t) + \gamma V(s_{t+1}) + \frac{1}{\beta} \log \beta_{t+1}(s_{t+1}))) \]</div>
<p>If we define the soft value function as <span class="math notranslate nohighlight">\(V_t(s_t) = \frac{1}{\beta} \log \beta_t(s_t)\)</span>, we can rewrite the above equation as:</p>
<div class="math notranslate nohighlight">
\[ V_t(s_t) = \frac{1}{\beta} \log \sum_{a_t} \sum_{s_{t+1}} p(s_{t+1} | s_t, a_t) \exp(\beta (r(s_t, a_t) + \gamma V_{t+1}(s_{t+1}))) \]</div>
<p>This is exactly the smooth Bellman equation we derived earlier, but now interpreted as the result of probabilistic inference in a graphical model.</p>
</section>
<section id="deriving-the-optimal-policy">
<h3><span class="section-number">14.2.2. </span>Deriving the Optimal Policy<a class="headerlink" href="#deriving-the-optimal-policy" title="Link to this heading">#</a></h3>
<p>The backward message recursion we derived earlier assumes a uniform prior policy <span class="math notranslate nohighlight">\(p(a_t | s_t)\)</span>. However, our goal is to find not just any policy, but an optimal one. We can extract this optimal policy efficiently by computing the posterior distribution over actions given our backward messages.</p>
<p>Starting from the definition of conditional probability and applying Bayes’ rule, we can write:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{align}
p(a_t | s_t, O_{1:T} = 1) &amp;= \frac{p(O_{1:T} = 1 | s_t, a_t) p(a_t | s_t)}{p(O_{1:T} = 1 | s_t)} \\
&amp;\propto p(a_t | s_t) p(O_t = 1 | s_t, a_t) p(O_{t+1:T} = 1 | s_t, a_t) \\
&amp;= p(a_t | s_t) p(O_t = 1 | s_t, a_t) \sum_{s_{t+1}} p(s_{t+1} | s_t, a_t) \beta_{t+1}(s_{t+1})
\end{align} \end{split}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\beta_{t+1}(s_{t+1}) = p(O_{t+1:T} = 1 | s_{t+1})\)</span> is our backward message.</p>
<p>Now, let’s substitute our definitions for the optimality probability and the soft value function:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{align}
p(a_t | s_t, O_{1:T} = 1) &amp;\propto p(a_t | s_t) \exp(\beta r(s_t, a_t)) \sum_{s_{t+1}} p(s_{t+1} | s_t, a_t) \exp(\beta \gamma V_{t+1}(s_{t+1})) \\
&amp;= p(a_t | s_t) \exp(\beta (r(s_t, a_t) + \gamma \sum_{s_{t+1}} p(s_{t+1} | s_t, a_t) V_{t+1}(s_{t+1})))
\end{align} \end{split}\]</div>
<p>After normalization, and assuming a uniform prior <span class="math notranslate nohighlight">\(p(a_t | s_t)\)</span>, we obtain the randomized decision rule:</p>
<div class="math notranslate nohighlight">
\[ d(a_t | s_t) = \frac{\exp(\beta (r(s_t, a_t) + \gamma \sum_{s_{t+1}} p(s_{t+1} | s_t, a_t) V_{t+1}(s_{t+1})))}{\sum_{a'_t} \exp(\beta (r(s_t, a'_t) + \gamma \sum_{s_{t+1}} p(s_{t+1} | s_t, a'_t) V_{t+1}(s_{t+1})))} \]</div>
</section>
</section>
<section id="regularized-markov-decision-processes">
<h2><span class="section-number">14.3. </span>Regularized Markov Decision Processes<a class="headerlink" href="#regularized-markov-decision-processes" title="Link to this heading">#</a></h2>
<p>Regularized MDPs <span id="id9">[<a class="reference internal" href="bibliography.html#id28" title="Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized Markov decision processes. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, 2160–2169. PMLR, 09–15 Jun 2019. URL: https://proceedings.mlr.press/v97/geist19a.html.">8</a>]</span> provide another perspective on how the smooth Bellman equations come to be. This framework offers a more general approach in which we seek to find optimal policies under the infinite horizon criterion while also accounting for a regularizer that influences the kind of policies we try to obtain.</p>
<p>Let’s set up some necessary notation. First, recall that the policy evaluation operator for a stationary policy with decision rule <span class="math notranslate nohighlight">\(d\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[ L_d v = r_d + \gamma P_d v \]</div>
<p>where <span class="math notranslate nohighlight">\(r_d\)</span> is the expected reward under policy <span class="math notranslate nohighlight">\(d\)</span>, <span class="math notranslate nohighlight">\(\gamma\)</span> is the discount factor, and <span class="math notranslate nohighlight">\(P_d\)</span> is the state transition probability matrix under <span class="math notranslate nohighlight">\(d\)</span>. A complementary object to the value function is the q-function (or Q-factor) representation:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{align*}
q_\gamma^{d^\infty}(s, a) &amp;= r(s, a) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a) v_\gamma^{d^\infty}(j) \\
v_\gamma^{d^\infty}(s) &amp;= \sum_{a \in \mathcal{A}_s} d(a | s) q_\gamma^{d^\infty}(s, a) 
\end{align*} \end{split}\]</div>
<p>The policy evaluation operator can then be written in terms of the q-function as:</p>
<div class="math notranslate nohighlight">
\[ [L_d v](s) = \langle d(\cdot | s), q(s, \cdot) \rangle \]</div>
<section id="legendre-fenchel-transform">
<h3><span class="section-number">14.3.1. </span>Legendre-Fenchel Transform<a class="headerlink" href="#legendre-fenchel-transform" title="Link to this heading">#</a></h3>
<p>A key concept in the theory of regularized MDPs is the Legendre-Fenchel transform, also known as the convex conjugate. For a strongly convex function <span class="math notranslate nohighlight">\(\Omega: \Delta_{\mathcal{A}} \rightarrow \mathbb{R}\)</span>, its Legendre-Fenchel transform <span class="math notranslate nohighlight">\(\Omega^*: \mathbb{R}^{\mathcal{A}} \rightarrow \mathbb{R}\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[ \Omega^*(q(s, \cdot)) = \max_{d(\cdot|s) \in \Delta_{\mathcal{A}}} \langle d(\cdot | s), q(s, \cdot) \rangle - \Omega(d(\cdot | s)) \]</div>
<p>An important property of this transform is that it has a unique maximizing argument, given by the gradient of <span class="math notranslate nohighlight">\(\Omega^*\)</span>. This gradient is Lipschitz and satisfies:</p>
<div class="math notranslate nohighlight">
\[ \nabla \Omega^*(q(s, \cdot)) = \arg\max_d \langle d(\cdot | s), q(s, \cdot) \rangle - \Omega(d(\cdot | s)) \]</div>
<p>An important example of a regularizer is the negative entropy, which gives rise to the smooth Bellman equations as we are about to see.</p>
</section>
</section>
<section id="regularized-bellman-operators">
<h2><span class="section-number">14.4. </span>Regularized Bellman Operators<a class="headerlink" href="#regularized-bellman-operators" title="Link to this heading">#</a></h2>
<p>With these concepts in place, we can now define the regularized Bellman operators:</p>
<ol class="arabic">
<li><p><strong>Regularized Policy Evaluation Operator</strong> <span class="math notranslate nohighlight">\((L_{d,\Omega})\)</span>:</p>
<div class="math notranslate nohighlight">
\[ [L_{d,\Omega} v](s) = \langle q(s,\cdot), d(\cdot | s) \rangle - \Omega(d(\cdot | s)) \]</div>
</li>
<li><p><strong>Regularized Bellman Optimality Operator</strong> <span class="math notranslate nohighlight">\((L_\Omega)\)</span>:</p>
<div class="math notranslate nohighlight">
\[ [L_\Omega v](s) = [\max_d L_{d,\Omega} v ](s) = \Omega^*(q(s, \cdot)) \]</div>
</li>
</ol>
<p>It can be shown that the addition of a regularizer in these regularized operators still preserves the contraction properties, and therefore the existence of a solution to the optimality equations and the convergence of successive approximation.</p>
<p>The regularized value function of a stationary policy with decision rule <span class="math notranslate nohighlight">\(d\)</span>, denoted by <span class="math notranslate nohighlight">\(v_{d,\Omega}\)</span>, is the unique fixed point of the operator equation:</p>
<div class="math notranslate nohighlight">
\[\text{find $v$ such that } \enspace v = L_{d,\Omega} v\]</div>
<p>Under the usual assumptions on the discount factor and the boundedness of the reward, the value of a policy can also be found in closed form by solving for <span class="math notranslate nohighlight">\(v\)</span> in the linear system of equations:</p>
<div class="math notranslate nohighlight">
\[ (I - \gamma P_d) v =  (r_d - \Omega(d)) \]</div>
<p>The associated state-action value function <span class="math notranslate nohighlight">\(q_{d,\Omega}\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
q_{d,\Omega}(s, a) &amp;= r(s, a) + \sum_{j \in \mathcal{S}} \gamma p(j|s,a) v_{d,\Omega}(j) \\
v_{d,\Omega}(s) &amp;= \sum_{a \in \mathcal{A}_s} d(a | s) q_{d,\Omega}(s, a) - \Omega(d(\cdot | s))
\end{align*} \end{split}\]</div>
<p>The regularized optimal value function <span class="math notranslate nohighlight">\(v^*_\Omega\)</span> is then the unique fixed point of <span class="math notranslate nohighlight">\(L_\Omega\)</span> in the fixed point equation:</p>
<div class="math notranslate nohighlight">
\[\text{find $v$ such that } v = L_\Omega v\]</div>
<p>The associated state-action value function <span class="math notranslate nohighlight">\(q^*_\Omega\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{align*}
q^*_\Omega(s, a) &amp;= r(s, a) + \sum_{j \in \mathcal{S}} \gamma p(j|s,a) v^*_\Omega(j) \\
v^*_\Omega(s) &amp;= \Omega^*(q^*_\Omega(s, \cdot))\end{align*} \end{split}\]</div>
<p>An important result in the theory of regularized MDPs is that there exists a unique optimal regularized policy. Specifically, if <span class="math notranslate nohighlight">\(d^*_\Omega\)</span> is a conserving decision rule (i.e., <span class="math notranslate nohighlight">\(d^*_\Omega = \arg\max_d L_{d,\Omega} v^*_\Omega\)</span>), then the randomized stationary policy <span class="math notranslate nohighlight">\((d^*_\Omega)^\infty\)</span> is the unique optimal regularized policy.</p>
<p>In practice, once we have found <span class="math notranslate nohighlight">\(v^*_\Omega\)</span>, we can derive the optimal decision rule by taking the gradient of the convex conjugate evaluated at the optimal action-value function:</p>
<div class="math notranslate nohighlight">
\[ d^*(\cdot | s) = \nabla \Omega^*(q^*_\Omega(s, \cdot)) \]</div>
<section id="recovering-the-smooth-bellman-equations">
<h3><span class="section-number">14.4.1. </span>Recovering the Smooth Bellman Equations<a class="headerlink" href="#recovering-the-smooth-bellman-equations" title="Link to this heading">#</a></h3>
<p>Under this framework, we can recover the smooth Bellman equations by choosing <span class="math notranslate nohighlight">\(\Omega\)</span> to be the negative entropy, and obtain the softmax policy as the gradient of the convex conjugate. Let’s show this explicitly:</p>
<ol class="arabic">
<li><p>Using the negative entropy regularizer:</p>
<div class="math notranslate nohighlight">
\[ \Omega(d(\cdot|s)) = \sum_{a \in \mathcal{A}_s} d(a|s) \ln d(a|s) \]</div>
</li>
<li><p>The convex conjugate:</p>
<div class="math notranslate nohighlight">
\[ \Omega^*(q(s, \cdot)) = \ln \sum_{a \in \mathcal{A}_s} \exp q(s,a) \]</div>
</li>
<li><p>Now, let’s write out the regularized Bellman optimality equation:</p>
<div class="math notranslate nohighlight">
\[ v^*_\Omega(s) = \Omega^*(q^*_\Omega(s, \cdot)) \]</div>
</li>
<li><p>Substituting the expressions for <span class="math notranslate nohighlight">\(\Omega^*\)</span> and <span class="math notranslate nohighlight">\(q^*_\Omega\)</span>:</p>
<div class="math notranslate nohighlight">
\[ v^*_\Omega(s) = \ln \sum_{a \in \mathcal{A}_s} \exp \left(r(s, a) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a) v^*_\Omega(j)\right) \]</div>
</li>
</ol>
<p>This is precisely the form of the smooth Bellman equation we derived earlier, with the log-sum-exp operation replacing the max operation of the standard Bellman equation.</p>
<p>Furthermore, the optimal policy is given by the gradient of <span class="math notranslate nohighlight">\(\Omega^*\)</span>:</p>
<div class="math notranslate nohighlight">
\[ d^*(a|s) = \nabla \Omega^*(q^*_\Omega(s, \cdot)) = \frac{\exp(q^*_\Omega(s,a))}{\sum_{a' \in \mathcal{A}_s} \exp(q^*_\Omega(s,a'))} \]</div>
<p>This is the familiar softmax policy we encountered in the smooth MDP setting.</p>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="parametric-dynamic-programming">
<h1><span class="section-number">15. </span>Parametric Dynamic Programming<a class="headerlink" href="#parametric-dynamic-programming" title="Link to this heading">#</a></h1>
<p>We have so far considered a specific kind of approximation: that of the Bellman operator itself. We explored a modified version of the operator with the desirable property of smoothness, which we deemed beneficial for optimization purposes and due to its rich multifaceted interpretations. We now turn our attention to another form of approximation, complementary to the previous kind, which seeks to address the challenge of applying the operator across the entire state space.</p>
<p>To be precise, suppose we can compute the Bellman operator <span class="math notranslate nohighlight">\(\mathrm{L}v\)</span> at some state <span class="math notranslate nohighlight">\(s\)</span>, producing a new function <span class="math notranslate nohighlight">\(U\)</span> whose value at state <span class="math notranslate nohighlight">\(s\)</span> is <span class="math notranslate nohighlight">\(u(s) = (\mathrm{L}v)(s)\)</span>. Then, putting aside the problem of pointwise evaluation, we want to carry out this update across the entire domain of <span class="math notranslate nohighlight">\(v\)</span>. When working with small state spaces, this is not an issue, and we can afford to carry out the update across the entirety of the state space. However, for larger or infinite state spaces, this becomes a major challenge.</p>
<p>So what can we do? Our approach will be to compute the operator at chosen “grid points,” then “fill in the blanks” for the states where we haven’t carried out the update by “fitting” the resulting output function on a dataset of input-output pairs. The intuition is that for sufficiently well-behaved functions and sufficiently expressive function approximators, we hope to generalize well enough. Our community calls this “learning,” while others would call it “function approximation” — a field of its own in mathematics. To truly have a “learning algorithm,” we’ll need to add one more piece of machinery: the use of samples — of simulation — to pick the grid points and perform numerical integration. But this is for the next section…</p>
<section id="partial-updates-in-the-tabular-case">
<h2><span class="section-number">15.1. </span>Partial Updates in the Tabular Case<a class="headerlink" href="#partial-updates-in-the-tabular-case" title="Link to this heading">#</a></h2>
<p>The ideas presented in this section apply more broadly to the successive approximation method applied to a fixed-point problem. Consider again the problem of finding the optimal value function <span class="math notranslate nohighlight">\(v_\gamma^\star\)</span> as the solution to the Bellman optimality operator <span class="math notranslate nohighlight">\(L\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathrm{L} \mathbf{v} \equiv \max_{d \in D^{MD}} \left\{\mathbf{r}_d + \gamma \mathbf{P}_d \mathbf{v}\right\}
\]</div>
<p>Value iteration — the name for the method of successive approximation applied to <span class="math notranslate nohighlight">\(L\)</span> — computes a sequence of iterates <span class="math notranslate nohighlight">\(v_{n+1} = \mathrm{L}v_n\)</span> from some arbitrary <span class="math notranslate nohighlight">\(v_0\)</span>. Let’s pause to consider what the equality sign in this expression means: it represents an assignment (perhaps better denoted as <span class="math notranslate nohighlight">\(:=\)</span>) across the entire domain. This becomes clearer when writing the update in component form:</p>
<div class="math notranslate nohighlight">
\[
v_{n+1}(s) := (\mathrm{L} v_n)(s) \equiv \max_{a \in \mathcal{A}_s} \left\{r(s,a) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a) v_n(j)\right\}, \, \forall s \in \mathcal{S}
\]</div>
<p>Pay particular attention to the <span class="math notranslate nohighlight">\(\forall s \in \mathcal{S}\)</span> notation: what happens when we can’t afford to update all components in each step of value iteration? A potential solution is to use Gauss-Seidel Value Iteration, which updates states sequentially, immediately using fresh values for subsequent updates.</p>
<div class="proof algorithm admonition" id="alg-gsvi">
<p class="admonition-title"><span class="caption-number">Algorithm 15.1 </span> (Gauss-Seidel Value Iteration)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, convergence threshold <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span><br />
<strong>Output:</strong> Value function <span class="math notranslate nohighlight">\(v\)</span> and policy <span class="math notranslate nohighlight">\(d\)</span></p>
<ol class="arabic">
<li><p><strong>Initialization:</strong></p>
<ul class="simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(v^0(s)\)</span> for all <span class="math notranslate nohighlight">\(s \in S\)</span></p></li>
<li><p>Set iteration counter <span class="math notranslate nohighlight">\(n = 0\)</span></p></li>
</ul>
</li>
<li><p><strong>Main Loop:</strong></p>
<ul class="simple">
<li><p>Set state index <span class="math notranslate nohighlight">\(j = 1\)</span></p></li>
</ul>
<p>a) <strong>State Update:</strong> Compute <span class="math notranslate nohighlight">\(v^{n+1}(s_j)\)</span> as:</p>
<div class="math notranslate nohighlight">
\[
      v^{n+1}(s_j) = \max_{a \in A_j} \left\{r(s_j, a) + \gamma \left[\sum_{i&lt;j} p(s_i|s_j,a)v^{n+1}(s_i) + \sum_{i \geq j} p(s_i|s_j,a)v^n(s_i)\right]\right\}
      \]</div>
<p>b) If <span class="math notranslate nohighlight">\(j = |S|\)</span>, proceed to step 3
Otherwise, increment <span class="math notranslate nohighlight">\(j\)</span> and return to step 2(a)</p>
</li>
<li><p><strong>Convergence Check:</strong></p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(\|v^{n+1} - v^n\| &lt; \varepsilon(1-\gamma)/(2\gamma)\)</span>, proceed to step 4</p></li>
<li><p>Otherwise, increment <span class="math notranslate nohighlight">\(n\)</span> and return to step 2</p></li>
</ul>
</li>
<li><p><strong>Policy Extraction:</strong>
For each <span class="math notranslate nohighlight">\(s \in S\)</span>, compute optimal policy:</p>
<div class="math notranslate nohighlight">
\[
   d(s) \in \operatorname{argmax}_{a \in A_s} \left\{r(s,a) + \gamma\sum_{j \in S} p(j|s,a)v^{n+1}(j)\right\}
   \]</div>
</li>
</ol>
<p><strong>Note:</strong> The algorithm differs from standard value iteration in that it immediately uses updated values within each iteration. This is reflected in the first sum of step 2(a), where <span class="math notranslate nohighlight">\(v^{n+1}\)</span> is used for already-updated states.</p>
</section>
</div><p>The Gauss-Seidel value iteration approach offers several advantages over standard value iteration: it can be more memory-efficient and often leads to faster convergence. This idea generalizes further (see for example <span id="id10">Bertsekas [<a class="reference internal" href="bibliography.html#id29" title="Dimitri P. Bertsekas. Distributed asynchronous computation of fixed points. Mathematical Programming, 27(1):107–120, September 1983. URL: http://dx.doi.org/10.1007/BF02591967, doi:10.1007/bf02591967.">4</a>]</span>) to accommodate fully asynchronous updates in any order. However, these methods, while more flexible in their update patterns, still fundamentally rely on a tabular representation—that is, they require storing and eventually updating a separate value for each state in memory. Even if we update states one at a time or in blocks, we must maintain this complete table of values, and our convergence guarantee assumes that every entry in this table will eventually be revised.</p>
<p>But what if maintaining such a table is impossible? This challenge arises naturally when dealing with continuous state spaces, where we cannot feasibly store values for every possible state, let alone update them. This is where function approximation comes into play.</p>
</section>
<section id="partial-updates-by-operator-fitting-parametric-value-iteration">
<h2><span class="section-number">15.2. </span>Partial Updates by Operator Fitting: Parametric Value Iteration<a class="headerlink" href="#partial-updates-by-operator-fitting-parametric-value-iteration" title="Link to this heading">#</a></h2>
<p>In the parametric approach to dynamic programming, instead of maintaining an explicit table of values, we represent the value function using a parametric function approximator <span class="math notranslate nohighlight">\(v(s; \boldsymbol{\theta})\)</span>, where <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> are parameters that get adjusted across iterations rather than the entries of a tabular representation. This idea traces back to the inception of dynamic programming and was described as early as 1963 by Bellman himself, who considered polynomial approximations. For a value function <span class="math notranslate nohighlight">\(v(s)\)</span>, we can write its polynomial approximation as:</p>
<div class="math notranslate nohighlight">
\[
v(s) \approx \sum_{i=0}^{n} \theta_i \phi_i(s)
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\{\phi_i(s)\}\)</span> is the set of basis functions</p></li>
<li><p><span class="math notranslate nohighlight">\(\theta_i\)</span> are the coefficients (our parameters)</p></li>
<li><p><span class="math notranslate nohighlight">\(n\)</span> is the degree of approximation</p></li>
</ul>
<p>As we discussed earlier in the context of trajectory optimization, we can choose from different polynomial bases beyond the usual monomial basis <span class="math notranslate nohighlight">\(\phi_i(s) = s^i\)</span>, such as Legendre or Chebyshev polynomials. While polynomials offer attractive mathematical properties, they become challenging to work with in higher dimensions due to the curse of dimensionality. This limitation motivates our later turn to neural network parameterizations, which scale better with dimensionality.</p>
<p>Given a parameterization, our value iteration procedure must now update the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> rather than tabular values directly. At each iteration, we aim to find parameters that best approximate the Bellman operator’s output at chosen base points. More precisely, we collect a dataset:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{D}_n = \{(s_i, (\mathrm{L}v)(s_i; \boldsymbol{\theta}_n)) \mid s_i \in B\}
\]</div>
<p>and fit a regressor <span class="math notranslate nohighlight">\(v(\cdot; \boldsymbol{\theta}_{n+1})\)</span> to this data.</p>
<p>This process differs from standard supervised learning in a specific way: rather than working with a fixed dataset, we iteratively generate our training targets using the previous value function approximation. During this process, the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span> remain “frozen”, entering only through dataset creation. This naturally leads to maintaining two sets of parameters:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span>: parameters of the target model used for generating training targets</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{n+1}\)</span>: parameters being optimized in the current iteration</p></li>
</ul>
<p>This target model framework emerges naturally from the structure of parametric value iteration — an insight that provides theoretical grounding for modern deep reinforcement learning algorithms where we commonly hear about the importance of the “target network trick” .</p>
<p>Parametric successive approximation, known in reinforcement learning literature as Fitted Value Iteration, offers a flexible template for deriving new algorithms by varying the choice of function approximator. Various instantiations of this approach have emerged across different fields:</p>
<ul class="simple">
<li><p>Using polynomial basis functions with linear regression yields Kortum’s method <span id="id11">[<a class="reference internal" href="bibliography.html#id31" title="Samuel Kortum. Value function approximation in an estimation routine. 1992. Manuscript, Boston University.">20</a>]</span>, known to econometricians. In reinforcement learning terms, this corresponds to value iteration with projected Bellman equations <span id="id12">[<a class="reference internal" href="bibliography.html#id32" title="John Rust. Chapter 14 Numerical dynamic programming in economics, pages 619–729. Elsevier, 1996. URL: http://dx.doi.org/10.1016/S1574-0021(96)01016-7, doi:10.1016/s1574-0021(96)01016-7.">29</a>]</span>.</p></li>
<li><p>Employing extremely randomized trees (via <code class="docutils literal notranslate"><span class="pre">ExtraTreesRegressor</span></code>) leads to the tree-based fitted value iteration of Ernst et al. <span id="id13">[<a class="reference internal" href="bibliography.html#id33" title="Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. Journal of Machine Learning Research, 6:503–556, 2005.">7</a>]</span>.</p></li>
<li><p>Neural network approximation (via <code class="docutils literal notranslate"><span class="pre">MLPRegressor</span></code>) gives rise to Neural Fitted Q-Iteration as developed by Riedmiller <span id="id14">[<a class="reference internal" href="bibliography.html#id34" title="Martin Riedmiller. Neural fitted q iteration – first experiences with a data efficient neural reinforcement learning method. In Proceedings of the 16th European Conference on Machine Learning (ECML), 317–328. Berlin, Heidelberg, 2005. Springer.">26</a>]</span>.</p></li>
</ul>
<p>The \texttt{fit} function in our algorithm represents this supervised learning step and can be implemented using any standard regression tool that follows the scikit-learn interface. This flexibility in choice of function approximator allows practitioners to leverage the extensive ecosystem of modern machine learning tools while maintaining the core dynamic programming structure.</p>
<div class="proof algorithm admonition" id="parametric-value-iteration">
<p class="admonition-title"><span class="caption-number">Algorithm 15.2 </span> (Parametric Value Iteration)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> Given an MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, base points <span class="math notranslate nohighlight">\(B \subset S\)</span>, function approximator class <span class="math notranslate nohighlight">\(v(s; \boldsymbol{\theta})\)</span>, maximum iterations <span class="math notranslate nohighlight">\(N\)</span>, tolerance <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span></p>
<p><strong>Output</strong> Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> for value function approximation</p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span> (e.g., for zero initialization)</p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow 0\)</span></p></li>
<li><p><strong>repeat</strong></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{D} \leftarrow \emptyset\)</span></p></li>
<li><p>For each <span class="math notranslate nohighlight">\(s \in B\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(y_s \leftarrow \max_{a \in A} \left\{r(s,a) + \gamma \sum_{j \in \mathcal{S}} p(j|s,a)v(j; \boldsymbol{\theta}_n)\right\}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D} \leftarrow \mathcal{D} \cup \{(s, y_s)\}\)</span></p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{n+1} \leftarrow \texttt{fit}(\mathcal{D})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\delta \leftarrow \frac{1}{|B|}\sum_{s \in B} (v(s; \boldsymbol{\theta}_{n+1}) - v(s; \boldsymbol{\theta}_n))^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow n + 1\)</span></p></li>
</ol>
</li>
<li><p><strong>until</strong> (<span class="math notranslate nohighlight">\(\delta &lt; \varepsilon\)</span> or <span class="math notranslate nohighlight">\(n \geq N\)</span>)</p></li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span></p></li>
</ol>
</section>
</div><p>The structure of the above algorithm mirrors value iteration in its core idea of iteratively applying the Bellman operator. However, several key modifications distinguish this fitted variant:</p>
<p>First, rather than applying updates across the entire state space, we compute the operator only at selected base points <span class="math notranslate nohighlight">\(B\)</span>. The resulting values are then stored implicitly through the parameter vector <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> via the fitting step, rather than explicitly as in the tabular case.</p>
<p>The fitting procedure itself may introduce an “inner optimization loop.” For instance, when using neural networks, this involves an iterative gradient descent procedure to optimize the parameters. This creates an interesting parallel with modified policy iteration: just as we might truncate policy evaluation steps there, we can consider variants where this inner loop runs for a fixed number of iterations rather than to convergence.</p>
<p>Finally, the termination criterion from standard value iteration may no longer hold. The classical criterion relied on the sup-norm contractivity property of the Bellman operator — a property that isn’t generally preserved under function approximation. While certain function approximation schemes can maintain this sup-norm contraction property (as we’ll see later), this is the exception rather than the rule.</p>
</section>
<section id="parametric-policy-iteration">
<h2><span class="section-number">15.3. </span>Parametric Policy Iteration<a class="headerlink" href="#parametric-policy-iteration" title="Link to this heading">#</a></h2>
<p>We can extend this idea of fitting partial operator updates to the policy iteration setting. Remember, policy iteration involves iterating in the space of policies rather than in the space of value functions. Given an initial guess on a deterministic decision rule <span class="math notranslate nohighlight">\(d_0\)</span>, we iteratively:</p>
<ol class="arabic simple">
<li><p>Compute the value function for the current policy (policy evaluation)</p></li>
<li><p>Derive a new improved policy (policy improvement)</p></li>
</ol>
<p>When computationally feasible and under the model-based setting, we can solve the policy evaluation step directly as a linear system equation. Alternatively, we could carry out policy evaluation by applying successive approximation to the operator <span class="math notranslate nohighlight">\(L_{d_n}\)</span> until convergence, or as in modified policy iteration, for just a few steps.</p>
<p>To apply the idea of fitting partial updates, we start at the level of the policy evaluation operator <span class="math notranslate nohighlight">\(L_{d_n}\)</span>. For a given decision rule <span class="math notranslate nohighlight">\(d_n\)</span>, this operator in component form is:</p>
<div class="math notranslate nohighlight">
\[
(L_{d_n}v)(s) = r(s,d_n(s)) + \gamma \int v(s')p(ds'|s,d_n(s))
\]</div>
<p>For a set of base points <span class="math notranslate nohighlight">\(B = \{s_1, ..., s_M\}\)</span>, we form our dataset:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{D}_n = \{(s_k, y_k) : s_k \in B\}
\]</div>
<p>where:</p>
<div class="math notranslate nohighlight">
\[
y_k = r(s_k,d_n(s_k)) + \gamma \int v_n(s')p(ds'|s_k,d_n(s_k))
\]</div>
<p>This gives us a way to perform approximate policy evaluation through function fitting. However, we now face the question of how to perform policy improvement in this parametric setting. A key insight comes from the the fact that in the exact form of policy iteration, we don’t need to improve the policy everywhere to guarantee progress. In fact, improving the policy at even a single state is sufficient for convergence.</p>
<p>This suggests a natural approach: rather than trying to approximate an improved policy over the entire state space, we can simply:</p>
<ol class="arabic simple">
<li><p>Compute improved actions at our base points:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
d_{n+1}(s_k) = \arg\max_{a \in \mathcal{A}} \left\{r(s_k,a) + \gamma \int v_n(s')p(ds'|s_k,a)\right\}, \quad \forall s_k \in B
\]</div>
<ol class="arabic simple" start="2">
<li><p>Let the function approximation of the value function implicitly generalize these improvements to other states during the next policy evaluation phase.</p></li>
</ol>
<p>This leads to the following algorithm:</p>
<div class="proof algorithm admonition" id="parametric-policy-iteration">
<p class="admonition-title"><span class="caption-number">Algorithm 15.3 </span> (Parametric Policy Iteration)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> Given an MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, base points <span class="math notranslate nohighlight">\(B \subset S\)</span>, function approximator class <span class="math notranslate nohighlight">\(v(s; \boldsymbol{\theta})\)</span>, maximum iterations <span class="math notranslate nohighlight">\(N\)</span>, tolerance <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span></p>
<p><strong>Output</strong> Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> for value function approximation</p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span>, decision rules <span class="math notranslate nohighlight">\(\{d_0(s_k)\}_{s_k \in B}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow 0\)</span></p></li>
<li><p><strong>repeat</strong></p>
<ol class="arabic simple">
<li><p>// Policy Evaluation</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D} \leftarrow \emptyset\)</span></p></li>
<li><p>For each <span class="math notranslate nohighlight">\(s_k \in B\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(y_k \leftarrow r(s_k,d_n(s_k)) + \gamma \int v_n(s')p(ds'|s_k,d_n(s_k))\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D} \leftarrow \mathcal{D} \cup \{(s_k, y_k)\}\)</span></p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{n+1} \leftarrow \texttt{fit}(\mathcal{D})\)</span></p></li>
<li><p>// Policy Improvement at Base Points</p></li>
<li><p>For each <span class="math notranslate nohighlight">\(s_k \in B\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(d_{n+1}(s_k) \leftarrow \arg\max_{a \in A} \{r(s_k,a) + \gamma \int v_n(s')p(ds'|s_k,a)\}\)</span></p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow n + 1\)</span></p></li>
</ol>
</li>
<li><p><strong>until</strong> (<span class="math notranslate nohighlight">\(n \geq N\)</span> or convergence criterion met)</p></li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span></p></li>
</ol>
</section>
</div></section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="approximate-component-wise-evaluation-of-the-bellman-operator">
<h1><span class="section-number">16. </span>Approximate Component-wise Evaluation of the Bellman Operator<a class="headerlink" href="#approximate-component-wise-evaluation-of-the-bellman-operator" title="Link to this heading">#</a></h1>
<p>So far, we’ve described a strategy that economizes computation by updating only a selected set of base points, then generalizing these updates to approximate the operator’s effect on other states. However, this approach assumes we can compute the operator exactly at these base points. What if even this limited computation proves infeasible?</p>
<p>The challenge arises naturally when we consider what computing the Bellman operator entails: evaluating an expectation over next states, which generally requires numerical integration. Even in the “best” scenario — the model-based setting we’ve worked with throughout this course — we assume access to an explicit representation of the transition probability function that we can evaluate everywhere.</p>
<p>But what if we lack these exact probabilities and instead only have access to samples of next states for given state-action pairs? In this case, we must turn to Monte Carlo integration methods, which brings us fully into what we would recognize as a learning setting.</p>
<section id="numerical-quadrature-methods">
<h2><span class="section-number">16.1. </span>Numerical Quadrature Methods<a class="headerlink" href="#numerical-quadrature-methods" title="Link to this heading">#</a></h2>
<p>In the general case, the Bellman operator in component-wise form is:</p>
<div class="math notranslate nohighlight">
\[
(\mathrm{L} v_n)(s) \equiv \max_{a \in \mathcal{A}_s} \left\{r(s,a) + \gamma \int v_n(s')p(ds'|s,a)\right\}, \, \forall s \in \mathcal{S}
\]</div>
<p>When we have direct access to <span class="math notranslate nohighlight">\(p\)</span> (rather than just samples from it), we can employ numerical integration methods to approximate the expectation. Let’s divide our state space into <span class="math notranslate nohighlight">\(m\)</span> equal intervals of width <span class="math notranslate nohighlight">\(\Delta s = (s_{\max} - s_{\min})/m\)</span>. Then, a quadrature approximation takes the form:</p>
<div class="math notranslate nohighlight">
\[
\int v_n(s')p(ds'|s,a) \approx \sum_{i=1}^m w_i v_n(s'_i)
\]</div>
<p>For instance, using the rectangle (midpoint) rule with points <span class="math notranslate nohighlight">\(s'_i = s_{\min} + (i-\frac{1}{2})\Delta s\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\int v_n(s')p(ds'|s,a) \approx \sum_{i=1}^m v_n(s'_i)p(s'_i|s,a)\Delta s
\]</div>
<p>or the trapezoidal rule using endpoints <span class="math notranslate nohighlight">\(s'_i = s_{\min} + i\Delta s\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\int v_n(s')p(ds'|s,a) \approx \frac{\Delta s}{2}\sum_{i=1}^{m} [v_n(s'_i)p(s'_i|s,a) + v_n(s'_{i+1})p(s'_{i+1}|s,a)]
\]</div>
<p>It’s instructive to contrast this approach with the one that we would apply in the deterministic case:</p>
<div class="math notranslate nohighlight">
\[
(\mathrm{L}v)(s) = \max_{a \in \mathcal{A}} \{r(s,a) + \gamma v(f(s,a))\}
\]</div>
<p>where <span class="math notranslate nohighlight">\(f(s,a)\)</span> is the deterministic transition function: a special case where <span class="math notranslate nohighlight">\(p(ds'|s,a)\)</span> is a Dirac delta measure concentrated at <span class="math notranslate nohighlight">\(f(s,a)\)</span>.
This setting does not require numerical integration but instead faces the challenge of evaluating <span class="math notranslate nohighlight">\(v\)</span> at <span class="math notranslate nohighlight">\(f(s,a)\)</span>, which might not coincide with our grid points. This is a problem which we handled through interpolation in the previous chapter by:</p>
<ol class="arabic simple">
<li><p>Maintaining values at grid points <span class="math notranslate nohighlight">\(\{s_i\}_{i=1}^N\)</span></p></li>
<li><p>Using interpolation to evaluate <span class="math notranslate nohighlight">\(v\)</span> at arbitrary points</p></li>
</ol>
<p>For example with linear interpolation between grid points, <span class="math notranslate nohighlight">\(L\)</span> took the form:</p>
<div class="math notranslate nohighlight">
\[
(\mathrm{L}v)(s_i) = \max_{a \in \mathcal{A}} \left\{r(s_i,a) + \gamma \left[(1-\alpha)v(s_k) + \alpha v(s_{k+1})\right]\right\}
\]</div>
<p>where <span class="math notranslate nohighlight">\(f(s_i,a)\)</span> falls between grid points <span class="math notranslate nohighlight">\(s_k\)</span> and <span class="math notranslate nohighlight">\(s_{k+1}\)</span>, and <span class="math notranslate nohighlight">\(\alpha\)</span> is the interpolation weight:</p>
<div class="math notranslate nohighlight">
\[
\alpha = \frac{f(s_i,a) - s_k}{s_{k+1} - s_k}
\]</div>
<p>Returning to the Bellman operator in the stochastic case, we can approximate the expectation using quadrature methods. Plugging such a quadrature approximation back into our operator, we obtain <span class="math notranslate nohighlight">\(\hat{\mathrm{L}}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
(\hat{\mathrm{L}} v_n)(s) \equiv \max_{a \in \mathcal{A}_s} \left\{r(s,a) + \gamma \sum_{i=1}^m v_n(s'_i)p(s'_i|s,a)\Delta s\right\}
\]</div>
<p>This strategy of approximating the operator at the level of expectation computation is complementary to our earlier approach of performing partial updates and then generalizing through function approximation. These approximations address different challenges:</p>
<ul class="simple">
<li><p>Quadrature handles the computation of expectations over continuous state spaces (not needed in deterministic case)</p></li>
<li><p>Function approximation deals with representing and generalizing value functions (analogous to interpolation in deterministic case)</p></li>
</ul>
<p>Combining both strategies leads to the following algorithm:</p>
<div class="proof algorithm admonition" id="fitted-value-iteration-quadrature">
<p class="admonition-title"><span class="caption-number">Algorithm 16.1 </span> (Fitted Value Iteration with Rectangle Rule)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> Given an MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, base points <span class="math notranslate nohighlight">\(B \subset S\)</span>, integration points <span class="math notranslate nohighlight">\(\{s'_i\}_{i=1}^m\)</span>, function approximator class <span class="math notranslate nohighlight">\(v(s; \boldsymbol{\theta})\)</span>, maximum iterations <span class="math notranslate nohighlight">\(N\)</span>, tolerance <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span></p>
<p><strong>Output</strong> Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> for value function approximation</p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span> (e.g., for zero initialization)</p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow 0\)</span></p></li>
<li><p><strong>repeat</strong></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{D} \leftarrow \emptyset\)</span></p></li>
<li><p>For each <span class="math notranslate nohighlight">\(s \in B\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(y_s \leftarrow \max_{a \in A} \left\{r(s,a) + \gamma \sum_{i=1}^m v_n(s'_i)p(s'_i|s,a)\Delta s\right\}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D} \leftarrow \mathcal{D} \cup \{(s, y_s)\}\)</span></p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{n+1} \leftarrow \texttt{fit}(\mathcal{D})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\delta \leftarrow \frac{1}{|B|}\sum_{s \in B} (v(s; \boldsymbol{\theta}_{n+1}) - v(s; \boldsymbol{\theta}_n))^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow n + 1\)</span></p></li>
</ol>
</li>
<li><p><strong>until</strong> (<span class="math notranslate nohighlight">\(\delta &lt; \varepsilon\)</span> or <span class="math notranslate nohighlight">\(n \geq N\)</span>)</p></li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span></p></li>
</ol>
</section>
</div></section>
<section id="monte-carlo-integration">
<h2><span class="section-number">16.2. </span>Monte Carlo Integration<a class="headerlink" href="#monte-carlo-integration" title="Link to this heading">#</a></h2>
<p>Numerical quadrature methods scale poorly with increasing dimension. Specifically, for a fixed error tolerance <span class="math notranslate nohighlight">\(\epsilon\)</span>, the number of required quadrature points grows exponentially with dimension <span class="math notranslate nohighlight">\(d\)</span> as <span class="math notranslate nohighlight">\(O((\frac{1}{\epsilon})^d)\)</span>. Furthermore, quadrature methods require explicit evaluation of the transition probability function <span class="math notranslate nohighlight">\(p(s'|s,a)\)</span> at specified points—a luxury we don’t have in the “model-free” setting where we only have access to samples from the MDP.</p>
<p>Let <span class="math notranslate nohighlight">\(\mathcal{B} = \{s_1, ..., s_M\}\)</span> be our set of base points where we will evaluate the operator. At each base point <span class="math notranslate nohighlight">\(s_k \in \mathcal{B}\)</span>, Monte Carlo integration approximates the expectation using <span class="math notranslate nohighlight">\(N\)</span> samples:</p>
<div class="math notranslate nohighlight">
\[
\int v_n(s')p(ds'|s_k,a) \approx \frac{1}{N} \sum_{i=1}^N v_n(s'_{k,i}), \quad s'_{k,i} \sim p(\cdot|s_k,a)
\]</div>
<p>where <span class="math notranslate nohighlight">\(s'_{k,i}\)</span> denotes the <span class="math notranslate nohighlight">\(i\)</span>-th sample drawn from <span class="math notranslate nohighlight">\(p(\cdot|s_k,a)\)</span> for base point <span class="math notranslate nohighlight">\(s_k\)</span>.</p>
<p>This approach has two remarkable properties:</p>
<ol class="arabic simple">
<li><p>The convergence rate is <span class="math notranslate nohighlight">\(O(\frac{1}{\sqrt{N}})\)</span> regardless of the number of dimensions</p></li>
<li><p>It only requires samples from <span class="math notranslate nohighlight">\(p(\cdot|s_k,a)\)</span>, not explicit probability values</p></li>
</ol>
<p>These properties make Monte Carlo integration particularly attractive for high-dimensional problems and model-free settings. Indeed, this is one of the key mathematical foundations that enables learning in general: the ability to estimate expectations using only samples from a distribution. This principle underlies the empirical risk minimization framework in statistical learning theory, where we approximate expected losses using finite samples.</p>
<p>The resulting approximate Bellman operator at each base point becomes:</p>
<div class="math notranslate nohighlight">
\[
(\hat{\mathrm{L}} v_n)(s_k) \equiv \max_{a \in \mathcal{A}_{s_k}} \left\{r(s_k,a) + \frac{\gamma}{N} \sum_{i=1}^N v_n(s'_{k,i})\right\}, \quad s'_{k,i} \sim p(\cdot|s_k,a)
\]</div>
<p>for each <span class="math notranslate nohighlight">\(s_k \in \mathcal{B}\)</span>. From these <span class="math notranslate nohighlight">\(M\)</span> point-wise evaluations, we will then fit our function approximator, just as we did in the quadrature case.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="dp.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">11. </span>Dynamic Programming</p>
      </div>
    </a>
    <a class="right-next"
       href="bibliography.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">17. </span>Bibliography</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">13. Approximate Dynamic Programming</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#smooth-optimality-equations-for-infinite-horizon-mdps">14. Smooth Optimality Equations for Infinite-Horizon MDPs</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gumbel-noise-on-the-rewards">14.1. Gumbel Noise on the Rewards</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#control-as-inference-perspective">14.2. Control as Inference Perspective</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#message-passing">14.2.1. Message Passing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deriving-the-optimal-policy">14.2.2. Deriving the Optimal Policy</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regularized-markov-decision-processes">14.3. Regularized Markov Decision Processes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#legendre-fenchel-transform">14.3.1. Legendre-Fenchel Transform</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regularized-bellman-operators">14.4. Regularized Bellman Operators</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recovering-the-smooth-bellman-equations">14.4.1. Recovering the Smooth Bellman Equations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#parametric-dynamic-programming">15. Parametric Dynamic Programming</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#partial-updates-in-the-tabular-case">15.1. Partial Updates in the Tabular Case</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#partial-updates-by-operator-fitting-parametric-value-iteration">15.2. Partial Updates by Operator Fitting: Parametric Value Iteration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parametric-policy-iteration">15.3. Parametric Policy Iteration</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#approximate-component-wise-evaluation-of-the-bellman-operator">16. Approximate Component-wise Evaluation of the Bellman Operator</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-quadrature-methods">16.1. Numerical Quadrature Methods</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#monte-carlo-integration">16.2. Monte Carlo Integration</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Pierre-Luc Bacon
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>