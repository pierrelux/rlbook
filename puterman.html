
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>4.3 OPTIMALITY EQUATIONS AND THE PRINCIPLE OF OPTIMALITY &#8212; Practical Reinforcement Learning: From Algorithms to Applications</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=b4b7a797" />
    <link rel="stylesheet" type="text/css" href="_static/graphviz.css?v=fd3f3429" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"bm": ["{\\boldsymbol #1}", 1]}, "processEscapes": true}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'puterman';</script>
    <script src="_static/iframe-modal.js?v=f72a1242"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Practical Reinforcement Learning: From Algorithms to Applications</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Why This Book?
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Modeling</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="modeling.html">Why Build a Model? For Whom?</a></li>

<li class="toctree-l1"><a class="reference internal" href="ssm.html">Dynamics Models for Decision Making</a></li>




<li class="toctree-l1"><a class="reference internal" href="simulation.html">Programs as Models</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Numerical Trajectory Optimization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ocp.html">Discrete-Time Trajectory Optimization</a></li>


<li class="toctree-l1"><a class="reference internal" href="cocp.html">Trajectory Optimization in Continuous Time</a></li>




</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">From Trajectories to Policies</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="mpc.html">Model Predictive Control</a></li>




<li class="toctree-l1"><a class="reference internal" href="dp.html">Dynamic Programming</a></li>



</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Learning from Data</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="adp.html">Approximate Dynamic Programming</a></li>





<li class="toctree-l1"><a class="reference internal" href="cadp.html">Policy Parametrization Methods</a></li>







</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="appendix_examples.html">Example COCPs</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix_ivps.html">Solving Initial Value Problems</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix_nlp.html">Nonlinear Programming</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/pierrelux/rlbook" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pierrelux/rlbook/edit/main/puterman.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pierrelux/rlbook/issues/new?title=Issue%20on%20page%20%2Fputerman.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/puterman.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>4.3 OPTIMALITY EQUATIONS AND THE PRINCIPLE OF OPTIMALITY</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">4.3 OPTIMALITY EQUATIONS AND THE PRINCIPLE OF OPTIMALITY</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#finite-horizon-markov-decision-processes">84 FINITE-HORIZON MARKOV DECISION PROCESSES</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#optimality-equations-and-the-principle-of-optimality-85">OPTIMALITY EQUATIONS AND THE PRINCIPLE OF OPTIMALITY 85</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">86 FINITE-HORIZON MARKOV DECISION PROCESSES</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#optimality-equations-and-the-principle-of-optimality-87">OPTIMALITY EQUATIONS AND THE PRINCIPLE OF OPTIMALITY 87</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">88 FINITE-HORIZON MARKOV DECISION PROCESSES</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimality-of-deterministic-markov-policies">4.4 OPTIMALITY OF DETERMINISTIC MARKOV POLICIES</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#optimality-of-deterministic-markov-policies-89">OPTIMALITY OF DETERMINISTIC MARKOV POLICIES 89</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#theorem-4-4-1">Theorem 4.4.1.</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">90 FINITE-HORIZON MARKOV DECISION PROCESSES</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#optimality-of-deterministic-markov-policies-91">OPTIMALITY OF DETERMINISTIC MARKOV POLICIES 91</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">92 FINITE-HORIZON MARKOV DECISION PROCESSES</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-induction">4.5 BACKWARD INDUCTION</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-backward-induction-algorithm">The Backward Induction Algorithm</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-induction-93">BACKWARD INDUCTION 93</a></li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="optimality-equations-and-the-principle-of-optimality">
<h1>4.3 OPTIMALITY EQUATIONS AND THE PRINCIPLE OF OPTIMALITY<a class="headerlink" href="#optimality-equations-and-the-principle-of-optimality" title="Link to this heading">#</a></h1>
<p>In this section we introduce optimality equations (sometimes referred to as Bellman equations or functional equations) and investigate their properties. We show that solutions of these equations correspond to optimal value functions and that they also provide a basis for determining optimal policies. We assume either finite or countable <span class="math notranslate nohighlight">\(S\)</span> to avoid technical subtleties.</p>
<p>Let</p>
<div class="math notranslate nohighlight">
\[u_t^*(h_t) = \sup_{\pi \in \Pi^{\text{HR}}} u_t^{\pi}(h_t). \tag{4.3.1}\]</div>
<p>It denotes the supremum over all policies of the expected total reward from decision epoch <span class="math notranslate nohighlight">\(t\)</span> onward when the history up to time <span class="math notranslate nohighlight">\(t\)</span> is <span class="math notranslate nohighlight">\(h_t\)</span>. For <span class="math notranslate nohighlight">\(t &gt; 1\)</span>, we need not consider all policies when taking the above supremum. Since we know <span class="math notranslate nohighlight">\(h_t\)</span>, we only consider portions of policies from decision epoch <span class="math notranslate nohighlight">\(t\)</span> onward; that is, we require only the supremum over <span class="math notranslate nohighlight">\((d_t, d_{t+1}, \ldots, d_{N-1}) \in D_t^{\text{HR}} \times D_{t+1}^{\text{HR}} \times \cdots \times D_{N-1}^{\text{HR}}\)</span>. When minimizing costs instead of maximizing rewards, we sometimes refer to <span class="math notranslate nohighlight">\(u_t^*\)</span> as a <em>cost-to-go</em> function.</p>
<p>The <em>optimality equations</em> are given by</p>
<div class="math notranslate nohighlight">
\[u_t(h_t) = \sup_{a \in A_{s_t}} \left\{ r_t(s_t, a) + \sum_{j \in S} p_t(j|s_t, a) u_{t+1}(h_t, a, j) \right\} \tag{4.3.2}\]</div>
<p>for <span class="math notranslate nohighlight">\(t = 1, \ldots, N - 1\)</span> and <span class="math notranslate nohighlight">\(h_t = (h_{t-1}, a_{t-1}, s_t) \in H_t\)</span>. For <span class="math notranslate nohighlight">\(t = N\)</span>, we add the boundary condition</p>
<div class="math notranslate nohighlight">
\[u_N(h_N) = r_N(s_N) \tag{4.3.3}\]</div>
<p>for <span class="math notranslate nohighlight">\(h_N = (h_{N-1}, a_{N-1}, s_N) \in H_N\)</span>.</p>
<p>These equations reduce to the policy evaluation equations (4.2.1) when we replace the supremum over all actions in state <span class="math notranslate nohighlight">\(s_t\)</span> by the action corresponding to a specified policy, or equivalently, when <span class="math notranslate nohighlight">\(A_s\)</span> is a singleton for each <span class="math notranslate nohighlight">\(s \in S\)</span>.</p>
<p>The operation “sup” in (4.3.2) is implemented by evaluating the quantity in brackets for each <span class="math notranslate nohighlight">\(a \in A_{s_t}\)</span> and then choosing the supremum over all of these values. When <span class="math notranslate nohighlight">\(A_{s_t}\)</span> is a continuum, the supremum might be found analytically. If the supremum in (4.3.2) is attained, for example, when each <span class="math notranslate nohighlight">\(A_{s_t}\)</span> is finite, it can be replaced by “max” so (4.3.2) becomes</p>
<div class="math notranslate nohighlight">
\[u_t(h_t) = \max_{a \in A_{s_t}} \left\{ r_t(s_t, a) + \sum_{j \in S} p_t(j|s_t, a) u_{t+1}(h_t, a, j) \right\}. \tag{4.3.4}\]</div>
<p>A solution to the system of equations (4.3.2) or (4.3.4) and boundary condition (4.3.3) is a sequence of functions <span class="math notranslate nohighlight">\(u_t: H_t \to R\)</span>, <span class="math notranslate nohighlight">\(t = 1, \ldots, N\)</span>, with the property that <span class="math notranslate nohighlight">\(u_N\)</span> satisfies (4.3.3), <span class="math notranslate nohighlight">\(u_{N-1}\)</span> satisfies the <span class="math notranslate nohighlight">\((N - 1)\)</span>th equation with <span class="math notranslate nohighlight">\(u_N\)</span> substituted into the right-hand side of the <span class="math notranslate nohighlight">\((N - 1)\)</span>th equation, and so forth.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="finite-horizon-markov-decision-processes">
<h1>84 FINITE-HORIZON MARKOV DECISION PROCESSES<a class="headerlink" href="#finite-horizon-markov-decision-processes" title="Link to this heading">#</a></h1>
<p>The optimality equations are fundamental tools in Markov decision theory, having the following important and useful properties.</p>
<p><strong>a.</strong> Solutions to the optimality equations are the optimal returns from period <span class="math notranslate nohighlight">\(t\)</span> onward for each <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p><strong>b.</strong> They provide a method for determining whether a policy is optimal. If the expected total reward of policy <span class="math notranslate nohighlight">\(\pi\)</span> from period <span class="math notranslate nohighlight">\(t\)</span> onward satisfies this system of equations for <span class="math notranslate nohighlight">\(t = 1, \ldots, N\)</span> it is optimal.</p>
<p><strong>c.</strong> They are the basis for an efficient procedure for computing optimal return functions and policies.</p>
<p><strong>d.</strong> They may be used to determine structural properties of optimal policies and return functions.</p>
<p>Before stating and proving the main result in this chapter, we introduce the following important yet simple lemma.</p>
<p><strong>Lemma 4.3.1.</strong> Let <span class="math notranslate nohighlight">\(w\)</span> be a real-valued function on an arbitrary discrete set <span class="math notranslate nohighlight">\(W\)</span> and let <span class="math notranslate nohighlight">\(q(\cdot)\)</span> be a probability distribution on <span class="math notranslate nohighlight">\(W\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[\sup_{u \in W} w(u) \geq \sum_{u \in W} q(u)w(u)\]</div>
<p><em>Proof.</em> Let <span class="math notranslate nohighlight">\(w^* = \sup_{u \in W} w(u)\)</span>. Then,</p>
<div class="math notranslate nohighlight">
\[w^* = \sum_{u \in W} q(u)w^* \geq \sum_{u \in W} q(u)w(u).\]</div>
<p><span class="math notranslate nohighlight">\(\square\)</span></p>
<p>Note that the lemma remains valid with <span class="math notranslate nohighlight">\(W\)</span> a Borel subset of a measurable space, <span class="math notranslate nohighlight">\(w(u)\)</span> an integrable function on <span class="math notranslate nohighlight">\(W\)</span>, and the summation replaced by integration.</p>
<p>The following theorem summarizes the optimality properties of solutions of the optimality equation. Its inductive proof illustrates several dynamic programming principles and consists of two parts. First we establish that solutions provide upper bounds on <span class="math notranslate nohighlight">\(u_t^*\)</span> and then we establish existence of a policy <span class="math notranslate nohighlight">\(\pi'\)</span> for which <span class="math notranslate nohighlight">\(u_t^{\pi'}\)</span> is arbitrarily close to <span class="math notranslate nohighlight">\(u_t\)</span>.</p>
<p><strong>Theorem 4.3.2.</strong> Suppose <span class="math notranslate nohighlight">\(u_t\)</span> is a solution of (4.3.2) for <span class="math notranslate nohighlight">\(t = 1, \ldots, N - 1\)</span>, and <span class="math notranslate nohighlight">\(u_N\)</span> satisfies (4.3.3). Then</p>
<p><strong>a.</strong> <span class="math notranslate nohighlight">\(u_t(h_t) = u_t^*(h_t)\)</span> for all <span class="math notranslate nohighlight">\(h_t \in H_t\)</span>, <span class="math notranslate nohighlight">\(t = 1, \ldots, N\)</span>, and</p>
<p><strong>b.</strong> <span class="math notranslate nohighlight">\(u_1(s_1) = v_N^*(s_1)\)</span> for all <span class="math notranslate nohighlight">\(s_1 \in S\)</span>.</p>
<p><em>Proof.</em> The proof is in two parts. First we establish by induction that <span class="math notranslate nohighlight">\(u_n(h_n) \geq u_n^*(h_n)\)</span> for all <span class="math notranslate nohighlight">\(h_n \in H_n\)</span> and <span class="math notranslate nohighlight">\(n = 1, 2, \ldots, N\)</span>.</p>
<p>Since no decision is made in period <span class="math notranslate nohighlight">\(N\)</span>, <span class="math notranslate nohighlight">\(u_N(h_N) = r_N(s_N) = u_N^*(h_N)\)</span> for all <span class="math notranslate nohighlight">\(h_N \in H_N\)</span> and <span class="math notranslate nohighlight">\(\pi \in \Pi^{\text{HR}}\)</span>. Therefore <span class="math notranslate nohighlight">\(u_N(h_N) = u_N^*(h_N)\)</span> for all <span class="math notranslate nohighlight">\(h_N \in H_N\)</span>. Now assume</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="optimality-equations-and-the-principle-of-optimality-85">
<h1>OPTIMALITY EQUATIONS AND THE PRINCIPLE OF OPTIMALITY 85<a class="headerlink" href="#optimality-equations-and-the-principle-of-optimality-85" title="Link to this heading">#</a></h1>
<p>that <span class="math notranslate nohighlight">\(u_t(h_t) \geq u_t^*(h_t)\)</span> for all <span class="math notranslate nohighlight">\(h_t \in H_t\)</span> for <span class="math notranslate nohighlight">\(t = n + 1, \ldots, N\)</span>. Let <span class="math notranslate nohighlight">\(\pi' = (d_1', d_2', \ldots, d_{N-1}')\)</span> be an arbitrary policy in <span class="math notranslate nohighlight">\(\Pi^{\text{HR}}\)</span>. For <span class="math notranslate nohighlight">\(t = n\)</span>, the optimality equation is</p>
<div class="math notranslate nohighlight">
\[u_n(h_n) = \sup_{a \in A_{s_n}} \left\{ r_n(s_n, a) + \sum_{j \in S} p_n(j|s_n, a) u_{n+1}(h_n, a, j) \right\}.\]</div>
<p>By the induction hypothesis</p>
<div class="math notranslate nohighlight">
\[u_n(h_n) \geq \sup_{a \in A_{s_n}} \left\{ r_n(s_n, a) + \sum_{j \in S} p_n(j|s_n, a) u_{n+1}^*(h_n, a, j) \right\} \tag{4.3.5}\]</div>
<div class="math notranslate nohighlight">
\[\geq \sup_{a \in A_{s_n}} \left\{ r_n(s_n, a) + \sum_{j \in S} p_n(j|s_n, a) u_n^{\pi'}(h_n, a, j) \right\} \tag{4.3.6}\]</div>
<div class="math notranslate nohighlight">
\[\geq \sum q_{d_n'(h_n)}(a) \left\{ r_n(s_n, a) + \sum_{j \in S} p_n(j|s_n, a) u_{n+1}^{\pi'}(h_n, a, j) \right\} \tag{4.3.7}\]</div>
<div class="math notranslate nohighlight">
\[= u_n^{\pi'}(h_n).\]</div>
<p>The inequality in (4.3.5) follows from the induction hypothesis and the non-negativity of <span class="math notranslate nohighlight">\(p_n\)</span>, and that in (4.3.6) from the definition of <span class="math notranslate nohighlight">\(u_{n+1}^*\)</span>. That in (4.3.7) follows from Lemma 4.3.1 with <span class="math notranslate nohighlight">\(W = A_{s_n}\)</span> and <span class="math notranslate nohighlight">\(w\)</span> equal to the expression in brackets. The last equality follows from (4.2.6) and Theorem 4.2.2. Since <span class="math notranslate nohighlight">\(\pi'\)</span> is arbitrary,</p>
<div class="math notranslate nohighlight">
\[u_n(h_n) \geq u_n^{\pi}(h_n) \quad \text{for all } \pi \in \Pi^{\text{HR}}.\]</div>
<p>Thus <span class="math notranslate nohighlight">\(u_n(h_n) \geq u_n^*(h_n)\)</span> and the induction hypothesis holds.</p>
<p>Now we establish that for any <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span>, there exists a <span class="math notranslate nohighlight">\(\pi' \in \Pi^{\text{HD}}\)</span> for which</p>
<div class="math notranslate nohighlight">
\[u_n^{\pi'}(h_n) + (N - n)\varepsilon \geq u_n(h_n) \tag{4.3.8}\]</div>
<p>for all <span class="math notranslate nohighlight">\(h_n \in H_n\)</span> and <span class="math notranslate nohighlight">\(n = 1, 2, \ldots, N\)</span>. To do this, construct a policy <span class="math notranslate nohighlight">\(\pi' = (d_1, d_2, \ldots, d_{N-1})\)</span> by choosing <span class="math notranslate nohighlight">\(d_n(h_n)\)</span> to satisfy</p>
<div class="math notranslate nohighlight">
\[r_n(s_n, d_n(h_n)) + \sum_{j \in S} p_n(j|s_n, d_n(h_n)) u_{n+1}(s_n, d_n(h_n), j) + \varepsilon \geq u_n(h_n).\]</div>
<div class="math notranslate nohighlight">
\[(4.3.9)\]</div>
<p>We establish (4.3.8) by induction. Since <span class="math notranslate nohighlight">\(u_N^{\pi}(h_N) = u_N(h_N)\)</span>, the induction hypothesis holds for <span class="math notranslate nohighlight">\(t = N\)</span>. Assume that <span class="math notranslate nohighlight">\(u_t^{\pi}(h_t) + (N - t)\varepsilon \geq u_t(h_t)\)</span> for <span class="math notranslate nohighlight">\(t = n + 1, \ldots, N\)</span>.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="id1">
<h1>86 FINITE-HORIZON MARKOV DECISION PROCESSES<a class="headerlink" href="#id1" title="Link to this heading">#</a></h1>
<p>Then it follows from Theorem 4.2.1 and (4.3.9) that</p>
<div class="math notranslate nohighlight">
\[u_n^{\pi'}(h_n) = r_n(s_n, d_n(h_n)) + \sum_{j \in S} p_n(j|s_n, d_n(h_n)) u_{n+1}^{\pi'}(s_n, d_n(h_n), j)\]</div>
<div class="math notranslate nohighlight">
\[\geq r_n(s_n, d_n(h_n)) + \sum_{j \in S} p_n(j|s_n, d_n(h_n)) u_{n+1}(s_n, d_n(h_n), j)\]</div>
<div class="math notranslate nohighlight">
\[-(N - n - 1)\varepsilon\]</div>
<div class="math notranslate nohighlight">
\[\geq u_n(h_n) - (N - n)\varepsilon.\]</div>
<p>Thus the induction hypothesis is satisfied and (4.3.8) holds for <span class="math notranslate nohighlight">\(n = 1, 2, \ldots, N\)</span>.</p>
<p>Therefore for any <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span>, there exists a <span class="math notranslate nohighlight">\(\pi' \in \Pi^{\text{HR}}\)</span> for which</p>
<div class="math notranslate nohighlight">
\[u_n^*(h_n) + (N - n)\varepsilon \geq u_n^{\pi'}(h_n) + (N - n)\varepsilon \geq u_n(h_n) \geq u_n^*(h_n)\]</div>
<p>so that (a) follows. Part (b) follows from the definitions of the quantities. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>Part (a) of Theorem 4.3.2 means that solutions of the optimality equation are the optimal value functions from period <span class="math notranslate nohighlight">\(t\)</span> onward, and result (b) means that the solution to the equation with <span class="math notranslate nohighlight">\(n = 1\)</span> is the value function for the MDP, that is, it is the optimal value from decision epoch 1 onward.</p>
<p>The following result shows how to use the optimality equations to find optimal policies, and to verify that a policy is optimal. Theorem 4.3.3 uses optimality equations (4.3.4) in which maxima are attained. Theorem 4.3.4 considers the case of suprema.</p>
<p><strong>Theorem 4.3.3.</strong> Suppose <span class="math notranslate nohighlight">\(u_t^*\)</span>, <span class="math notranslate nohighlight">\(t = 1, \ldots, N\)</span> are solutions of the optimality equations (4.3.4) subject to boundary condition (4.3.3), and that policy <span class="math notranslate nohighlight">\(\pi^* = (d_1^*, d_2^*, \ldots, d_{N-1}^*) \in \Pi^{\text{HD}}\)</span> satisfies</p>
<div class="math notranslate nohighlight">
\[r_t(s_t, d_t^*(h_t)) + \sum_{j \in S} p_t(j|s_t, d_t^*(h_t)) u_{t+1}^*(h_t, d_t^*(h_t), j)\]</div>
<div class="math notranslate nohighlight">
\[= \max_{a \in A_{s_t}} \left\{ r_t(s_t, a) + \sum_{j \in S} p_t(j|s_t, a) u_{t+1}^*(h_t, a, j) \right\} \tag{4.3.10}\]</div>
<p>for <span class="math notranslate nohighlight">\(t = 1, \ldots, N - 1\)</span>.</p>
<p>Then</p>
<p><strong>a.</strong> For each <span class="math notranslate nohighlight">\(t = 1, 2, \ldots, N\)</span>,</p>
<div class="math notranslate nohighlight">
\[u_t^{\pi^*}(h_t) = u_t^*(h_t), \quad h_t \in H_t. \tag{4.3.11}\]</div>
<p><strong>b.</strong> <span class="math notranslate nohighlight">\(\pi^*\)</span> is an optimal policy, and</p>
<div class="math notranslate nohighlight">
\[v_N^{\pi^*}(s) = v_N^*(s), \quad s \in S. \tag{4.3.12}\]</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="optimality-equations-and-the-principle-of-optimality-87">
<h1>OPTIMALITY EQUATIONS AND THE PRINCIPLE OF OPTIMALITY 87<a class="headerlink" href="#optimality-equations-and-the-principle-of-optimality-87" title="Link to this heading">#</a></h1>
<p><em>Proof.</em> We establish part (a); part (b) follows from Theorem 4.2.1 and Theorem 4.3.2b. The proof is by induction. Clearly</p>
<div class="math notranslate nohighlight">
\[u_N^{\pi^*}(h_n) = u_N^*(h_n), \quad h_n \in H_n.\]</div>
<p>Assume the result holds for <span class="math notranslate nohighlight">\(t = n + 1, \ldots, N\)</span>. Then, for <span class="math notranslate nohighlight">\(h_n = (h_{n-1}, d_{n-1}^*(h_{n-1})), s_n)\)</span>,</p>
<div class="math notranslate nohighlight">
\[u_n^*(h_n) = \max_{a \in A_{s_n}} \left\{ r_n(s_n, a) + \sum_{j \in S} p_n(j|s_n, a) u_{n+1}^*(h_n, a, j) \right\}\]</div>
<div class="math notranslate nohighlight">
\[= r_n(s_n, d_n^*(h_n)) + \sum_{j \in S} p_n(j|s_n, d_n^*(h_n)) u_{n+1}^*(h_n, d_n^*(h_n), j)\]</div>
<div class="math notranslate nohighlight">
\[= u_n^{\pi^*}(h_n).\]</div>
<p>The second equality is a consequence of (4.3.10), and the induction hypothesis and the last equality follows from Theorem 4.2.1. Thus the induction hypothesis is satisfied and the result follows. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>We frequently write equation (4.3.10) as</p>
<div class="math notranslate nohighlight">
\[d_t^*(h_t) \in \arg\max_{a \in A_{s_t}} \left\{ r_t(s_t, a) + \sum_{j \in S_t} p_t(j|s_t, a) u_{t+1}^*(h_t, a, j) \right\},\]</div>
<p>where “arg max” has been defined in Sec. 2.2.</p>
<p>The theorem implies that an optimal policy is found by first solving the optimality equations, and then for each history choosing a decision rule which selects any action which attains the maximum on the right-hand side of (4.3.10) for <span class="math notranslate nohighlight">\(t = 1, 2, \ldots, N\)</span>. When using this equation in computation, for each history the right-hand side is evaluated for all <span class="math notranslate nohighlight">\(a \in A_{s_t}\)</span> and the set of maximizing actions is recorded. When there is more than one maximizing action in this set, there is more than one optimal policy.</p>
<p>Note that we have restricted attention to history-dependent deterministic policies in Theorem 4.3.3. This is because if there existed a history-dependent randomized policy which satisfied the obvious generalization of (4.3.10), as a result of Lemma 4.3.1, we could find a deterministic policy which satisfied (4.3.10). We expand on this point in the next section.</p>
<p>This theorem provides a formal statement of “The Principle of Optimality,” a fundamental result of dynamic programming. An early verbal statement appeared in Bellman (1957, p. 83).</p>
<p>“An optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision.”</p>
<p>Denardo (1982, p. 15) provides a related statement.</p>
<p>“There exists a policy that is optimal for every state (at every stage).”</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="id2">
<h1>88 FINITE-HORIZON MARKOV DECISION PROCESSES<a class="headerlink" href="#id2" title="Link to this heading">#</a></h1>
<p>Any deterministic policy <span class="math notranslate nohighlight">\(\pi^*\)</span> satisfying (4.3.10) has these properties and such a policy must <em>exist</em> because maxima are attained. Alternatively, Theorem 4.3.3 provides <em>sufficient</em> conditions for verifying the optimality of a policy.</p>
<p>Note that “The Principle of Optimality” may not be valid for other optimality criteria. In Sec. 4.6.2, we provide an example in which it does not hold, albeit under a nonstandard optimality criteria.</p>
<p>In case the supremum in (4.3.2) is not attained, the decision maker must be content with <span class="math notranslate nohighlight">\(\varepsilon\)</span>-optimal policies. To account for this we modify Theorem 4.3.3 as follows. Arguments in the second part of the proof of Theorem 4.3.2 can be used to establish it.</p>
<p><strong>Theorem 4.3.4.</strong> Let <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span> be arbitrary and suppose <span class="math notranslate nohighlight">\(u_t^*\)</span>, <span class="math notranslate nohighlight">\(t = 1, \ldots, N\)</span> are solutions of the optimality equations (4.3.2) and (4.3.3). Let <span class="math notranslate nohighlight">\(\pi^{\varepsilon} = (d_1^{\varepsilon}, d_2^{\varepsilon}, \ldots, d_{N-1}^{\varepsilon}) \in \Pi^{\text{HD}}\)</span> satisfy</p>
<div class="math notranslate nohighlight">
\[r_t(s_t, d_t^{\varepsilon}(h_t)) + \sum_{j \in S} p_t(j|s_t, d_t^{\varepsilon}(h_t)) u_{t+1}^*(h_t, d_t^{\varepsilon}(h_t), j) + \frac{\varepsilon}{N-1}\]</div>
<div class="math notranslate nohighlight">
\[\geq \sup_{a \in A_{s_t}} \left\{ r_t(s_t, a) + \sum_{j \in S} p_t(j|s_t, a) u_{t+1}^*(h_t, a, j) \right\} \tag{4.3.13}\]</div>
<p>for <span class="math notranslate nohighlight">\(t = 1, 2, \ldots, N - 1\)</span>. Then,</p>
<p><strong>a.</strong> For each <span class="math notranslate nohighlight">\(t = 1, 2, \ldots, N - 1\)</span>,</p>
<div class="math notranslate nohighlight">
\[u_t^{\pi^{\varepsilon}}(h_t) + (N - t)\frac{\varepsilon}{N-1} \geq u_t^*(h_t), \quad h_t \in H_t. \tag{4.3.14}\]</div>
<p><strong>b.</strong> <span class="math notranslate nohighlight">\(\pi^{\varepsilon}\)</span> is an <span class="math notranslate nohighlight">\(\varepsilon\)</span>-optimal policy, that is</p>
<div class="math notranslate nohighlight">
\[v_N^{\pi^{\varepsilon}}(s) + \varepsilon \geq v_N^*(s), \quad s \in S. \tag{4.3.15}\]</div>
<section id="optimality-of-deterministic-markov-policies">
<h2>4.4 OPTIMALITY OF DETERMINISTIC MARKOV POLICIES<a class="headerlink" href="#optimality-of-deterministic-markov-policies" title="Link to this heading">#</a></h2>
<p>This section provides conditions under which there exists an optimal policy which is deterministic and Markovian, and illustrates how backward induction can be used to determine the structure of an optimal policy.</p>
<p>From the perspective of application, we find it comforting that by restricting attention to nonrandomized Markov policies, which are simple to implement and evaluate, we may achieve as large an expected total reward as if we used randomized history-dependent policies. We show that when the immediate rewards and transition probabilities depend on the past only through the current state of the system (as assumed throughout this book), the optimal value functions depend on the history only through the current state of the system. This enables us to impose assumptions on the action sets, rewards, and transition probabilities which ensure existence of optimal policies which depend only on the system state.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="optimality-of-deterministic-markov-policies-89">
<h1>OPTIMALITY OF DETERMINISTIC MARKOV POLICIES 89<a class="headerlink" href="#optimality-of-deterministic-markov-policies-89" title="Link to this heading">#</a></h1>
<p>Inspection of the proof of Theorem 4.3.2 reveals that it constructs an <span class="math notranslate nohighlight">\(\varepsilon\)</span>-optimal deterministic history-dependent policy. Theorem 4.3.3 and 4.3.4 identify optimal and <span class="math notranslate nohighlight">\(\varepsilon\)</span>-optimal policies. We summarize these results as follows.</p>
<section id="theorem-4-4-1">
<h2>Theorem 4.4.1.<a class="headerlink" href="#theorem-4-4-1" title="Link to this heading">#</a></h2>
<p><strong>a.</strong> For any <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span>, there exists an <span class="math notranslate nohighlight">\(\varepsilon\)</span>-optimal policy which is deterministic history dependent. Any policy in <span class="math notranslate nohighlight">\(\Pi^{\text{HD}}\)</span> which satisfies (4.3.13) is <span class="math notranslate nohighlight">\(\varepsilon\)</span>-optimal.</p>
<p><strong>b.</strong> Let <span class="math notranslate nohighlight">\(u_t^*\)</span> be a solution of (4.3.2) and (4.3.3) and suppose that for each <span class="math notranslate nohighlight">\(t\)</span> and <span class="math notranslate nohighlight">\(s_t \in S\)</span>, there exists an <span class="math notranslate nohighlight">\(a' \in A_{s_t}\)</span> for which</p>
<div class="math notranslate nohighlight">
\[r_t(s_t, a') + \sum_{j \in S} p_t(j|s_t, a') u_{t+1}^*(h_t, a', j)\]</div>
<div class="math notranslate nohighlight">
\[= \sup_{a \in A_{s_t}} \left\{ r_t(s_t, a) + \sum_{j \in S} p_t(j|s_t, a) u_{t+1}^*(h_t, a, j) \right\} \tag{4.4.1}\]</div>
<p>for all <span class="math notranslate nohighlight">\(h_t = (s_{t-1}, a_{t-1}, s_t) \in H_t\)</span>. Then there exists a deterministic history-dependent policy which is optimal.</p>
<p><em>Proof.</em> Part (a) follows from the second part of the proof of Theorem 4.3.2. The policy <span class="math notranslate nohighlight">\(\pi^{\varepsilon}\)</span> in Theorem 4.3.4 is optimal and deterministic. When there exists an <span class="math notranslate nohighlight">\(a' \in A_{s_t}\)</span> for which (4.4.1) holds, the policy <span class="math notranslate nohighlight">\(\pi^* \in \Pi^{\text{HD}}\)</span> of Theorem 4.3.3 is optimal. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>We next show by induction that there exists an optimal policy which is Markovian and deterministic.</p>
<p><strong>Theorem 4.4.2.</strong> Let <span class="math notranslate nohighlight">\(u_t^*\)</span>, <span class="math notranslate nohighlight">\(t = 1, \ldots, N\)</span> be solutions of (4.3.2) and (4.3.3). Then</p>
<p><strong>a.</strong> For each <span class="math notranslate nohighlight">\(t = 1, \ldots, N\)</span>, <span class="math notranslate nohighlight">\(u_t^*(h_t)\)</span> depends on <span class="math notranslate nohighlight">\(h_t\)</span> only through <span class="math notranslate nohighlight">\(s_t\)</span>.</p>
<p><strong>b.</strong> For any <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span>, there exists an <span class="math notranslate nohighlight">\(\varepsilon\)</span>-optimal policy which is deterministic and Markov.</p>
<p><strong>c.</strong> If there exists an <span class="math notranslate nohighlight">\(a' \in A_{s_t}\)</span> such that (4.4.1) holds for each <span class="math notranslate nohighlight">\(s_t \in S\)</span> and <span class="math notranslate nohighlight">\(t = 1, 2, \ldots, N - 1\)</span>, there exists an optimal policy which is deterministic and Markov.</p>
<p><em>Proof.</em> We show that (a) holds by induction. Since <span class="math notranslate nohighlight">\(u_N^*(h_N) = u_N^*(h_{N-1}, a_{N-1}, s) = r_N(s)\)</span> for all <span class="math notranslate nohighlight">\(h_{N-1} \in H_{N-1}\)</span> and <span class="math notranslate nohighlight">\(a_{N-1} \in A_{s_{N-1}}\)</span>, <span class="math notranslate nohighlight">\(u_N^*(h_N) = u_N^*(s_N)\)</span>. Assume now that (a) is valid for <span class="math notranslate nohighlight">\(n = t + 1, \ldots, N\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[u_t^*(h_t) = \sup_{a \in A_{s_t}} \left\{ r_t(s_t, a) + \sum_{j \in S} p_t(j|s_t, a) u_{t+1}^*(h_t, a, j) \right\},\]</div>
<p>which by the induction hypothesis gives</p>
<div class="math notranslate nohighlight">
\[u_t^*(h_t) = \sup_{a \in A_{s_t}} \left\{ r_t(s_t, a) + \sum_{j \in S} p_t(j|s_t, a) u_{t+1}^*(j) \right\}. \tag{4.4.2}\]</div>
<p>Since the quantity in brackets depend on <span class="math notranslate nohighlight">\(h_t\)</span> only through <span class="math notranslate nohighlight">\(s_t\)</span>, (a) holds for all <span class="math notranslate nohighlight">\(t\)</span>.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="id3">
<h1>90 FINITE-HORIZON MARKOV DECISION PROCESSES<a class="headerlink" href="#id3" title="Link to this heading">#</a></h1>
<p>Choose <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span>, and let <span class="math notranslate nohighlight">\(\pi^{\varepsilon} = (d_1^{\varepsilon}, d_2^{\varepsilon}, \ldots, d_{N-1}^{\varepsilon})\)</span> be any policy in <span class="math notranslate nohighlight">\(\Pi^{\text{MD}}\)</span> satisfying</p>
<div class="math notranslate nohighlight">
\[r_t(s_t, d_t^{\varepsilon}(s_t)) + \sum_{j \in S} p_t(j|s_t, d_t^{\varepsilon}(s_t)) u_{t+1}^*(j) + \frac{\varepsilon}{N-1}\]</div>
<div class="math notranslate nohighlight">
\[\geq \sup_{a \in A_{s_t}} \left\{ r_t(s_t, a) + \sum_{j \in S} p_t(j|s_t, a) u_{t+1}^*(j) \right\}. \tag{4.4.3}\]</div>
<p>Then, by part (a), <span class="math notranslate nohighlight">\(\pi^{\varepsilon}\)</span> satisfies the hypotheses of Theorem 4.3.4b so it is <span class="math notranslate nohighlight">\(\varepsilon\)</span>-optimal.</p>
<p>Part (c) follows by noting that under the hypotheses of Theorem 4.4.1b, there exists a <span class="math notranslate nohighlight">\(\pi^* = (d_1^*, d_2^*, \ldots, d_{N-1}^*) \in \Pi^{\text{MD}}\)</span>, which satisfies</p>
<div class="math notranslate nohighlight">
\[r_t(s_t, d_t^*(s_t)) + \sum_{j \in S} p_t(j|s_t, d_t^*(s_t)) u_{t+1}^*(j)\]</div>
<div class="math notranslate nohighlight">
\[= \max_{a \in A_{s_t}} \left\{ r_t(s_t, a) + \sum_{j \in S} p_t(j|s_t, a) u_{t+1}^*(j) \right\}. \tag{4.4.4}\]</div>
<p>Therefore by part (a) and Theorem 4.3.3b, <span class="math notranslate nohighlight">\(\pi^*\)</span> is optimal. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>Thus we have established that</p>
<div class="math notranslate nohighlight">
\[v_N^*(s) = \sup_{\pi \in \Pi^{\text{HR}}} v_N^{\pi}(s) = \sup_{\pi \in \Pi^{\text{MD}}} v_N^{\pi}(s), \quad s \in S.\]</div>
<p>We now provide conditions under which the supremum in (4.4.1) is attained, so that we may easily determine when there exists a deterministic Markovian policy which is optimal. Following Appendix B, we say that a set <span class="math notranslate nohighlight">\(X\)</span> is <em>compact</em> if it is a compact subset of a complete separable metric space. In many applications we consider only compact subsets of <span class="math notranslate nohighlight">\(R^n\)</span>. The proof of part (c) below is quite technical and relies on properties of semicontinuous functions described in Appendix B. Note that more subtle arguments are required for general <span class="math notranslate nohighlight">\(S\)</span>.</p>
<p><strong>Proposition 4.4.3.</strong> Assume <span class="math notranslate nohighlight">\(S\)</span> is finite or countable, and that</p>
<p><strong>a.</strong> <span class="math notranslate nohighlight">\(A_s\)</span> is finite for each <span class="math notranslate nohighlight">\(s \in S\)</span>, or</p>
<p><strong>b.</strong> <span class="math notranslate nohighlight">\(A_s\)</span> is compact, <span class="math notranslate nohighlight">\(r_t(s, a)\)</span> is continuous in <span class="math notranslate nohighlight">\(a\)</span> for each <span class="math notranslate nohighlight">\(s \in S\)</span>, there exists an <span class="math notranslate nohighlight">\(M &lt; \infty\)</span> for which <span class="math notranslate nohighlight">\(|r_t(s, a)| \leq M\)</span> for all <span class="math notranslate nohighlight">\(a \in A_s\)</span>, <span class="math notranslate nohighlight">\(s \in S\)</span>, and <span class="math notranslate nohighlight">\(p_t(j|s, a)\)</span> is continuous in <span class="math notranslate nohighlight">\(a\)</span> for each <span class="math notranslate nohighlight">\(j \in S\)</span> and <span class="math notranslate nohighlight">\(s \in S\)</span> and <span class="math notranslate nohighlight">\(t = 1, 2, \ldots, N\)</span>, or</p>
<p><strong>c.</strong> <span class="math notranslate nohighlight">\(A_s\)</span> is a compact, <span class="math notranslate nohighlight">\(r_t(s, a)\)</span> is upper semicontinuous (u.s.c.) in <span class="math notranslate nohighlight">\(a\)</span> for each <span class="math notranslate nohighlight">\(s \in S\)</span>, there exists an <span class="math notranslate nohighlight">\(M &lt; \infty\)</span> for which <span class="math notranslate nohighlight">\(|r_t(s, a)| \leq M\)</span> for all <span class="math notranslate nohighlight">\(a \in A_s\)</span>, <span class="math notranslate nohighlight">\(s \in S\)</span>, and for each <span class="math notranslate nohighlight">\(j \in S\)</span> and <span class="math notranslate nohighlight">\(s \in S\)</span>, <span class="math notranslate nohighlight">\(p_t(j|s, a)\)</span> is lower semi-continuous (l.s.c.) in <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(t = 1, 2, \ldots, N\)</span>.</p>
<p>Then there exists a deterministic Markovian policy which is optimal.</p>
<p><em>Proof.</em> We show that there exists an <span class="math notranslate nohighlight">\(a'\)</span> which satisfies (4.4.1) under hypothesis (a), (b), or (c), in which case the result follows from Theorem 4.4.2. Note that as a consequence of Theorem 4.4.2c, we require that, for each <span class="math notranslate nohighlight">\(s \in S\)</span>, there exists an</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="optimality-of-deterministic-markov-policies-91">
<h1>OPTIMALITY OF DETERMINISTIC MARKOV POLICIES 91<a class="headerlink" href="#optimality-of-deterministic-markov-policies-91" title="Link to this heading">#</a></h1>
<p><span class="math notranslate nohighlight">\(a' \in A_s\)</span>, for which</p>
<div class="math notranslate nohighlight">
\[r_t(s, a') + \sum_{j \in S} p_t(j|s_t, a') u_{t+1}^*(j)\]</div>
<div class="math notranslate nohighlight">
\[= \sup_{a \in A_s} \left\{ r_t(s, a) + \sum_{j \in S} p_t(j|s, a) u_{t+1}^*(j) \right\}. \tag{4.4.5}\]</div>
<p>Clearly such an <span class="math notranslate nohighlight">\(a'\)</span> exists when <span class="math notranslate nohighlight">\(A_s\)</span> is finite so the result follows under a.</p>
<p>Suppose that the hypotheses in part (c) hold. By assumption <span class="math notranslate nohighlight">\(|r_t(s, a)| \leq M\)</span> for all <span class="math notranslate nohighlight">\(s \in S\)</span> and <span class="math notranslate nohighlight">\(a \in A_s\)</span>, <span class="math notranslate nohighlight">\(|u_t^*(s)| \leq NM\)</span> for all <span class="math notranslate nohighlight">\(s \in S\)</span> and <span class="math notranslate nohighlight">\(t = 1, 2, \ldots, N\)</span>. Therefore, for each <span class="math notranslate nohighlight">\(t\)</span>, <span class="math notranslate nohighlight">\(u_t^*(s) - NM \leq 0\)</span>. Now apply Proposition B.3, with <span class="math notranslate nohighlight">\(s\)</span> fixed and <span class="math notranslate nohighlight">\(X\)</span> identified with <span class="math notranslate nohighlight">\(S_Y\)</span>, <span class="math notranslate nohighlight">\(A_s\)</span> identified with <span class="math notranslate nohighlight">\(q(x, y)\)</span>, <span class="math notranslate nohighlight">\(p_t(j|s, a)\)</span> identified with <span class="math notranslate nohighlight">\(f(w, x)\)</span> and <span class="math notranslate nohighlight">\(u_t^*(s) - NM\)</span> identified with <span class="math notranslate nohighlight">\(f(x)\)</span>, to obtain that</p>
<div class="math notranslate nohighlight">
\[\sum_{j \in S} p_t(j|s, a)[u_{t+1}^*(j) - NM]\]</div>
<p>is u.s.c., from which we conclude that <span class="math notranslate nohighlight">\(\sum_{j \in S} p_t(j|s, a)u_{t+1}^*(j)\)</span> is u.s.c. By Proposition B.1.a, <span class="math notranslate nohighlight">\(r_t(s, a) + \sum_{j \in S} p_t(j|s, a)u_{t+1}^*(j)\)</span> is u.s.c. in <span class="math notranslate nohighlight">\(a\)</span> for each <span class="math notranslate nohighlight">\(s \in S\)</span>. Therefore by Theorem B.2, the supremum over <span class="math notranslate nohighlight">\(a\)</span> in (4.4.5) is attained, from which the result follows.</p>
<p>Conclusion (b) follows from (c) since continuous functions are both upper and lower semicontinuous. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>To illustrate this result, consider the following example which we analyze in further detail in Sec. 6.4.</p>
<p><strong>Example 4.4.1.</strong> Let <span class="math notranslate nohighlight">\(N = 3\)</span>; <span class="math notranslate nohighlight">\(S = \{s_1, s_2\}\)</span>; <span class="math notranslate nohighlight">\(A_{s_1} = [0, 2]\)</span>, and <span class="math notranslate nohighlight">\(A_{s_2} = \{a_{2,1}\}\)</span>; <span class="math notranslate nohighlight">\(r_t(s_1, a) = -a^2\)</span>, <span class="math notranslate nohighlight">\(r_t(s_2, a_{2,1}) = -\frac{1}{2}\)</span>, <span class="math notranslate nohighlight">\(p_t(s_1|s_1, a) = \frac{1}{2}a\)</span>, <span class="math notranslate nohighlight">\(p_t(s_2|s_1, a) = 1 - \frac{1}{2}a\)</span>, and <span class="math notranslate nohighlight">\(p_t(s_2|s_2, a_{2,1}) = 1\)</span> for <span class="math notranslate nohighlight">\(t = 1, 2\)</span>, <span class="math notranslate nohighlight">\(r_3(s_1) = -1\)</span>, and <span class="math notranslate nohighlight">\(r_3(s_2) = -\frac{1}{2}\)</span>. (See Fig. 4.4.1)</p>
<p>Since <span class="math notranslate nohighlight">\(A_{s_1}\)</span> is compact and <span class="math notranslate nohighlight">\(r_t(s_1, \cdot)\)</span> and <span class="math notranslate nohighlight">\(p_t(j|s_1, \cdot)\)</span> are continuous functions on <span class="math notranslate nohighlight">\(A_{s_1}\)</span> there exists a deterministic Markov policy which is optimal.</p>
<p>[THIS IS FIGURE: A graphical representation showing two states <span class="math notranslate nohighlight">\(S_1\)</span> and <span class="math notranslate nohighlight">\(S_2\)</span> with arrows indicating transitions. From <span class="math notranslate nohighlight">\(S_1\)</span>, there’s a self-loop labeled “{ -a², a/2 }” and an arrow to <span class="math notranslate nohighlight">\(S_2\)</span> labeled “{ -a², 1 - a/2 }”. From <span class="math notranslate nohighlight">\(S_2\)</span>, there’s a self-loop labeled “{ -1/2, 1 }” and an arrow labeled “<span class="math notranslate nohighlight">\(a_{2,1}\)</span>”.]</p>
<p><strong>Figure 4.4.1</strong> Graphical representation of Example 4.4.1.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="id4">
<h1>92 FINITE-HORIZON MARKOV DECISION PROCESSES<a class="headerlink" href="#id4" title="Link to this heading">#</a></h1>
<section id="backward-induction">
<h2>4.5 BACKWARD INDUCTION<a class="headerlink" href="#backward-induction" title="Link to this heading">#</a></h2>
<p>Backward induction provides an efficient method for solving finite-horizon discrete-time MDPs. For stochastic problems, enumeration and evaluation of <em>all</em> policies is the only alternative, but forward induction and reaching methods provide alternative solution methods for deterministic systems. The terms “backward induction” and “dynamic programming” are synonymous, although the expression “dynamic programming” often refers to all results and methods for sequential decision processes. This section presents the backward induction algorithm and shows how to use it to find optimal policies and value functions. The algorithm generalizes the policy evaluation algorithm of Sec. 4.2.</p>
<p>We present the algorithm for a model in which maxima are obtained in (4.3.2), so that we are assured of obtaining an optimal (instead of an <span class="math notranslate nohighlight">\(\varepsilon\)</span>-optimal) Markovian deterministic policy. The algorithm solves optimality equations (4.3.4) subject to boundary condition (4.3.3). Generalization to models based on (4.3.2) is left as an exercise.</p>
<section id="the-backward-induction-algorithm">
<h3>The Backward Induction Algorithm<a class="headerlink" href="#the-backward-induction-algorithm" title="Link to this heading">#</a></h3>
<p><strong>1.</strong> Set <span class="math notranslate nohighlight">\(t = N\)</span> and</p>
<div class="math notranslate nohighlight">
\[u_N^*(s_N) = r_N(s_N) \quad \text{for all } s_N \in S,\]</div>
<p><strong>2.</strong> Substitute <span class="math notranslate nohighlight">\(t - 1\)</span> for <span class="math notranslate nohighlight">\(t\)</span> and compute <span class="math notranslate nohighlight">\(u_t^*(s_t)\)</span> for each <span class="math notranslate nohighlight">\(s_t \in S\)</span> by</p>
<div class="math notranslate nohighlight">
\[u_t^*(s_t) = \max_{a \in A_{s_t}} \left\{ r_t(s_t, a) + \sum_{j \in S} p_t(j|s_t, a) u_{t+1}^*(j) \right\}. \tag{4.5.1}\]</div>
<p>Set</p>
<div class="math notranslate nohighlight">
\[A_{s_t, t}^* = \arg\max_{a \in A_{s_t}} \left\{ r_t(s_t, a) + \sum_{j \in S} p_t(j|s_t, a) u_{t+1}^*(j) \right\}. \tag{4.5.2}\]</div>
<p><strong>3.</strong> If <span class="math notranslate nohighlight">\(t = 1\)</span>, stop. Otherwise return to step 2.</p>
<p>As a consequence of Theorem 4.4.2, which among other results shows that <span class="math notranslate nohighlight">\(u_t^*\)</span> depends on <span class="math notranslate nohighlight">\(h_t = (s_{t-1}, a_{t-1}, s_t)\)</span> only through <span class="math notranslate nohighlight">\(s_t\)</span>, we need not evaluate <span class="math notranslate nohighlight">\(u_t^*\)</span> for all <span class="math notranslate nohighlight">\(h_t \in H_t\)</span>. This significantly reduces computational effort. Combining results from Theorems 4.3.2 and 4.4.2 yields the following properties of the iterates of the backward induction algorithm.</p>
<p><strong>Theorem 4.5.1.</strong> Suppose <span class="math notranslate nohighlight">\(u_t^*\)</span>, <span class="math notranslate nohighlight">\(t = 1, \ldots, N\)</span> and <span class="math notranslate nohighlight">\(A_{s_t, t}^*\)</span>, <span class="math notranslate nohighlight">\(t = 1, \ldots, N - 1\)</span> satisfy (4.5.1) and (4.5.2); then,</p>
<p><strong>a.</strong> for <span class="math notranslate nohighlight">\(t = 1, \ldots, N\)</span> and <span class="math notranslate nohighlight">\(h_t = (h_{t-1}, a_{t-1}, s_t)\)</span></p>
<div class="math notranslate nohighlight">
\[u_t^*(s_t) = \sup_{\pi \in \Pi^{\text{HR}}} u_t^{\pi}(h_t), \quad s_t \in S.\]</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="backward-induction-93">
<h1>BACKWARD INDUCTION 93<a class="headerlink" href="#backward-induction-93" title="Link to this heading">#</a></h1>
<p><strong>b.</strong> Let <span class="math notranslate nohighlight">\(d^*(s_t) \in A_{s_t, t}^*\)</span> for all <span class="math notranslate nohighlight">\(s_t \in S\)</span>, <span class="math notranslate nohighlight">\(t = 1, \ldots, N - 1\)</span>, and let <span class="math notranslate nohighlight">\(\pi^* = (d_1^*, \ldots, d_{N-1}^*)\)</span>. Then <span class="math notranslate nohighlight">\(\pi^* \in \Pi^{\text{MD}}\)</span> is optimal and satisfies</p>
<div class="math notranslate nohighlight">
\[v_N^{\pi^*}(s) = \sup_{\pi \in \Pi^{\text{HR}}} v_N^{\pi}(s), \quad s \in S\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[u_t^{\pi^*}(s_t) = u_t^*(s_t), \quad s_t \in S\]</div>
<p>for <span class="math notranslate nohighlight">\(t = 1, \ldots, N\)</span>.</p>
<p>This theorem represents a formal statement of the following properties of the backward induction algorithm.</p>
<p><strong>a.</strong> For <span class="math notranslate nohighlight">\(t = 1, 2, \ldots, N - 1\)</span>, it finds sets <span class="math notranslate nohighlight">\(A_{s_t, t}^*\)</span> which contain all actions in <span class="math notranslate nohighlight">\(A_{s_t}\)</span> which attain the maximum in (4.5.1).</p>
<p><strong>b.</strong> It evaluates any policy which selects an action in <span class="math notranslate nohighlight">\(A_{s_t, t}^*\)</span> for each <span class="math notranslate nohighlight">\(s_t \in S\)</span> for all <span class="math notranslate nohighlight">\(t = 1, 2, \ldots, N - 1\)</span>.</p>
<p><strong>c.</strong> It computes the expected total reward for the entire decision-making horizon, and from each period to the end of the horizon for any optimal policy.</p>
<p>Let <span class="math notranslate nohighlight">\(D_t^* \equiv \times_{s \in S} A_{s, t}^*\)</span>. Then any <span class="math notranslate nohighlight">\(\pi^* \in \Pi^* \equiv D_1^* \times \ldots \times D_{N-1}^*\)</span> is an optimal policy. If more than one such <span class="math notranslate nohighlight">\(\pi^*\)</span> exists, each yields the same expected total reward. This occurs if for some <span class="math notranslate nohighlight">\(s_t\)</span>, <span class="math notranslate nohighlight">\(A_{s_t, t}^*\)</span> contains more than one action. To obtain a particular optimal policy, it is only necessary to retain a single action from <span class="math notranslate nohighlight">\(A_{s_t, t}^*\)</span> for each <span class="math notranslate nohighlight">\(t \leq N - 1\)</span> and <span class="math notranslate nohighlight">\(s_t \in S\)</span>.</p>
<p>Although this chapter emphasizes models with finite <span class="math notranslate nohighlight">\(S\)</span> and <span class="math notranslate nohighlight">\(A\)</span>, the algorithm and results of Theorem 4.5.1 are valid in greater generality. It applies to models with countable, compact, or Polish state and action spaces. In nondiscrete models, regularity conditions are required to ensure that <span class="math notranslate nohighlight">\(u_t\)</span> is measurable, integrals exist, and maxima are attained. Often we discretize the state and action spaces prior to computation; however, backward induction may be used to find optimal policies when the maxima and maximizing actions can be determined analytically. More importantly, a considerable portion of stochastic optimization literature uses backward induction to characterize the form of optimal policies under structural assumptions on rewards and transition probabilities. When such results can be obtained, specialized algorithms may be developed to determine the best policy of that type. We expand on this point in Sec. 4.7.</p>
<p>When there are <span class="math notranslate nohighlight">\(K\)</span> states with <span class="math notranslate nohighlight">\(L\)</span> actions in each, the backward induction algorithm requires <span class="math notranslate nohighlight">\((N - 1)LK^2\)</span> multiplications to evaluate and determine an optimal policy. Since there are <span class="math notranslate nohighlight">\((L^K)^{(N-1)}\)</span> deterministic Markovian policies, and direct evaluation of each requires <span class="math notranslate nohighlight">\((N - 1)K^2\)</span> multiplications, this represents considerable reduction in computation.</p>
<p>A further advantage of using backward induction is that, at pass <span class="math notranslate nohighlight">\(t\)</span> through the algorithm, only <span class="math notranslate nohighlight">\(r_t\)</span>, <span class="math notranslate nohighlight">\(p_t\)</span> and <span class="math notranslate nohighlight">\(u_{t+1}^*\)</span> need be in high-speed memory. The data from iterations <span class="math notranslate nohighlight">\(t + 1\)</span>, <span class="math notranslate nohighlight">\(t + 2, \ldots, N\)</span> are not required since they are summarized through <span class="math notranslate nohighlight">\(u_{t+1}^*\)</span>, and the data from decision epochs <span class="math notranslate nohighlight">\(1, 2, \ldots, t - 1\)</span> are not needed until subsequent iterations. Of course, <span class="math notranslate nohighlight">\(A_{s, t}^*\)</span> must be stored for all <span class="math notranslate nohighlight">\(t\)</span> to recover all optimal policies.</p>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">4.3 OPTIMALITY EQUATIONS AND THE PRINCIPLE OF OPTIMALITY</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#finite-horizon-markov-decision-processes">84 FINITE-HORIZON MARKOV DECISION PROCESSES</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#optimality-equations-and-the-principle-of-optimality-85">OPTIMALITY EQUATIONS AND THE PRINCIPLE OF OPTIMALITY 85</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">86 FINITE-HORIZON MARKOV DECISION PROCESSES</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#optimality-equations-and-the-principle-of-optimality-87">OPTIMALITY EQUATIONS AND THE PRINCIPLE OF OPTIMALITY 87</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">88 FINITE-HORIZON MARKOV DECISION PROCESSES</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimality-of-deterministic-markov-policies">4.4 OPTIMALITY OF DETERMINISTIC MARKOV POLICIES</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#optimality-of-deterministic-markov-policies-89">OPTIMALITY OF DETERMINISTIC MARKOV POLICIES 89</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#theorem-4-4-1">Theorem 4.4.1.</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">90 FINITE-HORIZON MARKOV DECISION PROCESSES</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#optimality-of-deterministic-markov-policies-91">OPTIMALITY OF DETERMINISTIC MARKOV POLICIES 91</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">92 FINITE-HORIZON MARKOV DECISION PROCESSES</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-induction">4.5 BACKWARD INDUCTION</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-backward-induction-algorithm">The Backward Induction Algorithm</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-induction-93">BACKWARD INDUCTION 93</a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Pierre-Luc Bacon
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>