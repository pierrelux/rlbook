
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>1. Discrete-Time Trajectory Optimization &#8212; Reinforcement Learning: beyond the Agent Interaction Loop</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=87e54e7c" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css?v=1f1b58a8" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"bm": ["{\\boldsymbol #1}", 1]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'ocp';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="4. Continuous-Time Trajectory Optimization" href="cocp.html" />
    <link rel="prev" title="Introduction" href="intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Reinforcement Learning: beyond the Agent Interaction Loop</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">1. Discrete-Time Trajectory Optimization</a></li>


<li class="toctree-l1"><a class="reference internal" href="cocp.html">4. Continuous-Time Trajectory Optimization</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/pierrelux/rlbook" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pierrelux/rlbook/edit/main/ocp.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pierrelux/rlbook/issues/new?title=Issue%20on%20page%20%2Focp.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/ocp.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Discrete-Time Trajectory Optimization</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">1. Discrete-Time Trajectory Optimization</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reduction-to-mayer-problems">1.1. Reduction to Mayer Problems</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-methods-for-solving-docps">2. Numerical Methods for Solving DOCPs</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#single-shooting-methods">2.1. Single Shooting Methods</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dealing-with-bound-constraints">2.1.1. Dealing with Bound Constraints</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#on-the-choice-of-optimizer">2.1.2. On the choice of optimizer</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#constrained-optimization-approach">2.2. Constrained Optimization Approach</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nonlinear-programming">2.2.1. Nonlinear Programming</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#karush-kuhn-tucker-kkt-conditions">2.2.1.1. Karush-Kuhn-Tucker (KKT) conditions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#lagrange-multiplier-theorem">2.2.1.2. Lagrange Multiplier Theorem</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#newton-s-method">2.2.1.3. Newton’s Method</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#efficient-implementation-of-newton-s-method">2.2.1.3.1. Efficient Implementation of Newton’s Method</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-equality-constrained-programs-with-newton-s-method">2.2.1.4. Solving Equality Constrained Programs with Newton’s Method</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#demonstration">2.2.1.4.1. Demonstration</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-sqp-approach-taylor-expansion-and-quadratic-approximation">2.2.2. The SQP Approach: Taylor Expansion and Quadratic Approximation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-newton-s-method-in-the-equality-constrained-case">2.2.2.1. Connection to Newton’s Method in the Equality-Constrained Case</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sqp-for-inequality-constrained-optimization">2.2.3. SQP for Inequality-Constrained Optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#demonstration-with-jax-and-cvxpy">2.2.3.1. Demonstration with JAX and CVXPy</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-arrow-hurwicz-uzawa-algorithm">2.2.4. The Arrow-Hurwicz-Uzawa algorithm</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#the-discrete-time-pontryagin-maximum-principle">3. The Discrete-Time Pontryagin Maximum Principle</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pmp-for-mayer-problems">3.1. PMP for Mayer Problems</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pmp-for-bolza-problems">3.2. PMP for Bolza Problems</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hamiltonian-formulation">3.3. Hamiltonian Formulation</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="discrete-time-trajectory-optimization">
<h1><span class="section-number">1. </span>Discrete-Time Trajectory Optimization<a class="headerlink" href="#discrete-time-trajectory-optimization" title="Link to this heading">#</a></h1>
<p>The three quantities <span class="math notranslate nohighlight">\((w_t, q_t, a_t)\)</span> appearing in the model of <span id="id1">Hall and Butcher [<a class="reference internal" href="#id14" title="Warren A. Hall and William S. Butcher. Optimal timing of irrigation. Journal of the Irrigation and Drainage Division, 94(2):267–275, June 1968. URL: http://dx.doi.org/10.1061/JRCEA4.0000569, doi:10.1061/jrcea4.0000569.">HB68</a>]</span> above have the property that they encompass all of the information necessary to simulate the process. We say that it is a “<strong>state variable</strong>”, and its time evolution is specified via so-called <strong>dynamics function</strong>, which we commonly denote by <span class="math notranslate nohighlight">\(f_t\)</span>. In discrete-time systems, the dynamics are often described by “difference equations,” as opposed to the “differential equations” used for continuous-time systems. When the dynamics function depends on the time index <span class="math notranslate nohighlight">\(t\)</span>, we refer to it as “non-stationary” dynamics. Conversely, if the function <span class="math notranslate nohighlight">\(f\)</span> remains constant across all time steps, we call it “stationary” dynamics. In the context of optimal control theory, we refer more generally to <span class="math notranslate nohighlight">\(u_t\)</span> as a “control” variable while in other communities it is called an “action”. Whether the problem is posed as a minimization problem or maximization problem is also a matter of communities, with control theory typically posing problems in terms of cost minimization while OR and RL communities usually adopt a reward maximization perspective. In this course, we will alternate between the two equivalent formulations while ensuring that context is sufficiently clear to understand which one is used.</p>
<p>The problem stated above, is known as a Discrete-Time Optimal Control Problem (DOCP), which we write more generically as:</p>
<div class="proof definition admonition" id="bolza-docp">
<p class="admonition-title"><span class="caption-number">Definition 1.1 </span> (Discrete-Time Optimal Control Problem of Bolza Type)</p>
<section class="definition-content" id="proof-content">
<div class="amsmath math notranslate nohighlight">
\[\begin{alignat*}{2}
\text{minimize} \quad &amp; c_T(\mathbf{x}_T) + \sum_{t=1}^T c_t(\mathbf{x}_t, \mathbf{u}_t) &amp; \\
\text{such that} \quad 
&amp; \mathbf{x}_{t+1} = \mathbf{f}_t(\mathbf{x}_t, \mathbf{u}_t), &amp; \quad &amp; t = 1, \dots, T-1, \\
&amp; \mathbf{u}_{lb} \leq \mathbf{u}_t \leq \mathbf{u}_{ub}, &amp; \quad &amp; t = 1, \dots, T, \\
&amp; \mathbf{x}_{lb} \leq \mathbf{x}_t \leq \mathbf{x}_{ub}, &amp; \quad &amp; t = 1, \dots, T, \\
\text{given} \quad &amp; \mathbf{x}_1. &amp;
\end{alignat*}\]</div>
</section>
</div><p>The objective function (sometimes called a “performance index”) in a Bolza problem comprises of two terms: the sum of immediate cost or rewards per stage, and a terminal cost function (sometimes called “scrap value”). If the terminal cost function is omitted from the objective function, then the resulting DOCP is said to be of Lagrange type.</p>
<div class="proof definition admonition" id="lagrange-docp">
<p class="admonition-title"><span class="caption-number">Definition 1.2 </span> (Discrete-Time Optimal Control Problem of Lagrange Type)</p>
<section class="definition-content" id="proof-content">
<div class="amsmath math notranslate nohighlight">
\[\begin{alignat*}{2}
\text{minimize} \quad &amp; \sum_{t=1}^T c_t(\mathbf{x}_t, \mathbf{u}_t) &amp; \\
\text{such that} \quad 
&amp; \mathbf{x}_{t+1} = \mathbf{f}_t(\mathbf{x}_t, \mathbf{u}_t), &amp; \quad &amp; t = 1, \dots, T-1, \\
&amp; \mathbf{u}_{lb} \leq \mathbf{u}_t \leq \mathbf{u}_{ub}, &amp; \quad &amp; t = 1, \dots, T, \\
&amp; \mathbf{x}_{lb} \leq \mathbf{x}_t \leq \mathbf{x}_{ub}, &amp; \quad &amp; t = 1, \dots, T, \\
\text{given} \quad &amp; \mathbf{x}_1. &amp;
\end{alignat*}\]</div>
</section>
</div><p>Finally, a Mayer problem is one in which the objective function only contains a terminal cost function without explicitly accounting for immediate costs encountered across stages:</p>
<div class="proof definition admonition" id="mayer-docp">
<p class="admonition-title"><span class="caption-number">Definition 1.3 </span> (Discrete-Time Optimal Control Problem of Mayer Type)</p>
<section class="definition-content" id="proof-content">
<div class="amsmath math notranslate nohighlight">
\[\begin{alignat*}{2}
\text{minimize} \quad &amp; c_T(\mathbf{x}_T) &amp; \\
\text{such that} \quad 
&amp; \mathbf{x}_{t+1} = \mathbf{f}_t(\mathbf{x}_t, \mathbf{u}_t), &amp; \quad &amp; t = 1, \dots, T-1, \\
&amp; \mathbf{u}_{lb} \leq \mathbf{u}_t \leq \mathbf{u}_{ub}, &amp; \quad &amp; t = 1, \dots, T, \\
&amp; \mathbf{x}_{lb} \leq \mathbf{x}_t \leq \mathbf{x}_{ub}, &amp; \quad &amp; t = 1, \dots, T, \\
\text{given} \quad &amp; \mathbf{x}_1. &amp;
\end{alignat*}\]</div>
</section>
</div><p>When writing the optimal control problem in any of those three forms, it is implied that both <span class="math notranslate nohighlight">\(u_1, ..., u_T\)</span> and the state trajectory <span class="math notranslate nohighlight">\(x_1, ..., x_T\)</span> are optimization variables. Since we ultimately care about the decisions themselves, the idea of posing the states themselves as optimization variables seems uncessary given that we have access to the dynamics. We will soon see that there indeed exists a way in which we get rid of the state variables as constraints through a process of explicit simulation with the class of “shooting methods”, thereby turning what would otherwise be an constrained optimization problem into an unconstrained one.</p>
<section id="reduction-to-mayer-problems">
<h2><span class="section-number">1.1. </span>Reduction to Mayer Problems<a class="headerlink" href="#reduction-to-mayer-problems" title="Link to this heading">#</a></h2>
<p>While it might appear at first glance that Bolza problems are somehow more expressive or powerful, we can show that both Lagrange and Bolza problems can be reduced to a Mayer problem through the idea of “state augmentation”.
The overall idea is that the explicit sum of costs can be eliminated by maintaining a running sum of costs as an additional state variable <span class="math notranslate nohighlight">\(y_t\)</span>. The augmented system equation then becomes:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\boldsymbol{\tilde{f}}_t\left(\boldsymbol{\boldsymbol{\tilde{x}}}_t, \boldsymbol{u}_t\right) \triangleq \left(\begin{array}{c}
\boldsymbol{f}_t\left(\boldsymbol{x}_t, \boldsymbol{u}_t\right) \\
y_t+c_t\left(\boldsymbol{x}_t, \boldsymbol{u}_t\right)
\end{array}\right) 
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\tilde{x}}_t \triangleq (\mathbf{x}_t, y_t)\)</span> is the concatenation of the running cost so far and the underlying state of the original system. The terminal cost function in the Bolza-to-Mayer transformation is computed with:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\tilde{c}_T(\mathbf{\tilde{x}}_T)  \triangleq c_T\left(\boldsymbol{x}_T\right)+y_T
\end{align*}\]</div>
<p>This transformation is often helpful to simplify mathematical derivations (as we are about to see shortly) but could also be used to streamline algorithmic implementation (by maintaining one version of the code rather than three with many if/else statements). That being said, there could also be computational advantages to working with the specific problem types rather than the one size-fits-for-all Mayer reduction.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="numerical-methods-for-solving-docps">
<h1><span class="section-number">2. </span>Numerical Methods for Solving DOCPs<a class="headerlink" href="#numerical-methods-for-solving-docps" title="Link to this heading">#</a></h1>
<p>Let’s assume that an optimal control problem has been formulated in one of the forms presented earlier and has been given to us to solve. The following section explores numerical solutions applicable to these problems, focusing on trajectory optimization. Our goal is to output an optimal control (and state trajectory) based on the given cost function and dynamics structure. It’s important to note that the methods presented here are not learning methods just yet; they don’t involve ingesting data or inferring unknown quantities from it. However, these methods represent a central component of any decision-learning system, and we will later explore how learning concepts can be incorporated.</p>
<p>Before delving into the solution methods, let’s consider an electric vehicle energy management problem which we will use this as a test bed throughout this section. Consider an electric vehicle traversing a planned route, where we aim to optimize its energy consumption over a 20-minute journey. Our simplified model represents the vehicle’s state using two variables: <span class="math notranslate nohighlight">\(x_1\)</span>, the battery state of charge as a percentage, and <span class="math notranslate nohighlight">\(x_2\)</span>, denoting the vehicle’s speed in meters per second. The control input <span class="math notranslate nohighlight">\(u\)</span>, ranging from -1 to 1, represents the motor power, with negative values indicating regenerative braking and positive values representing acceleration. The problem can be formally expressed as a mathematical program in Bolza form:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\min_{\mathbf{x}, \mathbf{u}} \quad &amp; J = \underbrace{x_{T,1}^2 + x_{T,2}^2}_{\text{Mayer term}} + \underbrace{\sum_{t=1}^{T-1} 0.1(x_{t,1}^2 + x_{t,2}^2 + u_t^2)}_{\text{Lagrange term}} \\
\text{subject to:} \quad &amp; x_{t+1} = f_t(x_t, u_t), \quad t = 1, \ldots, T-1 \\
&amp; x_1 = \begin{bmatrix} 1 \\ 0 \end{bmatrix} \\
&amp; -1 \leq u_t \leq 1, \quad t = 1, \ldots, T-1 \\
&amp; -5 \leq x_{t,1}, x_{t,2} \leq 5, \quad t = 1, \ldots, T \\
\text{where:} \quad &amp; f_t(x_t, u_t) = \begin{bmatrix}
   x_{t,1} + 0.1x_{t,2} + 0.05u_t \\
   x_{t,2} + 0.1u_t
   \end{bmatrix} \\
&amp; T = 20 \\
&amp; x_t = \begin{bmatrix} x_{t,1} \\ x_{t,2} \end{bmatrix} \in \mathbb{R}^2, \quad u_t \in \mathbb{R}.
\end{align*}\]</div>
<p>The system dynamics, represented by <span class="math notranslate nohighlight">\(f_t(x_t, u_t)\)</span>, describe how the battery charge and vehicle speed evolve based on the current state and control input. The initial condition <span class="math notranslate nohighlight">\(x_1 = [1, 0]^T\)</span> indicates that the vehicle starts with a fully charged battery and zero initial speed. The constraints <span class="math notranslate nohighlight">\(-1 \leq u_t \leq 1\)</span> and <span class="math notranslate nohighlight">\(-5 \leq x_{t,1}, x_{t,2} \leq 5\)</span> ensure that the control inputs and state variables remain within acceptable ranges throughout the journey. This model is of course highly simplistic and neglects the nonlinear nature of battery discharge and vehicle motion due to air resistance, road grade, and vehicle mass, etc. Furthermore, our model ignores the effect of environmental factors like wind and temperature on regenerative breaking. Route-specific information such as elevation changes and speed limits are absent, as is the consideration of auxiliary power consumption such as heating and entertainment. These are all possible improvements to our models which we ignore at the moment for the sake of simplicity.</p>
<section id="single-shooting-methods">
<h2><span class="section-number">2.1. </span>Single Shooting Methods<a class="headerlink" href="#single-shooting-methods" title="Link to this heading">#</a></h2>
<p>Given access to unconstrained optimization solver, the easiest method to implement is by far what is known as “single shooting” in control theory. The idea of simple: rather than having to solve for the state variables as equality constraints, we transform the original constrained problem into an unconstrained one through “simulation”, ie by recursively computing the evolution of our system for any given set of controls and initial state. In the deterministic setting, given an initial state, we can always exactly reconstruct the resulting sequence of states by “rolling out” our model, a process which some communities would refer to as “time marching”. Mathematically, this amounts to forming the following unconstrained program:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\min_{\substack{\mathbf{u}_1, \ldots, \mathbf{u}_{T-1} \\ \mathbf{u}_{lb} \leq \mathbf{u}_t \leq \mathbf{u}_{lb} \, t=1,...,T-1}} \quad c_T(\boldsymbol{\phi}_{T}(\boldsymbol{u}_{1:T-1}, \boldsymbol{x}_1)) + \sum_{t=1}^{T-1} c_t(\boldsymbol{\phi}_t(\boldsymbol{u}_{1:T-1}, \boldsymbol{x}_1), \boldsymbol{u}_{t})
\end{align*}
\end{split}\]</div>
<p>To implement this transform, we construct a set of helper functions <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_1, ..., \boldsymbol{\phi}_{T-1}\)</span> whose role is compute the state at any time <span class="math notranslate nohighlight">\(t=1, ..., T\)</span> resulting from applying the sequence of controls starting from the initial state. We can define those functions recursively as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
&amp;\boldsymbol{\phi}_t(\boldsymbol{u}_{1:T-1}, \boldsymbol{x}_1) \triangleq \boldsymbol{f}_{t-1}(\boldsymbol{\phi}_{t-1}(\boldsymbol{u}_{1:T-1}, \boldsymbol{x}_1), \boldsymbol{u}_{t-1}), \quad t=2,...,T\\
&amp;\text{with}\quad \boldsymbol{\phi}_1(\boldsymbol{u}_{1:T}, \boldsymbol{x}_1) \triangleq \boldsymbol{x}_1
\end{align*}
\end{split}\]</div>
<div class="proof algorithm admonition" id="naive-single-shooting">
<p class="admonition-title"><span class="caption-number">Algorithm 2.1 </span> (Naive Single Shooting: re-computation/checkpointing)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Inputs</strong> Initial state <span class="math notranslate nohighlight">\(\mathbf{x}_1\)</span>, time horizon <span class="math notranslate nohighlight">\(T\)</span>, control bounds <span class="math notranslate nohighlight">\(\mathbf{u}_{lb}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{u}_{ub}\)</span>, state transition functions <span class="math notranslate nohighlight">\(\mathbf{f}_t\)</span>, cost functions <span class="math notranslate nohighlight">\(c_t\)</span></p>
<p><strong>Output</strong> Optimal control sequence <span class="math notranslate nohighlight">\(\mathbf{u}^*_{1:T-1}\)</span></p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\mathbf{u}_{1:T-1}\)</span> within bounds <span class="math notranslate nohighlight">\([\mathbf{u}_{lb}, \mathbf{u}_{ub}]\)</span></p></li>
<li><p>Define <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_t(\mathbf{u}_{1:T-1}, \mathbf{x}_1)\)</span> for <span class="math notranslate nohighlight">\(t = 1, ..., T\)</span>:</p>
<ol class="arabic simple">
<li><p>If <span class="math notranslate nohighlight">\(t = 1\)</span>:</p>
<ol class="arabic simple">
<li><p>Return <span class="math notranslate nohighlight">\(\mathbf{x}_1\)</span></p></li>
</ol>
</li>
<li><p>Else:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{x} \leftarrow \mathbf{x}_1\)</span></p></li>
<li><p>For <span class="math notranslate nohighlight">\(i = 1\)</span> to <span class="math notranslate nohighlight">\(t-1\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{x} \leftarrow \mathbf{f}_{i}(\mathbf{x}, \mathbf{u}_{i})\)</span></p></li>
</ol>
</li>
<li><p>Return <span class="math notranslate nohighlight">\(\mathbf{x}\)</span></p></li>
</ol>
</li>
</ol>
</li>
<li><p>Define objective function <span class="math notranslate nohighlight">\(J(\mathbf{u}_{1:T-1})\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(J \leftarrow c_T(\boldsymbol{\phi}_T(\mathbf{u}_{1:T-1}, \mathbf{x}_1))\)</span></p></li>
<li><p>For <span class="math notranslate nohighlight">\(t = 1\)</span> to <span class="math notranslate nohighlight">\(T-1\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(J \leftarrow J + c_t(\boldsymbol{\phi}_t(\mathbf{u}_{1:T-1}, \mathbf{x}_1), \mathbf{u}_t)\)</span></p></li>
</ol>
</li>
<li><p>Return <span class="math notranslate nohighlight">\(J\)</span></p></li>
</ol>
</li>
<li><p>Solve optimization problem:
<span class="math notranslate nohighlight">\(\mathbf{u}^*_{1:T-1} \leftarrow \arg\min_{\mathbf{u}_{1:T-1}} J(\mathbf{u}_{1:T-1})\)</span>
subject to <span class="math notranslate nohighlight">\(\mathbf{u}_{lb} \leq \mathbf{u}_t \leq \mathbf{u}_{ub}, \, t=1,\ldots,T-1\)</span></p></li>
<li><p>Return <span class="math notranslate nohighlight">\(\mathbf{u}^*_{1:T-1}\)</span></p></li>
</ol>
</section>
</div><div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">grad</span><span class="p">,</span> <span class="n">jit</span>
<span class="kn">from</span> <span class="nn">jax.example_libraries</span> <span class="kn">import</span> <span class="n">optimizers</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">single_shooting_ev_optimization</span><span class="p">(</span><span class="n">T</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">num_iterations</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implements the single shooting method for the electric vehicle energy optimization problem.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">    T: time horizon</span>
<span class="sd">    num_iterations: number of optimization iterations</span>
<span class="sd">    step_size: step size for the optimizer</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    optimal_u: optimal control sequence</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
            <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.05</span> <span class="o">*</span> <span class="n">u</span><span class="p">,</span>
            <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">u</span>
        <span class="p">])</span>
    
    <span class="k">def</span> <span class="nf">c</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">t</span> <span class="o">==</span> <span class="n">T</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">u</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">phi</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x1</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">k</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">u</span><span class="p">[</span><span class="n">k</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
    
    <span class="k">def</span> <span class="nf">objective</span><span class="p">(</span><span class="n">u</span><span class="p">):</span>
        <span class="n">total_cost</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
            <span class="n">x_t</span> <span class="o">=</span> <span class="n">phi</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
            <span class="n">total_cost</span> <span class="o">+=</span> <span class="n">c</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">u</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">x_T</span> <span class="o">=</span> <span class="n">phi</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
        <span class="n">total_cost</span> <span class="o">+=</span> <span class="n">c</span><span class="p">(</span><span class="n">x_T</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>  <span class="c1"># No control at final step</span>
        <span class="k">return</span> <span class="n">total_cost</span>
    
    <span class="k">def</span> <span class="nf">clip_controls</span><span class="p">(</span><span class="n">u</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
    
    <span class="n">x1</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">])</span>  <span class="c1"># Initial state: full battery, zero speed</span>
    
    <span class="c1"># Initialize controls</span>
    <span class="n">u_init</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">T</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># Setup optimizer</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizers</span><span class="o">.</span><span class="n">adam</span><span class="p">(</span><span class="n">step_size</span><span class="p">)</span>
    <span class="n">opt_init</span><span class="p">,</span> <span class="n">opt_update</span><span class="p">,</span> <span class="n">get_params</span> <span class="o">=</span> <span class="n">optimizer</span>
    <span class="n">opt_state</span> <span class="o">=</span> <span class="n">opt_init</span><span class="p">(</span><span class="n">u_init</span><span class="p">)</span>
    
    <span class="nd">@jit</span>
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">):</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">get_params</span><span class="p">(</span><span class="n">opt_state</span><span class="p">)</span>
        <span class="n">value</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">objective</span><span class="p">)(</span><span class="n">u</span><span class="p">)</span>
        <span class="n">opt_state</span> <span class="o">=</span> <span class="n">opt_update</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">)</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">get_params</span><span class="p">(</span><span class="n">opt_state</span><span class="p">)</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">clip_controls</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
        <span class="n">opt_state</span> <span class="o">=</span> <span class="n">opt_init</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">value</span><span class="p">,</span> <span class="n">opt_state</span>
    
    <span class="c1"># Run optimization</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">):</span>
        <span class="n">value</span><span class="p">,</span> <span class="n">opt_state</span> <span class="o">=</span> <span class="n">step</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Iteration </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">, Cost: </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="n">optimal_u</span> <span class="o">=</span> <span class="n">get_params</span><span class="p">(</span><span class="n">opt_state</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">optimal_u</span>

<span class="k">def</span> <span class="nf">plot_results</span><span class="p">(</span><span class="n">optimal_u</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
    <span class="c1"># Compute state trajectory</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">])</span>
    <span class="n">x_trajectory</span> <span class="o">=</span> <span class="p">[</span><span class="n">x1</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">x_next</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
            <span class="n">x_trajectory</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">x_trajectory</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.05</span> <span class="o">*</span> <span class="n">optimal_u</span><span class="p">[</span><span class="n">t</span><span class="p">],</span>
            <span class="n">x_trajectory</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">optimal_u</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
        <span class="p">])</span>
        <span class="n">x_trajectory</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_next</span><span class="p">)</span>
    <span class="n">x_trajectory</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x_trajectory</span><span class="p">)</span>

    <span class="n">time</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">x_trajectory</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Battery State of Charge&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">x_trajectory</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Vehicle Speed&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time Step&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;State Value&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Optimal State Trajectories&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">time</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">optimal_u</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Motor Power Input&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time Step&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Control Input&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Optimal Control Inputs&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Run the optimization</span>
<span class="n">optimal_u</span> <span class="o">=</span> <span class="n">single_shooting_ev_optimization</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Optimal control sequence:&quot;</span><span class="p">,</span> <span class="n">optimal_u</span><span class="p">)</span>

<span class="c1"># Plot the results</span>
<span class="n">plot_results</span><span class="p">(</span><span class="n">optimal_u</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Iteration 0, Cost: 2.9000003337860107
Iteration 100, Cost: 1.4151428937911987
Iteration 200, Cost: 1.4088482856750488
Iteration 300, Cost: 1.409121036529541
Iteration 400, Cost: 1.409376859664917
Iteration 500, Cost: 1.409613013267517
Iteration 600, Cost: 1.4098304510116577
Iteration 700, Cost: 1.4100291728973389
Iteration 800, Cost: 1.410210371017456
Iteration 900, Cost: 1.410375714302063
Optimal control sequence: [-0.84949356 -0.76774836 -0.6298094  -0.4975385  -0.38879833 -0.3057558
 -0.2055591  -0.1496703  -0.07521398 -0.0221795   0.01586018  0.05557606
  0.09427628  0.11390017  0.13218477  0.1561778   0.1723089   0.17151853
  0.18140294]
</pre></div>
</div>
<img alt="_images/16365883047a2a786133a8b8ff1159eb302501657ca43cb5dd5b0378e9b76fdd.png" src="_images/16365883047a2a786133a8b8ff1159eb302501657ca43cb5dd5b0378e9b76fdd.png" />
</div>
</details>
</div>
<p>The approach outlined in <a class="reference internal" href="#naive-single-shooting">Algorithm 2.1</a> stems directly from the mathematical definition and involves recomputing the sequence of states from the begining every time that the instantenous cost function along the trajectory needs to be evaluated. This implementation has the benefit that it requires very little storage, as the only quantity that we have to maintain in addition to the running cost is the last state. However, this simplicitity and storage savings come at a steep computation cost as it requires re-computing the trajectory up to any given stage starting from the initial state.
A more practical and efficient implementation combines trajectory unrolling with cost accumulation. This process can be realized through a simple for-loop in frameworks like JAX, which can trace code execution through control flows. Alternatively, a more efficient <code class="docutils literal notranslate"><span class="pre">scan</span></code> operation could be employed. By simultaneously computing the trajectory and summing costs, we eliminate redundant calculations, effectively trading computation for storage—a strategy reminiscent of checkpointing in automatic differentiation.</p>
<div class="proof algorithm admonition" id="shooting-trajectory-storage">
<p class="admonition-title"><span class="caption-number">Algorithm 2.2 </span> (Single Shooting: Trajectory Storage)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Inputs</strong> Initial state <span class="math notranslate nohighlight">\(\mathbf{x}_1\)</span>, time horizon <span class="math notranslate nohighlight">\(T\)</span>, control bounds <span class="math notranslate nohighlight">\(\mathbf{u}_{lb}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{u}_{ub}\)</span>, state transition functions <span class="math notranslate nohighlight">\(\mathbf{f}_t\)</span>, cost functions <span class="math notranslate nohighlight">\(c_t\)</span></p>
<p><strong>Output</strong> Optimal control sequence <span class="math notranslate nohighlight">\(\mathbf{u}^*_{1:T-1}\)</span></p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\mathbf{u}_{1:T-1}\)</span> within bounds <span class="math notranslate nohighlight">\([\mathbf{u}_{lb}, \mathbf{u}_{ub}]\)</span></p></li>
<li><p>Define function ComputeTrajectoryAndCost(<span class="math notranslate nohighlight">\(\mathbf{u}_{1:T-1}, \mathbf{x}_1\)</span>):</p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\mathbf{x} \leftarrow [\mathbf{x}_1]\)</span>  // List to store states</p></li>
<li><p>Initialize <span class="math notranslate nohighlight">\(J \leftarrow 0\)</span>  // Total cost</p></li>
<li><p>For <span class="math notranslate nohighlight">\(t = 1\)</span> to <span class="math notranslate nohighlight">\(T-1\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(J \leftarrow J + c_t(\mathbf{x}[t], \mathbf{u}_t)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{x}_{\text{next}} \leftarrow \mathbf{f}_t(\mathbf{x}[t], \mathbf{u}_t)\)</span></p></li>
<li><p>Append <span class="math notranslate nohighlight">\(\mathbf{x}_{\text{next}}\)</span> to <span class="math notranslate nohighlight">\(\mathbf{x}\)</span></p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(J \leftarrow J + c_T(\mathbf{x}[T])\)</span>  // Add final state cost</p></li>
<li><p>Return <span class="math notranslate nohighlight">\(\mathbf{x}, J\)</span></p></li>
</ol>
</li>
<li><p>Define objective function <span class="math notranslate nohighlight">\(J(\mathbf{u}_{1:T-1})\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\_, J \leftarrow\)</span> ComputeTrajectoryAndCost(<span class="math notranslate nohighlight">\(\mathbf{u}_{1:T-1}, \mathbf{x}_1\)</span>)</p></li>
<li><p>Return <span class="math notranslate nohighlight">\(J\)</span></p></li>
</ol>
</li>
<li><p>Solve optimization problem:
<span class="math notranslate nohighlight">\(\mathbf{u}^*_{1:T-1} \leftarrow \arg\min_{\mathbf{u}_{1:T-1}} J(\mathbf{u}_{1:T-1})\)</span>
subject to <span class="math notranslate nohighlight">\(\mathbf{u}_{lb} \leq \mathbf{u}_t \leq \mathbf{u}_{ub}, \, t=1,\ldots,T-1\)</span></p></li>
<li><p>Return <span class="math notranslate nohighlight">\(\mathbf{u}^*_{1:T-1}\)</span></p></li>
</ol>
</section>
</div><div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code demonstration</span>
<span class="expanded">Hide code demonstration</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">grad</span><span class="p">,</span> <span class="n">jit</span>
<span class="kn">from</span> <span class="nn">jax.example_libraries</span> <span class="kn">import</span> <span class="n">optimizers</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">single_shooting_ev_optimization</span><span class="p">(</span><span class="n">T</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">num_iterations</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implements the single shooting method for the electric vehicle energy optimization problem.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">    T: time horizon</span>
<span class="sd">    num_iterations: number of optimization iterations</span>
<span class="sd">    step_size: step size for the optimizer</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    optimal_u: optimal control sequence</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
            <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.05</span> <span class="o">*</span> <span class="n">u</span><span class="p">,</span>
            <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">u</span>
        <span class="p">])</span>
    
    <span class="k">def</span> <span class="nf">c</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">t</span> <span class="o">==</span> <span class="n">T</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">u</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">compute_trajectory_and_cost</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">x1</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x1</span>
        <span class="n">total_cost</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
            <span class="n">total_cost</span> <span class="o">+=</span> <span class="n">c</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">u</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">t</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">u</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">total_cost</span> <span class="o">+=</span> <span class="n">c</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>  <span class="c1"># No control at final step</span>
        <span class="k">return</span> <span class="n">total_cost</span>
    
    <span class="k">def</span> <span class="nf">objective</span><span class="p">(</span><span class="n">u</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">compute_trajectory_and_cost</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">x1</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">clip_controls</span><span class="p">(</span><span class="n">u</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
    
    <span class="n">x1</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">])</span>  <span class="c1"># Initial state: full battery, zero speed</span>
    
    <span class="c1"># Initialize controls</span>
    <span class="n">u_init</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">T</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># Setup optimizer</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizers</span><span class="o">.</span><span class="n">adam</span><span class="p">(</span><span class="n">step_size</span><span class="p">)</span>
    <span class="n">opt_init</span><span class="p">,</span> <span class="n">opt_update</span><span class="p">,</span> <span class="n">get_params</span> <span class="o">=</span> <span class="n">optimizer</span>
    <span class="n">opt_state</span> <span class="o">=</span> <span class="n">opt_init</span><span class="p">(</span><span class="n">u_init</span><span class="p">)</span>
    
    <span class="nd">@jit</span>
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">):</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">get_params</span><span class="p">(</span><span class="n">opt_state</span><span class="p">)</span>
        <span class="n">value</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">objective</span><span class="p">)(</span><span class="n">u</span><span class="p">)</span>
        <span class="n">opt_state</span> <span class="o">=</span> <span class="n">opt_update</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">)</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">get_params</span><span class="p">(</span><span class="n">opt_state</span><span class="p">)</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">clip_controls</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
        <span class="n">opt_state</span> <span class="o">=</span> <span class="n">opt_init</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">value</span><span class="p">,</span> <span class="n">opt_state</span>
    
    <span class="c1"># Run optimization</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">):</span>
        <span class="n">value</span><span class="p">,</span> <span class="n">opt_state</span> <span class="o">=</span> <span class="n">step</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Iteration </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">, Cost: </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="n">optimal_u</span> <span class="o">=</span> <span class="n">get_params</span><span class="p">(</span><span class="n">opt_state</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">optimal_u</span>

<span class="k">def</span> <span class="nf">plot_results</span><span class="p">(</span><span class="n">optimal_u</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
    <span class="c1"># Compute state trajectory</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">])</span>
    <span class="n">x_trajectory</span> <span class="o">=</span> <span class="p">[</span><span class="n">x1</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">x_next</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
            <span class="n">x_trajectory</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">x_trajectory</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.05</span> <span class="o">*</span> <span class="n">optimal_u</span><span class="p">[</span><span class="n">t</span><span class="p">],</span>
            <span class="n">x_trajectory</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">optimal_u</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
        <span class="p">])</span>
        <span class="n">x_trajectory</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_next</span><span class="p">)</span>
    <span class="n">x_trajectory</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x_trajectory</span><span class="p">)</span>

    <span class="n">time</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">x_trajectory</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Battery State of Charge&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">x_trajectory</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Vehicle Speed&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time Step&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;State Value&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Optimal State Trajectories&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">time</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">optimal_u</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Motor Power Input&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time Step&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Control Input&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Optimal Control Inputs&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Run the optimization</span>
<span class="n">optimal_u</span> <span class="o">=</span> <span class="n">single_shooting_ev_optimization</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Optimal control sequence:&quot;</span><span class="p">,</span> <span class="n">optimal_u</span><span class="p">)</span>

<span class="c1"># Plot the results</span>
<span class="n">plot_results</span><span class="p">(</span><span class="n">optimal_u</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Iteration 0, Cost: 2.9000003337860107
Iteration 100, Cost: 1.4151430130004883
Iteration 200, Cost: 1.408848762512207
Iteration 300, Cost: 1.4091217517852783
Iteration 400, Cost: 1.4093775749206543
Iteration 500, Cost: 1.409614086151123
Iteration 600, Cost: 1.4098315238952637
Iteration 700, Cost: 1.4100300073623657
Iteration 800, Cost: 1.4102115631103516
Iteration 900, Cost: 1.4103766679763794
Optimal control sequence: [-0.84949344 -0.7677488  -0.6298097  -0.4975385  -0.38879827 -0.30575562
 -0.2055591  -0.14967042 -0.07521399 -0.02217961  0.01586013  0.05557559
  0.09427615  0.1138996   0.13218467  0.15617764  0.1723922   0.17151831
  0.18140292]
</pre></div>
</div>
<img alt="_images/7be885c5bf1716da04cc1070af32a86fd451119c4e327e700dacb98c8ad5d7e1.png" src="_images/7be885c5bf1716da04cc1070af32a86fd451119c4e327e700dacb98c8ad5d7e1.png" />
</div>
</details>
</div>
<section id="dealing-with-bound-constraints">
<h3><span class="section-number">2.1.1. </span>Dealing with Bound Constraints<a class="headerlink" href="#dealing-with-bound-constraints" title="Link to this heading">#</a></h3>
<p>While we have successfully eliminated the dynamics as explicit constraints through what essentially amounts to a “reparametrization” of our problem, we’ve been silent regarding the bound constraints. The view of single shooting as a perfect transformation from a constrained problem to an unconstrained one is not entirely accurate: we must leave something on the table, and that something is the ability to easily impose state constraints.</p>
<p>By directly simulating the process from the initial state, there is one and only one corresponding induced path, and there’s no way to let our optimizer know that it can adjust within some bounds, even if that means the generated trajectory is no longer feasible (realistic).</p>
<p>Fortunately, the situation is much better for bound constraints on the controls. If we choose gradient descent as our method for solving this problem, we can consider a simple extension to readily support these kinds of bound constraints. The approach, in this case, would be what we call projected gradient descent. The general form of a projected gradient descent step can be expressed as:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{u}_{k+1} = \mathcal{P}_C(\mathbf{u}_k - \alpha \nabla J(\mathbf{u}_k))
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{P}_C\)</span> denotes the projection onto the feasible set <span class="math notranslate nohighlight">\(C\)</span>, <span class="math notranslate nohighlight">\(\alpha\)</span> is the step size, and <span class="math notranslate nohighlight">\(\nabla J(\mathbf{u}_k)\)</span> is the gradient of the objective function at the current point <span class="math notranslate nohighlight">\(\mathbf{u}_k\)</span>. In general, the projection operation can be computationally expensive or even intractable. However, in the case of box constraints (i.e., bound constraints), the projection simplifies to an element-wise clipping operation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
[\mathcal{P}_C(\mathbf{u})]_i = \begin{cases}
    [\mathbf{u}_{lb}]_i &amp; \text{if } [\mathbf{u}]_i &lt; [\mathbf{u}_{lb}]_i \\
    [\mathbf{u}]_i &amp; \text{if } [\mathbf{u}_{lb}]_i \leq [\mathbf{u}]_i \leq [\mathbf{u}_{ub}]_i \\
    [\mathbf{u}_{ub}]_i &amp; \text{if } [\mathbf{u}]_i &gt; [\mathbf{u}_{ub}]_i
\end{cases}
\end{split}\]</div>
<p>With this simple change, we can maintain the computational simplicity of unconstrained optimization while enforcing the bound constraints at each iteration: ie ensuring that we are feasible throughout optimization. Moreover, it can be shown that this projection preserves the convergence properties of the gradient descent method, and that under suitable conditions (such as Lipschitz continuity of the gradient), projected gradient descent converges to a stationary point of the constrained problem.</p>
<p>Here’s the algorithm for projected gradient descent with bound constraint for a general problem of the form:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\min_{\mathbf{u}} \quad &amp; J(\mathbf{u}) \\
\text{subject to} \quad &amp; \mathbf{u}_{lb} \leq \mathbf{u} \leq \mathbf{u}_{ub}
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(J(\mathbf{u})\)</span> is our objective function, and <span class="math notranslate nohighlight">\(\mathbf{u}_{lb}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{u}_{ub}\)</span> are the lower and upper bounds on the control variables, respectively.</p>
<div class="proof algorithm admonition" id="proj-grad-descent-bound-constraints">
<p class="admonition-title"><span class="caption-number">Algorithm 2.3 </span> (Projected Gradient Descent for Bound Constraints)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> Initial point <span class="math notranslate nohighlight">\(\mathbf{u}_0\)</span>, learning rate <span class="math notranslate nohighlight">\(\alpha\)</span>, bounds <span class="math notranslate nohighlight">\(\mathbf{u}_{lb}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{u}_{ub}\)</span>,
maximum iterations <span class="math notranslate nohighlight">\(\max_\text{iter}\)</span>, tolerance <span class="math notranslate nohighlight">\(\varepsilon\)</span></p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(k = 0\)</span></p></li>
<li><p>While <span class="math notranslate nohighlight">\(k &lt; \max_\text{iter}\)</span> and not converged:</p>
<ol class="arabic simple">
<li><p>Compute gradient: <span class="math notranslate nohighlight">\(\mathbf{g}_k = \nabla J(\mathbf{u}_k)\)</span></p></li>
<li><p>Update: <span class="math notranslate nohighlight">\(\mathbf{u}_{k+1} = \text{clip}(\mathbf{u}_k - \alpha \mathbf{g}_k, \mathbf{u}_{lb}, \mathbf{u}_{ub})\)</span></p></li>
<li><p>Check convergence: if <span class="math notranslate nohighlight">\(\|\mathbf{u}_{k+1} - \mathbf{u}_k\| &lt; \varepsilon\)</span>, mark as converged</p></li>
<li><p><span class="math notranslate nohighlight">\(k = k + 1\)</span></p></li>
</ol>
</li>
<li><p>Return <span class="math notranslate nohighlight">\(\mathbf{u}_k\)</span></p></li>
</ol>
</section>
</div><p>In this algorithm, the <code class="docutils literal notranslate"><span class="pre">clip</span></code> function projects the updated point back onto the feasible region defined by the bounds:</p>
<div class="math notranslate nohighlight">
\[
\text{clip}(u, u_{lb}, u_{ub}) = \max(\min(u, u_{ub}), u_{lb})
\]</div>
</section>
<section id="on-the-choice-of-optimizer">
<h3><span class="section-number">2.1.2. </span>On the choice of optimizer<a class="headerlink" href="#on-the-choice-of-optimizer" title="Link to this heading">#</a></h3>
<p>Despite frequent mentions of automatic differentiation, it’s important to note that the single shooting approaches outlined in this section need not rely on gradient-based optimization methods. In fact, one could use any method provided by <code class="docutils literal notranslate"><span class="pre">scipy.optimize.minimize</span></code>, which offers a range of options such as:</p>
<ul class="simple">
<li><p>Derivative-free methods like <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-neldermead.html">Nelder-Mead Simplex</a>, suitable for problems where gradients are unavailable or difficult to compute.</p></li>
<li><p>Quasi-Newton methods like <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-bfgs.html">BFGS (Broyden-Fletcher-Goldfarb-Shanno)</a>, which by default uses finite differences rather than automatic differentiation to approximate gradients.</p></li>
</ul>
<p>Another common strategy for single shooting methods is to use stochastic optimization techniques. For instance, random search generates a number of candidate solutions randomly and evaluates them. This approach is useful for problems with badly behaved loss landscapes or when gradient information is unreliable. More sophisticated stochastic methods include:</p>
<ul class="simple">
<li><p>Genetic Algorithms: These mimic biological evolution, using mechanisms like selection, crossover, and mutation to evolve a population of solutions over generations <span id="id2">[<a class="reference internal" href="#id16" title="John H Holland. Genetic algorithms. Scientific american, 267(1):66–73, 1992.">Hol92</a>]</span>. (Implemented in <a class="reference external" href="https://github.com/DEAP/deap">DEAP</a> library)</p></li>
<li><p>Simulated Annealing: Inspired by the annealing process in metallurgy, this method allows for occasional “uphill” moves to escape local minima <span id="id3">[<a class="reference internal" href="#id17" title="Scott Kirkpatrick, C Daniel Gelatt Jr, and Mario P Vecchi. Optimization by simulated annealing. science, 220(4598):671–680, 1983.">KGJV83</a>]</span>. (Available in <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.dual_annealing.html">SciPy</a>)</p></li>
<li><p>Particle Swarm Optimization: This technique simulates the social behavior of organisms in a swarm, with particles (candidate solutions) moving through the search space and influencing each other <span id="id4">[<a class="reference internal" href="#id18" title="James Kennedy and Russell Eberhart. Particle swarm optimization. In Proceedings of ICNN'95-International Conference on Neural Networks, volume 4, 1942–1948. IEEE, 1995.">KE95</a>]</span>. (Implemented in <a class="reference external" href="https://github.com/ljvmiranda921/pyswarms">PySwarms</a> library)</p></li>
</ul>
<p>The selection of an optimization method for single shooting is influenced by multiple factors: problem-specific characteristics, available computational resources, and the balance between exploring the solution space and exploiting known good solutions. While gradient-based methods generally offer faster convergence when applicable, derivative-free and stochastic approaches tend to be more robust to complex non-convex loss landscapes, albeit at the cost of increased computational demands.</p>
<p>In practice, however, this choice is often guided by the tools at hand and the practitioners’ familiarity with them. For instance, researchers with a background in deep learning tend to gravitate towards first-order gradient-based optimization techniques along with automatic differentiation for efficient derivative computation.</p>
</section>
</section>
<section id="constrained-optimization-approach">
<h2><span class="section-number">2.2. </span>Constrained Optimization Approach<a class="headerlink" href="#constrained-optimization-approach" title="Link to this heading">#</a></h2>
<p>The mathematical programming formulation presented earlier lends itself readily to off-the-shelf solvers for nonlinear mathematical programs. For example, we can use the <code class="docutils literal notranslate"><span class="pre">scipy.optimize.minimize</span></code> function along with the SLSQP (Sequential Least Squares Programming) solver to obtain a solution to any feasible Bolza problem of the form presented below.</p>
<p>The following code demonstrate how the car charging problem can be solved directly using <code class="docutils literal notranslate"><span class="pre">scipy.optimize.minimize</span></code> in a black box fashion. In the next sections, we will dive deeper into the mathematical underpinnings of constrained optimization and implement our own solvers. For the moment, I simply want to bring to your attention that the solution to this problem, as expressed in its original form, involves solving for two interdependent quantities: the optimal sequence of controls, and that of the states encountered when applying them to the system. Is that bug, or a feature? We’ll see that it depends…</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">solve_docp</span><span class="p">(</span><span class="n">c_T</span><span class="p">,</span> <span class="n">c_t</span><span class="p">,</span> <span class="n">f_t</span><span class="p">,</span> <span class="n">x_1</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">u_lb</span><span class="p">,</span> <span class="n">u_ub</span><span class="p">,</span> <span class="n">x_lb</span><span class="p">,</span> <span class="n">x_ub</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Solve a Discrete-Time Optimal Control Problem of Bolza Type using scipy.minimize with SLSQP.</span>
<span class="sd">    </span>
<span class="sd">    Parameters:</span>
<span class="sd">    - c_T: function, terminal cost c_T(x_T)</span>
<span class="sd">    - c_t: function, stage cost c_t(x_t, u_t)</span>
<span class="sd">    - f_t: function, state transition f_t(x_t, u_t)</span>
<span class="sd">    - x_1: array, initial state</span>
<span class="sd">    - T: int, time horizon</span>
<span class="sd">    - u_lb, u_ub: arrays, lower and upper bounds for control inputs</span>
<span class="sd">    - x_lb, x_ub: arrays, lower and upper bounds for states</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    - result: OptimizeResult object from scipy.optimize.minimize</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">n_x</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_1</span><span class="p">)</span>
    <span class="n">n_u</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">u_lb</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">objective</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">z</span><span class="p">[:</span><span class="n">T</span><span class="o">*</span><span class="n">n_x</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">n_x</span><span class="p">)</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">z</span><span class="p">[</span><span class="n">T</span><span class="o">*</span><span class="n">n_x</span><span class="p">:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">n_u</span><span class="p">)</span>
        
        <span class="n">cost</span> <span class="o">=</span> <span class="n">c_T</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
            <span class="n">cost</span> <span class="o">+=</span> <span class="n">c_t</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="n">u</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
        
        <span class="k">return</span> <span class="n">cost</span>
    
    <span class="k">def</span> <span class="nf">constraints</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">z</span><span class="p">[:</span><span class="n">T</span><span class="o">*</span><span class="n">n_x</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">n_x</span><span class="p">)</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">z</span><span class="p">[</span><span class="n">T</span><span class="o">*</span><span class="n">n_x</span><span class="p">:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">n_u</span><span class="p">)</span>
        
        <span class="n">cons</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="c1"># State transition constraints</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">cons</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">f_t</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="n">u</span><span class="p">[</span><span class="n">t</span><span class="p">]))</span>
        
        <span class="c1"># Initial state constraint</span>
        <span class="n">cons</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">x_1</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">cons</span><span class="p">)</span>
    
    <span class="c1"># Set up bounds</span>
    <span class="n">bounds</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
        <span class="n">bounds</span><span class="o">.</span><span class="n">extend</span><span class="p">([(</span><span class="n">xl</span><span class="p">,</span> <span class="n">xu</span><span class="p">)</span> <span class="k">for</span> <span class="n">xl</span><span class="p">,</span> <span class="n">xu</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">x_lb</span><span class="p">,</span> <span class="n">x_ub</span><span class="p">)])</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
        <span class="n">bounds</span><span class="o">.</span><span class="n">extend</span><span class="p">([(</span><span class="n">ul</span><span class="p">,</span> <span class="n">uu</span><span class="p">)</span> <span class="k">for</span> <span class="n">ul</span><span class="p">,</span> <span class="n">uu</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">u_lb</span><span class="p">,</span> <span class="n">u_ub</span><span class="p">)])</span>
    
    <span class="c1"># Initial guess</span>
    <span class="n">z0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">T</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_x</span> <span class="o">+</span> <span class="n">n_u</span><span class="p">))</span>
    
    <span class="c1"># Solve the optimization problem</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span>
        <span class="n">objective</span><span class="p">,</span>
        <span class="n">z0</span><span class="p">,</span>
        <span class="n">method</span><span class="o">=</span><span class="s1">&#39;SLSQP&#39;</span><span class="p">,</span>
        <span class="n">constraints</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;eq&#39;</span><span class="p">,</span> <span class="s1">&#39;fun&#39;</span><span class="p">:</span> <span class="n">constraints</span><span class="p">},</span>
        <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span>
        <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;ftol&#39;</span><span class="p">:</span> <span class="mf">1e-6</span><span class="p">,</span> <span class="s1">&#39;maxiter&#39;</span><span class="p">:</span> <span class="mi">1000</span><span class="p">}</span>
    <span class="p">)</span>
    
    <span class="k">return</span> <span class="n">result</span>

<span class="k">def</span> <span class="nf">plot_results</span><span class="p">(</span><span class="n">x_opt</span><span class="p">,</span> <span class="n">u_opt</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Plot the optimal states and control inputs.</span>
<span class="sd">    </span>
<span class="sd">    Parameters:</span>
<span class="sd">    - x_opt: array, optimal states</span>
<span class="sd">    - u_opt: array, optimal control inputs</span>
<span class="sd">    - T: int, time horizon</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    
    <span class="c1"># Plot states</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">x_opt</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Battery State of Charge&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">x_opt</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Vehicle Speed&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time Step&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;State Value&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Optimal State Trajectories&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># Plot control inputs</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">u_opt</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Motor Power Input&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time Step&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Control Input&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Optimal Control Inputs&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">example_docp</span><span class="p">():</span>
    <span class="c1"># Define problem-specific functions and parameters</span>
    <span class="k">def</span> <span class="nf">c_T</span><span class="p">(</span><span class="n">x_T</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_T</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x_T</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>
    
    <span class="k">def</span> <span class="nf">c_t</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">u_t</span><span class="p">):</span>
        <span class="k">return</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="p">(</span><span class="n">x_t</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x_t</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">u_t</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">f_t</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">u_t</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
            <span class="n">x_t</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">x_t</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.05</span> <span class="o">*</span> <span class="n">u_t</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">x_t</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">u_t</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="p">])</span>
    
    <span class="n">x_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">])</span>
    <span class="n">T</span> <span class="o">=</span> <span class="mi">20</span>
    <span class="n">u_lb</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">1.0</span><span class="p">])</span>
    <span class="n">u_ub</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">])</span>
    <span class="n">x_lb</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">5.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.0</span><span class="p">])</span>
    <span class="n">x_ub</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">])</span>
    
    <span class="n">result</span> <span class="o">=</span> <span class="n">solve_docp</span><span class="p">(</span><span class="n">c_T</span><span class="p">,</span> <span class="n">c_t</span><span class="p">,</span> <span class="n">f_t</span><span class="p">,</span> <span class="n">x_1</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">u_lb</span><span class="p">,</span> <span class="n">u_ub</span><span class="p">,</span> <span class="n">x_lb</span><span class="p">,</span> <span class="n">x_ub</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Optimization successful:&quot;</span><span class="p">,</span> <span class="n">result</span><span class="o">.</span><span class="n">success</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Optimal cost:&quot;</span><span class="p">,</span> <span class="n">result</span><span class="o">.</span><span class="n">fun</span><span class="p">)</span>
    
    <span class="c1"># Extract optimal states and controls</span>
    <span class="n">x_opt</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">x</span><span class="p">[:</span><span class="n">T</span><span class="o">*</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">u_opt</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="n">T</span><span class="o">*</span><span class="mi">2</span><span class="p">:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Optimal states:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">x_opt</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Optimal controls:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">u_opt</span><span class="p">)</span>
    
    <span class="c1"># Plot the results</span>
    <span class="n">plot_results</span><span class="p">(</span><span class="n">x_opt</span><span class="p">,</span> <span class="n">u_opt</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">example_docp</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimization successful: True
Optimal cost: 1.4233413233781695
Optimal states:
[[ 1.00000000e+00 -4.38415751e-19]
 [ 9.53308083e-01 -9.33838348e-02]
 [ 9.05334198e-01 -1.70654837e-01]
 [ 8.56730665e-01 -2.33730936e-01]
 [ 8.08051721e-01 -2.84342636e-01]
 [ 7.59765028e-01 -3.24047495e-01]
 [ 7.12262141e-01 -3.54243770e-01]
 [ 6.65868050e-01 -3.76183199e-01]
 [ 6.20849825e-01 -3.90983009e-01]
 [ 5.77424459e-01 -3.99637139e-01]
 [ 5.35765971e-01 -4.03026688e-01]
 [ 4.96011845e-01 -4.01929602e-01]
 [ 4.58268833e-01 -3.97029704e-01]
 [ 4.22618229e-01 -3.88924971e-01]
 [ 3.89120605e-01 -3.78135227e-01]
 [ 3.57820102e-01 -3.65109185e-01]
 [ 3.28748325e-01 -3.50230903e-01]
 [ 3.01927831e-01 -3.33825710e-01]
 [ 2.77375326e-01 -3.16165578e-01]
 [ 2.55104530e-01 -2.97474055e-01]]
Optimal controls:
[[-9.33838348e-01]
 [-7.72710023e-01]
 [-6.30760993e-01]
 [-5.06117000e-01]
 [-3.97048581e-01]
 [-3.01962756e-01]
 [-2.19394288e-01]
 [-1.47998099e-01]
 [-8.65413015e-02]
 [-3.38954871e-02]
 [ 1.09708598e-02]
 [ 4.89989725e-02]
 [ 8.10473324e-02]
 [ 1.07897446e-01]
 [ 1.30260412e-01]
 [ 1.48782824e-01]
 [ 1.64051930e-01]
 [ 1.76601316e-01]
 [ 1.86915230e-01]
 [-3.21985557e-08]]
</pre></div>
</div>
<img alt="_images/cf7adb0d61523f821f694194e1cd7054b85f1cd5e0c779a8482d1b035b07413b.png" src="_images/cf7adb0d61523f821f694194e1cd7054b85f1cd5e0c779a8482d1b035b07413b.png" />
</div>
</div>
<section id="nonlinear-programming">
<h3><span class="section-number">2.2.1. </span>Nonlinear Programming<a class="headerlink" href="#nonlinear-programming" title="Link to this heading">#</a></h3>
<p>Unless specific assumptions are made on the dynamics and cost structure, a DOCP is, in its most general form, a nonlinear mathematical program (commonly referred to as an NLP, not to be confused with Natural Language Processing). An NLP can be formulated as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\text{minimize } &amp; f(\mathbf{x}) \\
\text{subject to } &amp; \mathbf{g}(\mathbf{x}) \leq \mathbf{0} \\
&amp; \mathbf{h}(\mathbf{x}) = \mathbf{0}
\end{aligned}
\end{split}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f: \mathbb{R}^n \to \mathbb{R}\)</span> is the objective function</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{g}: \mathbb{R}^n \to \mathbb{R}^m\)</span> represents inequality constraints</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{h}: \mathbb{R}^n \to \mathbb{R}^\ell\)</span> represents equality constraints</p></li>
</ul>
<p>Unlike unconstrained optimization commonly used in deep learning, the optimality of a solution in constrained optimization must consider both the objective value and constraint feasibility. To illustrate this, consider the following problem, which includes both equality and inequality constraints:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\text{Minimize} \quad &amp; f(x_1, x_2) = (x_1 - 1)^2 + (x_2 - 2.5)^2 \\
\text{subject to} \quad &amp; g(x_1, x_2) = (x_1 - 1)^2 + (x_2 - 1)^2 \leq 1.5, \\
&amp; h(x_1, x_2) = x_2 - \left(0.5 \sin(2 \pi x_1) + 1.5\right) = 0.
\end{align*}
\end{split}\]</div>
<p>In this example, the objective function <span class="math notranslate nohighlight">\(f(x_1, x_2)\)</span> is quadratic, the inequality constraint <span class="math notranslate nohighlight">\(g(x_1, x_2)\)</span> defines a circular feasible region centered at <span class="math notranslate nohighlight">\((1, 1)\)</span> with a radius of <span class="math notranslate nohighlight">\(\sqrt{1.5}\)</span> and the equality constraint <span class="math notranslate nohighlight">\(h(x_1, x_2)\)</span> requires <span class="math notranslate nohighlight">\(x_2\)</span> to lie on a sine wave function. The following code demonstrates the difference between the unconstrained, and constrained solutions to this problem.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span>

<span class="c1"># Define the objective function</span>
<span class="k">def</span> <span class="nf">objective</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mf">2.5</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="c1"># Define the inequality constraint function</span>
<span class="k">def</span> <span class="nf">constraint</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">1.5</span>

<span class="c1"># Define the gradient of the objective function</span>
<span class="k">def</span> <span class="nf">objective_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mf">2.5</span><span class="p">)])</span>

<span class="c1"># Define the gradient of the inequality constraint function</span>
<span class="k">def</span> <span class="nf">constraint_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)])</span>

<span class="c1"># Define the sine wave equality constraint function</span>
<span class="k">def</span> <span class="nf">sine_wave_equality_constraint</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="mf">1.5</span><span class="p">)</span>

<span class="c1"># Define the gradient of the sine wave equality constraint function</span>
<span class="k">def</span> <span class="nf">sine_wave_equality_constraint_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="mi">1</span><span class="p">])</span>

<span class="c1"># Define the constraints including the sine wave equality constraint</span>
<span class="n">sine_wave_constraints</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;ineq&#39;</span><span class="p">,</span> <span class="s1">&#39;fun&#39;</span><span class="p">:</span> <span class="n">constraint</span><span class="p">,</span> <span class="s1">&#39;jac&#39;</span><span class="p">:</span> <span class="n">constraint_gradient</span><span class="p">},</span>  <span class="c1"># Inequality constraint</span>
                         <span class="p">{</span><span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;eq&#39;</span><span class="p">,</span> <span class="s1">&#39;fun&#39;</span><span class="p">:</span> <span class="n">sine_wave_equality_constraint</span><span class="p">,</span> <span class="s1">&#39;jac&#39;</span><span class="p">:</span> <span class="n">sine_wave_equality_constraint_gradient</span><span class="p">}]</span>  <span class="c1"># Sine wave equality constraint</span>

<span class="c1"># Define only the inequality constraint</span>
<span class="n">inequality_constraints</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;ineq&#39;</span><span class="p">,</span> <span class="s1">&#39;fun&#39;</span><span class="p">:</span> <span class="n">constraint</span><span class="p">,</span> <span class="s1">&#39;jac&#39;</span><span class="p">:</span> <span class="n">constraint_gradient</span><span class="p">}]</span>

<span class="c1"># Initial guess</span>
<span class="n">x0</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.25</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">]</span>

<span class="c1"># Solve the optimization problem with the sine wave equality constraint</span>
<span class="n">res_sine_wave_constraint</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">objective</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;SLSQP&#39;</span><span class="p">,</span> <span class="n">jac</span><span class="o">=</span><span class="n">objective_gradient</span><span class="p">,</span> 
                                    <span class="n">constraints</span><span class="o">=</span><span class="n">sine_wave_constraints</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;disp&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">})</span>

<span class="n">x_opt_sine_wave_constraint</span> <span class="o">=</span> <span class="n">res_sine_wave_constraint</span><span class="o">.</span><span class="n">x</span>

<span class="c1"># Solve the optimization problem with only the inequality constraint</span>
<span class="n">res_inequality_only</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">objective</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;SLSQP&#39;</span><span class="p">,</span> <span class="n">jac</span><span class="o">=</span><span class="n">objective_gradient</span><span class="p">,</span> 
                               <span class="n">constraints</span><span class="o">=</span><span class="n">inequality_constraints</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;disp&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">})</span>

<span class="n">x_opt_inequality_only</span> <span class="o">=</span> <span class="n">res_inequality_only</span><span class="o">.</span><span class="n">x</span>

<span class="c1"># Solve the unconstrained optimization problem for reference</span>
<span class="n">res_unconstrained</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">objective</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;SLSQP&#39;</span><span class="p">,</span> <span class="n">jac</span><span class="o">=</span><span class="n">objective_gradient</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;disp&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">})</span>
<span class="n">x_opt_unconstrained</span> <span class="o">=</span> <span class="n">res_unconstrained</span><span class="o">.</span><span class="n">x</span>

<span class="c1"># Generate data for visualization</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="mf">2.5</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>  <span class="c1"># Objective function values</span>
<span class="n">constraint_values</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="c1"># Data for sine wave constraint</span>
<span class="n">x_sine</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
<span class="n">y_sine</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x_sine</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1.5</span>

<span class="c1"># Visualization with Improved Color Scheme</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>  <span class="c1"># Heatmap for the objective function</span>

<span class="c1"># Plot all the optimal points</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_opt_inequality_only</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_opt_inequality_only</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;ro&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Optimal Solution (Inequality Only)&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">markeredgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_opt_sine_wave_constraint</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_opt_sine_wave_constraint</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;mo&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Optimal Solution (Sine Wave Equality &amp; Inequality)&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">markeredgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_opt_unconstrained</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_opt_unconstrained</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;co&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Unconstrained Minimum&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">markeredgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>

<span class="c1"># Adjust constraint boundary colors</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">constraint_values</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="p">[</span><span class="mf">1.5</span><span class="p">],</span> <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;navy&#39;</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">constraint_values</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;skyblue&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Plot the sine wave equality constraint with a high contrast color</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_sine</span><span class="p">,</span> <span class="n">y_sine</span><span class="p">,</span> <span class="s1">&#39;lime&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Sine Wave Equality Constraint&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;x2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Example NLP&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="s1">&#39;small&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">fancybox</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># Set the aspect ratio to be equal so the circle appears correctly</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">,</span> <span class="n">adjustable</span><span class="o">=</span><span class="s1">&#39;box&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="_images/af05192d0312dcb3309548c3ceedd4468f282e991b9cef83452557dfe3eef22a.png" src="_images/af05192d0312dcb3309548c3ceedd4468f282e991b9cef83452557dfe3eef22a.png" />
</div>
</div>
<section id="karush-kuhn-tucker-kkt-conditions">
<h4><span class="section-number">2.2.1.1. </span>Karush-Kuhn-Tucker (KKT) conditions<a class="headerlink" href="#karush-kuhn-tucker-kkt-conditions" title="Link to this heading">#</a></h4>
<p>While this example is simple enough to convince ourselves visually of the solution to this particular problem, it falls short of providing us with actionable chracterization of what constitutes and optimal solution in general.
The Karush-Kuhn-Tucker (KKT) conditions provide us with an answer to this problem by generalizing the first-order optimality conditions in unconstrained optimization to problems involving both equality and inequality constraints.
This result relies on the construction of an auxiliary function called the Lagrangian, defined as:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\mathbf{x}, \boldsymbol{\mu}, \boldsymbol{\lambda})=f(\mathbf{x})+\boldsymbol{\mu}^{\top} \mathbf{g}(\mathbf{x})+\boldsymbol{\lambda}^{\top} \mathbf{h}(\mathbf{x})\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\mu} \in \mathbb{R}^m\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\lambda} \in \mathbb{R}^\ell\)</span> are known as Lagrange multipliers. The first-order optimality conditions then state that if <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span>, then there must exist corresponding Lagrange multipliers <span class="math notranslate nohighlight">\(\boldsymbol{\mu}^*\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\lambda}^*\)</span> such that:</p>
<div class="proof definition admonition" id="kkt-conditions">
<p class="admonition-title"><span class="caption-number">Definition 2.1 </span></p>
<section class="definition-content" id="proof-content">
<ol class="arabic">
<li><p>The gradient of the Lagrangian with respect to <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> must be zero at the optimal point (<strong>stationarity</strong>):</p>
<div class="math notranslate nohighlight">
\[\nabla_x \mathcal{L}(\mathbf{x}^*, \boldsymbol{\mu}^*, \boldsymbol{\lambda}^*) = \nabla f(\mathbf{x}^*) + \sum_{i=1}^m \mu_i^* \nabla g_i(\mathbf{x}^*) + \sum_{j=1}^\ell \lambda_j^* \nabla h_j(\mathbf{x}^*) = \mathbf{0}\]</div>
<p>In the case where we only have equality constraints, this means that the gradient of the objective and that of constraint are parallel to each other at the optimum but point in opposite directions.</p>
</li>
<li><p>A valid solution of a NLP is one which satisfies all the constraints (<strong>primal feasibility</strong>)</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
   \mathbf{g}(\mathbf{x}^*) &amp;\leq \mathbf{0}, \enspace \text{and} \enspace \mathbf{h}(\mathbf{x}^*) &amp;= \mathbf{0}
   \end{aligned}\]</div>
</li>
<li><p>Furthermore, the Lagrange multipliers for <strong>inequality</strong> constraints must be non-negative (<strong>dual feasibility</strong>)</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\mu}^* \geq \mathbf{0}\]</div>
<p>This condition stems from the fact that the inequality constraints can only push the solution in one direction.</p>
</li>
<li><p>Finally, for each inequality constraint, either the constraint is active (equality holds) or its corresponding Lagrange multiplier is zero at an optimal solution (<strong>complementary slackness</strong>)</p>
<div class="math notranslate nohighlight">
\[\mu_i^* g_i(\mathbf{x}^*) = 0, \quad \forall i = 1,\ldots,m\]</div>
</li>
</ol>
</section>
</div><p>Let’s now solve our example problem above, this time using <a class="reference external" href="https://coin-or.github.io/Ipopt/">Ipopt</a> via the <a class="reference external" href="http://www.pyomo.org/">Pyomo</a> interface so that we can access the Lagrange multipliers found by the solver.</p>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyomo.environ</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyomo.opt</span> <span class="kn">import</span> <span class="n">SolverFactory</span>
<span class="kn">from</span> <span class="nn">myst_nb</span> <span class="kn">import</span> <span class="n">glue</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="c1"># Define the Pyomo model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ConcreteModel</span><span class="p">()</span>

<span class="c1"># Define the variables</span>
<span class="n">model</span><span class="o">.</span><span class="n">x1</span> <span class="o">=</span> <span class="n">Var</span><span class="p">(</span><span class="n">initialize</span><span class="o">=</span><span class="mf">1.25</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">x2</span> <span class="o">=</span> <span class="n">Var</span><span class="p">(</span><span class="n">initialize</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>

<span class="c1"># Define the objective function</span>
<span class="k">def</span> <span class="nf">objective_rule</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">x1</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">x2</span> <span class="o">-</span> <span class="mf">2.5</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="n">model</span><span class="o">.</span><span class="n">obj</span> <span class="o">=</span> <span class="n">Objective</span><span class="p">(</span><span class="n">rule</span><span class="o">=</span><span class="n">objective_rule</span><span class="p">,</span> <span class="n">sense</span><span class="o">=</span><span class="n">minimize</span><span class="p">)</span>

<span class="c1"># Define the inequality constraint (circle)</span>
<span class="k">def</span> <span class="nf">inequality_constraint_rule</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">x1</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">x2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">&lt;=</span> <span class="mf">1.5</span>
<span class="n">model</span><span class="o">.</span><span class="n">ineq_constraint</span> <span class="o">=</span> <span class="n">Constraint</span><span class="p">(</span><span class="n">rule</span><span class="o">=</span><span class="n">inequality_constraint_rule</span><span class="p">)</span>

<span class="c1"># Define the equality constraint (sine wave) using Pyomo&#39;s math functions</span>
<span class="k">def</span> <span class="nf">equality_constraint_rule</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">x2</span> <span class="o">==</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">model</span><span class="o">.</span><span class="n">x1</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1.5</span>
<span class="n">model</span><span class="o">.</span><span class="n">eq_constraint</span> <span class="o">=</span> <span class="n">Constraint</span><span class="p">(</span><span class="n">rule</span><span class="o">=</span><span class="n">equality_constraint_rule</span><span class="p">)</span>

<span class="c1"># Create a suffix component to capture dual values</span>
<span class="n">model</span><span class="o">.</span><span class="n">dual</span> <span class="o">=</span> <span class="n">Suffix</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="n">Suffix</span><span class="o">.</span><span class="n">IMPORT</span><span class="p">)</span>

<span class="c1"># Create a solver</span>
<span class="n">solver</span><span class="o">=</span><span class="n">SolverFactory</span><span class="p">(</span><span class="s1">&#39;ipopt&#39;</span><span class="p">)</span>

<span class="c1"># Solve the problem</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">solver</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tee</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Check if the solver found an optimal solution</span>
<span class="k">if</span> <span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">solver</span><span class="o">.</span><span class="n">status</span> <span class="o">==</span> <span class="n">SolverStatus</span><span class="o">.</span><span class="n">ok</span> <span class="ow">and</span> 
    <span class="n">results</span><span class="o">.</span><span class="n">solver</span><span class="o">.</span><span class="n">termination_condition</span> <span class="o">==</span> <span class="n">TerminationCondition</span><span class="o">.</span><span class="n">optimal</span><span class="p">):</span>
    
    <span class="c1"># Print the results</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;x1: </span><span class="si">{</span><span class="n">value</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">x1</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;x2: </span><span class="si">{</span><span class="n">value</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">x2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="c1"># Print the objective value</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Objective value: </span><span class="si">{</span><span class="n">value</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">obj</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Print the Lagrange multipliers (dual values)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Lagrange multipliers:&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">component_objects</span><span class="p">(</span><span class="n">Constraint</span><span class="p">,</span> <span class="n">active</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">c</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">c</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">[</span><span class="si">{</span><span class="n">index</span><span class="si">}</span><span class="s2">]: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">dual</span><span class="p">[</span><span class="n">c</span><span class="p">[</span><span class="n">index</span><span class="p">]]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">glue</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">c</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">[</span><span class="si">{</span><span class="n">index</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">dual</span><span class="p">[</span><span class="n">c</span><span class="p">[</span><span class="n">index</span><span class="p">]],</span> <span class="n">display</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Solver did not find an optimal solution.&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Solver Status: </span><span class="si">{</span><span class="n">results</span><span class="o">.</span><span class="n">solver</span><span class="o">.</span><span class="n">status</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Termination Condition: </span><span class="si">{</span><span class="n">results</span><span class="o">.</span><span class="n">solver</span><span class="o">.</span><span class="n">termination_condition</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>x1: 1.2271417639244486
x2: 1.994852000302119
Objective value: 0.3067678825174803

Lagrange multipliers:
ineq_constraint[None]: -5.466075458072094e-09
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>eq_constraint[None]: -1.0102959885190541
</pre></div>
</div>
</div>
</details>
</div>
<p>After running the code, we find that the Lagrange multiplier associated with the inequality constraint is approximately <span class="pasted-text">-5.47e-09</span>. This very small value, close to zero, suggests that the inequality constraint is not active at the optimal solution, meaning that the solution point lies inside the circle defined by this constraint. This can be verified visually in the figure above. As for the equality constraint, its corresponding Lagrange multiplier is <span class="pasted-text">-1.01e+00</span> and the fact that it’s non-zero indicates that this constraint is active at the optimal solution. In general when we find a Lagrange multiplier close to zero (like the one for the inequality constraint), it means that constraint is not “binding”—the optimal solution does not lie on the boundary defined by this constraint. In contrast, a non-zero Lagrange multiplier, such as the one for the equality constraint, indicates that the constraint is active and that any relaxation would directly affect the objective function’s value, as required by the stationarity condition.</p>
</section>
<section id="lagrange-multiplier-theorem">
<h4><span class="section-number">2.2.1.2. </span>Lagrange Multiplier Theorem<a class="headerlink" href="#lagrange-multiplier-theorem" title="Link to this heading">#</a></h4>
<p>The KKT conditions introduced above characterize the solution structure of constrained optimization problems with equality constraints. In this particular context, these conditions are referred to as the first-order optimality conditions, as part of the Lagrange multiplier theorem. Let’s just re-state them in that simpler setting:</p>
<div class="proof definition admonition" id="definition-7">
<p class="admonition-title"><span class="caption-number">Definition 2.2 </span> (Lagrange Multiplier Theorem)</p>
<section class="definition-content" id="proof-content">
<p>Consider the constrained optimization problem:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\min_{\mathbf{x}} \quad &amp; f(\mathbf{x}) \\
\text{subject to} \quad &amp; g_i(\mathbf{x}) = 0, \quad i = 1, \ldots, m
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^n\)</span>, <span class="math notranslate nohighlight">\(f: \mathbb{R}^n \to \mathbb{R}\)</span>, and <span class="math notranslate nohighlight">\(g_i: \mathbb{R}^n \to \mathbb{R}\)</span> for <span class="math notranslate nohighlight">\(i = 1, \ldots, m\)</span>.</p>
<p>Assume that:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g_i\)</span> are continuously differentiable functions.</p></li>
<li><p>The gradients <span class="math notranslate nohighlight">\(\nabla g_i(\mathbf{x}^*)\)</span> are linearly independent at the optimal point <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span>.</p></li>
</ol>
<p>Then, there exist unique Lagrange multipliers <span class="math notranslate nohighlight">\(\lambda_i^* \in \mathbb{R}\)</span>, <span class="math notranslate nohighlight">\(i = 1, \ldots, m\)</span>, such that the following first-order optimality conditions hold:</p>
<ol class="arabic simple">
<li><p>Stationarity: <span class="math notranslate nohighlight">\(\nabla f(\mathbf{x}^*) + \sum_{i=1}^m \lambda_i^* \nabla g_i(\mathbf{x}^*) = \mathbf{0}\)</span></p></li>
<li><p>Primal feasibility: <span class="math notranslate nohighlight">\(g_i(\mathbf{x}^*) = 0\)</span>, for <span class="math notranslate nohighlight">\(i = 1, \ldots, m\)</span></p></li>
</ol>
</section>
</div><p>Note that both the stationarity and primal feasibility statements are simply saying that the derivative of the Lagrangian in either the primal or dual variables must be zero at an optimal constrained solution. In other words:</p>
<div class="math notranslate nohighlight">
\[
\nabla_{\mathbf{x}, \boldsymbol{\lambda}} L(\mathbf{x}^*, \boldsymbol{\lambda}^*) = \mathbf{0}
\]</div>
<p>Letting <span class="math notranslate nohighlight">\(\mathbf{F}(\mathbf{x}, \boldsymbol{\lambda})\)</span> stand for <span class="math notranslate nohighlight">\(\nabla_{\mathbf{x}, \boldsymbol{\lambda}} L(\mathbf{x}, \boldsymbol{\lambda})\)</span>, the Lagrange multipliers theorem tells us that an optimal primal-dual pair is actually a zero of that function <span class="math notranslate nohighlight">\(\mathbf{F}\)</span>: the derivative of the Lagrangian. Therefore, we can use this observation to craft a solution method for solving equality constrained optimization using Newton’s method, which is a numerical procedure for finding zeros of a nonlinear function.</p>
</section>
<section id="newton-s-method">
<h4><span class="section-number">2.2.1.3. </span>Newton’s Method<a class="headerlink" href="#newton-s-method" title="Link to this heading">#</a></h4>
<p>Newton’s method is a numerical procedure for solving root-finding problems. These are nonlinear systems of equations of the form:</p>
<p>Find <span class="math notranslate nohighlight">\(\mathbf{z}^* \in \mathbb{R}^n\)</span> such that <span class="math notranslate nohighlight">\(\mathbf{F}(\mathbf{z}^*) = \mathbf{0}\)</span></p>
<p>where <span class="math notranslate nohighlight">\(\mathbf{F}: \mathbb{R}^n \to \mathbb{R}^n\)</span> is a continuously differentiable function. Newton’s method then consists in applying the following sequence of iterates:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{z}^{k+1} = \mathbf{z}^k - [\nabla \mathbf{F}(\mathbf{z}^k)]^{-1} \mathbf{F}(\mathbf{z}^k)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{z}^k\)</span> is the k-th iterate, and <span class="math notranslate nohighlight">\(\nabla \mathbf{F}(\mathbf{z}^k)\)</span> is the Jacobian matrix of <span class="math notranslate nohighlight">\(\mathbf{F}\)</span> evaluated at <span class="math notranslate nohighlight">\(\mathbf{z}^k\)</span>.</p>
<p>Newton’s method exhibits local quadratic convergence: if the initial guess <span class="math notranslate nohighlight">\(\mathbf{z}^0\)</span> is sufficiently close to the true solution <span class="math notranslate nohighlight">\(\mathbf{z}^*\)</span>, and <span class="math notranslate nohighlight">\(\nabla \mathbf{F}(\mathbf{z}^*)\)</span> is nonsingular, the method converges quadratically to <span class="math notranslate nohighlight">\(\mathbf{z}^*\)</span> <span id="id5">[<a class="reference internal" href="#id20" title="J. M. Ortega and W. C. Rheinboldt. Iterative Solution of Nonlinear Equations in Several Variables. Computer Science and Applied Mathematics. Academic Press, New York, 1970.">OR70</a>]</span>. However, the method is sensitive to the initial guess; if it’s too far from the desired solution, Newton’s method might fail to converge or converge to a different root. To mitigate this problem, a set of techniques known as numerical continuation methods <span id="id6">[<a class="reference internal" href="#id21" title="E. L. Allgower and K. Georg. Numerical Continuation Methods: An Introduction. Volume 13 of Springer Series in Computational Mathematics. Springer-Verlag, Berlin, Heidelberg, 1990.">AG90</a>]</span> have been developed. These methods effectively enlarge the basin of attraction of Newton’s method by solving a sequence of related problems, progressing from an easy one to the target problem. This approach is reminiscent of several concepts in machine learning and statistical inference: curriculum learning in machine learning, where models are trained on increasingly complex data; tempering in Markov Chain Monte Carlo (MCMC) samplers, which gradually adjusts the target distribution to improve mixing; and modern diffusion models, which use a similar concept of gradually transforming noise into structured data.</p>
<section id="efficient-implementation-of-newton-s-method">
<h5><span class="section-number">2.2.1.3.1. </span>Efficient Implementation of Newton’s Method<a class="headerlink" href="#efficient-implementation-of-newton-s-method" title="Link to this heading">#</a></h5>
<p>Note that each step of Newton’s method involves computing the inverse of a Jacobian matrix. However, a cardinal rule in numerical linear algebra is to avoid computing matrix inverses explicitly: rarely, if ever, should there be a <code class="docutils literal notranslate"><span class="pre">np.lindex.inv</span></code> in your code. Instead, the numerically stable and computationally efficient approach is to solve a linear system of equations at each step.
Given the Newton’s method iterate:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{z}^{k+1} = \mathbf{z}^k - [\nabla \mathbf{F}(\mathbf{z}^k)]^{-1} \mathbf{F}(\mathbf{z}^k)
\]</div>
<p>We can reformulate this as a two-step procedure:</p>
<ol class="arabic simple">
<li><p>Solve the linear system: <span class="math notranslate nohighlight">\(\underbrace{[\nabla \mathbf{F}(\mathbf{z}^k)]}_{\mathbf{A}} \Delta \mathbf{z}^k = -\mathbf{F}(\mathbf{z}^k)\)</span></p></li>
<li><p>Update: <span class="math notranslate nohighlight">\(\mathbf{z}^{k+1} = \mathbf{z}^k + \Delta \mathbf{z}^k\)</span></p></li>
</ol>
<p>The structure of the linear system in step 1 often allows for specialized solution methods. In the context of automatic differentiation, matrix-free linear solvers are particularly useful. These solvers can find a solution without explicitly forming the matrix A, requiring only the ability to evaluate matrix-vector or vector-matrix products. Typical examples of such methods include classical matrix-splitting methods (e.g., Richardson iteration) or conjugate gradient methods through <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.cg.html"><code class="docutils literal notranslate"><span class="pre">sparse.linalg.cg</span></code></a> for example. Another useful method is the Generalized Minimal Residual method (GMRES) implemented in SciPy via <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.gmres.html"><code class="docutils literal notranslate"><span class="pre">sparse.linalg.gmres</span></code></a>, which is useful when facing non-symmetric and indefinite systems.</p>
<p>By inspecting the structure of matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> in the specific application where the function <span class="math notranslate nohighlight">\(\mathbf{F}\)</span> is the derivative of the Lagrangian, we will also uncover an important structure known as the KKT matrix. This structure will then allow us to derive a Quadratic Programming (QP) sub-problem as part of a larger iterative procedure for solving equality and inequality constrained problems via Sequential Quadratic Programming (SQP).</p>
</section>
</section>
<section id="solving-equality-constrained-programs-with-newton-s-method">
<h4><span class="section-number">2.2.1.4. </span>Solving Equality Constrained Programs with Newton’s Method<a class="headerlink" href="#solving-equality-constrained-programs-with-newton-s-method" title="Link to this heading">#</a></h4>
<p>To solve equality-constrained optimization problems using Newton’s method, we begin by recognizing that the problem reduces to finding a zero of the function <span class="math notranslate nohighlight">\(\mathbf{F}(\mathbf{z}) = \nabla_{\mathbf{x}, \boldsymbol{\lambda}} L(\mathbf{x}, \boldsymbol{\lambda})\)</span>. Here, <span class="math notranslate nohighlight">\(\mathbf{F}\)</span> represents the derivative of the Lagrangian function, and <span class="math notranslate nohighlight">\(\mathbf{z} = (\mathbf{x}, \boldsymbol{\lambda})\)</span> combines both the primal variables <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and the dual variables (Lagrange multipliers) <span class="math notranslate nohighlight">\(\boldsymbol{\lambda}\)</span>. Explicitly, we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{F}(\mathbf{z}) = \begin{bmatrix} \nabla_{\mathbf{x}} L(\mathbf{x}, \boldsymbol{\lambda}) \\ \mathbf{g}(\mathbf{x}) \end{bmatrix} = \begin{bmatrix} \nabla f(\mathbf{x}) + \sum_{i=1}^m \lambda_i \nabla g_i(\mathbf{x}) \\ \mathbf{g}(\mathbf{x}) \end{bmatrix}.
\end{split}\]</div>
<p>Newton’s method involves linearizing <span class="math notranslate nohighlight">\(\mathbf{F}(\mathbf{z})\)</span> around the current iterate <span class="math notranslate nohighlight">\(\mathbf{z}^k = (\mathbf{x}^k, \boldsymbol{\lambda}^k)\)</span> and then solving the resulting linear system. At each iteration <span class="math notranslate nohighlight">\(k\)</span>, Newton’s method updates the current estimate by solving the linear system:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{z}^{k+1} = \mathbf{z}^k - [\nabla \mathbf{F}(\mathbf{z}^k)]^{-1} \mathbf{F}(\mathbf{z}^k).
\]</div>
<p>However, instead of explicitly inverting the Jacobian matrix <span class="math notranslate nohighlight">\(\nabla \mathbf{F}(\mathbf{z}^k)\)</span>, we solve the linear system:</p>
<div class="math notranslate nohighlight">
\[
\underbrace{\nabla \mathbf{F}(\mathbf{z}^k)}_{\mathbf{A}} \Delta \mathbf{z}^k = -\mathbf{F}(\mathbf{z}^k),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\Delta \mathbf{z}^k = (\Delta \mathbf{x}^k, \Delta \boldsymbol{\lambda}^k)\)</span> represents the Newton step for the primal and dual variables. Substituting the expression for <span class="math notranslate nohighlight">\(\mathbf{F}(\mathbf{z})\)</span> and its Jacobian, the system becomes:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
\nabla^2_{\mathbf{x}\mathbf{x}} L(\mathbf{x}^k, \boldsymbol{\lambda}^k) &amp; \nabla \mathbf{g}(\mathbf{x}^k)^T \\
\nabla \mathbf{g}(\mathbf{x}^k) &amp; \mathbf{0}
\end{bmatrix}
\begin{bmatrix}
\Delta \mathbf{x}^k \\
\Delta \boldsymbol{\lambda}^k
\end{bmatrix}
=
-
\begin{bmatrix}
\nabla f(\mathbf{x}^k) + \nabla \mathbf{g}(\mathbf{x}^k)^T \boldsymbol{\lambda}^k \\
\mathbf{g}(\mathbf{x}^k)
\end{bmatrix}.
\end{split}\]</div>
<p>The matrix on the left-hand side is known as the KKT matrix, as it stems from the Karush-Kuhn-Tucker conditions for this optimization problem
The solution of this system provides the updates <span class="math notranslate nohighlight">\(\Delta \mathbf{x}^k\)</span> and <span class="math notranslate nohighlight">\(\Delta \boldsymbol{\lambda}^k\)</span>, which are then used to update the primal and dual variables:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}^{k+1} = \mathbf{x}^k + \Delta \mathbf{x}^k, \quad \boldsymbol{\lambda}^{k+1} = \boldsymbol{\lambda}^k + \Delta \boldsymbol{\lambda}^k.
\]</div>
<section id="demonstration">
<h5><span class="section-number">2.2.1.4.1. </span>Demonstration<a class="headerlink" href="#demonstration" title="Link to this heading">#</a></h5>
<p>The following code demonstates how we can implement this idea in Jax. In this demonstration, we are minimizing a quadratic objective function subject to a single equality constraint, a problem formally stated as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\min_{x \in \mathbb{R}^2} \quad &amp; f(x) = (x_1 - 2)^2 + (x_2 - 1)^2 \\
\text{subject to} \quad &amp; g(x) = x_1^2 + x_2^2 - 1 = 0
\end{aligned}
\end{split}\]</div>
<p>Geometrically speaking, the constraint <span class="math notranslate nohighlight">\(g(x)\)</span> describes a unit circle centered at the origin. To solve this problem using the method of Lagrange multipliers, we form the Lagrangian:</p>
<div class="math notranslate nohighlight">
\[
L(x, \lambda) = f(x) + \lambda g(x) = (x_1 - 2)^2 + (x_2 - 1)^2 + \lambda(x_1^2 + x_2^2 - 1)
\]</div>
<p>For this particular problem, it happens so that we can also find an analytical without even having to use Newton’s method. From the first-order optimality conditions, we obtain the following linear system of equations:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
   2(x_1 - 2) + 2\lambda x_1 &amp;= 0 \\
   2(x_2 - 1) + 2\lambda x_2 &amp;= 0 \\
   x_1^2 + x_2^2 - 1 &amp;= 0\\
\end{align*}\]</div>
<p>From the first two equations, we then get:</p>
<div class="math notranslate nohighlight">
\[x_1 = \frac{2}{1 + \lambda}, \quad x_2 = \frac{1}{1 + \lambda}\]</div>
<p>which we can substitute these into the 3rd constraint equation to obtain:</p>
<div class="math notranslate nohighlight">
\[(\frac{2}{1 + \lambda})^2 + (\frac{1}{1 + \lambda})^2 = 1 \Leftrightarrow \lambda = \sqrt{5} - 1$\]</div>
<p>This value of the Lagrange multiplier can then be backsubstituted into the above equations to obtain <span class="math notranslate nohighlight">\(x_1 = \frac{2}{\sqrt{5}}\)</span> and <span class="math notranslate nohighlight">\(x_2 =  \frac{1}{\sqrt{5}}\)</span>.
We can verify numerically (and visually on the following graph) that the point <span class="math notranslate nohighlight">\((2/\sqrt{5}, 1/\sqrt{5})\)</span> is indeed the point on the unit circle closest to <span class="math notranslate nohighlight">\((2, 1)\)</span>.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">grad</span><span class="p">,</span> <span class="n">jit</span><span class="p">,</span> <span class="n">jacfwd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define the objective function and constraint</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span>

<span class="c1"># Lagrangian</span>
<span class="k">def</span> <span class="nf">L</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">lambda_</span> <span class="o">*</span> <span class="n">g</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Gradient and Hessian of Lagrangian</span>
<span class="n">grad_L_x</span> <span class="o">=</span> <span class="n">jit</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="n">grad_L_lambda</span> <span class="o">=</span> <span class="n">jit</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">hess_L_xx</span> <span class="o">=</span> <span class="n">jit</span><span class="p">(</span><span class="n">jacfwd</span><span class="p">(</span><span class="n">grad_L_x</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="n">hess_L_xlambda</span> <span class="o">=</span> <span class="n">jit</span><span class="p">(</span><span class="n">jacfwd</span><span class="p">(</span><span class="n">grad_L_x</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># Newton&#39;s method</span>
<span class="nd">@jit</span>
<span class="k">def</span> <span class="nf">newton_step</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">):</span>
    <span class="n">grad_x</span> <span class="o">=</span> <span class="n">grad_L_x</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">)</span>
    <span class="n">grad_lambda</span> <span class="o">=</span> <span class="n">grad_L_lambda</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">)</span>
    <span class="n">hess_xx</span> <span class="o">=</span> <span class="n">hess_L_xx</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">)</span>
    <span class="n">hess_xlambda</span> <span class="o">=</span> <span class="n">hess_L_xlambda</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># Construct the full KKT matrix</span>
    <span class="n">kkt_matrix</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">block</span><span class="p">([</span>
        <span class="p">[</span><span class="n">hess_xx</span><span class="p">,</span> <span class="n">hess_xlambda</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)],</span>
        <span class="p">[</span><span class="n">hess_xlambda</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">]])]</span>
    <span class="p">])</span>
    
    <span class="c1"># Construct the right-hand side</span>
    <span class="n">rhs</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="o">-</span><span class="n">grad_x</span><span class="p">,</span> <span class="o">-</span><span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">grad_lambda</span><span class="p">])])</span>
    
    <span class="c1"># Solve the KKT system</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">kkt_matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">delta</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="n">lambda_</span> <span class="o">+</span> <span class="n">delta</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">solve_constrained_optimization</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">lambda0</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">lambda_</span> <span class="o">=</span> <span class="n">x0</span><span class="p">,</span> <span class="n">lambda0</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
        <span class="n">x_new</span><span class="p">,</span> <span class="n">lambda_new</span> <span class="o">=</span> <span class="n">newton_step</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">x_new</span> <span class="o">-</span> <span class="n">x</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">lambda_new</span> <span class="o">-</span> <span class="n">lambda_</span><span class="p">])]))</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">lambda_</span> <span class="o">=</span> <span class="n">x_new</span><span class="p">,</span> <span class="n">lambda_new</span>
    
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span>

<span class="c1"># Analytical solution</span>
<span class="k">def</span> <span class="nf">analytical_solution</span><span class="p">():</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">lambda_opt</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">]),</span> <span class="n">lambda_opt</span>

<span class="c1"># Solve the problem numerically</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
<span class="n">lambda0</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">x_opt_num</span><span class="p">,</span> <span class="n">lambda_opt_num</span><span class="p">,</span> <span class="n">iterations</span> <span class="o">=</span> <span class="n">solve_constrained_optimization</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">lambda0</span><span class="p">)</span>

<span class="c1"># Compute analytical solution</span>
<span class="n">x_opt_ana</span><span class="p">,</span> <span class="n">lambda_opt_ana</span> <span class="o">=</span> <span class="n">analytical_solution</span><span class="p">()</span>

<span class="c1"># Verify the result</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Numerical Solution:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Constraint violation: </span><span class="si">{</span><span class="n">g</span><span class="p">(</span><span class="n">x_opt_num</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Objective function value: </span><span class="si">{</span><span class="n">f</span><span class="p">(</span><span class="n">x_opt_num</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Analytical Solution:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Constraint violation: </span><span class="si">{</span><span class="n">g</span><span class="p">(</span><span class="n">x_opt_ana</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Objective function value: </span><span class="si">{</span><span class="n">f</span><span class="p">(</span><span class="n">x_opt_ana</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Comparison:&quot;</span><span class="p">)</span>
<span class="n">x_diff</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x_opt_num</span> <span class="o">-</span> <span class="n">x_opt_ana</span><span class="p">)</span>
<span class="n">lambda_diff</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">lambda_opt_num</span> <span class="o">-</span> <span class="n">lambda_opt_ana</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Difference in x: </span><span class="si">{</span><span class="n">x_diff</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Difference in lambda: </span><span class="si">{</span><span class="n">lambda_diff</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Precision test</span>
<span class="n">rtol</span> <span class="o">=</span> <span class="mf">1e-5</span>  <span class="c1"># relative tolerance</span>
<span class="n">atol</span> <span class="o">=</span> <span class="mf">1e-8</span>  <span class="c1"># absolute tolerance</span>

<span class="n">x_close</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">x_opt_num</span><span class="p">,</span> <span class="n">x_opt_ana</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="n">atol</span><span class="p">)</span>
<span class="n">lambda_close</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">lambda_opt_num</span><span class="p">,</span> <span class="n">lambda_opt_ana</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="n">atol</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Precision Test:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;x values are close: </span><span class="si">{</span><span class="n">x_close</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;lambda values are close: </span><span class="si">{</span><span class="n">lambda_close</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="n">x_close</span> <span class="ow">and</span> <span class="n">lambda_close</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The numerical solution matches the analytical solution within the specified tolerance.&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The numerical solution differs from the analytical solution more than the specified tolerance.&quot;</span><span class="p">)</span>

<span class="c1"># Visualize the result</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="c1"># Create a mesh for the contour plot</span>
<span class="n">x1_range</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">x2_range</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">X1</span><span class="p">,</span> <span class="n">X2</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x1_range</span><span class="p">,</span> <span class="n">x2_range</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">f</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">]))</span> <span class="k">for</span> <span class="n">x1</span> <span class="ow">in</span> <span class="n">x1_range</span><span class="p">]</span> <span class="k">for</span> <span class="n">x2</span> <span class="ow">in</span> <span class="n">x2_range</span><span class="p">])</span>

<span class="c1"># Plot filled contours</span>
<span class="n">contour</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">extent</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">contour</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Objective Function Value&#39;</span><span class="p">)</span>

<span class="c1"># Plot the constraint</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">jnp</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Constraint&#39;</span><span class="p">)</span>

<span class="c1"># Plot the optimal points (numerical and analytical) and initial point</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_opt_num</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_opt_num</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Numerical Optimal Point&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_opt_ana</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_opt_ana</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Analytical Optimal Point&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x0</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x0</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Initial Point&#39;</span><span class="p">)</span>

<span class="c1"># Add labels and title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;x2&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Constrained Optimization: Numerical vs Analytical Solution&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>

<span class="c1"># Set the axis limits explicitly</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Numerical Solution:
Constraint violation: 0.000000
Objective function value: 1.527864

Analytical Solution:
Constraint violation: -0.000000
Objective function value: 1.527864

Comparison:
Difference in x: 5.960464477539063e-08
Difference in lambda: 1.1920928955078125e-07

Precision Test:
x values are close: True
lambda values are close: True
The numerical solution matches the analytical solution within the specified tolerance.
</pre></div>
</div>
<img alt="_images/adb60c70705392b81d3a938949e3a17c1c82d7bb5f4a59c0622cd17801c0cdf6.png" src="_images/adb60c70705392b81d3a938949e3a17c1c82d7bb5f4a59c0622cd17801c0cdf6.png" />
</div>
</div>
</section>
</section>
</section>
<section id="the-sqp-approach-taylor-expansion-and-quadratic-approximation">
<h3><span class="section-number">2.2.2. </span>The SQP Approach: Taylor Expansion and Quadratic Approximation<a class="headerlink" href="#the-sqp-approach-taylor-expansion-and-quadratic-approximation" title="Link to this heading">#</a></h3>
<p>Sequential Quadratic Programming (SQP) tackles the problem of solving constrained programs by iteratively solving a sequence of simpler subproblems. Specifically, these subproblems are quadratic programs (QPs) that approximate the original problem around the current iterate by using a quadratic model of the objective function and a linear model of the constraints. Suppose we have the following optimization problem with equality constraints:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\min_{\mathbf{x}} \quad &amp; f(\mathbf{x}) \\
\text{subject to} \quad &amp; \mathbf{g}(\mathbf{x}) = \mathbf{0}.
\end{aligned}
\end{split}\]</div>
<p>At each iteration <span class="math notranslate nohighlight">\(k\)</span>, we approximate the objective function <span class="math notranslate nohighlight">\(f(\mathbf{x})\)</span> using a second-order Taylor expansion around the current iterate <span class="math notranslate nohighlight">\(\mathbf{x}^k\)</span>. The standard Taylor expansion for <span class="math notranslate nohighlight">\(f\)</span> would be:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
f(\mathbf{x}) \approx f(\mathbf{x}^k) + \nabla f(\mathbf{x}^k)^T (\mathbf{x} - \mathbf{x}^k) + \frac{1}{2} (\mathbf{x} - \mathbf{x}^k)^T \nabla^2 f(\mathbf{x}^k) (\mathbf{x} - \mathbf{x}^k).
\end{align*}\]</div>
<p>This expansion uses the <strong>Hessian of the objective function</strong> <span class="math notranslate nohighlight">\(\nabla^2 f(\mathbf{x}^k)\)</span> to capture the curvature of <span class="math notranslate nohighlight">\(f\)</span>. However, in the context of constrained optimization, we also need to account for the effect of the constraints on the local behavior of the solution. If we were to use only <span class="math notranslate nohighlight">\(\nabla^2 f(\mathbf{x}^k)\)</span>, we would not capture the influence of the constraints on the curvature of the feasible region. The resulting subproblem might then lead to steps that violate the constraints or are less effective in achieving convergence. The choice that we make instead is to use the Hessian of the Lagrangian, <span class="math notranslate nohighlight">\(\nabla^2_{\mathbf{x}\mathbf{x}} L(\mathbf{x}^k, \boldsymbol{\lambda}^k)\)</span>, leading to the following quadratic model:</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{x}) \approx f(\mathbf{x}^k) + \nabla f(\mathbf{x}^k)^T (\mathbf{x} - \mathbf{x}^k) + \frac{1}{2} (\mathbf{x} - \mathbf{x}^k)^T \nabla^2_{\mathbf{x}\mathbf{x}} L(\mathbf{x}^k, \boldsymbol{\lambda}^k) (\mathbf{x} - \mathbf{x}^k).
\]</div>
<p>Similarly, the equality constraints <span class="math notranslate nohighlight">\(\mathbf{g}(\mathbf{x})\)</span> are linearized around <span class="math notranslate nohighlight">\(\mathbf{x}^k\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{g}(\mathbf{x}) \approx \mathbf{g}(\mathbf{x}^k) + \nabla \mathbf{g}(\mathbf{x}^k) (\mathbf{x} - \mathbf{x}^k).
\]</div>
<p>Combining these approximations, we obtain a Quadratic Programming (QP) subproblem, which approximates our original problem locally at <span class="math notranslate nohighlight">\(\mathbf{x}^k\)</span> but is easier to solve:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\text{Minimize} \quad &amp; \nabla f(\mathbf{x}^k)^T \Delta \mathbf{x} + \frac{1}{2} \Delta \mathbf{x}^T \nabla^2_{\mathbf{x}\mathbf{x}} L(\mathbf{x}^k, \boldsymbol{\lambda}^k) \Delta \mathbf{x} \\
\text{subject to} \quad &amp; \nabla \mathbf{g}(\mathbf{x}^k) \Delta \mathbf{x} + \mathbf{g}(\mathbf{x}^k) = \mathbf{0},
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\Delta \mathbf{x} = \mathbf{x} - \mathbf{x}^k\)</span>. The QP subproblem solved at each iteration focuses on finding the optimal step direction <span class="math notranslate nohighlight">\(\Delta \mathbf{x}\)</span> for the primal variables.
While solving this QP, we obtain not only the step <span class="math notranslate nohighlight">\(\Delta \mathbf{x}\)</span> but also the associated Lagrange multipliers for the QP subproblem, which correspond to an updated dual variable vector <span class="math notranslate nohighlight">\(\boldsymbol{\lambda}^{k+1}\)</span>. More specifically, after solving the QP, we use <span class="math notranslate nohighlight">\(\Delta \mathbf{x}^k\)</span> to update the primal variables:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{x}^{k+1} = \mathbf{x}^k + \Delta \mathbf{x}^k.
\end{align*}\]</div>
<p>Simultaneously, the Lagrange multipliers from the QP provide the updated dual variables <span class="math notranslate nohighlight">\(\boldsymbol{\lambda}^{k+1}\)</span>.
We summarize the SQP algorithm in the following pseudo-code:</p>
<div class="proof algorithm admonition" id="alg-sqp">
<p class="admonition-title"><span class="caption-number">Algorithm 2.4 </span> (Sequential Quadratic Programming (SQP))</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> Initial estimate <span class="math notranslate nohighlight">\(\mathbf{x}^0\)</span>, initial Lagrange multipliers <span class="math notranslate nohighlight">\(\boldsymbol{\lambda}^0\)</span>, tolerance <span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span>.</p>
<p><strong>Output:</strong> Solution <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span>, Lagrange multipliers <span class="math notranslate nohighlight">\(\boldsymbol{\lambda}^*\)</span>.</p>
<p><strong>Procedure:</strong></p>
<ol class="arabic">
<li><p><strong>Compute the QP Solution:</strong> Solve the QP subproblem to obtain <span class="math notranslate nohighlight">\(\Delta \mathbf{x}^k\)</span>. The QP solver also provides the updated Lagrange multipliers <span class="math notranslate nohighlight">\(\boldsymbol{\lambda}^{k+1}\)</span> associated with the constraints.</p></li>
<li><p><strong>Update the Estimates:</strong> Update the primal variables:</p>
<div class="math notranslate nohighlight">
\[
   \mathbf{x}^{k+1} = \mathbf{x}^k + \Delta \mathbf{x}^k.
   \]</div>
<p>Set the dual variables to the updated values <span class="math notranslate nohighlight">\(\boldsymbol{\lambda}^{k+1}\)</span> from the QP solution.</p>
</li>
<li><p><strong>Repeat Until Convergence:</strong> Continue iterating until <span class="math notranslate nohighlight">\(\|\Delta \mathbf{x}^k\| &lt; \epsilon\)</span> and the KKT conditions are satisfied.</p></li>
</ol>
</section>
</div><section id="connection-to-newton-s-method-in-the-equality-constrained-case">
<h4><span class="section-number">2.2.2.1. </span>Connection to Newton’s Method in the Equality-Constrained Case<a class="headerlink" href="#connection-to-newton-s-method-in-the-equality-constrained-case" title="Link to this heading">#</a></h4>
<p>The QP subproblem in SQP is directly related to applying Newton’s method for equality-constrained optimization. To see this, note that the KKT matrix of the QP subproblem is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\begin{bmatrix}
\nabla^2_{\mathbf{x}\mathbf{x}} L(\mathbf{x}_k, \boldsymbol{\mu}_k) &amp; \nabla \mathbf{h}(\mathbf{x}_k)^T \\
\nabla \mathbf{h}(\mathbf{x}_k) &amp; \mathbf{0}
\end{bmatrix}
\begin{bmatrix}
\Delta \mathbf{x}_k \\
\Delta \boldsymbol{\mu}_k
\end{bmatrix}
=
-
\begin{bmatrix}
\nabla f(\mathbf{x}_k) + \nabla \mathbf{h}(\mathbf{x}_k)^T \boldsymbol{\mu}_k \\
\mathbf{h}(\mathbf{x}_k)
\end{bmatrix}
\end{align*}\]</div>
<p>This is exactly the same linear system that have to solve when applying Newton’s method to the KKT conditions of the original program! Thus, solving the QP subproblem at each iteration of SQP is equivalent to taking a Newton step on the KKT conditions of the original nonlinear problem.</p>
</section>
</section>
<section id="sqp-for-inequality-constrained-optimization">
<h3><span class="section-number">2.2.3. </span>SQP for Inequality-Constrained Optimization<a class="headerlink" href="#sqp-for-inequality-constrained-optimization" title="Link to this heading">#</a></h3>
<p>So far, we’ve applied the ideas behind Sequential Quadratic Programming (SQP) to problems with only equality constraints. Now, let’s extend this framework to handle optimization problems that also include inequality constraints.
Consider a general nonlinear optimization problem that includes both equality and inequality constraints:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\min_{\mathbf{x}} \quad &amp; f(\mathbf{x}) \\
\text{subject to} \quad &amp; \mathbf{g}(\mathbf{x}) = \mathbf{0}, \\
&amp; \mathbf{h}(\mathbf{x}) \leq \mathbf{0}.
\end{align*}\]</div>
<p>As we did earlier, we approximate this problem by constructing a quadratic approximation to the objective and a linearization of the constraints. QP subproblem at each iteration is then formulated as:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\text{Minimize} \quad &amp; \nabla f(\mathbf{x}^k)^T \Delta \mathbf{x} + \frac{1}{2} \Delta \mathbf{x}^T \nabla^2_{\mathbf{x}\mathbf{x}} L(\mathbf{x}^k, \boldsymbol{\lambda}^k, \boldsymbol{\nu}^k) \Delta \mathbf{x} \\
\text{subject to} \quad &amp; \nabla \mathbf{g}(\mathbf{x}^k) \Delta \mathbf{x} + \mathbf{g}(\mathbf{x}^k) = \mathbf{0}, \\
&amp; \nabla \mathbf{h}(\mathbf{x}^k) \Delta \mathbf{x} + \mathbf{h}(\mathbf{x}^k) \leq \mathbf{0},
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\Delta \mathbf{x} = \mathbf{x} - \mathbf{x}^k\)</span> represents the step direction for the primal variables. The following pseudocode outlines the steps involved in applying SQP to a problem with both equality and inequality constraints:</p>
<div class="proof algorithm admonition" id="alg-sqp-ineq">
<p class="admonition-title"><span class="caption-number">Algorithm 2.5 </span> (Sequential Quadratic Programming (SQP) with Inequality Constraints)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> Initial estimate <span class="math notranslate nohighlight">\(\mathbf{x}^0\)</span>, initial multipliers <span class="math notranslate nohighlight">\(\boldsymbol{\lambda}^0, \boldsymbol{\nu}^0\)</span>, tolerance <span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span>.</p>
<p><strong>Output:</strong> Solution <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span>, Lagrange multipliers <span class="math notranslate nohighlight">\(\boldsymbol{\lambda}^*, \boldsymbol{\nu}^*\)</span>.</p>
<p><strong>Procedure:</strong></p>
<ol class="arabic">
<li><p><strong>Initialization:</strong>
Set <span class="math notranslate nohighlight">\(k = 0\)</span>.</p></li>
<li><p><strong>Repeat:</strong></p>
<p>a. <strong>Construct the QP Subproblem:</strong>
Formulate the QP subproblem using the current iterate <span class="math notranslate nohighlight">\(\mathbf{x}^k\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\lambda}^k\)</span>, and <span class="math notranslate nohighlight">\(\boldsymbol{\nu}^k\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
   \begin{aligned}
   \text{Minimize} \quad &amp; \nabla f(\mathbf{x}^k)^T \Delta \mathbf{x} + \frac{1}{2} \Delta \mathbf{x}^T \nabla^2_{\mathbf{x}\mathbf{x}} L(\mathbf{x}^k, \boldsymbol{\lambda}^k, \boldsymbol{\nu}^k) \Delta \mathbf{x} \\
   \text{subject to} \quad &amp; \nabla \mathbf{g}(\mathbf{x}^k) \Delta \mathbf{x} + \mathbf{g}(\mathbf{x}^k) = \mathbf{0}, \\
   &amp; \nabla \mathbf{h}(\mathbf{x}^k) \Delta \mathbf{x} + \mathbf{h}(\mathbf{x}^k) \leq \mathbf{0}.
   \end{aligned}
   \end{split}\]</div>
<p>b. <strong>Solve the QP Subproblem:</strong>
Solve for <span class="math notranslate nohighlight">\(\Delta \mathbf{x}^k\)</span> and obtain the updated Lagrange multipliers <span class="math notranslate nohighlight">\(\boldsymbol{\lambda}^{k+1}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\nu}^{k+1}\)</span>.</p>
<p>c. <strong>Update the Estimates:</strong>
Update the primal variables and multipliers:</p>
<div class="math notranslate nohighlight">
\[
   \mathbf{x}^{k+1} = \mathbf{x}^k + \Delta \mathbf{x}^k.
   \]</div>
<p>d. <strong>Check for Convergence:</strong>
If <span class="math notranslate nohighlight">\(\|\Delta \mathbf{x}^k\| &lt; \epsilon\)</span> and the KKT conditions are satisfied, stop. Otherwise, set <span class="math notranslate nohighlight">\(k = k + 1\)</span> and repeat.</p>
</li>
<li><p><strong>Return:</strong>
<span class="math notranslate nohighlight">\(\mathbf{x}^* = \mathbf{x}^{k+1}, \boldsymbol{\lambda}^* = \boldsymbol{\lambda}^{k+1}, \boldsymbol{\nu}^* = \boldsymbol{\nu}^{k+1}\)</span>.</p></li>
</ol>
</section>
</div><section id="demonstration-with-jax-and-cvxpy">
<h4><span class="section-number">2.2.3.1. </span>Demonstration with JAX and CVXPy<a class="headerlink" href="#demonstration-with-jax-and-cvxpy" title="Link to this heading">#</a></h4>
<p>Consider the following equality and inequality-constrained problem:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\min_{x \in \mathbb{R}^2} \quad &amp; f(x) = (x_1 - 2)^2 + (x_2 - 1)^2 \\
\text{subject to} \quad &amp; g(x) = x_1^2 + x_2^2 - 1 = 0 \\
&amp; h(x) = x_1^2 - x_2 \leq 0
\end{align*}\]</div>
<p>This example builds on our previous one but adds a parabola-shaped inequality constraint. We require our solution to lie not only on the circle defining our equality constraint but also below the parabola. To solve the QP subproblem, we will be using the <a class="reference external" href="https://www.cvxpy.org/">CVXPY</a> package. While the Lagrangian and derivatives could be computed easily by hand, we use <a class="reference external" href="https://jax.readthedocs.io/">JAX</a> for generality:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">grad</span><span class="p">,</span> <span class="n">jit</span><span class="p">,</span> <span class="n">jacfwd</span><span class="p">,</span> <span class="n">hessian</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cvxpy</span> <span class="k">as</span> <span class="nn">cp</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define the objective function and constraints</span>
<span class="nd">@jit</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="nd">@jit</span>
<span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span>

<span class="nd">@jit</span>
<span class="k">def</span> <span class="nf">h</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>  <span class="c1"># Corrected inequality constraint: x[1] &lt;= x[0]^2</span>

<span class="c1"># Compute gradients and Jacobians using JAX</span>
<span class="n">grad_f</span> <span class="o">=</span> <span class="n">jit</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">))</span>
<span class="n">hess_f</span> <span class="o">=</span> <span class="n">jit</span><span class="p">(</span><span class="n">hessian</span><span class="p">(</span><span class="n">f</span><span class="p">))</span>
<span class="n">jac_g</span> <span class="o">=</span> <span class="n">jit</span><span class="p">(</span><span class="n">jacfwd</span><span class="p">(</span><span class="n">g</span><span class="p">))</span>
<span class="n">jac_h</span> <span class="o">=</span> <span class="n">jit</span><span class="p">(</span><span class="n">jacfwd</span><span class="p">(</span><span class="n">h</span><span class="p">))</span>

<span class="nd">@jit</span>
<span class="k">def</span> <span class="nf">lagrangian</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">,</span> <span class="n">nu</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">lambda_</span><span class="p">,</span> <span class="n">g</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">+</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">nu</span><span class="p">,</span> <span class="n">h</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">hess_L</span> <span class="o">=</span> <span class="n">jit</span><span class="p">(</span><span class="n">hessian</span><span class="p">(</span><span class="n">lagrangian</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">solve_qp_subproblem</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">,</span> <span class="n">nu</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">delta_x</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    
    <span class="c1"># Convert JAX arrays to numpy for cvxpy</span>
    <span class="n">grad_f_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">grad_f</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">hess_L_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">hess_L</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">,</span> <span class="n">nu</span><span class="p">))</span>
    <span class="n">jac_g_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">jac_g</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">jac_h_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">jac_h</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">g_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">g</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">h_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">h</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    
    <span class="n">obj</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Minimize</span><span class="p">(</span><span class="n">grad_f_np</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">delta_x</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">cp</span><span class="o">.</span><span class="n">quad_form</span><span class="p">(</span><span class="n">delta_x</span><span class="p">,</span> <span class="n">hess_L_np</span><span class="p">))</span>
    
    <span class="n">constraints</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">jac_g_np</span> <span class="o">@</span> <span class="n">delta_x</span> <span class="o">+</span> <span class="n">g_np</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">jac_h_np</span> <span class="o">@</span> <span class="n">delta_x</span> <span class="o">+</span> <span class="n">h_np</span> <span class="o">&lt;=</span> <span class="mi">0</span>
    <span class="p">]</span>
    
    <span class="n">prob</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Problem</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">constraints</span><span class="p">)</span>
    <span class="n">prob</span><span class="o">.</span><span class="n">solve</span><span class="p">()</span>
    
    <span class="k">return</span> <span class="n">delta_x</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">prob</span><span class="o">.</span><span class="n">constraints</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dual_value</span><span class="p">,</span> <span class="n">prob</span><span class="o">.</span><span class="n">constraints</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">dual_value</span>

<span class="k">def</span> <span class="nf">sqp</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x0</span>
    <span class="n">lambda_</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">nu</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
        <span class="n">delta_x</span><span class="p">,</span> <span class="n">new_lambda</span><span class="p">,</span> <span class="n">new_nu</span> <span class="o">=</span> <span class="n">solve_qp_subproblem</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">,</span> <span class="n">nu</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">delta_x</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
            <span class="k">break</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">delta_x</span>
        <span class="n">lambda_</span> <span class="o">=</span> <span class="n">new_lambda</span>
        <span class="n">nu</span> <span class="o">=</span> <span class="n">new_nu</span>
        
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">,</span> <span class="n">nu</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span>

<span class="c1"># Initial point</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>

<span class="c1"># Solve using SQP</span>
<span class="n">x_opt</span><span class="p">,</span> <span class="n">lambda_opt</span><span class="p">,</span> <span class="n">nu_opt</span><span class="p">,</span> <span class="n">iterations</span> <span class="o">=</span> <span class="n">sqp</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Optimal x: </span><span class="si">{</span><span class="n">x_opt</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Optimal lambda: </span><span class="si">{</span><span class="n">lambda_opt</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Optimal nu: </span><span class="si">{</span><span class="n">nu_opt</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Iterations: </span><span class="si">{</span><span class="n">iterations</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Visualize the result</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="c1"># Create a mesh for the contour plot</span>
<span class="n">x1_range</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">x2_range</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">X1</span><span class="p">,</span> <span class="n">X2</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x1_range</span><span class="p">,</span> <span class="n">x2_range</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">f</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">]))</span> <span class="k">for</span> <span class="n">x1</span> <span class="ow">in</span> <span class="n">x1_range</span><span class="p">]</span> <span class="k">for</span> <span class="n">x2</span> <span class="ow">in</span> <span class="n">x2_range</span><span class="p">])</span>

<span class="c1"># Plot filled contours</span>
<span class="n">contour</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">contour</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Objective Function Value&#39;</span><span class="p">)</span>

<span class="c1"># Plot the equality constraint</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">jnp</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">x1_eq</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
<span class="n">x2_eq</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1_eq</span><span class="p">,</span> <span class="n">x2_eq</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Equality Constraint&#39;</span><span class="p">)</span>

<span class="c1"># Plot the inequality constraint and shade the feasible region</span>
<span class="n">x1_ineq</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">x2_ineq</span> <span class="o">=</span> <span class="n">x1_ineq</span><span class="o">**</span><span class="mi">2</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1_ineq</span><span class="p">,</span> <span class="n">x2_ineq</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Inequality Constraint&#39;</span><span class="p">)</span>

<span class="c1"># Shade the feasible region for the inequality constraint</span>
<span class="n">x2_lower</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">x2_ineq</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x1_ineq</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">x2_lower</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hatch</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\\</span><span class="s1">/...&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Feasible Region&#39;</span><span class="p">)</span>

<span class="c1"># Plot the optimal and initial points</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_opt</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_opt</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Optimal Point&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x0</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x0</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Initial Point&#39;</span><span class="p">)</span>

<span class="c1"># Add labels and title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;x2&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;SQP for Inequality Constraints with CVXPY and JAX&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper center&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>

<span class="c1"># Set the axis limits explicitly</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Verify the result</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Equality constraint violation: </span><span class="si">{</span><span class="n">g</span><span class="p">(</span><span class="n">x_opt</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Inequality constraint violation: </span><span class="si">{</span><span class="n">h</span><span class="p">(</span><span class="n">x_opt</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Objective function value: </span><span class="si">{</span><span class="n">f</span><span class="p">(</span><span class="n">x_opt</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimal x: [0.7861514 0.618034 ]
Optimal lambda: [1.03215616]
Optimal nu: [0.5118832]
Iterations: 5
</pre></div>
</div>
<img alt="_images/8ec3fb9c4c78f9eb027dd3f9e2450e4de60e281a6948444785f2d9e84573efec.png" src="_images/8ec3fb9c4c78f9eb027dd3f9e2450e4de60e281a6948444785f2d9e84573efec.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Equality constraint violation: 0.000000
Inequality constraint violation: 0.000000
Objective function value: 1.619326
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="the-arrow-hurwicz-uzawa-algorithm">
<h3><span class="section-number">2.2.4. </span>The Arrow-Hurwicz-Uzawa algorithm<a class="headerlink" href="#the-arrow-hurwicz-uzawa-algorithm" title="Link to this heading">#</a></h3>
<p>While the SQP method addresses constrained optimization problems by sequentially solving quadratic subproblems, an alternative approach emerges from viewing constrained optimization as a min-max problem. This perspective leads to a simpler algorithm, originally introduced by the Arrow-Hurwicz-Uzawa <span id="id7">[<a class="reference internal" href="#id22" title="Kenneth J Arrow, Leonid Hurwicz, and Hirofumi Uzawa. Studies in linear and non-linear programming. Stanford University Press, 1958.">AHU58</a>]</span>. Consider the following general constrained optimization problem encompassing both equality and inequality constraints:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\min_{\mathbf{x}} \quad &amp; f(\mathbf{x}) \\
\text{subject to} \quad &amp; \mathbf{g}(\mathbf{x}) = \mathbf{0} \\
&amp; \mathbf{h}(\mathbf{x}) \leq \mathbf{0}
\end{aligned}
\end{split}\]</div>
<p>Using the Lagrangian function <span class="math notranslate nohighlight">\(L(\mathbf{x}, \boldsymbol{\lambda}, \boldsymbol{\mu}) = f(\mathbf{x}) + \boldsymbol{\lambda}^T \mathbf{g}(\mathbf{x}) + \boldsymbol{\mu}^T \mathbf{h}(\mathbf{x})\)</span>, we can reformulate this problem as the following min-max problem:</p>
<div class="math notranslate nohighlight">
\[
\min_{\mathbf{x}} \max_{\boldsymbol{\lambda}, \boldsymbol{\mu} \geq 0} L(\mathbf{x}, \boldsymbol{\lambda}, \boldsymbol{\mu})
\]</div>
<p>The role of each component in this min-max structure can be understood as follows:</p>
<ol class="arabic simple">
<li><p>The outer minimization over <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> finds the feasible point that minimizes the objective function <span class="math notranslate nohighlight">\(f(\mathbf{x})\)</span>.</p></li>
<li><p>The maximization over <span class="math notranslate nohighlight">\(\boldsymbol{\lambda}\)</span> ensures that equality constraints <span class="math notranslate nohighlight">\(\mathbf{g}(\mathbf{x}) = \mathbf{0}\)</span> are satisfied.</p></li>
<li><p>The maximization over <span class="math notranslate nohighlight">\(\boldsymbol{\mu} \geq 0\)</span> ensures that inequality constraints <span class="math notranslate nohighlight">\(\mathbf{h}(\mathbf{x}) \leq \mathbf{0}\)</span> are satisfied. If any inequality constraint is violated, the corresponding term in <span class="math notranslate nohighlight">\(\boldsymbol{\mu}^T \mathbf{h}(\mathbf{x})\)</span> can be made arbitrarily large by choosing a large enough <span class="math notranslate nohighlight">\(\mu_i\)</span>.</p></li>
</ol>
<p>Using this observation, we can devise an algorithm which, like SQP, will update both the primal and dual variables at every step. But rather than using second-order optimization, we will simply use a first-order gradient update step: a descent step in the primal variable, and an ascent step in the dual one. The corresponding procedure, when implemented by gradient descent, is called Gradient Ascent Descent in the learning and optimization communities. In the case of equality constraints only, the algorithm looks like the following:</p>
<div class="proof algorithm admonition" id="ahuz-eq">
<p class="admonition-title"><span class="caption-number">Algorithm 2.6 </span> (Arrow-Hurwicz-Uzawa for equality constraints only)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> Initial guess <span class="math notranslate nohighlight">\(\mathbf{x}^0\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\lambda}^0\)</span>, step sizes <span class="math notranslate nohighlight">\(\alpha\)</span>, <span class="math notranslate nohighlight">\(\beta\)</span>
<strong>Output:</strong> Optimal <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\lambda}^*\)</span></p>
<p>1: <strong>for</strong> <span class="math notranslate nohighlight">\(k = 0, 1, 2, \ldots\)</span> until convergence <strong>do</strong></p>
<p>2:     <span class="math notranslate nohighlight">\(\mathbf{x}^{k+1} = \mathbf{x}^k - \alpha \nabla_{\mathbf{x}} L(\mathbf{x}^k, \boldsymbol{\lambda}^k)\)</span>  <strong>(Primal update)</strong></p>
<p>3:     <span class="math notranslate nohighlight">\(\boldsymbol{\lambda}^{k+1} = \boldsymbol{\lambda}^k + \beta \nabla_{\boldsymbol{\lambda}} L(\mathbf{x}^{k+1}, \boldsymbol{\lambda}^k)\)</span>  <strong>(Dual update)</strong></p>
<p>4: <strong>end for</strong></p>
<p>5: <strong>return</strong> <span class="math notranslate nohighlight">\(\mathbf{x}^k\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\lambda}^k\)</span></p>
</section>
</div><p>Now to account for the fact that the Lagrange multiplier needs to be non-negative for inequality constraints, we can use our previous idea from projected gradient descent for bound constraints and consider a projection, or clipping step to ensure that this condition is satisfied throughout. In this case, the algorithm looks like the following:</p>
<div class="proof algorithm admonition" id="ahuz-full">
<p class="admonition-title"><span class="caption-number">Algorithm 2.7 </span> (Arrow-Hurwicz-Uzawa for equality and inequality constraints)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> Initial guess <span class="math notranslate nohighlight">\(\mathbf{x}^0\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\lambda}^0\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\mu}^0 \geq 0\)</span>, step sizes <span class="math notranslate nohighlight">\(\alpha\)</span>, <span class="math notranslate nohighlight">\(\beta\)</span>, <span class="math notranslate nohighlight">\(\gamma\)</span>
<strong>Output:</strong> Optimal <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\lambda}^*\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\mu}^*\)</span></p>
<p>1: <strong>for</strong> <span class="math notranslate nohighlight">\(k = 0, 1, 2, \ldots\)</span> until convergence <strong>do</strong></p>
<p>2:     <span class="math notranslate nohighlight">\(\mathbf{x}^{k+1} = \mathbf{x}^k - \alpha \nabla_{\mathbf{x}} L(\mathbf{x}^k, \boldsymbol{\lambda}^k, \boldsymbol{\mu}^k)\)</span>  <strong>(Primal update)</strong></p>
<p>3:     <span class="math notranslate nohighlight">\(\boldsymbol{\lambda}^{k+1} = \boldsymbol{\lambda}^k + \beta \nabla_{\boldsymbol{\lambda}} L(\mathbf{x}^{k+1}, \boldsymbol{\lambda}^k, \boldsymbol{\mu}^k)\)</span>  <strong>(Dual update for equality constraints)</strong></p>
<p>4:     <span class="math notranslate nohighlight">\(\boldsymbol{\mu}^{k+1} = [\boldsymbol{\mu}^k + \gamma \nabla_{\boldsymbol{\mu}} L(\mathbf{x}^{k+1}, \boldsymbol{\lambda}^k, \boldsymbol{\mu}^k)]_+\)</span>  <strong>(Dual update with clipping for inequality constraints)</strong></p>
<p>5: <strong>end for</strong></p>
<p>6: <strong>return</strong> <span class="math notranslate nohighlight">\(\mathbf{x}^k\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\lambda}^k\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\mu}^k\)</span></p>
</section>
</div><p>Here, <span class="math notranslate nohighlight">\([\cdot]_+\)</span> denotes the projection onto the non-negative orthant, ensuring that <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> remains non-negative.</p>
<p>However, as it is widely known from the lessons of GAN (Generative Adversarial Network) training <span id="id8">[<a class="reference internal" href="#id23" title="Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, volume 27. 2014.">GPAM+14</a>]</span>, Gradient Descent Ascent (GDA) can fail to converge or suffer from instability. The Arrow-Hurwicz-Uzawa algorithm, also known as the first-order Lagrangian method, is known to converge only locally, in the vicinity of an optimal primal-dual pair.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">grad</span><span class="p">,</span> <span class="n">jit</span><span class="p">,</span> <span class="n">value_and_grad</span>
<span class="kn">import</span> <span class="nn">optax</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define the objective function and constraints</span>
<span class="nd">@jit</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="nd">@jit</span>
<span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span>

<span class="nd">@jit</span>
<span class="k">def</span> <span class="nf">h</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>  <span class="c1"># Inequality constraint: x[1] &lt;= x[0]^2</span>

<span class="c1"># Define the Lagrangian</span>
<span class="nd">@jit</span>
<span class="k">def</span> <span class="nf">lagrangian</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">,</span> <span class="n">mu</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">lambda_</span><span class="p">,</span> <span class="n">g</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">+</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">h</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="c1"># Compute gradients of the Lagrangian</span>
<span class="n">grad_L_x</span> <span class="o">=</span> <span class="n">jit</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">lagrangian</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="n">grad_L_lambda</span> <span class="o">=</span> <span class="n">jit</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">lagrangian</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">grad_L_mu</span> <span class="o">=</span> <span class="n">jit</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">lagrangian</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>

<span class="c1"># Define the Arrow-Hurwicz-Uzawa update step</span>
<span class="nd">@jit</span>
<span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">carry</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">opt_state_x</span><span class="p">,</span> <span class="n">opt_state_lambda</span><span class="p">,</span> <span class="n">opt_state_mu</span> <span class="o">=</span> <span class="n">carry</span>
    
    <span class="c1"># Compute gradients</span>
    <span class="n">grad_x</span> <span class="o">=</span> <span class="n">grad_L_x</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">,</span> <span class="n">mu</span><span class="p">)</span>
    <span class="n">grad_lambda</span> <span class="o">=</span> <span class="n">grad_L_lambda</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">,</span> <span class="n">mu</span><span class="p">)</span>
    <span class="n">grad_mu</span> <span class="o">=</span> <span class="n">grad_L_mu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">,</span> <span class="n">mu</span><span class="p">)</span>
    
    <span class="c1"># Update primal variables (minimization)</span>
    <span class="n">updates_x</span><span class="p">,</span> <span class="n">opt_state_x</span> <span class="o">=</span> <span class="n">optimizer_x</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">grad_x</span><span class="p">,</span> <span class="n">opt_state_x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">optax</span><span class="o">.</span><span class="n">apply_updates</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">updates_x</span><span class="p">)</span>
    
    <span class="c1"># Update dual variables (maximization)</span>
    <span class="n">updates_lambda</span><span class="p">,</span> <span class="n">opt_state_lambda</span> <span class="o">=</span> <span class="n">optimizer_lambda</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">grad_lambda</span><span class="p">,</span> <span class="n">opt_state_lambda</span><span class="p">)</span>
    <span class="n">lambda_</span> <span class="o">=</span> <span class="n">optax</span><span class="o">.</span><span class="n">apply_updates</span><span class="p">(</span><span class="n">lambda_</span><span class="p">,</span> <span class="o">-</span><span class="n">updates_lambda</span><span class="p">)</span>  <span class="c1"># Positive update for maximization</span>
    
    <span class="n">updates_mu</span><span class="p">,</span> <span class="n">opt_state_mu</span> <span class="o">=</span> <span class="n">optimizer_mu</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">grad_mu</span><span class="p">,</span> <span class="n">opt_state_mu</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">optax</span><span class="o">.</span><span class="n">apply_updates</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="o">-</span><span class="n">updates_mu</span><span class="p">)</span>  <span class="c1"># Positive update for maximization</span>
    
    <span class="c1"># Project mu onto the non-negative orthant</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">opt_state_x</span><span class="p">,</span> <span class="n">opt_state_lambda</span><span class="p">,</span> <span class="n">opt_state_mu</span><span class="p">),</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">arrow_hurwicz_uzawa</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">lambda0</span><span class="p">,</span> <span class="n">mu0</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
    <span class="c1"># Initialize optimizers</span>
    <span class="k">global</span> <span class="n">optimizer_x</span><span class="p">,</span> <span class="n">optimizer_lambda</span><span class="p">,</span> <span class="n">optimizer_mu</span>
    <span class="n">optimizer_x</span> <span class="o">=</span> <span class="n">optax</span><span class="o">.</span><span class="n">adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">optimizer_lambda</span> <span class="o">=</span> <span class="n">optax</span><span class="o">.</span><span class="n">adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">optimizer_mu</span> <span class="o">=</span> <span class="n">optax</span><span class="o">.</span><span class="n">adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    
    <span class="n">opt_state_x</span> <span class="o">=</span> <span class="n">optimizer_x</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
    <span class="n">opt_state_lambda</span> <span class="o">=</span> <span class="n">optimizer_lambda</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">lambda0</span><span class="p">)</span>
    <span class="n">opt_state_mu</span> <span class="o">=</span> <span class="n">optimizer_mu</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">mu0</span><span class="p">)</span>
    
    <span class="n">init_carry</span> <span class="o">=</span> <span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">lambda0</span><span class="p">,</span> <span class="n">mu0</span><span class="p">,</span> <span class="n">opt_state_x</span><span class="p">,</span> <span class="n">opt_state_lambda</span><span class="p">,</span> <span class="n">opt_state_mu</span><span class="p">)</span>
    
    <span class="c1"># Use jax.lax.scan for the optimization loop</span>
    <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">),</span> <span class="n">trajectory</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span><span class="n">update</span><span class="p">,</span> <span class="n">init_carry</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_iter</span><span class="p">))</span>
    
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">trajectory</span>

<span class="c1"># Initial point</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
<span class="n">lambda0</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">mu0</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Solve using Arrow-Hurwicz-Uzawa</span>
<span class="n">x_opt</span><span class="p">,</span> <span class="n">lambda_opt</span><span class="p">,</span> <span class="n">mu_opt</span><span class="p">,</span> <span class="n">trajectory</span> <span class="o">=</span> <span class="n">arrow_hurwicz_uzawa</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">lambda0</span><span class="p">,</span> <span class="n">mu0</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Final x: </span><span class="si">{</span><span class="n">x_opt</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Final lambda: </span><span class="si">{</span><span class="n">lambda_opt</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Final mu: </span><span class="si">{</span><span class="n">mu_opt</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Visualize the result</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="c1"># Create a mesh for the contour plot</span>
<span class="n">x1_range</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">x2_range</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">X1</span><span class="p">,</span> <span class="n">X2</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x1_range</span><span class="p">,</span> <span class="n">x2_range</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">f</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">]))</span> <span class="k">for</span> <span class="n">x1</span> <span class="ow">in</span> <span class="n">x1_range</span><span class="p">]</span> <span class="k">for</span> <span class="n">x2</span> <span class="ow">in</span> <span class="n">x2_range</span><span class="p">])</span>

<span class="c1"># Plot filled contours</span>
<span class="n">contour</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">contour</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Objective Function Value&#39;</span><span class="p">)</span>

<span class="c1"># Plot the equality constraint</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">jnp</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">x1_eq</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
<span class="n">x2_eq</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1_eq</span><span class="p">,</span> <span class="n">x2_eq</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Equality Constraint&#39;</span><span class="p">)</span>

<span class="c1"># Plot the inequality constraint and shade the feasible region</span>
<span class="n">x1_ineq</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">x2_ineq</span> <span class="o">=</span> <span class="n">x1_ineq</span><span class="o">**</span><span class="mi">2</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1_ineq</span><span class="p">,</span> <span class="n">x2_ineq</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Inequality Constraint&#39;</span><span class="p">)</span>

<span class="c1"># Shade the feasible region for the inequality constraint</span>
<span class="n">x2_lower</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">x2_ineq</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x1_ineq</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">x2_lower</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hatch</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\\</span><span class="s1">/...&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Feasible Region&#39;</span><span class="p">)</span>

<span class="c1"># Plot the optimal and initial points</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_opt</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_opt</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Final Point&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x0</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x0</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Initial Point&#39;</span><span class="p">)</span>

<span class="c1"># Plot the optimization trajectory using scatter plot</span>
<span class="n">scatter</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">trajectory</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">trajectory</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">trajectory</span><span class="p">)),</span> 
                      <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;cool&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">scatter</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Iteration&#39;</span><span class="p">)</span>

<span class="c1"># Add labels and title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;x2&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Arrow-Hurwicz-Uzawa Algorithm with JAX and Adam (Corrected Min/Max)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper center&#39;</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.05</span><span class="p">),</span> <span class="n">ncol</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>

<span class="c1"># Set the axis limits explicitly</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="c1"># Verify the result</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Equality constraint violation: </span><span class="si">{</span><span class="n">g</span><span class="p">(</span><span class="n">x_opt</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Inequality constraint violation: </span><span class="si">{</span><span class="n">h</span><span class="p">(</span><span class="n">x_opt</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Objective function value: </span><span class="si">{</span><span class="n">f</span><span class="p">(</span><span class="n">x_opt</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Final x: [0.7861006 0.6180657]
Final lambda: [1.0322342]
Final mu: [0.5120836]
</pre></div>
</div>
<img alt="_images/2969475050904e8f5c16bbb03dd8909d609cffcd6749c6d5a33bb9a8e04280c4.png" src="_images/2969475050904e8f5c16bbb03dd8909d609cffcd6749c6d5a33bb9a8e04280c4.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Equality constraint violation: -0.000041
Inequality constraint violation: -0.000112
Objective function value: 1.619425
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="the-discrete-time-pontryagin-maximum-principle">
<h1><span class="section-number">3. </span>The Discrete-Time Pontryagin Maximum Principle<a class="headerlink" href="#the-discrete-time-pontryagin-maximum-principle" title="Link to this heading">#</a></h1>
<p>Discrete-time optimal control problems (DOCPs) form a specific class of nonlinear programming problems. Therefore, we can apply the general results from the Karush-Kuhn-Tucker (KKT) conditions to characterize the structure of optimal solutions to DOCPs in any of their three forms. The discrete-time analogue of the KKT conditions for DOCPs is known as the discrete-time Pontryagin Maximum Principle (PMP). The PMP was first described by Pontryagin in 1956 <span id="id9">[<a class="reference internal" href="#id24" title="Lev Semyonovich Pontryagin, Vladimir Grigor'evich Boltyanskii, Revaz Valerianovich Gamkrelidze, and Evgenii Frolovich Mishchenko. The Mathematical Theory of Optimal Processes. Interscience Publishers, 1962.">PBGM62</a>]</span> for continuous-time systems, with the discrete-time version following shortly after. Similar to the KKT conditions, the PMP is useful from both theoretical and practical perspectives. It not only allows us to sometimes find closed-form solutions but also inspires the development of algorithms.</p>
<p>Importantly, the PMP goes beyond the KKT conditions by demonstrating the existence of a particular recursive equation—the adjoint equation. This equation governs the evolution of the derivative of the Hamiltonian, a close cousin to the Lagrangian. The adjoint equation enables us to transform the PMP into an algorithmic procedure, which has much in common with backpropagation <span id="id10">[<a class="reference internal" href="#id25" title="David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating errors. Nature, 323(6088):533–536, 1986.">RHW86</a>]</span> in deep learning. This connection between optimal control theory has been noted by several researchers, including Griewank <span id="id11">[<a class="reference internal" href="#id26" title="Andreas Griewank. On automatic differentiation. Mathematical Programming: Recent Developments and Applications, 1989.">Gri89</a>]</span> in the context of automatic differentiation, and LeCun <span id="id12">[<a class="reference internal" href="#id27" title="Yann LeCun. A theoretical framework for back-propagation. Proceedings of the 1988 Connectionist Models Summer School, pages 21–28, 1988.">LeC88</a>]</span> in his early work on neural networks.</p>
<section id="pmp-for-mayer-problems">
<h2><span class="section-number">3.1. </span>PMP for Mayer Problems<a class="headerlink" href="#pmp-for-mayer-problems" title="Link to this heading">#</a></h2>
<p>Before delving into more general cases, let’s consider a Mayer problem where the goal is to minimize a terminal cost function <span class="math notranslate nohighlight">\(c_T(\mathbf{x}_T)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{alignat*}{2}
\text{minimize} \quad &amp; c_T(\mathbf{x}_T) &amp; \\
\text{such that} \quad 
&amp; \mathbf{x}_{t+1} = \mathbf{f}_t(\mathbf{x}_t, \mathbf{u}_t), &amp; \quad &amp; t = 1, \dots, T-1, \\
&amp; \mathbf{u}_{lb} \leq \mathbf{u}_t \leq \mathbf{u}_{ub}, &amp; \quad &amp; t = 1, \dots, T, \\
&amp; \mathbf{x}_{lb} \leq \mathbf{x}_t \leq \mathbf{x}_{ub}, &amp; \quad &amp; t = 1, \dots, T, \\
\text{given} \quad &amp; \mathbf{x}_1. &amp;
\end{alignat*}
\end{split}\]</div>
<p>As done previously using the single shooting method, we reformulate this problem as an unconstrained optimization problem (excluding the state bound constraints since we lack a straightforward way to incorporate them directly). This reformulation is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
J(\mathbf{u}_{1:T-1}) = c_T(\boldsymbol{\phi}_T(\mathbf{u}_{1:T-1}, \mathbf{x}_1)),
\end{align*}\]</div>
<p>where the state evolution functions <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_t\)</span> are defined recursively as:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\boldsymbol{\phi}_t(\mathbf{u}_{1:T-1}, \mathbf{x}_1) = 
\begin{cases}
\mathbf{x}_1, &amp; \text{if } t = 1, \\
\mathbf{f}_{t-1}(\boldsymbol{\phi}_{t-1}(\mathbf{u}_{1:T-1}, \mathbf{x}_1), \mathbf{u}_{t-1}), &amp; \text{if } t = 2, \ldots, T.
\end{cases}
\end{align*}\]</div>
<p>To find the first-order optimality condition, we differentiate the objective function <span class="math notranslate nohighlight">\(J(\mathbf{u}_{1:T-1})\)</span> with respect to each control variable <span class="math notranslate nohighlight">\(\mathbf{u}_t\)</span> and set it to zero:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial J(\mathbf{u}_{1:T-1})}{\partial \mathbf{u}_t} = \frac{\partial c_T(\boldsymbol{\phi}_T)}{\partial \mathbf{u}_t} = 0, \quad t = 1, \ldots, T-1.
\]</div>
<p>Applying the chain rule, we get:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial c_T(\boldsymbol{\phi}_T)}{\partial \mathbf{u}_t} = \frac{\partial c_T(\boldsymbol{\phi}_T)}{\partial \boldsymbol{\phi}_T} \frac{\partial \boldsymbol{\phi}_T}{\partial \mathbf{u}_t}.
\]</div>
<p>Now, let’s expand the derivative <span class="math notranslate nohighlight">\(\frac{\partial \boldsymbol{\phi}_T}{\partial \mathbf{u}_t}\)</span> using its non-recursive form. From the definition of the state evolution functions, we have:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\boldsymbol{\phi}_T = \mathbf{f}_{T-1}(\boldsymbol{\phi}_{T-1}, \mathbf{u}_{T-1}), \quad \boldsymbol{\phi}_{T-1} = \mathbf{f}_{T-2}(\boldsymbol{\phi}_{T-2}, \mathbf{u}_{T-2}), \quad \ldots, \quad \boldsymbol{\phi}_{t+1} = \mathbf{f}_t(\boldsymbol{\phi}_t, \mathbf{u}_t).
\end{align*}\]</div>
<p>The above can also be written more recursively. For <span class="math notranslate nohighlight">\(s \geq t\)</span>, the derivative of <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_s\)</span> with respect to <span class="math notranslate nohighlight">\(\mathbf{u}_t\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \boldsymbol{\phi}_s}{\partial \mathbf{u}_t} = \frac{\partial \mathbf{f}_{s-1}}{\partial \boldsymbol{\phi}_{s-1}} \frac{\partial \boldsymbol{\phi}_{s-1}}{\partial \mathbf{u}_t}, \quad s = t+1, \ldots, T,
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \boldsymbol{\phi}_t}{\partial \mathbf{u}_t} = \frac{\partial \mathbf{f}_{t-1}}{\partial \mathbf{u}_t}.
\]</div>
<p>The overall derivative is then of the form:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial J(\mathbf{u}_{1:T-1})}{\partial \mathbf{u}_t} = \underbrace{\underbrace{\underbrace{\frac{\partial c_T(\boldsymbol{\phi}_T)}{\partial \boldsymbol{\phi}_T}}_{\boldsymbol{\lambda}_T} \frac{\partial \mathbf{f}_{T-1}}{\partial \boldsymbol{\phi}_{T-1}}}_{\boldsymbol{\lambda}_{T-1}} \cdots \frac{\partial \mathbf{f}_{t+1}}{\partial \boldsymbol{\phi}_{t+1}}}_{\boldsymbol{\lambda}_{t+1}} \frac{\partial \mathbf{f}_t}{\partial \mathbf{u}_t}.
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\lambda}_t\)</span> is called the adjoint (co-state) variable, and contains the reverse accumulation of the derivative. The evolution of this variable also obeys a difference equation, but one which runs backward in time: the adjoint equation. The recursive relationship for the adjoint equation is then:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\lambda}_t = \frac{\partial \mathbf{f}_t}{\partial \boldsymbol{\phi}_t}^\top \boldsymbol{\lambda}_{t+1}, \quad t = 1, \ldots, T-1,
\]</div>
<p>with the terminal condition:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\lambda}_T = \frac{\partial c_T}{\partial \boldsymbol{\phi}_T}.
\]</div>
<p>The first-order optimality condition in terms of the adjoint variable can finally be written as:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial J(\mathbf{u}_{1:T-1})}{\partial \mathbf{u}_t} = \frac{\partial \mathbf{f}_t}{\partial \mathbf{u}_t}^\top \boldsymbol{\lambda}_{t+1} = 0, \quad t = 1, \ldots, T-1.
\]</div>
</section>
<section id="pmp-for-bolza-problems">
<h2><span class="section-number">3.2. </span>PMP for Bolza Problems<a class="headerlink" href="#pmp-for-bolza-problems" title="Link to this heading">#</a></h2>
<p>To derive the adjoint equation for the Bolza problem, we consider the optimal control problem where the objective is to minimize both a terminal cost <span class="math notranslate nohighlight">\(c_T(\mathbf{x}_T)\)</span> and the sum of intermediate costs <span class="math notranslate nohighlight">\(c_t(\mathbf{x}_t, \mathbf{u}_t)\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\text{minimize} \quad &amp; c_T(\mathbf{x}_T) + \sum_{t=1}^{T-1} c_t(\mathbf{x}_t, \mathbf{u}_t) \\
\text{such that} \quad 
&amp; \mathbf{x}_{t+1} = \mathbf{f}_t(\mathbf{x}_t, \mathbf{u}_t), \quad t = 1, \dots, T-1, \\
\text{given} \quad &amp; \mathbf{x}_1.
\end{align*}\]</div>
<p>To handle the constraints, we introduce the Lagrangian function with multipliers <span class="math notranslate nohighlight">\(\boldsymbol{\lambda}_t\)</span> for each constraint <span class="math notranslate nohighlight">\(\mathbf{x}_{t+1} = \mathbf{f}_t(\mathbf{x}_t, \mathbf{u}_t)\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
L(\mathbf{x}, \mathbf{u}, \boldsymbol{\lambda}) &amp;\triangleq c_T(\mathbf{x}_T) + \sum_{t=1}^{T-1} c_t(\mathbf{x}_t, \mathbf{u}_t) + \sum_{t=1}^{T-1} \boldsymbol{\lambda}_{t+1}^\top \left( \mathbf{f}_t(\mathbf{x}_t, \mathbf{u}_t) - \mathbf{x}_{t+1} \right).
\end{align*}\]</div>
<p>The existence of a feasible solution <span class="math notranslate nohighlight">\((\mathbf{x}, \mathbf{u})\)</span> implies that there exists a unique set of Lagrange multipliers <span class="math notranslate nohighlight">\(\boldsymbol{\lambda}_t\)</span> such that the derivative of the Lagrangian with respect to all variables equals zero:</p>
<div class="math notranslate nohighlight">
\[
\nabla L(\mathbf{x}, \mathbf{u}, \boldsymbol{\lambda}) = 0.
\]</div>
<p>To simplify, we rearrange the Lagrangian so that each state variable <span class="math notranslate nohighlight">\(\mathbf{x}_t\)</span> appears only once in the summation:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
L(\mathbf{x}, \mathbf{u}, \boldsymbol{\lambda}) &amp;= c_T(\mathbf{x}_T) + \sum_{t=1}^{T-1} \left( c_t(\mathbf{x}_t, \mathbf{u}_t) + \boldsymbol{\lambda}_{t+1}^\top \left( \mathbf{f}_t(\mathbf{x}_t, \mathbf{u}_t) - \mathbf{x}_{t+1} \right) \right) \\
&amp;= c_T(\mathbf{x}_T) - \boldsymbol{\lambda}_1^\top \mathbf{x}_1 + \sum_{t=1}^{T-1} \left( c_t(\mathbf{x}_t, \mathbf{u}_t) + \boldsymbol{\lambda}_{t+1}^\top \mathbf{f}_t(\mathbf{x}_t, \mathbf{u}_t) - \boldsymbol{\lambda}_t^\top \mathbf{x}_t \right).
\end{align*}\]</div>
<p>This follows from noting that:</p>
<div class="math notranslate nohighlight">
\[
\sum_{t=1}^{T-1} \boldsymbol{\lambda}_{t+1}^\top \mathbf{x}_{t+1} = \boldsymbol{\lambda}_T^\top \mathbf{x}_T - \mathbf{x}_1^\top \boldsymbol{\lambda}_1 + \sum_{t=1}^{T-1} \boldsymbol{\lambda}_t^\top \mathbf{x}_t.
\]</div>
<p>If <span class="math notranslate nohighlight">\((\mathbf{x}, \mathbf{u})\)</span> is a feasible local minimum, there exists a Lagrange multiplier <span class="math notranslate nohighlight">\(\boldsymbol{\lambda}\)</span> such that:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial L(\mathbf{x}, \mathbf{u}, \boldsymbol{\lambda})}{\partial \mathbf{x}_i} = 0.
\]</div>
<p>By differentiating the Lagrangian with respect to each state <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span>, we obtain:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial L(\mathbf{x}, \mathbf{u}, \boldsymbol{\lambda})}{\partial \mathbf{x}_i} = 
\begin{cases}
\frac{\partial c_T (\mathbf{x}_T)}{\partial \mathbf{x}_T} + \boldsymbol{\lambda}_T, &amp; \text{if } i = T, \\
\frac{\partial c_t(\mathbf{x}_t, \mathbf{u}_t)}{\partial \mathbf{x}_t} + \boldsymbol{\lambda}_{t+1}^\top \frac{\partial \mathbf{f}_t(\mathbf{x}_t, \mathbf{u}_t)}{\partial \mathbf{x}_t} - \boldsymbol{\lambda}_t, &amp; \text{if } i = 1, \dots, T-1.
\end{cases}
\end{split}\]</div>
<p>Rearranging gives the adjoint equations:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\lambda}_T = \frac{\partial c_T(\mathbf{x}_T)}{\partial \mathbf{x}_T},
\]</div>
<p>and for <span class="math notranslate nohighlight">\(t = T-1, \dots, 1\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\lambda}_t = \frac{\partial c_t(\mathbf{x}_t, \mathbf{u}_t)}{\partial \mathbf{x}_t} + \boldsymbol{\lambda}_{t+1}^\top \frac{\partial \mathbf{f}_t(\mathbf{x}_t, \mathbf{u}_t)}{\partial \mathbf{x}_t}.
\]</div>
<p>The optimality condition for the controls is obtained by differentiating the Lagrangian with respect to <span class="math notranslate nohighlight">\(\mathbf{u}_t\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial L(\mathbf{x}, \mathbf{u}, \boldsymbol{\lambda})}{\partial \mathbf{u}_t} = \frac{\partial c_t(\mathbf{x}_t, \mathbf{u}_t)}{\partial \mathbf{u}_t} + \boldsymbol{\lambda}_{t+1}^\top \frac{\partial \mathbf{f}_t(\mathbf{x}_t, \mathbf{u}_t)}{\partial \mathbf{u}_t} = 0.
\]</div>
<p>Note that the systems dynamics are respected, we must have:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial L(\mathbf{x}, \mathbf{u}, \boldsymbol{\lambda})}{\partial \boldsymbol{\lambda}_{t+1}} = \mathbf{f}_t(\mathbf{x}_t, \mathbf{u}_t) - \mathbf{x}_{t+1} = 0.
\]</div>
</section>
<section id="hamiltonian-formulation">
<h2><span class="section-number">3.3. </span>Hamiltonian Formulation<a class="headerlink" href="#hamiltonian-formulation" title="Link to this heading">#</a></h2>
<p>The first-order optimality condition for the Bolza problem obtained above can be expressed using the so-called Hamiltonian function:</p>
<div class="math notranslate nohighlight">
\[
H_t(\mathbf{x}_t, \mathbf{u}_t, \boldsymbol{\lambda}_{t+1}) = c_t(\mathbf{x}_t, \mathbf{u}_t) + \boldsymbol{\lambda}_{t+1}^\top \mathbf{f}_t(\mathbf{x}_t, \mathbf{u}_t).
\]</div>
<p>If <span class="math notranslate nohighlight">\((\mathbf{x}^*, \mathbf{u}^*)\)</span> is a local minimum control trajectory, then:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial H_t(\mathbf{x}_t^*, \mathbf{u}_t^*, \boldsymbol{\lambda}_{t+1}^*)}{\partial \mathbf{u}_t} = 0, \quad t = 1, \ldots, T-1,
\]</div>
<p>where the adjoint variables (costate vectors) (\boldsymbol{\lambda}_t^*) are computed from:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\lambda}_t^* = \frac{\partial H_t(\mathbf{x}_t^*, \mathbf{u}_t^*, \boldsymbol{\lambda}_{t+1}^*)}{\partial \mathbf{x}_t}, \quad t = 1, \ldots, T-1, \quad \boldsymbol{\lambda}_T^* = \frac{\partial c_T(\mathbf{x}_T^*)}{\partial \mathbf{x}_T}.
\]</div>
<div class="docutils container" id="id13">
<div role="list" class="citation-list">
<div class="citation" id="id21" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">AG90</a><span class="fn-bracket">]</span></span>
<p>E. L. Allgower and K. Georg. <em>Numerical Continuation Methods: An Introduction</em>. Volume 13 of Springer Series in Computational Mathematics. Springer-Verlag, Berlin, Heidelberg, 1990.</p>
</div>
<div class="citation" id="id22" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">AHU58</a><span class="fn-bracket">]</span></span>
<p>Kenneth J Arrow, Leonid Hurwicz, and Hirofumi Uzawa. <em>Studies in linear and non-linear programming</em>. Stanford University Press, 1958.</p>
</div>
<div class="citation" id="id23" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">GPAM+14</a><span class="fn-bracket">]</span></span>
<p>Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In <em>Advances in neural information processing systems</em>, volume 27. 2014.</p>
</div>
<div class="citation" id="id26" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">Gri89</a><span class="fn-bracket">]</span></span>
<p>Andreas Griewank. <em>On automatic differentiation</em>. Mathematical Programming: Recent Developments and Applications, 1989.</p>
</div>
<div class="citation" id="id14" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">HB68</a><span class="fn-bracket">]</span></span>
<p>Warren A. Hall and William S. Butcher. Optimal timing of irrigation. <em>Journal of the Irrigation and Drainage Division</em>, 94(2):267–275, June 1968. URL: <a class="reference external" href="http://dx.doi.org/10.1061/JRCEA4.0000569">http://dx.doi.org/10.1061/JRCEA4.0000569</a>, <a class="reference external" href="https://doi.org/10.1061/jrcea4.0000569">doi:10.1061/jrcea4.0000569</a>.</p>
</div>
<div class="citation" id="id16" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">Hol92</a><span class="fn-bracket">]</span></span>
<p>John H Holland. Genetic algorithms. <em>Scientific american</em>, 267(1):66–73, 1992.</p>
</div>
<div class="citation" id="id19" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>IRS20<span class="fn-bracket">]</span></span>
<p>Fedor Iskhakov, John Rust, and Bertel Schjerning. Machine learning and structural econometrics: contrasts and synergies. <em>The Econometrics Journal</em>, 23(3):S81–S124, August 2020. URL: <a class="reference external" href="http://dx.doi.org/10.1093/ectj/utaa019">http://dx.doi.org/10.1093/ectj/utaa019</a>, <a class="reference external" href="https://doi.org/10.1093/ectj/utaa019">doi:10.1093/ectj/utaa019</a>.</p>
</div>
<div class="citation" id="id18" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">KE95</a><span class="fn-bracket">]</span></span>
<p>James Kennedy and Russell Eberhart. Particle swarm optimization. In <em>Proceedings of ICNN'95-International Conference on Neural Networks</em>, volume 4, 1942–1948. IEEE, 1995.</p>
</div>
<div class="citation" id="id17" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">KGJV83</a><span class="fn-bracket">]</span></span>
<p>Scott Kirkpatrick, C Daniel Gelatt Jr, and Mario P Vecchi. Optimization by simulated annealing. <em>science</em>, 220(4598):671–680, 1983.</p>
</div>
<div class="citation" id="id27" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">LeC88</a><span class="fn-bracket">]</span></span>
<p>Yann LeCun. A theoretical framework for back-propagation. <em>Proceedings of the 1988 Connectionist Models Summer School</em>, pages 21–28, 1988.</p>
</div>
<div class="citation" id="id20" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">OR70</a><span class="fn-bracket">]</span></span>
<p>J. M. Ortega and W. C. Rheinboldt. <em>Iterative Solution of Nonlinear Equations in Several Variables</em>. Computer Science and Applied Mathematics. Academic Press, New York, 1970.</p>
</div>
<div class="citation" id="id24" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">PBGM62</a><span class="fn-bracket">]</span></span>
<p>Lev Semyonovich Pontryagin, Vladimir Grigor'evich Boltyanskii, Revaz Valerianovich Gamkrelidze, and Evgenii Frolovich Mishchenko. <em>The Mathematical Theory of Optimal Processes</em>. Interscience Publishers, 1962.</p>
</div>
<div class="citation" id="id25" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">RHW86</a><span class="fn-bracket">]</span></span>
<p>David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating errors. <em>Nature</em>, 323(6088):533–536, 1986.</p>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Introduction</p>
      </div>
    </a>
    <a class="right-next"
       href="cocp.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">4. </span>Continuous-Time Trajectory Optimization</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">1. Discrete-Time Trajectory Optimization</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reduction-to-mayer-problems">1.1. Reduction to Mayer Problems</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-methods-for-solving-docps">2. Numerical Methods for Solving DOCPs</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#single-shooting-methods">2.1. Single Shooting Methods</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dealing-with-bound-constraints">2.1.1. Dealing with Bound Constraints</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#on-the-choice-of-optimizer">2.1.2. On the choice of optimizer</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#constrained-optimization-approach">2.2. Constrained Optimization Approach</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nonlinear-programming">2.2.1. Nonlinear Programming</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#karush-kuhn-tucker-kkt-conditions">2.2.1.1. Karush-Kuhn-Tucker (KKT) conditions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#lagrange-multiplier-theorem">2.2.1.2. Lagrange Multiplier Theorem</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#newton-s-method">2.2.1.3. Newton’s Method</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#efficient-implementation-of-newton-s-method">2.2.1.3.1. Efficient Implementation of Newton’s Method</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-equality-constrained-programs-with-newton-s-method">2.2.1.4. Solving Equality Constrained Programs with Newton’s Method</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#demonstration">2.2.1.4.1. Demonstration</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-sqp-approach-taylor-expansion-and-quadratic-approximation">2.2.2. The SQP Approach: Taylor Expansion and Quadratic Approximation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-newton-s-method-in-the-equality-constrained-case">2.2.2.1. Connection to Newton’s Method in the Equality-Constrained Case</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sqp-for-inequality-constrained-optimization">2.2.3. SQP for Inequality-Constrained Optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#demonstration-with-jax-and-cvxpy">2.2.3.1. Demonstration with JAX and CVXPy</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-arrow-hurwicz-uzawa-algorithm">2.2.4. The Arrow-Hurwicz-Uzawa algorithm</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#the-discrete-time-pontryagin-maximum-principle">3. The Discrete-Time Pontryagin Maximum Principle</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pmp-for-mayer-problems">3.1. PMP for Mayer Problems</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pmp-for-bolza-problems">3.2. PMP for Bolza Problems</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hamiltonian-formulation">3.3. Hamiltonian Formulation</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Pierre-Luc Bacon
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>