{"version":2,"myst":"1.6.3","options":{"logo":"/logo-439a25d1a718f5f15fe2838e2a61a91a.png","logo_text":"RL & Control"},"nav":[],"actions":[{"title":"View on GitHub","url":"https://github.com/pierrelux/rlbook","internal":false,"static":false}],"projects":[{"bibliography":["/Users/pierre-luc.bacon/Documents/rlbook/references.bib"],"math":{"\\Proj":{"macro":"\\mathsf{P}"},"\\Residual":{"macro":"\\mathsf{N}"},"\\Contraction":{"macro":"\\mathsf{T}"},"\\Bellman":{"macro":"\\mathsf{L}"},"\\BellmanPi":{"macro":"\\mathsf{L}_\\pi"},"\\BellmanQuad":{"macro":"\\widetilde{\\mathsf{L}}"},"\\EulerResidual":{"macro":"\\mathsf{H}"},"\\Policy":{"macro":"\\pi"}},"exports":[{"format":"pdf","filename":"book.pdf","url":"/book-bceca6481db041911b084ebd03cc1269.pdf"}],"title":"Reinforcement Learning Beyond the Agent Loop: Models, Control, and Data","description":"A graduate-level introduction to reinforcement learning as a framework for modeling, optimization, and control, connecting dynamic models, data, and applications beyond standard benchmarks.","authors":[{"nameParsed":{"literal":"Pierre-Luc Bacon","given":"Pierre-Luc","family":"Bacon"},"name":"Pierre-Luc Bacon","id":"contributors-myst-generated-uid-0"}],"github":"https://github.com/pierrelux/rlbook","id":"3c112bb5-93ac-429f-8216-c0b9947c00d4","toc":[{"file":"intro.md"},{"children":[{"file":"modeling.md"},{"file":"dynamics.md"},{"file":"simulation.md"}],"title":"Modeling"},{"children":[{"file":"trajectories.md"},{"file":"collocation.md"}],"title":"Numerical Trajectory Optimization"},{"children":[{"file":"mpc.md"},{"file":"dp.md"}],"title":"From Trajectories to Policies"},{"children":[{"file":"smoothing.md"},{"file":"projection.md"},{"children":[{"file":"montecarlo.md"},{"file":"fqi.md"},{"file":"amortization.md"},{"file":"pg.md"}],"title":"Simulation-Based Methods"}],"title":"Approximate Dynamic Programming"},{"children":[{"file":"appendix_examples.md"},{"file":"appendix_ivps.md"},{"file":"appendix_nlp.md"}],"title":"Appendix"}],"index":"index","pages":[{"level":1,"title":"Modeling"},{"slug":"modeling","title":"Why Build a Model? For Whom?","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"dynamics","title":"Dynamics Models for Decision Making","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"simulation","title":"Programs as Models","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Numerical Trajectory Optimization"},{"slug":"trajectories","title":"Discrete-Time Trajectory Optimization","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"collocation","title":"Trajectory Optimization in Continuous Time","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"From Trajectories to Policies"},{"slug":"mpc","title":"Model Predictive Control","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"dp","title":"Dynamic Programming","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Approximate Dynamic Programming"},{"slug":"smoothing","title":"Smooth Bellman Optimality Equations","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"projection","title":"Weighted Residual Methods for Functional Equations","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":2,"title":"Simulation-Based Methods"},{"slug":"montecarlo","title":"Monte Carlo Integration in Approximate Dynamic Programming","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"fqi","title":"Fitted Q-Iteration Methods","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"amortization","title":"Fitted Q-Iteration for Continuous Action Spaces","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"pg","title":"Policy Gradient Methods","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"level":1,"title":"Appendix"},{"slug":"appendix-examples","title":"Example COCPs","description":"","date":"","thumbnail":"/heat_exchanger-acfdd83b1501b4c220f686fe21df7820.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"appendix-ivps","title":"Solving Initial Value Problems","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"appendix-nlp","title":"Nonlinear Programming","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2}]}]}