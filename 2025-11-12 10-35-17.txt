I'm going to say you've got to find out, you know, if you push it left or right. If you push it right. Looking at the other one. Hey, the first one you got off there, you know, what the hell broke when you said that? Oh, there. Yeah, but here's the...  So, I'm going to be doing... I'm going to take that one. So, I'm going to start the one there. Oops, oops. Which one is it? Okay. I've uploaded, by the way, the lectures from a couple of classes ago. Going back to October. So, sometimes the sound is a bit faint. Maybe I should, like, be pointing that microphone over there. They used to be, like, a portable microphone. Anyways, I think you can hear. But you just have to crank up the sound a little bit. And the camera was pointing when it was present, it was pointing here. So, maybe, now that I moved it, maybe we'll get to see a little bit what I'm doing on the board. Actually, maybe I could zoom in a little bit, too, to make it better. I guess the recordings are more for you to know what's being covered more than actually really deeply understanding. Okay. I really don't have a lot of whiteboard space to it. It's annoying. Okay. I should have pushed the latest changes. And let me go back to overestimation bias. I'm going to try to do a better job this time. I think there are lots of subtleties into this. So, it's easy to miss the technicalities as to what's going on for real and why the method we're proposing actually works. So, let's go back to what we're doing. So, here we're not even talking about using projection methods or minimum residual methods to do function approximations. So, imagine everything here is tabular. So, we're doing the updates at every single state and action pairs. But we're doing this with the Monte Carlo method to deal with the problem of numerical integration. So, we're just going to talk about what that additional challenge means. And so, you can imagine when you combine Monte Carlo integration in addition to function approximation, things might get even more difficult. But we can study those things in isolation. Okay. So, what we're doing here is that we're going to be substituting. Whenever we have a term that looks like this, remember there's a continuation value, which is an integral over the next state. So, we evaluate the value function at the next state, condition on a state and action pair. So, this is what our notation means here. So, it's a condition. Really, this is a conditional expectation, by the way. And so, and then we use the sample average estimator. Simply, we take like n realization of the next states. We average them. At least conceptually, that's how we do in practice. You know, we might not be able to actually stop the simulator and gather n samples. But it's another discussion. And then, the actual Bellman operator, which now we denote as L hat, so that it's the empirical Bellman operator using n samples, is going to use that sample average instead of the actual integral. Now, what's great with this is that actually it allows us to be what we call model free, which means that all we require is to, sorry, I've lost, all we require is actually to be able to sample from the transition kernel, but we don't need to know the actual probabilities. So, all you've got to give me is a black box. A black box where you enter some statement action pairs. And then, the black box outputs the next state. This is what we call model free, and it's a setting that has been the favorite setting for most of the reinforcement learning literature over the last three decades. Right. So, the problem here is actually that this estimator in isolation, there's no issue with this. As you take the expectation of mu hat n, this will be unbiased. So, in the limit of the n going to infinity, this estimator is going to give us the right expectation. The problem is at this level, the fact that actually then this estimator, which by the way is a random variable, whenever you sample n states and you repeat that process, you're going to get a different number. This thing here is a random variable. And as you plug it inside this update, you add this constant and you take the max. This is now a non-linear transformation. And the resulting random variable, we write it as a function evaluated at state s, but really you think of this whole thing here as a random variable as well. The random variable here won't be unbiased. It won't be an unbiased estimator of LV at s, the actual Bellman operator. That's a bit surprising given the fact that we're just doing sample average here, but the max causes a lot of issues here. To understand why it's causing issues, we've got to write down what actually we mean by a biased and unbiased estimator. So, the thing we want to study is the expectation of that LNV at s. So, plug it in there. And the inequality that we then derive is actually the following one. So, we could actually skip directly from this to this by simply noting, well, if you know that beforehand, that actually the max over action here is a convex function. And the theorem you need to use here would be to note that actually this is Jensen's inequality. Or, with the English accent, it's Jensen inequality, but I think it's pronounced Jensen. And so, what's the difference here? So, here, this is the expectation over the max, whereas this is really the max of an expectation. So, this inequality, we could get zero shot, basically. What we did last time is simply to reuse that little property that we had derived earlier, which was to say that let's ignore the expectation for a sec. We have this little inequality that if you have a collection of objects and you take the maximum, then sort of a trivial inequality is to say that the maximum is going to be greater or equal than any item out of this collection, including a star, which is the maximum. And we can preserve this inequality by taking the expectation on both sides, and we can do this because the expectation is a monotone transformation, and so it allows us to do this. So, this is kind of an argument that does not rely on Jensen, but ultimately it's the same thing going on. It's Jensen's inequality that tells us this. I've added the little note here because Diego pointed out that actually we had done something similar like this, and indeed, we had it in theorem three of the Peirce chapter. We use a property like this when establishing that when doing dynamic programming, it was sufficient for us to just consider the class of deterministic policies rather than the full-blown class of randomized policies. And that was the same theorem being used there. Of course, the theorem is a bit different. Theorem three says something else, whereas here we're talking about the noise inherent to the approximation of the target value. So, it's not that we had derived that result earlier, but we used a similar tool. Things get even worse as we apply that biased procedure recursively. So, when we do a value iteration, we use the empirical Bellman operator, and because we are going to feed back the output as the input at the next iteration, the bias will just get into play. It's something that you really see in practice. If you ever play with these algorithms in practice, you're going to quickly realize that oftentimes your Q values are really off from what they should be. In some cases, it doesn't matter. Sometimes, it doesn't matter. The absolute values of your Q values may be really, really far off, yet you might still be able to retain the policy. You might be able to retain the optimal policy, but that would be a bit of a gamble. Does anyone have an intuition as to why it could be that your Q values are actually way off, probably due to the amplification of the overestimation bias, yet you might still be able, in some cases, to retain an optimal policy? Yeah, exactly, the ranking. Actually, really what matters, and we see that a lot in RL, is the action selection doesn't care about what are the absolute values. All it cares about is actually ranking them. A quantity of interest and a concept that our colleague, Amir Rasoud, now who's a professor at Polytechnique, has been developing a lot is this notion of the action gap, which is a distance between the optimal solution and the next value in the ranking. That action gap tends to be a determining quantity in many of the hardness bounds for solving MDPs. Problems for which the action gap is very large tend to be easier in a way, whereas those for which the action gap is small need more work, because we need to be able to disambiguate the two values in the ranking. Nevertheless, overestimation bias might really cause you some problem and could lead to very suboptimal behaviors, because of the noise and the fact that we now select actions in a noisy fashion. I've added another way of thinking about this, which is maybe a bit more explicit. We can think of these estimators, a sample average estimator, and we can decompose it in this way. We're not making any strong assumption on what's the distribution of the noise, but we write the estimator as the mean, the actual quantity of interest, plus some noise. What we say is, however, that the noise is zero mean. We're not saying it's Gaussian, but it's zero mean. We can rewrite what's going on out there in the following way. What we're describing in this section is going to be a technique. We talked, by the way, about learning the bias as a way that people in the econometrics community have studied. The most common approach for tackling this in the RL community is using this general strategy of decoupling selection and evaluation. What do we mean here by selection and evaluation? By selection, we mean the choice, this operation here of picking the actual action A. This is selection. This thing, we're going to decouple it by basically drawing two independent samples of the continuation value. What we need to stare at is here. This is consensual. Algorithmically, we'll think it'll be a bit different, but statistically, that's what's at play here. Imagine that now you carry two experiments in parallel. Remember that mu is the expected continuation value. It's the sample average of the integral. The way it works is you condition on S and A, and you pick a bunch of next state samples. That gives you an estimate of the expected next value. Then you repeat this twice. Let's say you use unsampled here and unsampled there. You do that process independently. This is very important. You sample these two things independently. That gives you two values. Mu hat n1, mu hat n2. Both of them have their own noise associated with them because we sampled them independently. This noise variable depends on the action. Whenever we take the action, we repeat that sample. We sample n, n realizations. We average again. That gives you our values. What we're going to do now is the following trick. What's hidden in this expression 50, but we're going to see it in 51, is we're going to nest two estimators. This part here, if we map it in the language of selection and evaluation, what would that be? Do you think? The decoupling really is what line 50 is showing. This is the selection process here. We select our optimal action A star according to mu n1. Although the notation doesn't suggest it like this, keep in mind that mu n1 is a random variable. As we compute arg max of blah blah of a random variable, A star is also a random variable. We're going to keep that in mind. A star is a random variable. It's a random variable which is going to be produced by an independent process from the second process, which is the process of evaluation. Y now is a new random variable. That random variable is perceived as whole. Now, we plug directly A star, which we've obtained here from mu 1, and we plug it into mu n2. Here, we're not searching over arg max. We're just saying, pick action, let's say, number three, which I deem to be optimal, and then give me a fresh bunch of mu n samples so that we can now evaluate what's the value of that action number three, which I think is optimal. There's a nesting of really this. We can maybe write as y is equal to blah blah of arg max here and there. If we replace the qualities here in y by their definition, we have that. We have arg s a plus gamma of mu s a epsilon 2, but actually now it's epsilon at A star. A star, again, we got it here. Now, we're going to study the bias of the random variable y. We're going to take the expectation around y, but now we've got two sources of randomness. Really, as we take the expectation by the law of total expectation, there's a nesting of the sources of randomness. This follows from the law of total expectation. The inner expectation is the expectation condition on the first stage selection process. The outside is selection. Second stage is evaluation. Let's study this thing a little bit and look at the inner expectation. These quantities here, condition on A star, r, and this here doesn't change at all. We're left with this and this. By the linearity of expectation, we can split this expectation into three terms, study them independently. The expectation of r, condition on A star, is r at A star. The expectation of mu at A star, condition on A star, is this. Then, there's this guy. We need to think about it more carefully. We have an equality here, so it means that somehow this term, expectation of the noise given A star, must have gone to zero. Why would that be the case? Let's think about that a bit carefully. It really depends on two things. One of them is the conditional independence assumption. The second thing has to do with that zero noise property, which is not really a stringent condition, but one that we've assumed earlier. Remember how the noise variable of the second state, where is it coming from? The way it works is we lay out all of our actions, and then we sample a bunch of next states. We do that basically ahead of time. That sampling of the next state that gives us epsilon 2, we're going to be doing it for all actions anyway. There's no conditioning. There's no dependence on how we sample next state due to A star. These two sampling processes, really, they're independent from one another. Remember the definition of conditional independence, what it means here is that the expectation of epsilon A star, and again, remember that A star is a random variable, the condition on A star is simply going to be equal to the expectation of epsilon evaluated at this particular A star that you gave me. It's a conditional independence property, and we have this assumption earlier that the expectation of the noise variable was zero. All in all, what we've established here is that by doing this nesting of estimators, where we select with one independent sample and we evaluate with another n independent samples, we've established that this estimator is unbiased. That was the quantity of interest. Now, line 53 is expectation of epsilon 2, but keep in mind, actually, though, that the full analysis we wanted to do here was to study the process, the nested process, so the joint distribution over epsilon 1, epsilon 2. This thing here is going to be unbiased, but we're still left with some expectation over epsilon 1. There's really two sources of bias, and W2 doesn't really tackle both at the same time. This is what I'm saying here. At least we're getting rid of one source of bias. Still left with some bias, but we've controlled it a lot, just through conditional independence. Now, in practice, you're not going to be doing this thing. of actually interrupting the rollouts you're taking in the environment and thinking and fresh samples. This is more of a conceptual perspective. The way that's going to work algorithmically is that you're going to be maintaining instead two copies of your estimates of the Q values, which will play a kind of a similar role as this independent sampling procedure that I described. However, we won't be able to control and guarantee as much the fact that we've got an actual conditional independence happening between those two, but the same phenomenon will still take place by maintaining these two copies and one that you use for you're maintaining two copies in parallel and you use one for selection, the other one for evaluation. I think what's missing here that I missed is that you're also going to be swapping it once in a while. You're going to change the roles. You're going to be using one for evaluation, you have one for selection, vice versa, and that swapping of the roles will allow us to decouple, to create that independence that we need for removing that source of bias. So this is the double Q strategy and it's very, very much used in practice because all it involves is actually maintaining a separate copy of your network. It can come with some memory costs, but it's usually not a big deal for most of RL research applications, which are not super memory hungry compared to, let's say, for that. Let's now jump into the algorithms themselves. You thought that this would be just an actual theory course, but I swear by the end of this course, or maybe today, you're gonna, you should know in practice what is NFQI, FQI with extra trees, you should know about DQM, soft Q learning, Q learning, the traditional Watkins Q learning, pretty much all that. What I want to share with you is going to be this, well, I don't like these unified view titles. I'm guilty. I've written papers with that term. Sometimes these unified views, they tend to be overly, over-promising in a way. I still think it's a pretty general perspective I'm going to be The pattern we're going to choose here is that of fitted Q iteration. We're doing function iteration, but we're doing it through projections. We keep on fitting our operator. Remember again why we're doing this. Of course, we want to do machine learning, so that's one of the reasons. Another way that I like to explain things is that, let's say you work over V values, but keep in mind, you can work over Q values too. And can anyone remember us why we want to work with Q values sometimes instead of V? This side. Does anyone can remind us? Is it because the only thing I think about is that it is easier to select the action using the Q values, but I'm not sure. Yes, you can do an arc map directly in the Q values. There you go. There you go. If Q is given, you just need to arc map. If Q is not given, you just need to arcmap.  If Q is not given, you could also do an arc map, but what's the problem? It's just that at inference time, let's say, you will need to carry around your model or to do numerical integration on the fly. Not practical. It's very costly. If you're model free, you can't do it too. This is arc max, oops, max A of R plus gamma integral V S prime. Yes, that's something I know. It's not convenient, so work with Q. Okay, so I was saying it doesn't matter, but let's say we work with V values. The mathematical operation we'd like to do is this. Let's write it with an arrow to make it clearer because it's an assignment. Pick a current guess of a function V, and then we use L to transform that function. Think of it as a function transformation process. Then we assign Vk plus one to that L. What's the challenge? The challenge, of course, is that if the state space is continuous or very large, you cannot do this assignment everywhere. However, we can compute this operator pointwise. That's fine. Give me an S, I can compute it. Sure, it might have an integral, but we can do it. What I cannot do is carry out the update. I'm going to write it like this right now, but I cannot do that for all S in the state space. It's just computationally infeasible. So the story I get here, and it's really, again, it's the same thing as a minimum residual framework. I just find this story to be maybe algorithmically more intuitive. So what is it that I do? Well, if I can compute the Bellman operator at some designated points, I just pick a bunch of them. Let's say I pick N. We're going to call them base points. So and then I compute the Bellman operator at these N base points. And then what is it that I do afterwards? Imagine you're a machine learning hacker. All you know in life is PyTorch, and then you want to find a solution to this using the tools you know. So I fit, I pose that, and I do, in other words, I try to generalize it. From the ground truth outputs that I'm computing, I'm just trying to guess what things ought to be around. And if you've got structure, and if the problem is learnable in some way, you should be able to generalize, given the inductive biases you've got. This is not linear, but it becomes a regression problem. So what I'm going to do is I'm going to create a data set of tuples, just like I do in supervised learning, of tuples of base points. This is the input. And output. And I have a data set like this of N such input-output pairs. And maybe let's be even a bit clearer here. This is how I define my data set at step k. My data set will keep on changing. The output, the targets, will keep on changing. This is a regression problem where this is x, you know, call it x. This is your y variable. And then I create that data set. And what is it that I do? Well, I compute fit. Fit on that data set at step k. And our computers, when we fit on our network, we don't return the full function. What is it that we return? Typically, we're going to be returning a set of parameters. This is exactly the projection framework story, except that it's generalized here, assuming that we can also have nonlinear function approximators. Fit can be random forest, can be neural network. It can use gradient descent or not. And we're even more general here in that we don't even say that actually fit needs to be using L2 loss, by the way. It could be using some other loss function. Yeah? I don't want to go to backpropagate the target. Because what we're doing here is value iteration. We're trying to do approximate value iteration. This is within a loop. So let's say you're doing it for m steps. So it's a nested optimization procedure. Yeah? What do you choose? So here, the SU eval will be oftentimes samples along your distributions as you execute your policy. This would be in the online setting. You could also just have it handed to you as an offline data set. All right. So these are chosen points, chosen states that have been observed. Absolutely. Absolutely. And you can see also how overestimation might become even more of an issue there. In a way, this is the ultimate old generalization setting. Say if we're in the offline setting that Tom just described, you're given some poor quality offline data set where you've just seen stuff in the same region, yet you're trying to imagine what would be the value at places you've never been that would give rise to behavior that is clearly different from the sampling distribution. That's a challenge. Definitely a challenge. Which is also why, as of now, offline reinforcement learning is not that great. Made some progress, but it's not that great. And it's always a discussion that I have to have with the partnership team at Mila. I like to work with companies and stuff, but they're oftentimes coming to me with a supervised learning mentality to tell me they have a data set or what kind of data do you have. We're really in a game where, of course, we want to work from data, but offline data sets are tricky, too. Which is why I want to give you tools to go beyond being purely model-free so that you can use physics models, physics-informed stuff like this, trying the devices, simulators, this and that. On states which are not seen in the data set, would you expect the value to be overestimated? Absolutely. Absolutely. Overestimated. So a large part of the literature on offline RL has to do with making sure that the updates you carry are in the support of the data. So one of the big advances that has been made recently was by a student, Miguel, a master's student. I was a PhD student when Scott Foguiboto worked on this, and Scott wrote the paper on TD3, which became very important. Yeah, was it in TD3? Scott made some important contribution, and part of it was to implement this idea that you want to make sure that things you make are supported by the data. In addition to doing tricks that look essentially like W2, but fancier. So TD3 is implementing a lot of those things. Another design choice, which we're going to add here, is when you fit typically, and really this is like scikit-learn Fitbit, any algorithm of scikit-learn that gives you a supervised learning interface is a valid choice for you to implement a fitted Q or fitted value iteration method. So in my consulting job, by the way, whenever we have things that look like RL problems, my suggestions for the data scientists when they start their job is to benchmark it with the simplest thing. And the simplest thing is to reuse their toolbox of data science, but then wrap it in a FQI loop like this. Because then all you got to do is code up a data set transformation procedure. This is really all that's good. This is really, it's a repeated, we're solving a sequence of regression problem. We're not solving a single one. And that goes back to the question also of what happens if you're back problem, you know. Really, if what you want to do is approximate value iteration, you're solving a sequence of regression problem, not a single one, because you're making up your own targets, which is also another big distinction with supervised learning. It's that feedback loop that gets created because we create our Y variables. We're not giving them. If we were given samples of the true value function, there wouldn't be much to do. That's a big difference with RL and supervised learning. We're not given the target. We're getting rewards, but rewards, as you know, are loosely correlated with values. Rewards are immediate points. By the way, this procedure here, let's say that you fit with the linear regression method with non-linear features, and you initialize your weight vector to be zero. What is the first iteration of FBI or FQI? What is it going to be doing? What is it going to stare at it for a second? What is it that we're going to be fitting? Anyone heard? What is it that we're going to be fitting in the first step? Are we going to be fitting the dynamics? Are we going to be fitting the discount factor? Are we going to be fitting the rewards? Why? Because L, remember, is R plus expected next value. We use a function approximator, which has zero weights, linear. The inner product of the features and the weight vector, which is zero, will give us zero values. The first step will be that we're going to be trying to fit the immediate reward function. Then things will bootstrap from there. It's also why this procedure here, sometimes we call that phenomenon, what we're doing here, making up our own targets from our own targets, the phenomenon of bootstrapping. It's a terminology that Sutton has introduced. We're bootstrapping our estimates. Bootstrap from R, and then from R, we make a new target. And then we bootstrap from the old value, V, to make new targets. What would, let's go back again to why not backprop through this thing. If, well, I'm telling you that backpropping through it would not be coherent with the perspective of fitting value iteration, of the fact that you want to do value iteration approximately. However, it would be consistent with another perspective. What is the complementary tool to the successive approximation method? Let's rewind, go back to minimum residual. We said there are two main approaches, although I've been focusing mostly on successive approximation. What would be the second class of method, and the second way that you could go about solving the discretized functional equation problem? Newton methods to treat as a root-finding problem. And in that case, we're getting close to your perspective. If you backprop through it, that becomes a lot like treating your problem as a root-finding problem, but not using second order information. So that would be very much similar to this. And in the RL community, this has been described and credited oftentimes to John Baird. I think it's John, yeah. Baird. And that's in 95, if I remember correctly. And this is called, I don't know, I think he calls it residual Bellman equation or residual methods. It's another term, another use of residual here, but it's different. It's a different context, and it's been studied, but not used in popular, been popular in RL. The reason is subtle, maybe you can go later with this, but the long story short is if you work under the assumption that you're model-free, and a very sudden assumption to make is that the life goes on in one long trajectory for which you have no control over. Sutton is a cognitive psychologist, a cognitive science person, but he has literally a bachelor in psychology. So that's his whole career has been about formalizing what human intelligence means algorithmically. But for him, it's really strict about this principle. Humans live only one life, and they cannot stop their life and then try out different outcomes. The time arrow goes one way. So in the strict Suttonian perspective of RL, you don't have what we call resets, abilities to stop at one state and sample a bunch of potential outcomes. But clearly, it's an assumption that is being made that for solving practical problems, it's not necessarily something you might be facing. If you're the person controlling the simulator, the person holding up the simulator, you can certainly pause the simulator. a bunch of samples. What would you know? Now going back to Baird, there's a subtlety here in the updates of the algorithm of Baird that backdrops to the targets, such that you would have to take independent samples if you want to be unbiased. And in the strict settlement view, that's a no-no. But in a way, it's a made-up restriction. And because of that, although it's been popularized in 95, the majority of the RL community, which is closely aligned with Rich Hutton's way of thinking, has an inflected that perspective. Oh, we need to do double sampling, and we're not allowed. It's a taboo. So we have very few of those algorithms now. There's been, you know, interest recently around these things. I mean, recently, I'm getting old, but for me now, recently, it's 2017, but I guess that's ancient now. It's one of those algorithms that addresses that double sampling trick using some optimization business. There's been probably more, but one of them is based on Fenchel duality. Terrible name. It's unpronounceable. Zbid. Not speed, but zbid. It's one of them. I think it's 2017. And if you're interested, anyway, they get rid of that double sampling by rewriting the optimization problem and leveraging Fenchel duality such that you've got a square function that you can get rid of by Fenchel duality. Long story short, there is some work on minimum residual that kind of corresponds to this idea of back propping, but not that much. Not that much because of this, I'd say, taboo. It's my interpretation of the events. So yeah. But clearly, if you implement DQN and these sort of things, well, DQN wants to be an approximate value iteration algorithm. It really wants to be that. So if you're back propping threat, you're no longer doing value iteration. You're doing something else, but it doesn't fit that framework anymore. All right. So the last design decision I wanted to, well, there are going to be a bunch of them, but that fit procedure really, and it's going to be important because if we want to lay out the full spectrum of algorithms, we're going to consider one more important modification to that. It's going to be fit, and then it's going to have another parameter here, which is a guess. Yes. Let me call it like, I'm going to write it as if it's Python. So I'm going to write it as, I hope it's not too convoluted, but theta is a zero, like a guess. And then I can pass it an optional guess. So in other words, what we're going to be allowing ourself to do is to warm start, and that's going to be an important design consideration. Because clearly, let's say you refit the neural network across time, well, that'd be kind of wasteful to just reset from initialization randomly at every fit. Might as well start from the last effort to the last fit, because most likely that's going to be a good initialization. So very natural design decision, same way as an MPC where we oftentimes do warm start, although in MPCs, it's more for performance reason, whereas here, well, performance also. But especially in the nonlinear deep learning setting, it's really about starting in a right, in a good place on the loss landscape. So with this, we'll keep on adding some parameters to that, and you'll see we'll be able to explain all of it, all the algorithms. All right, take a break. We'll be back in 10 minutes. Yeah, it's all uploaded. It is? Yeah, I did yesterday. I'm just waiting for the interview for this course in 10 minutes, I assure you. The interview is in 10 minutes, I got it. Come on, go, go. Tell me everything you have to say. It's kind of absurd. Yeah. Yeah. Next, next. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah.  Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Mm.  Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm.  Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm.  Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm. Mm.   I think the new RAM to the West Island has opened also today or yesterday. Five days. Okay. I've never tried the RAM at all. It looks fun. When it works. But does it stop at the airport or it's not, it's probably not finished, eh? In a decade, probably. One day. 2027, so 2030 probably. It's going to be amazing, though. So we're here. It's a bit messy.  I have to erase some stuff. Okay. I think of this as value iteration. Keep on making new data set. You refit it and you refit it sometimes with a previous guess. If you do gradient-based optimization, that's quite easy to do. In other cases, it might not be as intuitive.  So this is not all models and scikit-learn support Worm Start, but some of them do. And in particular, this first one, which I want to highlight, it's often in the literature for some reason. Oftentimes when people talk about FQI, which as you might notice here, I use FQI in kind of a general way. But oftentimes when people cite the literature, they say FQI, they will then cite Damien Ernst 2005. Yet these ideas in the RL community are much older than that. As you know, it was described already in Gordon 95. So it's just that Damien, I think, popularized it. Damien Ernst is a researcher in Belgium, and he's always been really a very unique RL researcher. While everybody in the RL community were following the computational neuroscience kind of agenda of Rich Sutton, Damien Ernst was trying to get RL to work for his concrete applications. At this time in 2005, I'm not too sure what he was working on, but I know that after this he started working on grid applications and controlling the grid. So he's an engineer by training. It's always been very practical. And maybe it got popular because actually it was truly kind of working on real applications. And it showed the world that you could solve real problems, not just toy problems. Or at least things that look like real problems back then. So what is Damien Ernst's FQI? It's this pattern. But FIT uses random forests. Another thing to know, actually, Damien Ernst also proposed and introduced a variant of random forests in 2004. And I think just because he did that in 2005, it was like, all right, I have my own favorite model now. It's called extremely randomized random forests, extra trees. So I'm going to use extra trees for my algorithm. It's kind of historically how it happened, 2004, 2005. So Damien Ernst's FQI used extra trees. They are implemented in Scikit-learn, by the way. But they're essentially a variant of random forests. And I'm trying to remember exactly. So in random forests, what is it that you randomize again? You randomize the features you pick for your splits. And what's the other layer of randomization he has? I think he randomizes the thresholds as well. Two levels of randomizations. And honestly, we don't hear too much about extra trees. I'm not sure if you've ever used it in Scikit-learn. But it's in there. It's in there. Anyway, I think of FQI as random forests, collection trees. And random forests are pretty strong as, you know, one of the first things you want to try when you do data science is, so nowadays people do XGBoost. But random forests are pretty good in general as well, especially for tabular data. So it's a really good baseline, FQI. And the way that Damien Ernst presented it too, he really did some marketing around the idea that at least conceptually, this algorithm works. You could feed it a fixed offline data set. Let's just maybe change things a little bit here. Because if you want to derive a policy and all you got is a data set, we'd still be facing this issue of not knowing what the data set is. transition probabilities are. So in order to do, to actually use the policy you found, you would typically want to work over Q values instead, right? In that case the base points are just going to be now tuples of state-action pairs. I'm just going to add another parenthesis here. All right. And of course we're going to be doing multicolor integration. So the operator we're going to use here is the empirical downline operator. All right. And let's actually, I think that's one aspect which maybe is not explicit in my big table of all design choices. For the majority of work in RL, and I should add a note in the text to say why that probably is, LN usually for most of what we see is going to be one. Why one? It goes back to the story I told you just before the break, that a lot of the model-free work works under the assumption that you are not allowed to do resets. Your life is a trajectory, a single trajectory. So in that perspective, then your Monte Carlo estimator of the Bellman operator, what is L hat N1 at B? Okay, let's write it explicitly. Let's say we're at step K, okay? And we look at this thing, let's use, yeah, we said we use Q, and then we look at some state SMA. So this is going to be Q. So this is where also you might want to collect rewards in your data set, which you can do here. If you know the function, if you know the reward function and you have it as a piece of code or a formula, then you could plug your samples SMA in there and get your rewards. Or you could also just use samples of R that your black box has computed for you. You can be fully model-free in that manner. So typically, we would write this instead as R. Okay, I'm going to put my indices again here so that we remember we're using samples. Ri plus gamma, and then here, you still have an integral, a conditional expectation. But I'm going to use a sample average instead, with one sample. Actually, no summation in there. All right? And then what is it going to be? What is it going to be like? Well, it's going to be my Q. It's K. But usually here, what I would have is V. But we know that V and Q's are related from one another as follow. Max A prime of Q K S I A I. Sometimes the states and the iterates might coincide. If you're strictly online, you take one sample, you update. These two sets of indices will coincide. But in the case here that we're doing, it's more like batch mode, then they don't need to coincide. So yeah. So this is going to be what my data set's going to look like. And in a way, in order to compute this thing here, your data set, you're going to be doing it from your, oops, this should be actually S prime I. Sorry, it's not the previous notation, but this is the next state. This is what I want to say, right? And this is A, sorry, A prime. Okay? So really the way you'd want to organize your data in all of these algorithms would be as quadruples of state. I write I here to mean the I-th sample. The action I've observed under that state. The reward I've observed by taking action A in state S. And the observed next state, S prime I. All right? So when I say, like, FQI and these other algorithms are, like, offline, could be working offline data, it means they would be working over So you're going to keep on creating new targets in that manner. So if I want to be more explicit, right, the data set D here would be constructed like that. So I'd be mapping from SA to RI plus gamma max A prime of QK S prime I A prime. Okay. So I have a bunch of quadruples I've collected, historical data, and then I keep on solving these regression problems using my favorite procedure that may be a neural network or not, that could use gradient descent or not. If you're using random forest, what's the optimizer for random forest? It's a greedy optimization-like procedure. It's not gradient descent, because it's a non-differentiable process. There's been work, by the way, on coming up with smooth approximation to random forest. As far as I know, they don't work as well as the greedy variant, but the procedure for fitting a random forest is not doing gradient descent under the hood, if you remember your fundamental machine learning course. Yeah, that's a pretty good template here, so let's keep it there. So data in urns is fit is going to be an extra tree, and we feed it an offline data set. We don't recollect new base points along the way. I can't remember if in the 2005 paper he does explain explicitly whether you can use warm starting or not. It might be like half a sentence where he says you can do it. I know from personal interaction with him, because I was trying to debug my own algorithm as a master's student using a QI, I emailed him, and he told me, yeah, I think that it's very important in my 2005 paper was to warm start my random forest, and there's a way to do it. Back then, scikit-learn was not exposing the same warm starting strategy as what he was telling me, so I had some back and forth back then. Anyways, in principle, you could also warm start, but the simplest description of FQI looks like the following. So it's an offline method. So after this paper, a lot of people in clinical applications, clinical research, started using RL for their problem using FQI, because in many of those settings, they've got, let's say, a cohort of patients, and they've got historical data, they rarely have the ability to go in and collect data online and sample transitions dynamically. So it's a method that's been used a lot. My postdoc advisor, Emma Brunskill, was using this extensively for quite a while, clinical applications, but she was mostly doing education-like education, like e-learning. Intelligent tutoring systems, and then she's done a lot of that. All right, let's talk now about a very, very important paper that got swept under the rug after DQM. However, the paper does cite it, I think, properly, but then in the way that history is told, it's rarely, sometimes forgotten. This algorithm is called NFQI, Neural Fitted Q iteration, which, if my memory is correct, is 2009, although I think I've seen earlier presentations of that idea by Red Miller. I think it kind of happened at the same time as Damien Ernst, and Red Miller just studied this thing in the context of neural networks, which in 2009 were still not that great. In 2006, we had restricted Boltzmann machines. Things started working a little bit by doing layer-wise retraining. It was getting there, but not quite. Still very popular, so this was Red Miller. He went to work at DeepMind afterwards, so he's still there, by the way. And it's probably on the DQM paper. I can't remember, but it's not the first one. So Red Miller took this, actually. What did he do? Neural network. I read it carefully last year, that paper, so there might be a few more things. Let's see. Yeah, yeah, I re-read it. Okay, that's really all it does. NFQI, but again, very important. I want you to remember this. This is an outer, inner-level, it's a nested loop. There's a repeat here, and then fifth year will be another loop. I didn't write it explicitly, but when you use a neural network, you're going to be doing gradient descent. It's going to have its own inner loop. Back then, RMSProp was not a thing, and Adam was not a thing. They had their own optimizer. I can't remember. It's not really relevant anymore. AdaProp, maybe. Doesn't matter. It's gradient descent. In my reading of the paper, it seems to imply that he was doing full optimization, because when we say fit, really, we're solving the L2 regression problem. We want to do it until convergence. There's a thing that has been done here. No, it's fine, it's fine, okay. No, gradient descent is not written here. We wrote argument, but really, to argument this whole thing, we'd be doing it by gradient descent. It's a regression problem, a neural network. So we're doing deep RL back in 2009, even probably earlier. It's one of the papers we like to cite the most when we talk about this. Any reaction on this idea while I take a sip of my coffee? Let's add some more options. The other option we're going to add is, I'm going to call it for now, number of iterations. Let's say I do truncated, fix the budget of the number of gradient descent steps I want to be doing to minimize my L2 loss. Okay, so N enter, and I'm going to call it, we have I, we have K, we have N, M. I have the data set that keeps on changing. I have the word starting. Here on the outside, there's a set of base points, which is different from the data set D by the way. I think it's preferable to call them base points, A, S, S prime, R, S prime. So this is on the outside. It's provided as an input to that procedure. All right, and now let's play with this parameter over there, the number of iterations. One extreme, we take one gradient step, and then we recompute our data set, and then we keep on warm starting. It's kind of one extreme. DQN, you can think of it as this, but where you do truncated iterations, truncated optimization, but also going to allow yourself to gather a new sample as you go along. This is very important difference from NFQI. NFQI, at least, it highly suggests that it's as you go along, but it's mostly offline. DQN takes NFQI and says, really, maybe that's the biggest difference, says, we're going to be collecting data online. It's not going to be a set of base points from the outside. Whenever I do an update here, it gives me an estimate of Q, which gives me a policy, and I'll use that policy to now gather new samples. It means that my data set will keep on growing over time, and to manage that growing data set, what is it that we're going to be doing? I could keep it all. That's one solution. The other solution is what to do. Yeah, so you use a circular buffer, so you override the dot, and just the way that we do mini-batches in supervised learning, you've got a batch of data, you sample uniformly for it, you're going to be sampling from a circular buffer. That's a replay buffer strategy. Okay, so we need to add, really, the difference is we've got replay, which is online data collection on a circular buffer. Case, well, I'm going to use K here, but truncated initialization. That's it. Notice, there's no notion of target networks here. Why? One of the selling points of BQN is that it uses a target network. You don't even have to talk about this, because by construction, this algorithm already uses this notion of a target network. What is a target network? It's the fact that we don't backprop to this thing here, because really, when we use a neural network, it might be more adequate to actually write it like this. My network takes as an input a set of parameters, we evaluate that SNA, and then we fit, gives you a new set of parameters. We're going to be maintaining these two sets of parameters in time. Although these ones, we treat them as frozen, stop gradient, but this is by construction. It is what this thing implies. If you call this up this way in JAX, there's not even a need to put a stop gradient in your code. The whole, by the way, I'm kind of anti-stop gradient, because it messes up our understanding of calculus. Stop gradient is a made-up concept. There's no mathematical need for that concept. You can re-express everything that stop gradient does through the notion of partial derivative. That's all we need, and this is what's going on here. When we take a derivative of a loss, we take it with respect to our parameter argument, not the data argument. Hence, the reason why there's no need to even have stop gradient. It's not a discussion, it's a rant I could have, but it's a very bad habit. It's a habit that comes from the view of deep learning as the computational graph perspective, I think, has led to that paradigm in frameworks like TensorFlow. I don't know if PyTorch, I have no use for PyTorch, but TensorFlow used to be that same. I want to take the gradient of that tensor variable with respect to another tensor variable. Mathematically, you don't take derivatives of a variable with respect to another variable. You take the derivative of a function with respect to an argument of that function. Anyway, not important for this discussion here. So DQM, you take NFQI, which means that the fit is gradient descent. You do truncated gradient descent for the budget of your choosing, so it means you don't do convergence to the regression problem perfectly. And we gather a sample in a growing data set, but a data set whose size is managed by using a circular buffer. This is a replay buffer, and that gives you DQM. Now, the way that DQM is written as an algorithm, if you open up a pseudocode in the 2013-14 paper, you might not see it written in the same way. It's because, there you go, this is the section here I wrote. You can basically nest these two optimization loops into only one. You can flatten these two loops into one. And this is just a little algorithmic exercise that, using modular arithmetic, you can just flatten everything into one single loop where there is a condition in which I'm going to swap my target. I'll let you maybe read those things a bit more carefully, because I don't know how else I can explain it, but this would be the explicit outer-inner structure. Outer loop here is the outer loop of the value iteration. Inner loop is the inner loop of L2 minimization. By the way, DQM uses a different loss, not quite the L2, but for all practical purposes it's the same. And then this is what it would look like. This is gradient descent over L2 loss, and see, like, it's the partial derivative of the loss with respect to the theta argument. It's a partial derivative, it's not a total derivative. It's also another source of confusion I find. Mixing up total versus partial derivative. This matlab data says the partial derivative with respect to that. Hence, there's no back-propping through D here. This is unambiguous as an expression. So this optimization procedure here is going to be doing k-step of optimization from the same initialization and with the same fixed data set. You can flatten this into only one loop, because what's going to happen is as I finish this loop over there, next time I go there, what changes that now? I rebuild a new data set. That's all that means here. So the flattening of that procedure can be rewritten in this way, whereas now at every step of that other loop, I do gradient descent, but I do my swap here of my target network. Okay, DQM, let's talk about which one. Oh yeah, the other design choice is you might also want to pick the smooth Bellman operator rather than the hard Bellman operator. Do smooth Bellman with extra trees. It's never been written in the paper, and now it's too obvious to write a paper about this. Maybe back in 2006, you could have done it, but not anymore. But now, you know that that's a perfectly valid choice. There's no name for that algorithm. Smooth Bellman equation with extra trees. Smooth Bellman equation with a neural network and an online data collection and truncated partial inner optimization. That, there's a name for this. Yeah. We're just too much in a hurry. We're just, we're a way through. So there's going to be a deeper explanation to that. And the deeper explanation I can give to you is this distinction in stochastic optimization problem by, the stochastic optimization problem would look like something of an expectation of a function f of x where x is a random variable. And this thing has theta in there. And then we want to do min theta of this, this would be stochastic optimization. There are two ways you could go about this to do the same. Form an empirical surrogate to this by the sample average. The surrogate. This is actually exactly like empirical risk minimization. Okay, we could do this and then minimize the surrogate. This is called sample average approximation, the SAA method. The other method is where you'd be doing this, using n sample, the surrogate. You would be only using one sample. This is called stochastic approximation. And in stochastic approximation, the intuition is that you don't want to be wasting time to do an average ahead of time. You're going to be fine with only one sample, which by the way is an unbiased estimator of that expectation, but a high variance, it's unbiased. But you're going to be averaging instead through time across your iterates in time. In practical terms that you might know, this is a mini-batch versus SGD, which sometimes you say n is equal to one. I think it's so, so SGD. This is exactly the same thing. Now, yeah, go back to this. Same story. Same story. It's like stochastic approximation versus SAA. However, here the problem would be fixed in that the loss function wouldn't change. Here, because we change our data set, we're chasing a moving target. So the SAA perspective is true with the additional complexity that now you're also tracking a non-stationary distribution. Convergence can still be shown, by the way, for SAA in the non-stationary case. It's a whole technicality about learning rates. But yeah. Okay. So if you use DQM and instead you do it over a smooth Bellman equation, you get essentially an algorithm that was proposed in 2017. It's the smooth Q-learning algorithm. No, not Subtexture Critic. No. No, that one is good too. But I'd have to think about it where it fits in that perspective. Bad consistency in learning is like smooth Q-learning, assuming deterministic transitions, which allows you to make simplifications to the form of your updates. Because things are deterministic and you know exactly what's next. This one didn't last very long because no, not smooth Q-learning. Yeah, I used soft, but indeed it's soft. So 2018, there was this AC and I think this one has been largely been forgotten afterwards. A thing that they did, which maybe is why the algorithm was not so popular, is that they tackled the full-blown continuous MDP case. And so not only they had to deal with an integral for the next states, which they dealt with using Monte Carlo the way you were used to, that's fine. But also they considered continuous actions. And because of that, in order to deal with the continuous action spaces, they use... Let me remember, they use, what is it called again? The thing from physics, where we'll find it. There you go. Stein variational gradient descent. It's very fancy. It's very fancy. So, but quite complicated to explain, but it's a fancy bell and whistle on this algorithm. Stein variational gradient descent is a sampling approach that looks like gradient descent. Essentially, it's gradient descent with constant learning rate that you can show that asymptotically this thing is going to be sampling from the distribution of interest. And they use that in order to sample and deal with the problem of numerical integration and to integrate the softmax policy, which if you have a continuous action space, the softmax policy is an energy-based policy. It's intractable. So to sample from that energy-based policy, they use Stein variational gradient descent kind of FCMC approach. So that approach is very, very expensive, but you could do smooth Q-learning in the discrete case, and you wouldn't have to do this if your action space is quite small, and then it would be perfectly fine. It would be like DQN, but with the smooth Bellman equation. It's a very simple change in your code. Just the way that you build the data set, and when you need to gather new actions from your policy, you don't derive the policy by taking the softmax policy at all. That would be smooth or soft DQN. Hasn't been published, but I think it's obvious afterwards. Am I missing something else in my table? I want to say double DQN is DQN using the double Q trick that we talked about. We presented it in the case where you assume tabular representation, but you could do it if you have function approximators. You're maintaining two sets of networks, and then you use one for evaluation and one for selection, the other one for evaluation, and you swap the roles. And that's it. That's already like a good decade of influential deep learning algorithms, which in my opinion, if you read carefully the unified perspective, there's no need for me to dedicate one course for each of them because if you see the big pattern, they're just special cases. Not to diminish their importance, and the importance of special algorithmic details and sometimes hyperparameters if you want to get the same performance, but the big picture on them is mostly captured through this characterization I'm giving you here. All right. Let's talk about traditional. Oh, you can use EMA and something else. Don't care about this for now. Let's talk about again this thing here and go to traditional algorithms. So let's consider the case where it's not a neural network. We do a linear regression with nonlinear features. And we do online data collection. Not only this, we discard all of the experience we collect, and we only keep the most recent experience. So a circular buffer with a capacity of one transition. And we do one step of inner optimization. That's going to give us this algorithm over there, the update on line 61 is an update where the S, A, and S prime, A prime, well, actually not A prime, but all of these are obtained on the fly, online, along the trajectory of your current policy. It's a special case of that. And this algorithm there, 61, is a classic algorithm. If you read Sutton and Barthold, that's what you're going to learn. And that algorithm is called Q-learning by Watkins. And does anyone know what's the reference for Watkins Q-learning? It's 89. So a bit of an old algorithm. It was born in 88. So even older than that, 36 years old, I guess. I'm running out of time. Watkins. I really wish someone would take care of that for me. Watkins. He wrote that in his PhD thesis and became very influential afterwards. And for the majority of RL researchers that have been trained starting from Watkins 89 all the way to BQM 2014-13, your perspective on RL was embodied to algorithms like this. These are strict, strict online algorithms. You don't retain any information except the current sample. And you do an update. And immediately afterwards, you do another update as you go along. And so the way that people got to the point of BQM 2013 back then when it was introduced, the mental model of RL was this. So you can see that when you view BQM and you compare it with this, they're like, wow, BQM has so many more. It's really like a paradigm shift from this. And it's quite a paradigm shift because it's essentially an extreme case of this general procedure, which is that infinite value iteration. The way that you analyze and think of this extreme case is through the lens of stochastic approximation, SA. Stochastic approximation is what I've told you earlier. But another more general perspective on stochastic approximation is it's a tool to find the roots of a function. A function is an expectation. And you want to find the roots of that expectation via samples of the product distribution underlying the distribution. So root finding, your brain goes Newton's method. Yes, except that Newton's method on a noisy sample is not great. It would be expensive and quite sensitive to the noise. Stochastic approximation is like a first order Newton's method that has been specifically analyzed and designed for finding the roots of a noisy function. 61 is the stochastic approximation step applied to the root finding problem, which is that of minimizing the expected squared residual of the Bellman equation. So if you use your intuition from supervised learning, what's the downside of 61? In terms of, yeah. Yes, but let's say at a deeper level, why wouldn't it be data efficient? I guess each of these has a lot of values. Yeah, exactly. So it's just like SGD and the discussion about mini-batches. So these algorithms are really not data efficient because of that bias variance trade-off. But again, from a strict Satanian perspective, this is the only game you're allowed to play. So that's important to keep in mind sometimes when you read some of these papers or the papers that come from the group around there at U of A, because it's the game they play. If you want to solve applications, that might not be a restriction you need to have. So when you read the textbooks about RL written by OR people, oh, I'm done. I'm sorry. I'll just finish here. By OR people, it's why it's called here the section simulation-based approximate dynamic programming. You read Berserkius, you read Warren Powell, and so on. And for them, RL, things like this, or any of the stuff that uses multicolored integration, well, it's just approximate dynamic programming, but where you use a simulator to solve your problem. They really view this really pragmatically like this. Let's say I have a complicated functional equation I want to solve. I'm going to do function approximation, use Galerkin, whatever. There are a bunch of nasty integrals. It's all too complicated for me. It's too computationally heavy. Or I might say all I got is actually a simulator or program that computes output without necessarily having an explicit mathematical representation of the transition probabilities. Hence, I'm going to do simulation-based approximate dynamic programming. So from this more pragmatic perspective of the OR people, well, you're a simulator. You can do whatever you want. It's a piece of code that you own that runs on a computer. You can reset it if you want. It's a simulator. It's a digital twin. It's a substitute to something real. If you're in the business of solving real practical problem, then think about it this way, simulation-based approximate DP. Do what you want with a simulator. But a Sartanian RL person wouldn't want to be, wouldn't adopt this perspective because the simulation exists on their computer just because we're writing papers and we need to have experiments. But really, in the Sartanian perspective, there is only one problem, and that problem is the real life. The real life is not in a simulator. Hence, there's no such thing as resetting your simulator, drawing multiple samples, because there's one life, one trajectory. That's the whole philosophy around this, and I think it's important to keep that in mind, to give you context around why some choices were made. And I don't want you to be stuck in those design choices that were made because of this other research agenda, which is a respectable research agenda. I don't have any problem with this, but the rules that have been set have been set for a reason, and we need to be mindful of that. Now, just to conclude, again, one more sentence. If you're Sartanian, I guess Sartanian is a term that I made up and some other people, I guess, sometimes might call it this way. It's not yet written in a textbook like this, Sartanian RL, but he's really distinct about his way of thinking about things. So in Sartanian RL, probably he would say that these restrictions are necessary to get a true AGI, because these are the inherent limitations of our own human intelligence, and human intelligence, in a way, is the gold standard, is the thing we want to algorithm. We would want to put these kind of limitations of one life, one check, three kind of thing. Maybe, but to me, this is too much on the speculation side. I'm not willing to lock myself in to these principles out of speculation. Voil. Let's go back over this again on Monday, next week. Thank you.