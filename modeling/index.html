<!DOCTYPE html><html lang="en" class="" style="scroll-padding:60px"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width,initial-scale=1"/><title>Why Build a Model? For Whom? - Reinforcement Learning Beyond the Agent Loop: Models, Control, and Data</title><meta property="og:title" content="Why Build a Model? For Whom? - Reinforcement Learning Beyond the Agent Loop: Models, Control, and Data"/><meta name="generator" content="mystmd"/><meta name="description" content="A graduate-level introduction to reinforcement learning as a framework for modeling, optimization, and control, connecting dynamic models, data, and applications beyond standard benchmarks."/><meta property="og:description" content="A graduate-level introduction to reinforcement learning as a framework for modeling, optimization, and control, connecting dynamic models, data, and applications beyond standard benchmarks."/><meta name="keywords" content=""/><link rel="stylesheet" href="/rlbook/build/_assets/app-IZWEOBHI.css"/><link rel="stylesheet" href="/rlbook/build/_assets/thebe-core-VKVHG5VY.css"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jupyter-matplotlib@0.11.3/css/mpl_widget.css"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.css"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous"/><link rel="icon" href="/rlbook/favicon.ico"/><link rel="stylesheet" href="/rlbook/myst-theme.css"/><script>
  const savedTheme = localStorage.getItem("myst:theme");
  const theme = window.matchMedia("(prefers-color-scheme: light)").matches ? 'light' : 'dark';
  const classes = document.documentElement.classList;
  const hasAnyTheme = classes.contains('light') || classes.contains('dark');
  if (!hasAnyTheme) classes.add(savedTheme ?? theme);
</script></head><body class="m-0 transition-colors duration-500 bg-white dark:bg-stone-900"><div class="fixed top-1 left-1 h-[0px] w-[0px] focus-within:z-40 focus-within:h-auto focus-within:w-auto bg-white overflow-hidden focus-within:p-2 focus-within:ring-1" aria-label="skip to content options"><a href="#skip-to-frontmatter" class="block px-2 py-1 text-black underline">Skip to article frontmatter</a><a href="#skip-to-article" class="block px-2 py-1 text-black underline">Skip to article content</a></div><div class="bg-white/80 backdrop-blur dark:bg-stone-900/80 shadow dark:shadow-stone-700 p-3 md:px-8 sticky w-screen top-0 z-30 h-[60px]"><nav class="flex items-center justify-between flex-nowrap max-w-[1440px] mx-auto"><div class="flex flex-row xl:min-w-[19.5rem] mr-2 sm:mr-7 justify-start items-center shrink-0"><div class="block xl:hidden"><button class="flex items-center border-stone-400 text-stone-800 hover:text-stone-900 dark:text-stone-200 hover:dark:text-stone-100"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" width="2rem" height="2rem" class="m-1"><path fill-rule="evenodd" d="M3 6.75A.75.75 0 0 1 3.75 6h16.5a.75.75 0 0 1 0 1.5H3.75A.75.75 0 0 1 3 6.75ZM3 12a.75.75 0 0 1 .75-.75h16.5a.75.75 0 0 1 0 1.5H3.75A.75.75 0 0 1 3 12Zm0 5.25a.75.75 0 0 1 .75-.75h16.5a.75.75 0 0 1 0 1.5H3.75a.75.75 0 0 1-.75-.75Z" clip-rule="evenodd"></path></svg><span class="sr-only">Open Menu</span></button></div><a class="flex items-center ml-3 dark:text-white w-fit md:ml-5 xl:ml-7" href="/rlbook/"><div class="p-1 mr-3 dark:bg-white dark:rounded"><img src="/rlbook/build/logo-439a25d1a718f5f15fe2838e2a61a91a.png" class="h-9" alt="RL &amp; Control" height="2.25rem"/></div><span class="text-md sm:text-xl tracking-tight sm:mr-5">RL &amp; Control</span></a></div><div class="flex items-center flex-grow w-auto"><div class="flex-grow hidden text-md lg:block"></div><div class="flex-grow block"></div><button type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:R74op:" data-state="closed" class="flex items-center h-10 aspect-square sm:w-64 text-left text-gray-400 border border-gray-300 dark:border-gray-600 rounded-lg bg-gray-50 dark:bg-gray-700 hover:ring-blue-500 dark:hover:ring-blue-500 hover:border-blue-500 dark:hover:border-blue-500"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="p-2.5 h-10 w-10 aspect-square"><path fill-rule="evenodd" d="M10.5 3.75a6.75 6.75 0 1 0 0 13.5 6.75 6.75 0 0 0 0-13.5ZM2.25 10.5a8.25 8.25 0 1 1 14.59 5.28l4.69 4.69a.75.75 0 1 1-1.06 1.06l-4.69-4.69A8.25 8.25 0 0 1 2.25 10.5Z" clip-rule="evenodd"></path></svg><span class="hidden sm:block grow">Search</span><div aria-hidden="true" class="items-center hidden mx-1 font-mono text-sm text-gray-400 sm:flex gap-x-1"><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none hide-mac">CTRL</kbd><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none show-mac">⌘</kbd><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none ">K</kbd><script>
;(() => {
const script = document.currentScript;
const root = script.parentElement;

const isMac = /mac/i.test(
      window.navigator.userAgentData?.platform ?? window.navigator.userAgent,
    );
root.querySelectorAll(".hide-mac").forEach(node => {node.classList.add(isMac ? "hidden" : "block")});
root.querySelectorAll(".show-mac").forEach(node => {node.classList.add(!isMac ? "hidden" : "block")});
})()</script></div></button><button class="theme rounded-full aspect-square border border-stone-700 dark:border-white hover:bg-neutral-100 border-solid overflow-hidden text-stone-700 dark:text-white hover:text-stone-500 dark:hover:text-neutral-800 w-8 h-8 mx-3" title="Toggle theme between light and dark mode" aria-label="Toggle theme between light and dark mode"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="h-full w-full p-0.5 hidden dark:block"><path fill-rule="evenodd" d="M9.528 1.718a.75.75 0 0 1 .162.819A8.97 8.97 0 0 0 9 6a9 9 0 0 0 9 9 8.97 8.97 0 0 0 3.463-.69.75.75 0 0 1 .981.98 10.503 10.503 0 0 1-9.694 6.46c-5.799 0-10.5-4.7-10.5-10.5 0-4.368 2.667-8.112 6.46-9.694a.75.75 0 0 1 .818.162Z" clip-rule="evenodd"></path></svg><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-full w-full p-0.5 dark:hidden"><path stroke-linecap="round" stroke-linejoin="round" d="M12 3v2.25m6.364.386-1.591 1.591M21 12h-2.25m-.386 6.364-1.591-1.591M12 18.75V21m-4.773-4.227-1.591 1.591M5.25 12H3m4.227-4.773L5.636 5.636M15.75 12a3.75 3.75 0 1 1-7.5 0 3.75 3.75 0 0 1 7.5 0Z"></path></svg></button><div class="block sm:hidden"><div class="relative" data-headlessui-state=""><div><button class="flex text-sm bg-transparent rounded-full focus:outline-none" id="headlessui-menu-button-:Rr4op:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open Menu</span><div class="flex items-center text-stone-200 hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" width="2rem" height="2rem" class="p-1"><path fill-rule="evenodd" d="M10.5 6a1.5 1.5 0 1 1 3 0 1.5 1.5 0 0 1-3 0Zm0 6a1.5 1.5 0 1 1 3 0 1.5 1.5 0 0 1-3 0Zm0 6a1.5 1.5 0 1 1 3 0 1.5 1.5 0 0 1-3 0Z" clip-rule="evenodd"></path></svg></div></button></div></div></div><div class="hidden sm:block"><a href="https://github.com/pierrelux/rlbook" target="_blank" rel="noopener noreferrer" class="inline-block px-4 py-2 mx-1 mt-0 leading-none border rounded text-md border-stone-700 dark:border-white text-stone-700 dark:text-white hover:text-stone-500 dark:hover:text-neutral-800 hover:bg-neutral-100">View on GitHub</a></div></div></nav></div><div class="fixed xl:article-grid grid-gap xl:w-screen xl:pointer-events-none overflow-auto max-xl:min-w-[300px] hidden z-10" style="top:60px"><div class="pointer-events-auto xl:col-margin-left flex-col overflow-hidden hidden xl:flex"><div class="flex-grow py-6 overflow-y-auto primary-scrollbar"><nav aria-label="Navigation" class="overflow-y-hidden transition-opacity ml-3 xl:ml-0 mr-3 max-w-[350px] lg:hidden"><div class="w-full px-1 dark:text-white font-medium"></div></nav><div class="my-3 border-b-2 lg:hidden"></div><nav aria-label="Table of Contents" class="flex-grow overflow-y-hidden transition-opacity ml-3 xl:ml-0 mr-3 max-w-[350px]"><div class="w-full px-1 dark:text-white"><a title="Reinforcement Learning Beyond the Agent Loop: Models, Control, and Data" class="block break-words focus:outline outline-blue-200 outline-2 rounded p-2 my-1 rounded-lg hover:bg-slate-300/30 font-bold" href="/rlbook/">Reinforcement Learning Beyond the Agent Loop: Models, Control, and Data</a><div data-state="closed" class="w-full"><div class="flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="Modeling" class="block break-words rounded py-2 grow cursor-pointer">Modeling</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:Rmp8p:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:Rmp8p:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="closed" class="w-full"><div class="flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="Numerical Trajectory Optimization" class="block break-words rounded py-2 grow cursor-pointer">Numerical Trajectory Optimization</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:Rup8p:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:Rup8p:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="closed" class="w-full"><div class="flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="From Trajectories to Policies" class="block break-words rounded py-2 grow cursor-pointer">From Trajectories to Policies</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:R16p8p:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:R16p8p:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="closed" class="w-full"><div class="flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="Approximate Dynamic Programming" class="block break-words rounded py-2 grow cursor-pointer">Approximate Dynamic Programming</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:R1ep8p:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:R1ep8p:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="closed" class="w-full"><div class="flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="Appendix" class="block break-words rounded py-2 grow cursor-pointer">Appendix</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:R1mp8p:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:R1mp8p:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div></div></nav></div><div class="flex-none py-6 transition-all duration-700 translate-y-6 opacity-0"><a class="flex mx-auto text-gray-700 w-fit hover:text-blue-700 dark:text-gray-200 dark:hover:text-blue-400" href="https://mystmd.org/made-with-myst" target="_blank" rel="noreferrer"><svg style="width:24px;height:24px" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100" stroke="none"><g id="icon"><path fill="currentColor" d="M23.8,54.8v-3.6l4.7-0.8V17.5l-4.7-0.8V13H36l13.4,31.7h0.2l13-31.7h12.6v3.6l-4.7,0.8v32.9l4.7,0.8v3.6h-15
          v-3.6l4.9-0.8V20.8H65L51.4,53.3h-3.8l-14-32.5h-0.1l0.2,17.4v12.1l5,0.8v3.6H23.8z"></path><path fill="#F37726" d="M47,86.9c0-5.9-3.4-8.8-10.1-8.8h-8.4c-5.2,0-9.4-1.3-12.5-3.8c-3.1-2.5-5.4-6.2-6.8-11l4.8-1.6
          c1.8,5.6,6.4,8.6,13.8,8.8h9.2c6.4,0,10.8,2.5,13.1,7.5c2.3-5,6.7-7.5,13.1-7.5h8.4c7.8,0,12.7-2.9,14.6-8.7l4.8,1.6
          c-1.4,4.9-3.6,8.6-6.8,11.1c-3.1,2.5-7.3,3.7-12.4,3.8H63c-6.7,0-10,2.9-10,8.8"></path></g></svg><span class="self-center ml-2 text-sm">Made with MyST</span></a></div></div></div><main class="article-grid grid-gap"><article class="article-grid subgrid-gap col-screen article content"><div class="hidden"></div><div id="skip-to-frontmatter" aria-label="article frontmatter" class="mb-8 pt-9"><div class="flex items-center mb-5 h-6 text-sm font-light"><div class="flex-grow"></div><a href="https://github.com/pierrelux/rlbook" title="GitHub Repository: pierrelux/rlbook" target="_blank" rel="noopener noreferrer" class="text-inherit hover:text-inherit"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="1.25rem" height="1.25rem" class="inline-block mr-1 opacity-60 hover:opacity-100"><path d="M12 2.5c-5.4 0-9.8 4.4-9.8 9.7 0 4.3 2.8 8 6.7 9.2.5.1.7-.2.7-.5v-1.8c-2.4.5-3.1-.6-3.3-1.1-.1-.3-.6-1.1-1-1.4-.3-.2-.8-.6 0-.6s1.3.7 1.5 1c.9 1.5 2.3 1.1 2.8.8.1-.6.3-1.1.6-1.3-2.2-.2-4.4-1.1-4.4-4.8 0-1.1.4-1.9 1-2.6-.1-.2-.4-1.2.1-2.6 0 0 .8-.3 2.7 1 .8-.2 1.6-.3 2.4-.3.8 0 1.7.1 2.4.3 1.9-1.3 2.7-1 2.7-1 .5 1.3.2 2.3.1 2.6.6.7 1 1.5 1 2.6 0 3.7-2.3 4.6-4.4 4.8.4.3.7.9.7 1.8V21c0 .3.2.6.7.5 3.9-1.3 6.6-4.9 6.6-9.2 0-5.4-4.4-9.8-9.8-9.8z"></path></svg></a><a href="https://github.com/pierrelux/rlbook/edit/main/modeling.md" title="Edit This Page" target="_blank" rel="noopener noreferrer" class="text-inherit hover:text-inherit"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.25rem" height="1.25rem" class="inline-block mr-1 opacity-60 hover:opacity-100"><path stroke-linecap="round" stroke-linejoin="round" d="m16.862 4.487 1.687-1.688a1.875 1.875 0 1 1 2.652 2.652L10.582 16.07a4.5 4.5 0 0 1-1.897 1.13L6 18l.8-2.685a4.5 4.5 0 0 1 1.13-1.897l8.932-8.931Zm0 0L19.5 7.125M18 14v4.75A2.25 2.25 0 0 1 15.75 21H5.25A2.25 2.25 0 0 1 3 18.75V8.25A2.25 2.25 0 0 1 5.25 6H10"></path></svg></a><div class="relative flex inline-block mx-1 grow-0" data-headlessui-state=""><button class="relative ml-2 -mr-1" id="headlessui-menu-button-:Rs8top:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Downloads</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.25rem" height="1.25rem"><title>Download</title><path stroke-linecap="round" stroke-linejoin="round" d="M3 16.5v2.25A2.25 2.25 0 0 0 5.25 21h13.5A2.25 2.25 0 0 0 21 18.75V16.5M16.5 12 12 16.5m0 0L7.5 12m4.5 4.5V3"></path></svg></button></div></div><h1 class="mb-0">Why Build a Model? For Whom?</h1><header class="mt-4 not-prose"><div><span class="font-semibold text-sm inline-block"><button class="focus:shadow-[0_0_0_2px] focus:shadow-black outline-none hover:underline" aria-label="Author Details" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:R78top:" data-state="closed">Pierre-Luc Bacon</button></span></div></header></div><div class="block my-10 lg:sticky lg:z-10 lg:h-0 lg:pt-0 lg:my-0 lg:ml-10 lg:col-margin-right" style="top:60px"><nav></nav></div><div id="skip-to-article"></div><h1 id="why-build-a-model-for-whom" class="relative group"><span class="heading-text">Why Build a Model? For Whom?</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#why-build-a-model-for-whom" title="Link to this Section" aria-label="Link to this Section">¶</a></h1><blockquote><p>“The sciences do not try to explain, they hardly even try to interpret, they mainly make models. By a model is meant a mathematical construct which, with the addition of certain verbal interpretations, describes observed phenomena. The justification of such a mathematical construct is solely and precisely that it is expected to work.”
— John von Neumann</p></blockquote><p>The word <em>model</em> means different things depending on who you ask.</p><p>In machine learning, it typically refers to a parameterized function (often a neural network) fit to data. When we say <em>“we trained a model,”</em> we usually mean adjusting parameters so it makes good predictions. But that’s a narrow view.</p><p>In control, operations research, or structural economics, a model refers more broadly to a formal specification of a decision problem. It includes how a system evolves over time, what parts of the world we choose to represent, what decisions are available, what can be observed or measured, and how outcomes are evaluated. It also encodes assumptions about time (discrete or continuous, finite or infinite horizon), uncertainty, and information structure.</p><p>To clarify terminology, I’ll use the term decision-making model to refer to this broader object: one that includes not just system dynamics, but also a specification of state, control, observations, objectives, time structure, and information assumptions. In this sense, the model defines the structure of the decision problem. It’s the formal scaffold on which we build optimization or learning procedures.</p><p>Depending on the setting, we may ask different things from a decision-making model. Sometimes we want a model that supports counterfactual reasoning or policy evaluation, and are willing to bake in more assumptions to get there. Other times, we just need a model that supports prediction or simulation, even if it remains agnostic about internal mechanisms.</p><p>This mirrors a interesting distinction in econometrics between structural and reduced-form approaches. Structural models aim to capture the underlying process that generates behavior, enabling reasoning about what would happen under alternative policies or conditions. Reduced-form models, by contrast, focus on capturing statistical regularities (often to estimate causal effects) without necessarily modeling the mechanisms that generate them. Both are forms of modeling, just with different goals. The same applies in control and RL: some models are built to support simulation and optimization, while others serve more diagnostic or predictive roles, with fewer assumptions about how the system works internally.</p><p>This chapter steps back from algorithms to focus on the modeling side. What kinds of models do we need to support decision-making from data? What are their assumptions? What do they let us express or ignore? And how do they shape what learning and optimization can even mean?</p><h1 id="modeling-realism-and-control" class="relative group"><span class="heading-text">Modeling, Realism, and Control</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#modeling-realism-and-control" title="Link to this Section" aria-label="Link to this Section">¶</a></h1><p>Realism is only one way to assess a model. When the purpose of modeling is to support control or decision making, accuracy in reproducing every detail of the system is not always necessary. What matters more is whether the model leads to decisions that perform well when applied in practice. A model may simplify the physics, ignore some variables, or group complex interactions into a disturbance term. As long as it retains the core feedback structure relevant to the control task, it can still be effective.</p><p>In some cases, high-fidelity models can be counterproductive. Their complexity makes them harder to understand, slower to simulate, and more difficult to tune. Worse, they may include uncertain parameters that do not affect the control decisions but still influence the outcome of optimization. The resulting decisions can become fragile or overfitted to details that are not stable across different operating conditions.</p><p>A useful model for control is one that focuses on the variables, dynamics, and constraints that shape the decisions to be made. It should capture the key trade-offs without trying to account for every effect. In traditional control design, this principle appears through model simplification: engineers reduce the system to a manageable form, then use feedback to absorb remaining uncertainty. Reinforcement learning adopts a similar mindset, though often implicitly. It allows for model error and evaluates success based on the quality of the policy when deployed, rather than on the accuracy of the model itself.</p><h2 id="example-a-simple-model-that-supports-better-decisions" class="relative group"><span class="heading-text">Example: A simple model that supports better decisions</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#example-a-simple-model-that-supports-better-decisions" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>Researchers at the U.S. National Renewable Energy Laboratory investigated how to reduce cooling costs in a typical home in Austin, Texas <span class="cite-group parenthetical"><cite class="" data-state="closed"><a href="https://doi.org/10.1016/j.enbuild.2014.01.033" target="_blank" rel="noreferrer" class="hover-link">Cole <em>et al.</em>, 2014</a></cite></span>. They had access to a detailed EnergyPlus simulation of the building, which included thousands of internal variables: layered wall models, HVAC cycling behavior, occupancy schedules, and detailed weather inputs.</p><p>Although this simulator could closely reproduce indoor temperatures, it was too slow and too complex to use as a planning tool. Instead, the researchers constructed a much simpler model using just two parameters: an effective thermal resistance and an effective thermal capacitance. This reduced model did not capture short-term temperature fluctuations and could be off by as much as two degrees on hot afternoons.</p><p>Despite these inaccuracies, the simplified model proved useful for testing different cooling strategies. One such strategy involved cooling the house early in the morning when electricity prices were low, letting the temperature rise slowly during the expensive late-afternoon period, and reheating only slightly overnight. When this strategy was simulated in the full EnergyPlus model, it reduced peak compressor power by approximately 70 percent and lowered total cooling cost by about 60 percent compared to a standard thermostat schedule.</p><p>The reason this worked is that the simple model captured the most important structural feature of the system: the thermal mass of the building acts as a buffer that allows load shifting over time. That was enough to discover a control strategy that exploited this property. The many other effects present in the full simulation did not change the main conclusions and could be treated as part of the background variability.</p><p>This example shows that a model can be inaccurate in detail but still highly effective in guiding decisions. For control, what matters is not whether the model matches reality in every respect, but whether it helps identify actions that perform well under real-world conditions.</p><div></div><section id="references" class="article-grid subgrid-gap col-screen"><div><header class="text-lg font-semibold text-stone-900 dark:text-white group">References<a class="no-underline text-inherit hover:text-inherit ml-2 select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#references" title="Link to References" aria-label="Link to References">¶</a></header></div><div class="pl-3 mb-8 text-xs text-stone-500 dark:text-stone-300"><ol><li class="break-words" id="cite-COLE201469">Cole, W. J., Powell, K. M., Hale, E. T., & Edgar, T. F. (2014). Reduced-order residential home modeling for model predictive control. <i>Energy and Buildings</i>, <i>74</i>, 69–77. <a target="_blank" rel="noreferrer" href="https://doi.org/10.1016/j.enbuild.2014.01.033">https://doi.org/10.1016/j.enbuild.2014.01.033</a></li></ol></div></section><div class="flex pt-10 mb-10 space-x-4"><a class="flex-1 block p-4 font-normal text-gray-600 no-underline border border-gray-200 rounded shadow-sm group hover:border-blue-600 dark:hover:border-blue-400 hover:text-blue-600 dark:hover:text-blue-400 dark:text-gray-100 dark:border-gray-500 hover:shadow-lg dark:shadow-neutral-700" href="/rlbook/"><div class="flex h-full align-middle"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.5rem" height="1.5rem" class="self-center transition-transform group-hover:-translate-x-1 shrink-0"><path stroke-linecap="round" stroke-linejoin="round" d="M10.5 19.5 3 12m0 0 7.5-7.5M3 12h18"></path></svg><div class="flex-grow text-right"><div class="text-xs text-gray-500 dark:text-gray-400">Reinforcement Learning Beyond the Agent Loop: Models, Control, and Data</div>Reinforcement Learning Beyond the Agent Loop: Models, Control, and Data</div></div></a><a class="flex-1 block p-4 font-normal text-gray-600 no-underline border border-gray-200 rounded shadow-sm group hover:border-blue-600 dark:hover:border-blue-400 hover:text-blue-600 dark:hover:text-blue-400 dark:text-gray-100 dark:border-gray-500 hover:shadow-lg dark:shadow-neutral-700" href="/rlbook/ssm"><div class="flex h-full align-middle"><div class="flex-grow"><div class="text-xs text-gray-500 dark:text-gray-400">Modeling</div>Dynamics Models for Decision Making</div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.5rem" height="1.5rem" class="self-center transition-transform group-hover:translate-x-1 shrink-0"><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 4.5 21 12m0 0-7.5 7.5M21 12H3"></path></svg></div></a></div></article></main><script>((a,d)=>{if(!window.history.state||!window.history.state.key){let h=Math.random().toString(32).slice(2);window.history.replaceState({key:h},"")}try{let f=JSON.parse(sessionStorage.getItem(a)||"{}")[d||window.history.state.key];typeof f=="number"&&window.scrollTo(0,f)}catch(h){console.error(h),sessionStorage.removeItem(a)}})("positions", null)</script><link rel="modulepreload" href="/rlbook/build/entry.client-UNPC4GT3.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-OCTKKCIL.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-UAI5KRM7.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-2NH4LW52.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-F7G67JTZ.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-HBJK6BW3.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-HYMQ7M2K.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-OHOXABTA.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-OCWQY3HK.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-CPTH56EW.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-3CVK3PYF.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-J6FHCSRC.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-S4SWV34C.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-GUCIBHGO.js"/><link rel="modulepreload" href="/rlbook/build/root-7TUVC4ZT.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-INOWNUZ6.js"/><link rel="modulepreload" href="/rlbook/build/routes/$-P6PGXPYX.js"/><script>window.__remixContext = {"url":"/modeling","state":{"loaderData":{"root":{"config":{"version":2,"myst":"1.6.3","options":{"logo":"/rlbook/build/logo-439a25d1a718f5f15fe2838e2a61a91a.png","logo_text":"RL \u0026 Control"},"nav":[],"actions":[{"title":"View on GitHub","url":"https://github.com/pierrelux/rlbook","internal":false,"static":false}],"projects":[{"bibliography":["/Users/pierre-luc.bacon/Documents/rlbook/references.bib"],"math":{"\\Proj":{"macro":"\\mathsf{P}"},"\\Residual":{"macro":"\\mathsf{N}"},"\\Contraction":{"macro":"\\mathsf{T}"},"\\Bellman":{"macro":"\\mathsf{L}"},"\\BellmanPi":{"macro":"\\mathsf{L}_\\pi"},"\\BellmanQuad":{"macro":"\\widetilde{\\mathsf{L}}"},"\\EulerResidual":{"macro":"\\mathsf{H}"},"\\Policy":{"macro":"\\pi"}},"exports":[{"format":"pdf","filename":"book.pdf","url":"/rlbook/build/book-bceca6481db041911b084ebd03cc1269.pdf"}],"title":"Reinforcement Learning Beyond the Agent Loop: Models, Control, and Data","description":"A graduate-level introduction to reinforcement learning as a framework for modeling, optimization, and control, connecting dynamic models, data, and applications beyond standard benchmarks.","authors":[{"nameParsed":{"literal":"Pierre-Luc Bacon","given":"Pierre-Luc","family":"Bacon"},"name":"Pierre-Luc Bacon","id":"contributors-myst-generated-uid-0"}],"github":"https://github.com/pierrelux/rlbook","id":"3c112bb5-93ac-429f-8216-c0b9947c00d4","toc":[{"file":"intro.md"},{"children":[{"file":"modeling.md"},{"file":"ssm.md"},{"file":"simulation.md"}],"title":"Modeling"},{"children":[{"file":"ocp.md"},{"file":"cocp.md"}],"title":"Numerical Trajectory Optimization"},{"children":[{"file":"mpc.md"},{"file":"dp.md"}],"title":"From Trajectories to Policies"},{"children":[{"file":"regmdp.md"},{"file":"projdp.md"},{"children":[{"file":"simadp.md"},{"file":"fqi.md"}],"title":"Simulation-Based Methods"},{"file":"cadp.md"},{"file":"pg.md"}],"title":"Approximate Dynamic Programming"},{"children":[{"file":"appendix_examples.md"},{"file":"appendix_ivps.md"},{"file":"appendix_nlp.md"}],"title":"Appendix"}],"index":"index","pages":[{"level":1,"title":"Modeling"},{"slug":"modeling","title":"Why Build a Model? For Whom?","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"ssm","title":"Dynamics Models for Decision Making","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"simulation","title":"Programs as Models","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Numerical Trajectory Optimization"},{"slug":"ocp","title":"Discrete-Time Trajectory Optimization","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"cocp","title":"Trajectory Optimization in Continuous Time","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"From Trajectories to Policies"},{"slug":"mpc","title":"Model Predictive Control","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"dp","title":"Dynamic Programming","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Approximate Dynamic Programming"},{"slug":"regmdp","title":"Smooth Bellman Optimality Equations","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"projdp","title":"Weighted Residual Methods for Functional Equations","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":2,"title":"Simulation-Based Methods"},{"slug":"simadp","title":"Monte Carlo Integration in Approximate Dynamic Programming","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"fqi","title":"Fitted Q-Iteration Methods","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"cadp","title":"Fitted Q-Iteration for Continuous Action Spaces","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"pg","title":"Policy Gradient Methods","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Appendix"},{"slug":"appendix-examples","title":"Example COCPs","description":"","date":"","thumbnail":"/rlbook/build/heat_exchanger-acfdd83b1501b4c220f686fe21df7820.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"appendix-ivps","title":"Solving Initial Value Problems","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"appendix-nlp","title":"Nonlinear Programming","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2}]}]},"CONTENT_CDN_PORT":"3102","MODE":"static","BASE_URL":"/rlbook"},"routes/$":{"config":{"version":2,"myst":"1.6.3","options":{"logo":"/rlbook/build/logo-439a25d1a718f5f15fe2838e2a61a91a.png","logo_text":"RL \u0026 Control"},"nav":[],"actions":[{"title":"View on GitHub","url":"https://github.com/pierrelux/rlbook","internal":false,"static":false}],"projects":[{"bibliography":["/Users/pierre-luc.bacon/Documents/rlbook/references.bib"],"math":{"\\Proj":{"macro":"\\mathsf{P}"},"\\Residual":{"macro":"\\mathsf{N}"},"\\Contraction":{"macro":"\\mathsf{T}"},"\\Bellman":{"macro":"\\mathsf{L}"},"\\BellmanPi":{"macro":"\\mathsf{L}_\\pi"},"\\BellmanQuad":{"macro":"\\widetilde{\\mathsf{L}}"},"\\EulerResidual":{"macro":"\\mathsf{H}"},"\\Policy":{"macro":"\\pi"}},"exports":[{"format":"pdf","filename":"book.pdf","url":"/rlbook/build/book-bceca6481db041911b084ebd03cc1269.pdf"}],"title":"Reinforcement Learning Beyond the Agent Loop: Models, Control, and Data","description":"A graduate-level introduction to reinforcement learning as a framework for modeling, optimization, and control, connecting dynamic models, data, and applications beyond standard benchmarks.","authors":[{"nameParsed":{"literal":"Pierre-Luc Bacon","given":"Pierre-Luc","family":"Bacon"},"name":"Pierre-Luc Bacon","id":"contributors-myst-generated-uid-0"}],"github":"https://github.com/pierrelux/rlbook","id":"3c112bb5-93ac-429f-8216-c0b9947c00d4","toc":[{"file":"intro.md"},{"children":[{"file":"modeling.md"},{"file":"ssm.md"},{"file":"simulation.md"}],"title":"Modeling"},{"children":[{"file":"ocp.md"},{"file":"cocp.md"}],"title":"Numerical Trajectory Optimization"},{"children":[{"file":"mpc.md"},{"file":"dp.md"}],"title":"From Trajectories to Policies"},{"children":[{"file":"regmdp.md"},{"file":"projdp.md"},{"children":[{"file":"simadp.md"},{"file":"fqi.md"}],"title":"Simulation-Based Methods"},{"file":"cadp.md"},{"file":"pg.md"}],"title":"Approximate Dynamic Programming"},{"children":[{"file":"appendix_examples.md"},{"file":"appendix_ivps.md"},{"file":"appendix_nlp.md"}],"title":"Appendix"}],"index":"index","pages":[{"level":1,"title":"Modeling"},{"slug":"modeling","title":"Why Build a Model? For Whom?","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"ssm","title":"Dynamics Models for Decision Making","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"simulation","title":"Programs as Models","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Numerical Trajectory Optimization"},{"slug":"ocp","title":"Discrete-Time Trajectory Optimization","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"cocp","title":"Trajectory Optimization in Continuous Time","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"From Trajectories to Policies"},{"slug":"mpc","title":"Model Predictive Control","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"dp","title":"Dynamic Programming","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Approximate Dynamic Programming"},{"slug":"regmdp","title":"Smooth Bellman Optimality Equations","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"projdp","title":"Weighted Residual Methods for Functional Equations","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":2,"title":"Simulation-Based Methods"},{"slug":"simadp","title":"Monte Carlo Integration in Approximate Dynamic Programming","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"fqi","title":"Fitted Q-Iteration Methods","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"cadp","title":"Fitted Q-Iteration for Continuous Action Spaces","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"pg","title":"Policy Gradient Methods","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Appendix"},{"slug":"appendix-examples","title":"Example COCPs","description":"","date":"","thumbnail":"/rlbook/build/heat_exchanger-acfdd83b1501b4c220f686fe21df7820.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"appendix-ivps","title":"Solving Initial Value Problems","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"appendix-nlp","title":"Nonlinear Programming","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2}]}]},"page":{"version":2,"kind":"Article","sha256":"a105650b9b66d91d518a6ab2080957fc2b19712941ebc3d7d6e78d7ee9e2edd0","slug":"modeling","location":"/modeling.md","dependencies":[],"frontmatter":{"title":"Why Build a Model? For Whom?","content_includes_title":true,"authors":[{"nameParsed":{"literal":"Pierre-Luc Bacon","given":"Pierre-Luc","family":"Bacon"},"name":"Pierre-Luc Bacon","id":"contributors-myst-generated-uid-0"}],"github":"https://github.com/pierrelux/rlbook","math":{"\\Proj":{"macro":"\\mathsf{P}"},"\\Residual":{"macro":"\\mathsf{N}"},"\\Contraction":{"macro":"\\mathsf{T}"},"\\Bellman":{"macro":"\\mathsf{L}"},"\\BellmanPi":{"macro":"\\mathsf{L}_\\pi"},"\\BellmanQuad":{"macro":"\\widetilde{\\mathsf{L}}"},"\\EulerResidual":{"macro":"\\mathsf{H}"},"\\Policy":{"macro":"\\pi"}},"numbering":{"title":{"offset":1}},"source_url":"https://github.com/pierrelux/rlbook/blob/main/modeling.md","edit_url":"https://github.com/pierrelux/rlbook/edit/main/modeling.md","exports":[{"format":"md","filename":"modeling.md","url":"/rlbook/build/modeling-71b8dea943fd7886aa2b1583f8ce0bd3.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"heading","depth":1,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Why Build a Model? For Whom?","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RnqTo4drMx"}],"identifier":"why-build-a-model-for-whom","label":"Why Build a Model? For Whom?","html_id":"why-build-a-model-for-whom","implicit":true,"key":"C4lZ9jg8yD"},{"type":"blockquote","position":{"start":{"line":3,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"“The sciences do not try to explain, they hardly even try to interpret, they mainly make models. By a model is meant a mathematical construct which, with the addition of certain verbal interpretations, describes observed phenomena. The justification of such a mathematical construct is solely and precisely that it is expected to work.”\n— John von Neumann","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"CPVG20K24d"}],"key":"phgiE5kxA1"}],"key":"sPXOWrJCG3"},{"type":"paragraph","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"The word ","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"HjFQKnfsx2"},{"type":"emphasis","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"model","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"nTMZfpnLIu"}],"key":"WWe62q8sJj"},{"type":"text","value":" means different things depending on who you ask.","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"iXDPTSpXqE"}],"key":"V8UmqPEsqT"},{"type":"paragraph","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"In machine learning, it typically refers to a parameterized function (often a neural network) fit to data. When we say ","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"BAzX87PCNp"},{"type":"emphasis","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"“we trained a model,”","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"SWChiu4Il9"}],"key":"p3ZHqVW8EQ"},{"type":"text","value":" we usually mean adjusting parameters so it makes good predictions. But that’s a narrow view.","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"YcMoaZO71f"}],"key":"HUSQaBaKhq"},{"type":"paragraph","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"In control, operations research, or structural economics, a model refers more broadly to a formal specification of a decision problem. It includes how a system evolves over time, what parts of the world we choose to represent, what decisions are available, what can be observed or measured, and how outcomes are evaluated. It also encodes assumptions about time (discrete or continuous, finite or infinite horizon), uncertainty, and information structure.","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"RNfBZqUoFg"}],"key":"IAMPxF75jq"},{"type":"paragraph","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"To clarify terminology, I’ll use the term decision-making model to refer to this broader object: one that includes not just system dynamics, but also a specification of state, control, observations, objectives, time structure, and information assumptions. In this sense, the model defines the structure of the decision problem. It’s the formal scaffold on which we build optimization or learning procedures.","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"sBAQsteNkI"}],"key":"SPhyu3vyG1"},{"type":"paragraph","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"Depending on the setting, we may ask different things from a decision-making model. Sometimes we want a model that supports counterfactual reasoning or policy evaluation, and are willing to bake in more assumptions to get there. Other times, we just need a model that supports prediction or simulation, even if it remains agnostic about internal mechanisms.","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"dH7yqC3JEf"}],"key":"xid1E5xGMk"},{"type":"paragraph","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"This mirrors a interesting distinction in econometrics between structural and reduced-form approaches. Structural models aim to capture the underlying process that generates behavior, enabling reasoning about what would happen under alternative policies or conditions. Reduced-form models, by contrast, focus on capturing statistical regularities (often to estimate causal effects) without necessarily modeling the mechanisms that generate them. Both are forms of modeling, just with different goals. The same applies in control and RL: some models are built to support simulation and optimization, while others serve more diagnostic or predictive roles, with fewer assumptions about how the system works internally.","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"sd3W1q0JCb"}],"key":"zd0nkN6LcS"},{"type":"paragraph","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"text","value":"This chapter steps back from algorithms to focus on the modeling side. What kinds of models do we need to support decision-making from data? What are their assumptions? What do they let us express or ignore? And how do they shape what learning and optimization can even mean?","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"UtUjneuMIO"}],"key":"a690Grf4Wh"},{"type":"heading","depth":1,"position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"text","value":"Modeling, Realism, and Control","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"zVCY9qgrIw"}],"identifier":"modeling-realism-and-control","label":"Modeling, Realism, and Control","html_id":"modeling-realism-and-control","implicit":true,"key":"bbMeVvL7l4"},{"type":"paragraph","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"text","value":"Realism is only one way to assess a model. When the purpose of modeling is to support control or decision making, accuracy in reproducing every detail of the system is not always necessary. What matters more is whether the model leads to decisions that perform well when applied in practice. A model may simplify the physics, ignore some variables, or group complex interactions into a disturbance term. As long as it retains the core feedback structure relevant to the control task, it can still be effective.","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"aiMPPKzHvZ"}],"key":"tnMin3XEHl"},{"type":"paragraph","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"children":[{"type":"text","value":"In some cases, high-fidelity models can be counterproductive. Their complexity makes them harder to understand, slower to simulate, and more difficult to tune. Worse, they may include uncertain parameters that do not affect the control decisions but still influence the outcome of optimization. The resulting decisions can become fragile or overfitted to details that are not stable across different operating conditions.","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"key":"BsJhKoDmCt"}],"key":"dnrAEq9i8D"},{"type":"paragraph","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"children":[{"type":"text","value":"A useful model for control is one that focuses on the variables, dynamics, and constraints that shape the decisions to be made. It should capture the key trade-offs without trying to account for every effect. In traditional control design, this principle appears through model simplification: engineers reduce the system to a manageable form, then use feedback to absorb remaining uncertainty. Reinforcement learning adopts a similar mindset, though often implicitly. It allows for model error and evaluates success based on the quality of the policy when deployed, rather than on the accuracy of the model itself.","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"key":"slTOIREKcy"}],"key":"s3TSuoYM0I"},{"type":"heading","depth":2,"position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"children":[{"type":"text","value":"Example: A simple model that supports better decisions","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"key":"womUKfAny6"}],"identifier":"example-a-simple-model-that-supports-better-decisions","label":"Example: A simple model that supports better decisions","html_id":"example-a-simple-model-that-supports-better-decisions","implicit":true,"key":"xUpdBWGHI7"},{"type":"paragraph","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"children":[{"type":"text","value":"Researchers at the U.S. National Renewable Energy Laboratory investigated how to reduce cooling costs in a typical home in Austin, Texas ","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"key":"G1AllwBJ88"},{"type":"citeGroup","kind":"parenthetical","children":[{"type":"cite","kind":"parenthetical","label":"COLE201469","identifier":"cole201469","children":[{"type":"text","value":"Cole ","key":"RSwf2H8VW2"},{"type":"emphasis","children":[{"type":"text","value":"et al.","key":"KVRFUzfZxr"}],"key":"FstC5pTY7O"},{"type":"text","value":", 2014","key":"KurtbdXPef"}],"enumerator":"1","key":"OdJ24YN9t9"}],"key":"M1Rl8GeE8G"},{"type":"text","value":". They had access to a detailed EnergyPlus simulation of the building, which included thousands of internal variables: layered wall models, HVAC cycling behavior, occupancy schedules, and detailed weather inputs.","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"key":"wOonwWr5gb"}],"key":"cNOIEa175n"},{"type":"paragraph","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"children":[{"type":"text","value":"Although this simulator could closely reproduce indoor temperatures, it was too slow and too complex to use as a planning tool. Instead, the researchers constructed a much simpler model using just two parameters: an effective thermal resistance and an effective thermal capacitance. This reduced model did not capture short-term temperature fluctuations and could be off by as much as two degrees on hot afternoons.","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"key":"VUwXKOx12m"}],"key":"XVC7AtPCEb"},{"type":"paragraph","position":{"start":{"line":36,"column":1},"end":{"line":36,"column":1}},"children":[{"type":"text","value":"Despite these inaccuracies, the simplified model proved useful for testing different cooling strategies. One such strategy involved cooling the house early in the morning when electricity prices were low, letting the temperature rise slowly during the expensive late-afternoon period, and reheating only slightly overnight. When this strategy was simulated in the full EnergyPlus model, it reduced peak compressor power by approximately 70 percent and lowered total cooling cost by about 60 percent compared to a standard thermostat schedule.","position":{"start":{"line":36,"column":1},"end":{"line":36,"column":1}},"key":"vpTQTVTrVw"}],"key":"pBlbVAp9na"},{"type":"paragraph","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"children":[{"type":"text","value":"The reason this worked is that the simple model captured the most important structural feature of the system: the thermal mass of the building acts as a buffer that allows load shifting over time. That was enough to discover a control strategy that exploited this property. The many other effects present in the full simulation did not change the main conclusions and could be treated as part of the background variability.","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"key":"LEoekKimFb"}],"key":"iYUdbfZeEO"},{"type":"paragraph","position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"children":[{"type":"text","value":"This example shows that a model can be inaccurate in detail but still highly effective in guiding decisions. For control, what matters is not whether the model matches reality in every respect, but whether it helps identify actions that perform well under real-world conditions.","position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"key":"DDM4JnuV9Y"}],"key":"Yw3yqamzKd"}],"key":"l0TMJMobXO"}],"key":"auGEgtecWb"},"references":{"cite":{"order":["COLE201469"],"data":{"COLE201469":{"label":"COLE201469","enumerator":"1","doi":"https://doi.org/10.1016/j.enbuild.2014.01.033","html":"Cole, W. J., Powell, K. M., Hale, E. T., \u0026 Edgar, T. F. (2014). Reduced-order residential home modeling for model predictive control. \u003ci\u003eEnergy and Buildings\u003c/i\u003e, \u003ci\u003e74\u003c/i\u003e, 69–77. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.1016/j.enbuild.2014.01.033\"\u003ehttps://doi.org/10.1016/j.enbuild.2014.01.033\u003c/a\u003e","url":"https://doi.org/10.1016/j.enbuild.2014.01.033"}}}},"footer":{"navigation":{"prev":{"title":"Reinforcement Learning Beyond the Agent Loop: Models, Control, and Data","url":"/","group":"Reinforcement Learning Beyond the Agent Loop: Models, Control, and Data"},"next":{"title":"Dynamics Models for Decision Making","url":"/ssm","group":"Modeling"}}},"domain":"http://localhost:3003"},"project":{"bibliography":["/Users/pierre-luc.bacon/Documents/rlbook/references.bib"],"math":{"\\Proj":{"macro":"\\mathsf{P}"},"\\Residual":{"macro":"\\mathsf{N}"},"\\Contraction":{"macro":"\\mathsf{T}"},"\\Bellman":{"macro":"\\mathsf{L}"},"\\BellmanPi":{"macro":"\\mathsf{L}_\\pi"},"\\BellmanQuad":{"macro":"\\widetilde{\\mathsf{L}}"},"\\EulerResidual":{"macro":"\\mathsf{H}"},"\\Policy":{"macro":"\\pi"}},"exports":[{"format":"pdf","filename":"book.pdf","url":"/rlbook/build/book-bceca6481db041911b084ebd03cc1269.pdf"}],"title":"Reinforcement Learning Beyond the Agent Loop: Models, Control, and Data","description":"A graduate-level introduction to reinforcement learning as a framework for modeling, optimization, and control, connecting dynamic models, data, and applications beyond standard benchmarks.","authors":[{"nameParsed":{"literal":"Pierre-Luc Bacon","given":"Pierre-Luc","family":"Bacon"},"name":"Pierre-Luc Bacon","id":"contributors-myst-generated-uid-0"}],"github":"https://github.com/pierrelux/rlbook","id":"3c112bb5-93ac-429f-8216-c0b9947c00d4","toc":[{"file":"intro.md"},{"children":[{"file":"modeling.md"},{"file":"ssm.md"},{"file":"simulation.md"}],"title":"Modeling"},{"children":[{"file":"ocp.md"},{"file":"cocp.md"}],"title":"Numerical Trajectory Optimization"},{"children":[{"file":"mpc.md"},{"file":"dp.md"}],"title":"From Trajectories to Policies"},{"children":[{"file":"regmdp.md"},{"file":"projdp.md"},{"children":[{"file":"simadp.md"},{"file":"fqi.md"}],"title":"Simulation-Based Methods"},{"file":"cadp.md"},{"file":"pg.md"}],"title":"Approximate Dynamic Programming"},{"children":[{"file":"appendix_examples.md"},{"file":"appendix_ivps.md"},{"file":"appendix_nlp.md"}],"title":"Appendix"}],"index":"index","pages":[{"level":1,"title":"Modeling"},{"slug":"modeling","title":"Why Build a Model? For Whom?","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"ssm","title":"Dynamics Models for Decision Making","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"simulation","title":"Programs as Models","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Numerical Trajectory Optimization"},{"slug":"ocp","title":"Discrete-Time Trajectory Optimization","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"cocp","title":"Trajectory Optimization in Continuous Time","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"From Trajectories to Policies"},{"slug":"mpc","title":"Model Predictive Control","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"dp","title":"Dynamic Programming","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Approximate Dynamic Programming"},{"slug":"regmdp","title":"Smooth Bellman Optimality Equations","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"projdp","title":"Weighted Residual Methods for Functional Equations","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":2,"title":"Simulation-Based Methods"},{"slug":"simadp","title":"Monte Carlo Integration in Approximate Dynamic Programming","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"fqi","title":"Fitted Q-Iteration Methods","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"cadp","title":"Fitted Q-Iteration for Continuous Action Spaces","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"pg","title":"Policy Gradient Methods","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Appendix"},{"slug":"appendix-examples","title":"Example COCPs","description":"","date":"","thumbnail":"/rlbook/build/heat_exchanger-acfdd83b1501b4c220f686fe21df7820.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"appendix-ivps","title":"Solving Initial Value Problems","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"appendix-nlp","title":"Nonlinear Programming","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2}]}}},"actionData":null,"errors":null},"future":{"unstable_dev":false,"unstable_postcss":false,"unstable_tailwind":false,"v2_errorBoundary":true,"v2_headers":true,"v2_meta":true,"v2_normalizeFormMethod":true,"v2_routeConvention":true}};</script><script type="module" async="">import "/rlbook/build/manifest-3481E987.js";
import * as route0 from "/rlbook/build/root-7TUVC4ZT.js";
import * as route1 from "/rlbook/build/routes/$-P6PGXPYX.js";
window.__remixRouteModules = {"root":route0,"routes/$":route1};

import("/rlbook/build/entry.client-UNPC4GT3.js");</script></body></html>