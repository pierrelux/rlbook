---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.16.3
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# Policy Gradient Methods

The previous chapter showed how to handle continuous action spaces in fitted Q-iteration by amortizing action selection with policy networks. Methods like NFQCA, DDPG, TD3, and SAC all learn both a Q-function and a policy, using the Q-function to guide policy improvement. This chapter explores a different approach: optimizing policies directly without maintaining explicit value functions.

Direct policy optimization offers several advantages. First, it naturally handles stochastic policies, which can be essential for partially observable environments or problems requiring explicit exploration. Second, it avoids the detour through value function approximation, which may introduce errors that compound during policy extraction. Third, for problems with simple policy classes but complex value landscapes, directly searching in policy space can be more efficient than searching in value space.

The foundation of policy gradient methods rests on computing gradients of expected returns with respect to policy parameters. This chapter develops the mathematical machinery needed for this computation, starting with general derivative estimation techniques from stochastic optimization, then specializing to reinforcement learning settings, and finally examining variance reduction methods that make these estimators practical.

## Derivative Estimation for Stochastic Optimization

Consider optimizing an objective that involves an expectation:

$$
J(\theta) = \mathbb{E}_{x \sim p(x;\theta)}[f(x,\theta)]
$$

For concreteness, consider a simple example where $x \sim \mathcal{N}(\theta,1)$ and $f(x,\theta) = x^2\theta$. The derivative we seek is:

$$
\frac{d}{d\theta}J(\theta) = \frac{d}{d\theta}\int x^2\theta p(x;\theta)dx
$$

While we can compute this exactly for the Gaussian example, this is often impossible for more general problems. We might then be tempted to approximate our objective using samples:

$$
J(\theta) \approx \frac{1}{N}\sum_{i=1}^N f(x_i,\theta), \quad x_i \sim p(x;\theta)
$$

Then differentiate this approximation:

$$
\frac{d}{d\theta}J(\theta) \approx \frac{1}{N}\sum_{i=1}^N \frac{\partial}{\partial \theta}f(x_i,\theta)
$$

However, this naive approach ignores that the samples themselves depend on $\theta$. The correct derivative requires the product rule:

$$
\frac{d}{d\theta}J(\theta) = \int \frac{\partial}{\partial \theta}[f(x,\theta)p(x;\theta)]dx = \int \left[\frac{\partial f}{\partial \theta}p(x;\theta) + f(x,\theta)\frac{\partial p(x;\theta)}{\partial \theta}\right]dx
$$

The issue here is that while the first term could be numerically integrated using Monte Carlo, the second one cannot as it is not in the form of an expectation. 

Would there be a way to transform our objective in such a way that the Monte Carlo estimator for the objective could be differentiated directly while ensuring that the resulting derivative is unbiased? We will see that there are two main solutions to that problem: by doing a change of measure, or a change of variables. 

### The Likelihood Ratio Method

One solution comes from rewriting our objective using any distribution $q(x)$:

$$
J(\theta) = \int f(x,\theta)\frac{p(x;\theta)}{q(x)}q(x)dx = \mathbb{E}_{x \sim q(x)}\left[f(x,\theta)\frac{p(x;\theta)}{q(x)}\right]
$$

Write this more functionally by defining:

$$
J(\theta) = \mathbb{E}_{x \sim q(x)}[h(x,\theta)]
, \enspace h(x,\theta) \equiv f(x,\theta)\frac{p(x;\theta)}{q(x)}
$$

When we differentiate $J$, we must take the partial derivative of $h$ with respect to its second argument:

$$
\frac{d}{d\theta}J(\theta) = \mathbb{E}_{x \sim q(x)}\left[\frac{\partial h}{\partial \theta}(x,\theta)\right] = \mathbb{E}_{x \sim q(x)}\left[f(x,\theta)\frac{\partial}{\partial \theta}\frac{p(x;\theta)}{q(x)} + \frac{p(x;\theta)}{q(x)}\frac{\partial f}{\partial \theta}(x,\theta)\right]
$$

The so-called "score function" derivative estimator is obtained for the choice of $q(x) = p(x;\theta)$, where the ratio simplifies to $1$ and its derivative becomes the score function:

$$
\frac{d}{d\theta}J(\theta) = \mathbb{E}_{x \sim p(x;\theta)}\left[f(x,\theta)\frac{\partial \log p(x,\theta)}{\partial \theta} + \frac{\partial f(x,\theta)}{\partial \theta}\right]
$$


### The Reparameterization Trick

An alternative approach eliminates the $\theta$-dependence in the sampling distribution by expressing $x$ through a deterministic transformation of the noise:

$$
x = g(\epsilon,\theta), \quad \epsilon \sim q(\epsilon)
$$

Therefore if we want to sample from some target distribution $p(x;\theta)$, we can do so by first sampling from a simple base distribution $q(\epsilon)$ (like a standard normal) and then transforming those samples through a carefully chosen function $g$. If $g(\cdot,\theta)$ is invertible, the change of variables formula tells us how these distributions relate:

$$
p(x;\theta) = q(g^{-1}(x,\theta))\left|\det\frac{\partial g^{-1}(x,\theta)}{\partial x}\right| = q(\epsilon)\left|\det\frac{\partial g(\epsilon,\theta)}{\partial \epsilon}\right|^{-1}
$$


For example, if we want to sample from any multivariate Gaussian distributions with covariance matrix $\Sigma$ and mean $\mu$, it suffices to be able to sample from a standard normal noise and compute the linear transformation:

$$
x = \mu + \Sigma^{1/2}\epsilon, \quad \epsilon \sim \mathcal{N}(0,I)
$$

where $\Sigma^{1/2}$ is the matrix square root obtained via Cholesky decomposition. In the univariate case, this transformation is simply: 

$$
x = \mu + \sigma \epsilon, \quad \epsilon \sim \mathcal{N}(0,1)
$$

where $\sigma = \sqrt{\sigma^2}$ is the standard deviation (square root of the variance).



#### Common Examples of Reparameterization

##### The Truncated Normal Distribution
When we need samples constrained to an interval $[a,b]$, we can use the truncated normal distribution. To sample from it, we transform uniform noise through the inverse cumulative distribution function (CDF) of the standard normal:

$$
x = \Phi^{-1}(u\Phi(b) + (1-u)\Phi(a)), \quad u \sim \text{Uniform}(0,1)
$$

Here:
- $\Phi(z) = \frac{1}{2}\left[1 + \text{erf}\left(\frac{z}{\sqrt{2}}\right)\right]$ is the CDF of the standard normal distribution
- $\Phi^{-1}$ is its inverse (the quantile function)
- $\text{erf}(z) = \frac{2}{\sqrt{\pi}}\int_0^z e^{-t^2}dt$ is the error function

The resulting samples follow a normal distribution restricted to $[a,b]$, with the density properly normalized over this interval.

##### The Kumaraswamy Distribution
When we need samples in the unit interval [0,1], a natural choice might be the Beta distribution. However, its inverse CDF doesn't have a closed form. Instead, we can use the Kumaraswamy distribution as a convenient approximation, which allows for a simple reparameterization:

$$
x = (1-(1-u^{\alpha})^{1/\beta}), \quad u \sim \text{Uniform}(0,1)
$$

where:
- $\alpha, \beta > 0$ are shape parameters that control the distribution
- $\alpha$ determines the concentration around 0 
- $\beta$ determines the concentration around 1
- The distribution is similar to Beta(α,β) but with analytically tractable CDF and inverse CDF

The Kumaraswamy distribution has density:

$$
f(x; \alpha, \beta) = \alpha\beta x^{\alpha-1}(1-x^{\alpha})^{\beta-1}, \quad x \in [0,1]
$$

##### The Gumbel-Softmax Distribution 

When sampling from a categorical distribution with probabilities $\{\pi_i\}$, one approach uses $\text{Gumbel}(0,1)$ noise combined with the argmax of log-perturbed probabilities:

$$
\text{argmax}_i(\log \pi_i + g_i), \quad g_i \sim \text{Gumbel}(0,1)
$$

This approach, known in machine learning as the Gumbel-Max trick, relies on sampling Gumbel noise from uniform random variables through the transformation $g_i = -\log(-\log(u_i))$ where $u_i \sim \text{Uniform}(0,1)$. To see why this gives us samples from the categorical distribution, consider the probability of selecting category $i$:

$$
\begin{align*}
P(\text{argmax}_j(\log \pi_j + g_j) = i) &= P(\log \pi_i + g_i > \log \pi_j + g_j \text{ for all } j \neq i) \\
&= P(g_i - g_j > \log \pi_j - \log \pi_i \text{ for all } j \neq i)
\end{align*}
$$

Since the difference of two Gumbel random variables follows a logistic distribution, $g_i - g_j \sim \text{Logistic}(0,1)$, and these differences are independent for different $j$ (due to the independence of the original Gumbel variables), we can write:

$$
\begin{align*}
P(\text{argmax}_j(\log \pi_j + g_j) = i) &= \prod_{j \neq i} P(g_i - g_j > \log \pi_j - \log \pi_i) \\
&= \prod_{j \neq i} \frac{\pi_i}{\pi_i + \pi_j} = \pi_i
\end{align*}
$$

The last equality requires some additional algebra to show, but follows from the fact that these probabilities must sum to 1 over all $i$.

While we have shown that the Gumbel-Max trick gives us exact samples from a categorical distribution, the argmax operation isn't differentiable. For stochastic optimization problems of the form:

$$
\mathbb{E}_{x \sim p(x;\theta)}[f(x)] = \mathbb{E}_{\epsilon \sim \text{Gumbel}(0,1)}[f(g(\epsilon,\theta))]
$$

we need $g$ to be differentiable with respect to $\theta$. This leads us to consider a continuous relaxation where we replace the hard argmax with a temperature-controlled softmax:

$$
z_i = \frac{\exp((\log \pi_i + g_i)/\tau)}{\sum_j \exp((\log \pi_j + g_j)/\tau)}
$$

As $\tau \to 0$, this approximation approaches the argmax:

$$
\lim_{\tau \to 0} \frac{\exp(x_i/\tau)}{\sum_j \exp(x_j/\tau)} = \begin{cases} 1 & \text{if } x_i = \max_j x_j \\ 0 & \text{otherwise} \end{cases}
$$

The resulting distribution over the probability simplex is called the Gumbel-Softmax (or Concrete) distribution. The temperature parameter $\tau$ controls the discreteness of our samples: smaller values give samples closer to one-hot vectors but with less stable gradients, while larger values give smoother gradients but more diffuse samples.


### Numerical Analysis of Gradient Estimators

Let us examine the behavior of our three gradient estimators for the stochastic optimization objective: 

$$J(\theta) = \mathbb{E}_{x \sim \mathcal{N}(\theta,1)}[x^2\theta]$$ 

To get an analytical expression for the derivative, first note that we can factor out $\theta$ to obtain $J(\theta) = \theta\mathbb{E}[x^2]$ where $x \sim \mathcal{N}(\theta,1)$. By definition of the variance, we know that $\text{Var}(x) = \mathbb{E}[x^2] - (\mathbb{E}[x])^2$, which we can rearrange to $\mathbb{E}[x^2] = \text{Var}(x) + (\mathbb{E}[x])^2$. Since $x \sim \mathcal{N}(\theta,1)$, we have $\text{Var}(x) = 1$ and $\mathbb{E}[x] = \theta$, therefore $\mathbb{E}[x^2] = 1 + \theta^2$. This gives us:

$$J(\theta) = \theta(1 + \theta^2)$$

Now differentiating with respect to $\theta$ using the product rule yields:

$$\frac{d}{d\theta}J(\theta) = 1 + 3\theta^2$$ 

For concreteness, we fix $\theta = 1.0$ and analyze samples drawn using Monte Carlo estimation with batch size 1000 and 1000 independent trials. Evaluating at $\theta = 1$ gives us $\frac{d}{d\theta}J(\theta)\big|_{\theta=1} = 1 + 3(1)^2 = 4$, which serves as our ground truth against which we compare our estimators:

1.  First, we consider the naive estimator that incorrectly differentiates the Monte Carlo approximation:

    $$\hat{g}_{\text{naive}}(\theta) = \frac{1}{N}\sum_{i=1}^N x_i^2$$

    For $x \sim \mathcal{N}(1,1)$, we have $\mathbb{E}[x^2] = \theta^2 + 1 = 2.0$ and $\mathbb{E}[\hat{g}_{\text{naive}}] = 2.0$. We should therefore expect a bias of about $-2$ in our experiment. 

2. Then we compute the score function estimator:

    $$\hat{g}_{\text{SF}}(\theta) = \frac{1}{N}\sum_{i=1}^N \left[x_i^2\theta(x_i - \theta) + x_i^2\right]$$

    This estimator is unbiased with $\mathbb{E}[\hat{g}_{\text{SF}}] = 4$

3. Finally, through the reparameterization $x = \theta + \epsilon$ where $\epsilon \sim \mathcal{N}(0,1)$, we obtain:

    $$\hat{g}_{\text{RT}}(\theta) = \frac{1}{N}\sum_{i=1}^N \left[2\theta(\theta + \epsilon_i) + (\theta + \epsilon_i)^2\right]$$

    This estimator is also unbiased with $\mathbb{E}[\hat{g}_{\text{RT}}] = 4$.


```{code-cell} python
:tags: [hide-input]

%config InlineBackend.figure_format = 'retina'
import jax
import jax.numpy as jnp
import matplotlib.pyplot as plt

# Apply book style
try:
    import scienceplots
    plt.style.use(['science', 'notebook'])
except (ImportError, OSError):
    pass  # Use matplotlib defaults

key = jax.random.PRNGKey(0)

# Define the objective function f(x,θ) = x²θ where x ~ N(θ, 1)
def objective(x, theta):
    return x**2 * theta

# Naive Monte Carlo gradient estimation
@jax.jit
def naive_gradient_batch(key, theta):
    samples = jax.random.normal(key, (1000,)) + theta
    # Use jax.grad on the objective with respect to theta
    grad_fn = jax.grad(lambda t: jnp.mean(objective(samples, t)))
    return grad_fn(theta)

# Score function estimator (REINFORCE)
@jax.jit
def score_function_batch(key, theta):
    samples = jax.random.normal(key, (1000,)) + theta
    # f(x,θ) * ∂logp(x|θ)/∂θ + ∂f(x,θ)/∂θ
    # score function for N(θ,1) is (x-θ)
    score = samples - theta
    return jnp.mean(objective(samples, theta) * score + samples**2)

# Reparameterization gradient
@jax.jit
def reparam_gradient_batch(key, theta):
    eps = jax.random.normal(key, (1000,))
    # Use reparameterization x = θ + ε, ε ~ N(0,1)
    grad_fn = jax.grad(lambda t: jnp.mean(objective(t + eps, t)))
    return grad_fn(theta)

# Run trials
n_trials = 1000
theta = 1.0
true_grad = 1 + 3 * theta**2

keys = jax.random.split(key, n_trials)
naive_estimates = jnp.array([naive_gradient_batch(k, theta) for k in keys])
score_estimates = jnp.array([score_function_batch(k, theta) for k in keys])
reparam_estimates = jnp.array([reparam_gradient_batch(k, theta) for k in keys])

# Create violin plots with individual points
plt.figure(figsize=(12, 6))
data = [naive_estimates, score_estimates, reparam_estimates]
colors = ['#ff9999', '#66b3ff', '#99ff99']

parts = plt.violinplot(data, showextrema=False)
for i, pc in enumerate(parts['bodies']):
    pc.set_facecolor(colors[i])
    pc.set_alpha(0.7)

# Add box plots
plt.boxplot(data, notch=True, showfliers=False)

# Add true gradient line
plt.axhline(y=true_grad, color='r', linestyle='--', label='True Gradient')

plt.xticks([1, 2, 3], ['Naive', 'Score Function', 'Reparam'])
plt.ylabel('Gradient Estimate')
plt.title(f'Gradient Estimators (θ={theta}, true grad={true_grad:.2f})')
plt.grid(True, alpha=0.3)
plt.legend()

# Print statistics
methods = {
    'Naive': naive_estimates,
    'Score Function': score_estimates, 
    'Reparameterization': reparam_estimates
}

for name, estimates in methods.items():
    bias = jnp.mean(estimates) - true_grad
    variance = jnp.var(estimates)
    print(f"\n{name}:")
    print(f"Mean: {jnp.mean(estimates):.6f}")
    print(f"Bias: {bias:.6f}")
    print(f"Variance: {variance:.6f}")
    print(f"MSE: {bias**2 + variance:.6f}")

```

The numerical experiments corroborate our theory. The naive estimator consistently underestimates the true gradient by 2.0, though it maintains a relatively small variance. This systematic bias would make it unsuitable for optimization despite its low variance. The score function estimator corrects this bias but introduces substantial variance. While unbiased, this estimator would require many samples to achieve reliable gradient estimates. Finally, the reparameterization trick achieves a much lower variance while remaining unbiased. While this experiment is for didactic purposes only, it reproduces what is commonly found in practice: that when applicable, the reparameterization estimator tends to perform better than the score function counterpart. 

## Reparameterization Methods in Reinforcement Learning

When dynamics are known or can be learned, reparameterization provides an alternative to score function methods. By expressing actions and state transitions as deterministic functions of noise, we can backpropagate through trajectories to compute policy gradients with lower variance than score function estimators.

### Stochastic Value Gradients

The reparameterization trick requires that we can express our random variable as a deterministic function of noise. In reinforcement learning, this applies naturally when we have a learned model of the dynamics. Consider a stochastic policy $\pi_{\boldsymbol{w}}(a|s)$ that we can reparameterize as $a = \pi_{\boldsymbol{w}}(s,\epsilon)$ where $\epsilon \sim p(\epsilon)$, and a dynamics model $s' = f(s,a,\xi)$ where $\xi \sim p(\xi)$ represents environment stochasticity. Both transformations are deterministic given the noise variables.

With these reparameterizations, we can write an $n$-step return as a differentiable function of the noise:

$$
R_n(s_0,\{\epsilon_i\},\{\xi_i\}) = \sum_{i=0}^{n-1} \gamma^i r(s_i,a_i)
$$

where $a_i = \pi_{\boldsymbol{w}}(s_i,\epsilon_i)$ and $s_{i+1} = f(s_i,a_i,\xi_i)$ for $i=0,...,n-1$. The objective becomes:

$$
J(\boldsymbol{w}) = \mathbb{E}_{\{\epsilon_i\},\{\xi_i\}}[R_n(s_0,\{\epsilon_i\},\{\xi_i\})]
$$

We can now apply the reparameterization gradient estimator:

$$
\nabla_{\boldsymbol{w}}J(\boldsymbol{w}) = \mathbb{E}_{\{\epsilon_i\},\{\xi_i\}}\left[\nabla_{\boldsymbol{w}}R_n(s_0,\{\epsilon_i\},\{\xi_i\})\right]
$$

This gradient can be computed by automatic differentiation through the sequence of policy and model evaluations. The computation requires backpropagating through $n$ steps of model rollouts, which becomes expensive for large $n$ but avoids the high variance of score function estimators.

The Stochastic Value Gradients (SVG) framework {cite:p}`Heess2015` uses this approach while introducing a hybrid objective that combines model rollouts with value function bootstrapping:

$$
J^{\text{SVG}(n)}(\boldsymbol{w}) = \mathbb{E}_{\{\epsilon_i\},\{\xi_i\}}\left[\sum_{i=0}^{n-1} \gamma^i r(s_i,a_i) + \gamma^n q(s_n,a_n;\theta)\right]
$$

The terminal value function $q(s_n,a_n;\theta)$ approximates the value beyond horizon $n$, allowing shorter rollouts while still capturing long-term value. This creates a spectrum of algorithms parameterized by $n$.

#### SVG(0): Model-Free Reparameterization

When $n=0$, the objective collapses to:

$$
J^{\text{SVG}(0)}(\boldsymbol{w}) = \mathbb{E}_{s \sim \rho}\mathbb{E}_{\epsilon \sim p(\epsilon)}\left[q(s,\pi_{\boldsymbol{w}}(s,\epsilon);\theta)\right]
$$

No model is required—we simply differentiate the critic with respect to actions sampled from the reparameterized policy. This is the approach used in DDPG {cite:p}`lillicrap2015continuous` (with a deterministic policy where $\epsilon$ is absent) and SAC {cite:p}`haarnoja2018sac` (where $\epsilon$ produces the stochastic component). The gradient is:

$$
\nabla_{\boldsymbol{w}} J^{\text{SVG}(0)} = \mathbb{E}_{s,\epsilon}\left[\nabla_a q(s,a;\theta)\big|_{a=\pi_{\boldsymbol{w}}(s,\epsilon)} \nabla_{\boldsymbol{w}} \pi_{\boldsymbol{w}}(s,\epsilon)\right]
$$

This requires only that the critic $q$ be differentiable with respect to actions, not a learned dynamics model. All bias comes from errors in the value function approximation.

#### SVG(1) to SVG($n$): Model-Based Rollouts

For $n \geq 1$, we unroll a learned dynamics model for $n$ steps before bootstrapping with the critic. Consider SVG(1):

$$
J^{\text{SVG}(1)}(\boldsymbol{w}) = \mathbb{E}_{s,\epsilon,\xi}\left[r(s,\pi_{\boldsymbol{w}}(s,\epsilon)) + \gamma q(f(s,\pi_{\boldsymbol{w}}(s,\epsilon),\xi), \pi_{\boldsymbol{w}}(s',\epsilon');\theta)\right]
$$

where $s' = f(s,\pi_{\boldsymbol{w}}(s,\epsilon),\xi)$ is the next state predicted by the model. The gradient now flows through both the reward and the model transition. Increasing $n$ propagates reward information more directly through the model rollout, reducing reliance on the critic. However, model errors compound over the horizon—if the model is inaccurate, longer rollouts can degrade performance.

#### SVG($\infty$): Pure Model-Based Optimization

As $n \to \infty$, we eliminate the critic entirely:

$$
J^{\text{SVG}(\infty)}(\boldsymbol{w}) = \mathbb{E}_{\{\epsilon_i\},\{\xi_i\}}\left[\sum_{i=0}^{T-1} \gamma^i r(s_i,\pi_{\boldsymbol{w}}(s_i,\epsilon_i))\right]
$$

This is pure model-based policy optimization, differentiating through the entire trajectory. Approaches like PILCO {cite:p}`deisenroth2011pilco` and Dreamer {cite:p}`hafner2019dreamer` operate in this regime. With an accurate model, this provides the most direct gradient signal. The tradeoff is computational: backpropagating through hundreds of time steps is expensive, and gradient magnitudes can explode or vanish over long horizons.

The choice of $n$ reflects a fundamental bias-variance tradeoff. Small $n$ relies on the critic for long-term value estimation, inheriting its approximation errors. Large $n$ relies on the model, accumulating its prediction errors. In practice, intermediate values like $n=5$ or $n=10$ often work well when combined with a reasonably accurate learned model.

#### Noise Inference for Off-Policy Learning

A subtle issue arises when combining reparameterization with experience replay. SVG naturally supports off-policy learning—states $s$ can be sampled from a replay buffer rather than the current policy. However, reparameterization requires the noise variables $\epsilon$ that generated each action.

For on-policy data, we can simply store $\epsilon$ alongside each transition $(s, a, r, s')$. For off-policy data collected under a different policy, the noise is unknown. To apply reparameterization gradients to such data, we must *infer* the noise that would have produced the observed action under the current policy.

For invertible policies, this is straightforward. If $a = \pi_{\boldsymbol{w}}(s, \epsilon)$ with $\epsilon \sim \mathcal{N}(0, I)$, and the policy takes the form $a = \mu_{\boldsymbol{w}}(s) + \sigma_{\boldsymbol{w}}(s) \odot \epsilon$ (as in a Gaussian policy), we can recover the noise exactly:

$$
\epsilon = \frac{a - \mu_{\boldsymbol{w}}(s)}{\sigma_{\boldsymbol{w}}(s)}
$$

This recovered $\epsilon$ can then be used for gradient computation. However, this introduces a subtle dependence: the inferred $\epsilon$ depends on the current policy parameters $\boldsymbol{w}$, not just the data. As the policy changes during training, the same action $a$ corresponds to different noise values.

For dynamics noise $\xi$, the situation is more complex. If we have a probabilistic model $s' = f(s, a, \xi)$ and observe the actual next state $s'$, we could in principle infer $\xi$. In practice, environment stochasticity is often treated as irreducible—we cannot replay the exact same noise realization. SVG handles this by either: (1) using deterministic models and ignoring environment stochasticity, (2) re-simulating from the model rather than using observed next states, or (3) using importance weighting to correct for the distribution mismatch.

The noise inference perspective connects reparameterization gradients to the broader question of credit assignment in RL. By explicitly tracking which noise realizations led to which outcomes, we can more precisely attribute value to policy parameters rather than to lucky or unlucky samples.

When dynamics are deterministic or can be accurately reparameterized, SVG-style methods offer an efficient alternative to score function estimators. However, many reinforcement learning problems involve unknown dynamics or dynamics that resist accurate modeling. For these settings, we turn to the score function approach, which requires only the ability to sample trajectories under the policy.

### Sampling-Based Model Predictive Control

Both SVG and the policy gradient methods above optimize a parametric policy $\pi_{\boldsymbol{w}}$ that is then deployed. An alternative approach uses the model purely for planning: at each state, optimize an action sequence and execute only the first action. This is **model predictive control** (MPC). This avoids training a policy network and eliminates generalization error from function approximation, but requires solving an optimization problem at every decision.

For continuous action spaces with complex dynamics, gradient-based trajectory optimization (as in the [trajectory optimization chapter](trajectories.md)) can be expensive. Model Predictive Path Integral control (MPPI) {cite:p}`williams2017mppi` replaces gradient-based optimization with **importance sampling**. Given a dynamics model $s_{t+1} = f(s_t, a_t)$ (deterministic) and current state $s_0$, sample $K$ action sequences $\{\boldsymbol{a}^{(i)}\}_{i=1}^K$ and roll them out to get costs $C^{(i)} = \sum_{t=0}^{H-1} c(s_t^{(i)}, a_t^{(i)})$. The optimal action is computed as:

$$
a_0^* = \sum_{i=1}^K w^{(i)} a_0^{(i)}, \quad w^{(i)} = \frac{\exp(-C^{(i)}/\lambda)}{\sum_j \exp(-C^{(j)}/\lambda)}
$$

where $\lambda > 0$ is a temperature parameter. Execute $a_0^*$, observe the next state, and repeat.

The weighting $w^{(i)} \propto \exp(-C^{(i)}/\lambda)$ is the Boltzmann distribution from entropy-regularized control. MPPI solves:

$$
\min_{\boldsymbol{a}} \mathbb{E}_{\boldsymbol{\xi}}\left[\sum_{t=0}^{H-1} c(s_t, a_t) + \lambda H(\pi)\right]
$$

where $\pi$ is the distribution over action sequences and $H(\pi) = -\mathbb{E}[\log \pi(\boldsymbol{a})]$ is entropy. The importance sampling estimate approximates the optimal action under this entropy-regularized objective. The temperature $\lambda$ controls the trade-off between exploitation (focus on low-cost sequences) and exploration (maintain entropy over sequences).

The Boltzmann weighting connects MPPI to entropy-regularized methods from the [amortization chapter](amortization.md). SAC (Algorithm {prf:ref}`sac`) learns a stochastic policy $\pi_{\boldsymbol{\phi}}$ that approximates the Boltzmann distribution over actions at each state. PCL (Algorithm {prf:ref}`pcl`) minimizes path consistency residuals, exploiting the exact relationship $v^*(s) = q^*(s,a) - \alpha\log\pi^*(a|s)$ under deterministic dynamics. MPPI uses the Boltzmann distribution directly for action sequence selection but performs no learning. At each state, it samples action sequences, weights them by exponentiated costs, and returns the weighted average.

All three use entropy regularization with Boltzmann weighting. SAC and PCL amortize optimization by learning policies that generalize across states. MPPI performs full optimization at every decision, trading computation for avoiding policy approximation error.

MPPI and SVG represent two ways to use learned models. SVG backpropagates through $n$-step model rollouts using the reparameterization trick, requiring differentiable dynamics and policy. MPPI samples many action sequences and aggregates using importance weights, requiring only forward simulation. SVG is more sample-efficient (gradients provide richer signal) but needs differentiable components and backpropagation through long horizons. MPPI is simpler to implement and applies to non-differentiable dynamics, but requires many rollout samples ($K \approx 100-1000$) per action selection.

```{prf:algorithm} Model Predictive Path Integral Control (MPPI)
:label: mppi

**Input:** Dynamics model $s_{t+1} = f(s_t, a_t)$, cost function $c(s,a)$, horizon $H$, number of samples $K$, temperature $\lambda$, noise distribution $\epsilon \sim \mathcal{N}(0, \Sigma)$

**Output:** Action $a_0^*$

1. Observe current state $s_0$
2. **for** $i = 1, \ldots, K$ **do**
    1. Sample action sequence: $a_t^{(i)} \leftarrow \bar{a}_t + \epsilon_t^{(i)}$ for $t = 0, \ldots, H-1$ $\quad$ // Perturb nominal
    2. Roll out: $s_{t+1}^{(i)} \leftarrow f(s_t^{(i)}, a_t^{(i)})$ for $t = 0, \ldots, H-1$
    3. Compute cost: $C^{(i)} \leftarrow \sum_{t=0}^{H-1} c(s_t^{(i)}, a_t^{(i)})$
3. Compute Boltzmann weights: $w^{(i)} \leftarrow \exp(-C^{(i)}/\lambda) / \sum_j \exp(-C^{(j)}/\lambda)$
4. **return** $a_0^* = \sum_{i=1}^K w^{(i)} a_0^{(i)}$
```

The algorithm samples perturbed action sequences around a nominal trajectory $\{\bar{a}_t\}$ (often the previous optimal sequence, shifted forward). The Boltzmann weights assign high probability to low-cost sequences. The temperature $\lambda$ balances exploitation (small $\lambda$ focuses on best samples) and exploration (large $\lambda$ gives uniform averaging).

MPPI excels at real-time control for systems with fast, accurate models (robotics, autonomous vehicles). The replanning handles model errors and disturbances. However, the per-step computation scales as $O(KH)$ model evaluations, making it expensive for complex dynamics or long horizons. For problems requiring learning from experience rather than planning with a known model, policy gradient methods (developed in the next section) provide complementary strengths.

## Score Function Methods in Reinforcement Learning 

Let $G(\tau) \equiv \sum_{t=0}^T r(s_t, a_t)$ be the sum of undiscounted rewards in a trajectory $\tau$. The stochastic optimization problem we face is to maximize:

$$
J(\boldsymbol{w}) = \mathbb{E}_{\tau \sim p(\tau;\boldsymbol{w})}[G(\tau)]
$$

where $\tau = (s_0,a_0,s_1,a_1,...)$ is a trajectory and $G(\tau)$ is the total return. 
Applying the score function estimator, we get:

$$
\begin{align*}
\nabla_{\boldsymbol{w}}J(\boldsymbol{w}) &= \nabla_{\boldsymbol{w}}\mathbb{E}_{\tau}[G(\tau)] \\
&= \mathbb{E}_{\tau}\left[G(\tau)\nabla_{\boldsymbol{w}}\log p(\tau;\boldsymbol{w})\right] \\
&= \mathbb{E}_{\tau}\left[G(\tau)\nabla_{\boldsymbol{w}}\sum_{t=0}^T\log \pi_{\boldsymbol{w}}(a_t|s_t)\right] \\
&= \mathbb{E}_{\tau}\left[G(\tau)\sum_{t=0}^T\nabla_{\boldsymbol{w}}\log \pi_{\boldsymbol{w}}(a_t|s_t)\right]
\end{align*}
$$

We have eliminated the need to know the transition probabilities in this estimator since the probability of a trajectory factorizes as:

$$
p(\tau;\boldsymbol{w}) = p(s_0)\prod_{t=0}^T \pi_{\boldsymbol{w}}(a_t|s_t)p(s_{t+1}|s_t,a_t)
$$

Therefore, only the policy depends on $\boldsymbol{w}$. When taking the logarithm of this product, we get a sum where all the $\boldsymbol{w}$-independent terms vanish. The final estimator samples trajectories under the distribution $p(\tau; \boldsymbol{w})$ and computes:

$$
\nabla_{\boldsymbol{w}}J(\boldsymbol{w}) \approx \frac{1}{N}\sum_{i=1}^N\left[G(\tau^{(i)})\sum_{t=0}^T\nabla_{\boldsymbol{w}}\log d(a_t^{(i)}|s_t^{(i)};\boldsymbol{w})\right]
$$

This is a direct application of the score function estimator. However, we rarely use this form in practice and instead make several improvements to further reduce the variance.

## Leveraging Conditional Independence 

Given the Markov property of the MDP, rewards $r_k$ for $k < t$ are conditionally independent of action $a_t$ given the history $h_t = (s_0,a_0,...,s_{t-1},a_{t-1},s_t)$. This allows us to only need to consider future rewards when computing policy gradients.

$$
\begin{align*}
\nabla_{\boldsymbol{w}}J(\boldsymbol{w}) &= \mathbb{E}_{\tau}\left[\sum_{t=0}^T\nabla_{\boldsymbol{w}}\log \pi_{\boldsymbol{w}}(a_t|s_t)\sum_{k=0}^T r_k\right] \\
&= \mathbb{E}_{\tau}\left[\sum_{t=0}^T\nabla_{\boldsymbol{w}}\log \pi_{\boldsymbol{w}}(a_t|s_t)\left(\sum_{k=0}^{t-1} r_k + \sum_{k=t}^T r_k\right)\right] \\
&= \mathbb{E}_{\tau}\left[\sum_{t=0}^T\nabla_{\boldsymbol{w}}\log \pi_{\boldsymbol{w}}(a_t|s_t)\sum_{k=t}^T r_k\right]
\end{align*}
$$

The conditional independence assumption means that the term $\mathbb{E}_{\tau}\left[\sum_{t=0}^T\nabla_{\boldsymbol{w}}\log \pi_{\boldsymbol{w}}(a_t|s_t)\sum_{k=0}^{t-1} r_k \right]$ vanishes. To see this, factor the trajectory distribution as:

$$
p(\tau) = p(s_0,...,s_t,a_0,...,a_{t-1})\cdot \pi_{\boldsymbol{w}}(a_t|s_t)\cdot p(s_{t+1},...,s_T,a_{t+1},...,a_T|s_t,a_t)
$$

We can now re-write a single term of this summation as: 

$$
\mathbb{E}_{\tau}\left[\nabla_{\boldsymbol{w}}\log \pi_{\boldsymbol{w}}(a_t|s_t)\sum_{k=0}^{t-1} r_k\right] = \mathbb{E}_{s_{0:t},a_{0:t-1}}\left[\sum_{k=0}^{t-1} r_k \cdot \mathbb{E}_{a_t}\left[\nabla_{\boldsymbol{w}}\log \pi_{\boldsymbol{w}}(a_t|s_t)\right]\right]
$$

The inner expectation is zero because 

$$
\begin{align*}
\mathbb{E}_{a_t}\left[\nabla_{\boldsymbol{w}}\log \pi_{\boldsymbol{w}}(a_t|s_t)\right] &= \int \nabla_{\boldsymbol{w}}\log \pi_{\boldsymbol{w}}(a_t|s_t)\pi_{\boldsymbol{w}}(a_t|s_t)da_t \\
&= \int \frac{\nabla_{\boldsymbol{w}}\pi_{\boldsymbol{w}}(a_t|s_t)}{\pi_{\boldsymbol{w}}(a_t|s_t)}\pi_{\boldsymbol{w}}(a_t|s_t)da_t \\
&= \int \nabla_{\boldsymbol{w}}\pi_{\boldsymbol{w}}(a_t|s_t)da_t \\
&= \nabla_{\boldsymbol{w}}\int \pi_{\boldsymbol{w}}(a_t|s_t)da_t \\
&= \nabla_{\boldsymbol{w}}1 = 0
\end{align*}
$$

The Monte Carlo estimator becomes:

$$
\nabla_{\boldsymbol{w}}J(\boldsymbol{w}) \approx \frac{1}{N}\sum_{i=1}^N\left[\sum_{t=0}^T\nabla_{\boldsymbol{w}}\log d(a_t^{(i)}|s_t^{(i)};\boldsymbol{w})\sum_{k=t}^T r_k^{(i)}\right]
$$

The benefit of this estimator compared to the naive one is that it generally has less variance. More formally, we can show that this estimator arises from the application of a variance reduction technique known as the Extended Conditional Monte Carlo Method. 

## Variance Reduction via Control Variates

A control variate is a zero-mean random variable that we subtract from our estimator to reduce variance. Given an estimator $Z$ and a control variate $C$ with $\mathbb{E}[C]=0$, we can construct a new unbiased estimator:

$$
Z_{\text{cv}} = Z - \alpha C
$$

where $\alpha$ is a coefficient we can choose. The variance of this new estimator is:

$$
\text{Var}(Z_{\text{cv}}) = \text{Var}(Z) + \alpha^2\text{Var}(C) - 2\alpha\text{Cov}(Z,C)
$$

The optimal $\alpha$ that minimizes this variance is:

$$
\alpha^* = \frac{\text{Cov}(Z,C)}{\text{Var}(C)}
$$

In the reinforcement learning setting, we usually choose $C_t = \nabla_{\boldsymbol{w}}\log \pi_{\boldsymbol{w}}(a_t|s_t)$ as our control variate at each timestep. For a given state $s_t$, our estimator at time $t$ is:

$$
Z_t = \nabla_{\boldsymbol{w}}\log \pi_{\boldsymbol{w}}(a_t|s_t)\sum_{k=t}^T r_k
$$

Our control variate estimator becomes:

$$
Z_{t,\text{cv}} = Z_t - \alpha_t^* C_t = \nabla_{\boldsymbol{w}}\log \pi_{\boldsymbol{w}}(a_t|s_t)(\sum_{k=t}^T r_k - \alpha_t^*)
$$

Following the general theory, and using the fact that $\mathbb{E}[C_t|s_t] = 0$ the optimal coefficient is:

$$
\begin{align*}
\alpha^*_t = \frac{\text{Cov}(Z_t,C_t|s_t)}{\text{Var}(C_t|s_t)} &= \frac{\mathbb{E}[Z_tC_t^T|s_t] - \mathbb{E}[Z_t|s_t]\mathbb{E}[C_t^T|s_t]}{\mathbb{E}[C_tC_t^T|s_t] - \mathbb{E}[C_t|s_t]\mathbb{E}[C_t^T|s_t]} \\
&= \frac{\mathbb{E}[\nabla_{\boldsymbol{w}}\log \pi_{\boldsymbol{w}}(a_t|s_t)\nabla_{\boldsymbol{w}}\log \pi_{\boldsymbol{w}}(a_t|s_t)^T\sum_{k=t}^T r_k|s_t] - 0}{\mathbb{E}[\nabla_{\boldsymbol{w}}\log \pi_{\boldsymbol{w}}(a_t|s_t)\nabla_{\boldsymbol{w}}\log \pi_{\boldsymbol{w}}(a_t|s_t)^T|s_t] - 0} \\
&= \frac{\mathbb{E}[\|\nabla_{\boldsymbol{w}}\log \pi_{\boldsymbol{w}}(a_t|s_t)\|^2\sum_{k=t}^T r_k|s_t]}{\mathbb{E}[\|\nabla_{\boldsymbol{w}}\log \pi_{\boldsymbol{w}}(a_t|s_t)\|^2|s_t]} \\
&= \frac{\mathbb{E}_{a_t|s_t}[\|\nabla_{\boldsymbol{w}}\log \pi_{\boldsymbol{w}}(a_t|s_t)\|^2]\mathbb{E}[\sum_{k=t}^T r_k|s_t]}{\mathbb{E}_{a_t|s_t}[\|\nabla_{\boldsymbol{w}}\log \pi_{\boldsymbol{w}}(a_t|s_t)\|^2]} \\
&= \mathbb{E}[\sum_{k=t}^T r_k|s_t] = v^{d_{\boldsymbol{w}}}(s_t)
\end{align*}
$$

Therefore, our variance-reduced estimator becomes:

$$
Z_{\text{cv},t} = \nabla_{\boldsymbol{w}}\log \pi_{\boldsymbol{w}}(a_t|s_t)\left(\sum_{k=t}^T r_k - v^{d_{\boldsymbol{w}}}(s_t)\right)
$$


In practice when implementing this estimator, we won't have access to the true value function. So as we did earlier for NFQCA or SAC, we commonly learn that value function simultaneously with the policy. We could either use a fitted value approach, or even more simply just regress from states to sum of rewards to learn what Williams (1992) called a "baseline": 

```{prf:algorithm} Policy Gradient with Simple Baseline
:label: policy-grad-baseline

**Input:** Policy parameterization $\pi_{\boldsymbol{w}}(a|s)$, baseline function $b(s;\boldsymbol{\theta})$  
**Output:** Updated policy parameters $\boldsymbol{w}$  
**Hyperparameters:** Learning rates $\alpha_w$, $\alpha_\theta$, number of episodes $N$, episode length $T$

1. Initialize parameters $\boldsymbol{w}$, $\boldsymbol{\theta}$
2. For episode = 1, ..., $N$ do:
   1. Collect trajectory $\tau = (s_0, a_0, r_0, ..., s_T, a_T, r_T)$ using policy $\pi_{\boldsymbol{w}}(a|s)$
   2. Compute returns for each timestep: $R_t = \sum_{k=t}^T r_k$
   3. Update baseline: $\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} + \alpha_\theta \nabla_{\boldsymbol{\theta}}\sum_{t=0}^T (R_t - b(s_t;\boldsymbol{\theta}))^2$
   4. For $t = 0, ..., T$ do:
      1. Update policy: $\boldsymbol{w} \leftarrow \boldsymbol{w} + \alpha_w \nabla_{\boldsymbol{w}}\log \pi_{\boldsymbol{w}}(a_t|s_t)(R_t - b(s_t;\boldsymbol{\theta}))$
3. Return $\boldsymbol{w}$
```

When implementing this algorithm nowadays, we always use mini-batching to make full use of our GPUs. Therefore, a more representative variant for this algorithm would be: 


```{prf:algorithm} Policy Gradient with Optimal Control Variate and Mini-batches
:label: policy-grad-cv-batch

**Input:** Policy parameterization $\pi_{\boldsymbol{w}}(a|s)$, value function $v(s;\boldsymbol{\theta})$  
**Output:** Updated policy parameters $\boldsymbol{w}$  
**Hyperparameters:** Learning rates $\alpha_w$, $\alpha_\theta$, number of iterations $N$, episode length $T$, batch size $B$, mini-batch size $M$

1. Initialize parameters $\boldsymbol{w}$, $\boldsymbol{\theta}$
2. For iteration = 1, ..., N:
    1. Initialize empty buffer $\mathcal{D}$
    2. For b = 1, ..., B:
        1. Collect trajectory $\tau_b = (s_0, a_0, r_0, ..., s_T, a_T, r_T)$ using policy $\pi_{\boldsymbol{w}}(a|s)$
        2. Compute returns: $R_t = \sum_{k=t}^T r_k$ for all t
        3. Store tuple $(s_t, a_t, R_t)_{t=0}^T$ in $\mathcal{D}$
    3. Compute value targets: $v_{\text{target}}(s) = \frac{1}{|\mathcal{D}_s|}\sum_{(s,\cdot,R) \in \mathcal{D}_s} R$
    4. For value_epoch = 1, ..., K:
        1. Sample mini-batch $\mathcal{B}_v$ of size $M$ from $\mathcal{D}$
        2. Compute value loss: $L_v = \frac{1}{M}\sum_{(s,\cdot,R) \in \mathcal{B}_v} (v(s;\boldsymbol{\theta}) - v_{\text{target}}(s))^2$
        3. Update value function: $\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} - \alpha_\theta \nabla_{\boldsymbol{\theta}}L_v$
    5. Compute advantages: $A(s,a) = R - v(s;\boldsymbol{\theta})$ for all $(s,a,R) \in \mathcal{D}$
    6. Normalize advantages: $A \leftarrow \frac{A - \mu_A}{\sigma_A}$
    7. For policy_epoch = 1, ..., J:
        1. Sample mini-batch $\mathcal{B}_\pi$ of size $M$ from $\mathcal{D}$
        2. Compute policy loss: $L_\pi = -\frac{1}{M}\sum_{(s,a,\cdot) \in \mathcal{B}_\pi} \log \pi_{\boldsymbol{w}}(a|s)A(s,a)$
        3. Update policy: $\boldsymbol{w} \leftarrow \boldsymbol{w} - \alpha_w \nabla_{\boldsymbol{w}}L_\pi$
3. Return $\boldsymbol{w}$
```

## Generalized Advantage Estimator 


Given our control variate estimator with baseline $v(s)$, we have:

$$
\nabla_{\boldsymbol{w}}\log \pi_{\boldsymbol{w}}(a_t|s_t)(G_t - v(s_t))
$$

where $G_t$ is the return $\sum_{k=t}^T r_k$. We can improve this estimator by considering how it relates to the advantage function, defined as:

$$
A(s_t,a_t) = q(s_t,a_t) - v(s_t)
$$

where $q(s_t,a_t)$ is the action-value function. Due to the Bellman equation:

$$
q(s_t,a_t) = \mathbb{E}_{s_{t+1},r_t}[r_t + \gamma v(s_{t+1})|s_t,a_t]
$$

This leads to the one-step TD error:

$$
\delta_t = r_t + \gamma v(s_{t+1}) - v(s_t)
$$

Decompose our original term:

$$
\begin{align*}
G_t - v(s_t) &= r_t + \gamma G_{t+1} - v(s_t) \\
&= r_t + \gamma v(s_{t+1}) - v(s_t) + \gamma(G_{t+1} - v(s_{t+1})) \\
&= \delta_t + \gamma(G_{t+1} - v(s_{t+1}))
\end{align*}
$$

Expanding recursively:

$$
\begin{align*}
G_t - v(s_t) &= \delta_t + \gamma(G_{t+1} - v(s_{t+1})) \\
&= \delta_t + \gamma[\delta_{t+1} + \gamma(G_{t+2} - v(s_{t+2}))] \\
&= \delta_t + \gamma\delta_{t+1} + \gamma^2\delta_{t+2} + ... + \gamma^{T-t}\delta_T
\end{align*}
$$

GAE generalizes this by introducing a weighted version with parameter $\lambda$:

$$
A^{\text{GAE}(\gamma,\lambda)}(s_t,a_t) = (1-\lambda)\sum_{k=0}^{\infty}\lambda^k\sum_{l=0}^k \gamma^l\delta_{t+l}
$$

Which simplifies to:

$$
A^{\text{GAE}(\gamma,\lambda)}(s_t,a_t) = \sum_{l=0}^{\infty}(\gamma\lambda)^l\delta_{t+l}
$$

This formulation allows us to trade off bias and variance through $\lambda$:
- $\lambda=0$: one-step TD error (low variance, high bias)
- $\lambda=1$: Monte Carlo estimate (high variance, low bias)


The general GAE algorithm with mini-batches is the following: 

```{prf:algorithm} Policy Gradient with GAE and Mini-batches
:label: policy-grad-gae-batch

**Input:** Policy parameterization $\pi_{\boldsymbol{w}}(a|s)$, value function $v(s;\boldsymbol{\theta})$  
**Output:** Updated policy parameters $\boldsymbol{w}$  
**Hyperparameters:** Learning rates $\alpha_w$, $\alpha_\theta$, number of iterations $N$, episode length $T$, batch size $B$, mini-batch size $M$, discount $\gamma$, GAE parameter $\lambda$

1. Initialize parameters $\boldsymbol{w}$, $\boldsymbol{\theta}$
2. For iteration = 1, ..., N:
    1. Initialize empty buffer $\mathcal{D}$
    2. For b = 1, ..., B:
        1. Collect trajectory $\tau_b = (s_0, a_0, r_0, ..., s_T, a_T, r_T)$ using policy $\pi_{\boldsymbol{w}}(a|s)$
        2. Compute TD errors: $\delta_t = r_t + \gamma v(s_{t+1};\boldsymbol{\theta}) - v(s_t;\boldsymbol{\theta})$ for all t
        3. Compute GAE advantages:
            1. Initialize $A_T = 0$
            2. For t = T-1, ..., 0:
                1. $A_t = \delta_t + (\gamma\lambda)A_{t+1}$
        4. Store tuple $(s_t, a_t, A_t, v(s_t;\boldsymbol{\theta}))_{t=0}^T$ in $\mathcal{D}$
    3. For value_epoch = 1, ..., K:
        1. Sample mini-batch $\mathcal{B}_v$ of size $M$ from $\mathcal{D}$
        2. Compute value loss: $L_v = \frac{1}{M}\sum_{(s,\cdot,\cdot,v_{\text{old}}) \in \mathcal{B}_v} (v(s;\boldsymbol{\theta}) - v_{\text{old}})^2$
        3. Update value function: $\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} - \alpha_\theta \nabla_{\boldsymbol{\theta}}L_v$
    4. Normalize advantages: $A \leftarrow \frac{A - \mu_A}{\sigma_A}$
    5. For policy_epoch = 1, ..., J:
        1. Sample mini-batch $\mathcal{B}_\pi$ of size $M$ from $\mathcal{D}$
        2. Compute policy loss: $L_\pi = -\frac{1}{M}\sum_{(s,a,A,\cdot) \in \mathcal{B}_\pi} \log \pi_{\boldsymbol{w}}(a|s)A$
        3. Update policy: $\boldsymbol{w} \leftarrow \boldsymbol{w} - \alpha_w \nabla_{\boldsymbol{w}}L_\pi$
3. Return $\boldsymbol{w}$
```

With $\lambda = 0$, the GAE advantage estimator becomes just the one-step TD error:

$$
A^{\text{GAE}(\gamma,0)}(s_t,a_t) = \delta_t = r_t + \gamma v(s_{t+1}) - v(s_t)
$$

The non-batched, purely online, GAE(0) algorithm then becomes: 

```{prf:algorithm} Actor-Critic with TD(0)
:label: actor-critic-td0

**Input:** Policy parameterization $\pi_{\boldsymbol{w}}(a|s)$, value function $v(s;\boldsymbol{\theta})$  
**Output:** Updated policy parameters $\boldsymbol{w}$  
**Hyperparameters:** Learning rates $\alpha_w$, $\alpha_\theta$, number of episodes $N$, episode length $T$, discount $\gamma$

1. Initialize parameters $\boldsymbol{w}$, $\boldsymbol{\theta}$
2. For episode = 1, ..., $N$ do:
   1. Initialize state $s_0$
   2. For $t = 0, ..., T$ do:
      1. Sample action: $a_t \sim \pi_{\boldsymbol{w}}(\cdot|s_t)$
      2. Execute $a_t$, observe $r_t$, $s_{t+1}$
      3. Compute TD error: $\delta_t = r_t + \gamma v(s_{t+1};\boldsymbol{\theta}) - v(s_t;\boldsymbol{\theta})$
      4. Update value function: $\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} + \alpha_\theta \delta_t \nabla_{\boldsymbol{\theta}}v(s_t;\boldsymbol{\theta})$
      5. Update policy: $\boldsymbol{w} \leftarrow \boldsymbol{w} + \alpha_w \nabla_{\boldsymbol{w}}\log \pi_{\boldsymbol{w}}(a_t|s_t)\delta_t$
3. Return $\boldsymbol{w}$
```
This version was first derived by Richard Sutton in his 1984 PhD thesis. He called it the Adaptive Heuristic Actor-Critic algorithm. As far as I know, it was not derived using the score function method outlined here, but rather through intuitive reasoning (great intuition!).

## The Policy Gradient Theorem

Sutton (1999) provided an expression for the gradient of the infinite discounted return with respect to the parameters of a parameterized policy. Here is an alternative derivation by considering a bilevel optimization problem:

$$
\max_{\mathbf{w}} \alpha^\top \mathbf{v}_\gamma^{d^\infty}
$$

subject to:

$$
(\mathbf{I} - \gamma \mathbf{P}_d) \mathbf{v}_\gamma^{d^\infty} = \mathbf{r}_d
$$

The Implicit Function Theorem states that if there is a solution to the problem $F(\mathbf{v}, \mathbf{w}) = 0$, then we can "reparameterize" our problem as $F(\mathbf{v}(\mathbf{w}), \mathbf{w})$ where $\mathbf{v}(\mathbf{w})$ is an implicit function of $\mathbf{w}$. If the Jacobian $\frac{\partial F}{\partial \mathbf{v}}$ is invertible, then:

$$
\frac{d\mathbf{v}(\mathbf{w})}{d\mathbf{w}} = -\left(\frac{\partial F(\mathbf{v}(\mathbf{w}), \mathbf{w})}{\partial \mathbf{x}}\right)^{-1}\frac{\partial F(\mathbf{v}(\mathbf{w}), \mathbf{w})}{\partial \mathbf{w}}
$$

Here we made it clear in our notation that the derivative must be evaluated at root $(\mathbf{v}(\mathbf{w}), \mathbf{w})$ of $F$. For the remaining of this derivation, we will drop this dependence to make notation more compact. 

Applying this to our case with $F(\mathbf{v}, \mathbf{w}) = (\mathbf{I} - \gamma \mathbf{P}_d)\mathbf{v} - \mathbf{r}_d$:

$$
\frac{\partial \mathbf{v}_\gamma^{d^\infty}}{\partial \mathbf{w}} = (\mathbf{I} - \gamma \mathbf{P}_d)^{-1}\left(\frac{\partial \mathbf{r}_d}{\partial \mathbf{w}} + \gamma \frac{\partial \mathbf{P}_d}{\partial \mathbf{w}}\mathbf{v}_\gamma^{d^\infty}\right)
$$

Then:

$$
\begin{align*}
\nabla_{\mathbf{w}}J(\mathbf{w}) &= \alpha^\top \frac{\partial \mathbf{v}_\gamma^{d^\infty}}{\partial \mathbf{w}} \\
&= \mathbf{x}_\alpha^\top\left(\frac{\partial \mathbf{r}_d}{\partial \mathbf{w}} + \gamma \frac{\partial \mathbf{P}_d}{\partial \mathbf{w}}\mathbf{v}_\gamma^{d^\infty}\right)
\end{align*}
$$

where we have defined the discounted state visitation distribution:

$$
\mathbf{x}_\alpha^\top \equiv \alpha^\top(\mathbf{I} - \gamma \mathbf{P}_d)^{-1}.
$$

Remember the vector notation for MDPs:

$$
\begin{align*}
[\mathbf{r}_d]_s &= \sum_a d(a|s)r(s,a) \\
[\mathbf{P}_d]_{ss'} &= \sum_a d(a|s)P(s'|s,a)
\end{align*}
$$

Then taking the derivatives gives us: 

$$
\begin{align*}
\left[\frac{\partial \mathbf{r}_d}{\partial \mathbf{w}}\right]_s &= \sum_a \nabla_{\mathbf{w}}d(a|s)r(s,a) \\
\left[\frac{\partial \mathbf{P}_d}{\partial \mathbf{w}}\mathbf{v}_\gamma^{d^\infty}\right]_s &= \sum_a \nabla_{\mathbf{w}}d(a|s)\sum_{s'} P(s'|s,a)v_\gamma^{d^\infty}(s')
\end{align*}
$$

Substituting back:

$$
\begin{align*}
\nabla_{\mathbf{w}}J(\mathbf{w}) &= \sum_s x_\alpha(s)\left(\sum_a \nabla_{\mathbf{w}}d(a|s)r(s,a) + \gamma\sum_a \nabla_{\mathbf{w}}d(a|s)\sum_{s'} P(s'|s,a)v_\gamma^{d^\infty}(s')\right) \\
&= \sum_s x_\alpha(s)\sum_a \nabla_{\mathbf{w}}d(a|s)\left(r(s,a) + \gamma \sum_{s'} P(s'|s,a)v_\gamma^{d^\infty}(s')\right)
\end{align*}
$$

This is the policy gradient theorem, where $x_\alpha(s)$ is the discounted state visitation distribution and the term in parentheses is the state-action value function $q(s,a)$.


## Normalized Discounted State Visitation Distribution

The discounted state visitation $x_\alpha(s)$ is not normalized. Therefore the expression we obtained above is not an expectation. However, we can transform it into one by normalizing by $1 - \gamma$. Note that for any initial distribution $\alpha$:

$$
\sum_s x_\alpha(s) = \alpha^\top(\mathbf{I} - \gamma \mathbf{P}_d)^{-1}\mathbf{1} = \frac{\alpha^\top\mathbf{1}}{1-\gamma} = \frac{1}{1-\gamma}
$$

Therefore, defining the normalized state distribution $\mu_\alpha(s) = (1-\gamma)x_\alpha(s)$, we can write:

$$
\begin{align*}
\nabla_{\mathbf{w}}J(\mathbf{w}) &= \frac{1}{1-\gamma}\sum_s \mu_\alpha(s)\sum_a \nabla_{\mathbf{w}}d(a|s)\left(r(s,a) + \gamma \sum_{s'} P(s'|s,a)v_\gamma^{d^\infty}(s')\right) \\
&= \frac{1}{1-\gamma}\mathbb{E}_{s\sim\mu_\alpha}\left[\sum_a \nabla_{\mathbf{w}}d(a|s)Q(s,a)\right]
\end{align*}
$$

Now we have expressed the policy gradient theorem in terms of expectations under the normalized discounted state visitation distribution. But what does sampling from $\mu_\alpha$ mean? Recall that $\mathbf{x}_\alpha^\top = \alpha^\top(\mathbf{I} - \gamma \mathbf{P}_d)^{-1}$. Using the Neumann series expansion (valid when $\|\gamma \mathbf{P}_d\| < 1$, which holds for $\gamma < 1$ since $\mathbf{P}_d$ is a stochastic matrix) we have:

$$
\boldsymbol{\mu}_\alpha^\top = (1-\gamma)\alpha^\top\sum_{k=0}^{\infty} (\gamma \mathbf{P}_d)^k
$$

We can then factor out the first term from this summation to obtain:

$$
\begin{align*}
\boldsymbol{\mu}_\alpha^\top &= (1-\gamma)\alpha^\top\sum_{k=0}^{\infty} (\gamma \mathbf{P}_d)^k \\
&= (1-\gamma)\alpha^\top + (1-\gamma)\alpha^\top\sum_{k=1}^{\infty} (\gamma \mathbf{P}_d)^k \\
&= (1-\gamma)\alpha^\top + (1-\gamma)\alpha^\top\gamma\mathbf{P}_d\sum_{k=0}^{\infty} (\gamma \mathbf{P}_d)^k \\
&= (1-\gamma)\alpha^\top + \gamma\boldsymbol{\mu}_\alpha^\top \mathbf{P}_d
\end{align*}
$$


The balance equation:

$$
\boldsymbol{\mu}_\alpha^\top = (1-\gamma)\alpha^\top + \gamma\boldsymbol{\mu}_\alpha^\top \mathbf{P}_d
$$


shows that $\boldsymbol{\mu}_\alpha$ is a mixture distribution: with probability $1-\gamma$ you draw a state from the initial distribution $\alpha$ (reset), and with probability $\gamma$ you follow the policy dynamics $\mathbf{P}_d$ from the current state (continue). This interpretation directly connects to the geometric process: at each step you either terminate and resample from $\alpha$ (with probability $1-\gamma$) or continue following the policy (with probability $\gamma$).

```{code-cell} python
import numpy as np

def sample_from_discounted_visitation(
    alpha, 
    policy, 
    transition_model, 
    gamma, 
    n_samples=1000
):
    """Sample states from the discounted visitation distribution.
    
    Args:
        alpha: Initial state distribution (vector of probabilities)
        policy: Function (state -> action probabilities)
        transition_model: Function (state, action -> next state probabilities)
        gamma: Discount factor
        n_samples: Number of states to sample
    
    Returns:
        Array of sampled states
    """
    samples = []
    n_states = len(alpha)
    
    # Initialize state from alpha
    current_state = np.random.choice(n_states, p=alpha)
    
    for _ in range(n_samples):
        samples.append(current_state)
        
        # With probability (1-gamma): reset
        if np.random.random() > gamma:
            current_state = np.random.choice(n_states, p=alpha)
        # With probability gamma: continue
        else:
            # Sample action from policy
            action_probs = policy(current_state)
            action = np.random.choice(len(action_probs), p=action_probs)
            
            # Sample next state from transition model
            next_state_probs = transition_model(current_state, action)
            current_state = np.random.choice(n_states, p=next_state_probs)
    
    return np.array(samples)

# Example usage for a simple 2-state MDP
alpha = np.array([0.7, 0.3])  # Initial distribution
policy = lambda s: np.array([0.8, 0.2])  # Dummy policy
transition_model = lambda s, a: np.array([0.9, 0.1])  # Dummy transitions
gamma = 0.9

samples = sample_from_discounted_visitation(alpha, policy, transition_model, gamma)

# Check empirical distribution
print("Empirical state distribution:")
print(np.bincount(samples) / len(samples))
```

While the math shows that sampling from the discounted visitation distribution $\boldsymbol{\mu}_\alpha$ would give us unbiased policy gradient estimates, Thomas (2014) demonstrated that this implementation can be detrimental to performance in practice. The issue arises because terminating trajectories early (with probability $1-\gamma$) reduces the effective amount of data we collect from each trajectory. This early termination weakens the learning signal, as many trajectories don't reach meaningful terminal states or rewards.

Therefore, in practice, we typically sample complete trajectories from the undiscounted process (running the policy until natural termination or a fixed horizon) while still using $\gamma$ in the advantage estimation. This approach preserves the full learning signal from each trajectory and has been empirically shown to lead to better performance. 

This is one of several cases in RL where the theoretically optimal procedure differs from the best practical implementation.

# Summary

This chapter developed the mathematical foundations for policy gradient methods. Starting from general derivative estimation techniques in stochastic optimization, we saw two main approaches: the likelihood ratio (score function) method and the reparameterization trick. While the reparameterization trick typically offers lower variance, it requires that the sampling distribution be reparameterizable, making it inapplicable to discrete actions or environments with complex dynamics.

For reinforcement learning, the score function estimator provides a model-free gradient that depends only on the policy parametrization, not the transition dynamics. Through variance reduction techniques (leveraging conditional independence, using control variates, and the Generalized Advantage Estimator), we can make these gradients practical for learning. The resulting algorithms form the foundation of modern policy optimization methods.

The next chapter explores advanced policy optimization techniques including trust region methods, proximal policy optimization, and connections to optimal control theory.