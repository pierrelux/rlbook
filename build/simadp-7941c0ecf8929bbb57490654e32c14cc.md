---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.16.3
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# Simulation-Based Approximate Dynamic Programming 

The projection methods from the previous chapter showed how to transform the infinite-dimensional fixed-point problem $\Bellman v = v$ into a finite-dimensional one by choosing basis functions $\{\varphi_i\}$ and imposing conditions that make the residual $R(s) = \Bellman v(s) - v(s)$ small. Different projection conditions (Galerkin orthogonality, collocation at points, least squares minimization) yield different finite-dimensional systems to solve.

However, we left unresolved the question of how to evaluate the Bellman operator itself. Applying $\Bellman$ at any state requires computing an expectation:

$$
(\Bellman v)(s) = \max_{a \in \mathcal{A}_s} \left\{r(s,a) + \gamma \int v(s')p(ds'|s,a)\right\}
$$

For discrete state spaces with a manageable number of states, this expectation is a finite sum we can compute exactly. For continuous or high-dimensional state spaces, we need numerical integration. The projection methods framework is compatible with any quadrature scheme, but leaves the choice of integration method unspecified.

This chapter addresses the **integration subproblem** that arises at two levels in approximate dynamic programming. First, we must evaluate the transition expectation $\int v(s')p(ds'|s,a)$ within the Bellman operator itself. Second, when using projection methods like Galerkin or least squares, we encounter outer integrations for enforcing orthogonality conditions or minimizing residuals over distributions of states. Both require numerical approximation in continuous or high-dimensional spaces.

We begin by examining deterministic numerical integration methods: quadrature rules that approximate integrals by evaluating integrands at carefully chosen points with associated weights. We discuss how to coordinate the choice of quadrature with the choice of basis functions to balance approximation accuracy and computational cost. Then we turn to **Monte Carlo integration**, which approximates expectations using random samples rather than deterministic quadrature points. This shift from deterministic to stochastic integration is what brings us into machine learning territory. When we replace exact transition probabilities with samples drawn from simulations or real interactions, projection methods combined with Monte Carlo integration become what the operations research community calls **simulation-based approximate dynamic programming** and what the machine learning community calls **reinforcement learning**. By relying on samples rather than explicit probability functions, we move from model-based planning to data-driven learning.

## Evaluating the Bellman Operator with Numerical Quadrature

Before turning to Monte Carlo methods, we examine the structure of the numerical integration problem. When we apply the Bellman operator to an approximate value function $\hat{v}(s; \theta) = \sum_i \theta_i \varphi_i(s)$, we must evaluate integrals of the form:

$$ \int \hat{v}(s'; \theta) \, p(s'|s,a) \, ds' = \int \left(\sum_i \theta_i \varphi_i(s')\right) p(s'|s,a) \, ds' = \sum_i \theta_i \int \varphi_i(s') \, p(s'|s,a) \, ds' $$

This shows two independent approximations:

1. **Value function approximation**: We represent $v$ using basis functions $\{\varphi_i\}$ and coefficients $\theta$
2. **Quadrature approximation**: We approximate each integral $\int \varphi_i(s') p(s'|s,a) ds'$ numerically

These choices are independent but should be coordinated. To see why, consider what happens in projection methods when we iterate $\hat{v}^{(k+1)} = \Proj \Bellman \hat{v}^{(k)}$. In practice, we cannot evaluate $\Bellman$ exactly due to the integrals, so we compute $\hat{v}^{(k+1)} = \Proj \BellmanQuad \hat{v}^{(k)}$ instead, where $\BellmanQuad$ denotes the Bellman operator with numerical quadrature.

The error in a single iteration can be bounded by the triangle inequality. Let $v$ denote the true fixed point $v = \Bellman v$. Then:

$$
\begin{aligned}
\|v - \hat{v}^{(k+1)}\| &= \|v - \Proj \BellmanQuad \hat{v}^{(k)}\| \\
&\le \|v - \Proj \Bellman v\| + \|\Proj \Bellman v - \Proj \Bellman \hat{v}^{(k)}\| + \|\Proj \Bellman \hat{v}^{(k)} - \Proj \BellmanQuad \hat{v}^{(k)}\| \\
&= \underbrace{\|v - \Proj \Bellman v\|}_{\text{best approximation error}} + \underbrace{\|\Proj \Bellman v - \Proj \Bellman \hat{v}^{(k)}\|}_{\text{contraction of iterate error}} + \underbrace{\|\Proj \Bellman \hat{v}^{(k)} - \Proj \BellmanQuad \hat{v}^{(k)}\|}_{\text{quadrature error}}
\end{aligned}
$$

The first term is the best we can do with our basis (how well $\Proj$ approximates the true solution). The second term decreases with iterations when $\Proj \Bellman$ is a contraction. The third term is the error from replacing exact integrals with quadrature, and it does not vanish as iterations proceed.

To make this concrete, consider evaluating $(\Bellman \hat{v})(s)$ for our current approximation $\hat{v}(s; \theta) = \sum_i \theta_i \varphi_i(s)$. We want:

$$ (\Bellman \hat{v})(s) = \max_{a \in \mathcal{A}_s} \left\{r(s,a) + \gamma \sum_i \theta_i \int \varphi_i(s') p(s'|s,a) ds' \right\} $$

But we compute instead:

$$ (\BellmanQuad \hat{v})(s) = \max_{a \in \mathcal{A}_s} \left\{r(s,a) + \gamma \sum_i \theta_i \sum_j w_j \varphi_i(s'_j) \right\} $$

where $\{(s'_j, w_j)\}$ are quadrature nodes and weights. If the quadrature error $\|(\Bellman \hat{v})(s) - (\BellmanQuad \hat{v})(s)\|$ is large relative to the basis approximation quality, we cannot exploit the expressive power of the basis. For instance, degree-10 Chebyshev polynomials represent smooth functions to $O(10^{-8})$ accuracy, but combined with rectangle-rule quadrature (error $O(h^2) \approx 10^{-2}$), the quadrature term dominates the error bound. We pay the cost of storing and manipulating 10 coefficients but achieve only $O(h^2)$ convergence in the quadrature mesh size.

This echoes the coordination principle from continuous optimal control (Chapter on trajectory optimization): when transcribing a continuous-time problem, we use the same quadrature nodes for both the running cost integral and the dynamics integral. There, coordination ensures that "where we pay" aligns with "where we enforce" the dynamics. Here, coordination ensures that integration accuracy matches approximation accuracy. Both are instances of balancing multiple sources of error in numerical methods.

Standard basis-quadrature pairings achieve this balance:

- Piecewise constant or linear elements with midpoint or trapezoidal rules
- Chebyshev polynomials with Gauss-Chebyshev quadrature
- Legendre polynomials with Gauss-Legendre quadrature
- Hermite polynomials with Gauss-Hermite quadrature (for Gaussian shocks)

To make this concrete, we examine what these pairings look like in practice for collocation and Galerkin projection.

### Orthogonal Collocation with Chebyshev Polynomials

Consider approximating the value function using Chebyshev polynomials of degree $n-1$:

$$
\hat{v}(s; \theta) = \sum_{i=0}^{n-1} \theta_i T_i(s)
$$

For orthogonal collocation, we place collocation points at the zeros of $T_n(s)$, denoted $\{s_j\}_{j=1}^n$. At each collocation point, we require the Bellman equation to hold exactly:

$$
\hat{v}(s_j; \theta) = \max_{a \in \mathcal{A}} \left\{r(s_j,a) + \gamma \int \hat{v}(s'; \theta) p(ds'|s_j,a)\right\}
$$

The integral on the right must be approximated using quadrature. With Chebyshev-Gauss quadrature using the same nodes $\{s_k\}_{k=1}^n$ and weights $\{w_k\}_{k=1}^n$, this becomes:

$$
\hat{v}(s_j; \theta) = \max_{a \in \mathcal{A}} \left\{r(s_j,a) + \gamma \sum_{k=1}^n w_k \hat{v}(s_k; \theta) p(s_k|s_j,a)\right\}
$$

Substituting the basis representation $\hat{v}(s_k; \theta) = \sum_{i=0}^{n-1} \theta_i T_i(s_k)$:

$$
\sum_{i=0}^{n-1} \theta_i T_i(s_j) = \max_{a \in \mathcal{A}} \left\{r(s_j,a) + \gamma \sum_{k=1}^n w_k p(s_k|s_j,a) \sum_{i=0}^{n-1} \theta_i T_i(s_k)\right\}
$$

Rearranging:

$$
\sum_{i=0}^{n-1} \theta_i T_i(s_j) = \max_{a \in \mathcal{A}} \left\{r(s_j,a) + \gamma \sum_{i=0}^{n-1} \theta_i \underbrace{\sum_{k=1}^n w_k T_i(s_k) p(s_k|s_j,a)}_{B_{ji}^a}\right\}
$$

This yields a system of $n$ nonlinear equations (one per collocation point):

$$
\sum_{i=0}^{n-1} T_i(s_j) \theta_i = \max_{a \in \mathcal{A}} \left\{r(s_j,a) + \gamma \sum_{i=0}^{n-1} B_{ji}^a \theta_i\right\}, \quad j=1,\ldots,n
$$

The matrix elements $B_{ji}^a$ can be precomputed once the quadrature nodes, weights, and transition probabilities are known. Solving this system gives the coefficient vector $\theta$.

### Galerkin Projection with Hermite Polynomials

For a problem with Gaussian shocks, we might use Hermite polynomials $\{H_i(s)\}_{i=0}^{n-1}$ weighted by the Gaussian density $\phi(s)$. The Galerkin condition requires:

$$
\int \left(\Bellman \hat{v}(s; \theta) - \hat{v}(s; \theta)\right) H_j(s) \phi(s) ds = 0, \quad j=0,\ldots,n-1
$$

Expanding the Bellman operator:

$$
\int \left[\max_{a \in \mathcal{A}} \left\{r(s,a) + \gamma \int \hat{v}(s'; \theta) p(ds'|s,a)\right\} - \hat{v}(s; \theta)\right] H_j(s) \phi(s) ds = 0
$$

We approximate this outer integral using Gauss-Hermite quadrature with nodes $\{s_\ell\}_{\ell=1}^m$ and weights $\{w_\ell\}_{\ell=1}^m$:

$$
\sum_{\ell=1}^m w_\ell \left[\max_{a \in \mathcal{A}} \left\{r(s_\ell,a) + \gamma \int \hat{v}(s'; \theta) p(ds'|s_\ell,a)\right\} - \hat{v}(s_\ell; \theta)\right] H_j(s_\ell) = 0
$$

The inner integral (transition expectation) is also approximated using Gauss-Hermite quadrature:

$$
\int \hat{v}(s'; \theta) p(ds'|s_\ell,a) \approx \sum_{k=1}^m w_k \hat{v}(s_k; \theta) p(s_k|s_\ell,a)
$$

Substituting the basis representation and collecting terms:

$$
\sum_{\ell=1}^m w_\ell H_j(s_\ell) \max_{a \in \mathcal{A}} \left\{r(s_\ell,a) + \gamma \sum_{i=0}^{n-1} \theta_i \sum_{k=1}^m w_k H_i(s_k) p(s_k|s_\ell,a)\right\} = \sum_{\ell=1}^m w_\ell H_j(s_\ell) \sum_{i=0}^{n-1} \theta_i H_i(s_\ell)
$$

This gives $n$ nonlinear equations in $n$ unknowns. The right-hand side simplifies using orthogonality: when using the same quadrature nodes for projection and integration, $\sum_{\ell=1}^m w_\ell H_j(s_\ell) H_i(s_\ell) = \delta_{ij} \|H_j\|^2$.

In both cases, the basis-quadrature pairing ensures that:
1. The quadrature nodes appear in both the transition expectation and the outer projection
2. The quadrature accuracy matches the polynomial approximation order
3. Precomputed matrices capture the dynamics, making iterations efficient

## Monte Carlo Integration 

Deterministic quadrature rules work well when the state space has low dimension, the transition density is smooth, and we can evaluate it cheaply at arbitrary points. In many stochastic control problems none of these conditions truly hold. The state may be high dimensional, the dynamics may be given by a simulator rather than an explicit density, and the cost of each call to the model may be large. In that regime, deterministic quadrature becomes brittle. Monte Carlo methods offer a different way to approximate expectations, one that relies only on the ability to **sample** from the relevant distributions.

### Monte Carlo as randomized quadrature

Consider a single expectation of the form

$$
J = \int f(x)\,p(dx) = \mathbb{E}[f(X)],
$$

where $X \sim p$ and $f$ is some integrable function. Monte Carlo integration approximates $J$ by drawing independent samples $X^{(1)},\ldots,X^{(N)} \sim p$ and forming the sample average

$$
\hat{J}_N \equiv \frac{1}{N}\sum_{n=1}^N f\bigl(X^{(n)}\bigr).
$$

This estimator has two basic properties that will matter throughout this chapter.

First, it is **unbiased**:

$$
\mathbb{E}[\hat{J}_N]
= \mathbb{E}\left[\frac{1}{N}\sum_{n=1}^N f(X^{(n)})\right]
= \frac{1}{N}\sum_{n=1}^N \mathbb{E}[f(X^{(n)})]
= \mathbb{E}[f(X)]
= J.
$$

Second, its variance scales as $1/N$. If we write

$$
\sigma^2 \equiv \mathrm{Var}[f(X)],
$$

then independence of the samples gives

$$
\mathrm{Var}(\hat{J}_N)
= \frac{1}{N^2}\sum_{n=1}^N \mathrm{Var}\bigl(f(X^{(n)})\bigr)
= \frac{\sigma^2}{N}.
$$

The central limit theorem then says that for large $N$,

$$
\sqrt{N}\,(\hat{J}_N - J) \Longrightarrow \mathcal{N}(0,\sigma^2),
$$

so the integration error decays at rate $O(N^{-1/2})$. This rate is slow compared to high-order quadrature in low dimension, but it has one crucial advantage: it does not explicitly depend on the dimension of $x$. Monte Carlo integration pays in variance, not in an exponential growth in the number of nodes.

It is often helpful to view Monte Carlo as **randomized quadrature**. A deterministic quadrature rule selects nodes $x_j$ and weights $w_j$ in advance and computes

$$
\sum_j w_j f(x_j).
$$

Monte Carlo can be written in the same form: if we draw $X^{(n)}$ from density $p$, the sample average

$$
\hat{J}_N = \frac{1}{N}\sum_{n=1}^N f(X^{(n)})
$$

is just a quadrature rule with random nodes and equal weights. More advanced Monte Carlo schemes, such as importance sampling, change both the sampling distribution and the weights, but the basic idea remains the same.

### Monte Carlo evaluation of the Bellman operator

We now apply this to the Bellman operator. For a fixed value function $v$ and a given state-action pair $(s,a)$, the transition part of the Bellman operator is

$$
\int v(s')\,p(ds' \mid s,a)
= \mathbb{E}\left[v(S') \mid S = s, A = a\right].
$$

If we can simulate next states $S'^{(1)},\ldots,S'^{(N)}$ from the transition kernel $p(\cdot \mid s,a)$, either by calling a simulator or by interacting with the environment, we can approximate this expectation by

$$
\widehat{\mathbb{E}}_N\bigl[v(S') \mid s,a\bigr]
\equiv
\frac{1}{N}\sum_{n=1}^N v\bigl(S'^{(n)}\bigr).
$$

If the immediate reward is also random, say

$$
r = r(S,A,S') \quad \text{with} \quad (S', r) \sim p(\cdot \mid s,a),
$$

we can approximate the full one-step return

$$
\mathbb{E}\bigl[r + \gamma v(S') \mid S = s, A = a\bigr]
$$

by

$$
\widehat{G}_N(s,a)
\equiv
\frac{1}{N}\sum_{n=1}^N \bigl[r^{(n)} + \gamma v(S'^{(n)})\bigr],
$$

where $(r^{(n)}, S'^{(n)})$ are independent samples given $(s,a)$. Again, this is an unbiased estimator of the Bellman expectation for fixed $v$.

Plugging this into the Bellman operator gives a **Monte Carlo Bellman operator**:

$$
(\widehat{\Bellman}_N v)(s)
\equiv
\max_{a \in \mathcal{A}_s}
\left\{
\widehat{G}_N(s,a)
\right\}.
$$

The expectation inside the braces is now replaced by a random sample average. In model-based settings, we implement this by simulating many next states for each candidate action $a$ at state $s$. In model-free settings, we obtain samples from real interactions and re-use them to estimate the expectation.

At this stage nothing about approximation or projection has entered yet. For a fixed value function $v$, Monte Carlo provides unbiased, noisy evaluations of $(\Bellman v)(s)$. The approximation question arises once we couple this stochastic evaluation with basis functions and projections.

### Sampling the outer expectations

Projection methods introduce a second layer of integration. In Galerkin and least squares schemes, we choose a distribution $\mu$ over states (and sometimes actions) and enforce conditions of the form

$$
\int R(s; \theta)\,p_i(s)\, \mu(ds) = 0
\quad \text{or} \quad
\int R(s; \theta)^2\,\mu(ds) \text{ is minimized}.
$$

Here $R(s; \theta)$ is the residual function, such as

$$
R(s; \theta) = (\Bellman \hat{v})(s;\theta) - \hat{v}(s;\theta),
$$

and $p_i$ are test functions or derivatives of the residual with respect to parameters.

Note a subtle but important shift in perspective from the previous chapter. There, the weight function $w(s)$ in the inner product $\langle f, g \rangle_w = \int f(s) g(s) w(s) ds$ could be any positive weight function (not necessarily normalized). For Monte Carlo integration, however, we need a probability distribution we can sample from. We write $\mu$ for this sampling distribution and express projection conditions as integrals with respect to $\mu$: $\int R(s; \theta) p_i(s) \mu(ds)$. If a problem was originally formulated with an unnormalized weight $w(s)$, we must either (i) normalize it to define $\mu$, or (ii) use importance sampling with a different $\mu$ and reweight samples by $w(s)/\mu(s)$. In reinforcement learning, $\mu$ is typically the empirical state visitation distribution from collected trajectories.

These outer integrals over $s$ are generally not easier to compute than the inner transition expectations. Monte Carlo gives a way to approximate them as well. If we can draw states $S^{(1)},\ldots,S^{(M)} \sim \mu$, we can approximate, for example, the Galerkin orthogonality conditions by

$$
\int R(s; \theta)\,p_i(s)\,\mu(ds)
\approx
\frac{1}{M}\sum_{m=1}^M R\bigl(S^{(m)}; \theta\bigr)\,p_i\bigl(S^{(m)}\bigr).
$$

Similarly, a least squares objective

$$
\int R(s; \theta)^2\,\mu(ds)
$$

is approximated by the empirical risk

$$
\frac{1}{M}\sum_{m=1}^M R\bigl(S^{(m)}; \theta\bigr)^2.
$$

If we now substitute Monte Carlo estimates for both the inner transition expectations and the outer projection conditions, we obtain fully simulation-based schemes. We no longer need explicit access to the transition kernel $p(\cdot \mid s,a)$ or the state distribution $\mu$. It is enough to be able to **sample** from them, either through a simulator or by interacting with the real system. 

### Simulation-Based Projection Methods

Monte Carlo integration replaces both levels of integration in approximate dynamic programming. The transition expectation in the Bellman operator becomes $\frac{1}{N}\sum_{n=1}^N v(S'^{(n)})$, and the outer integrals for projection become empirical averages over sampled states. Writing $\Proj_M$ for projection using $M$ state samples and $\widehat{\Bellman}_N$ for the Monte Carlo Bellman operator with $N$ transition samples, the iteration becomes

$$
\hat{v}^{(k+1)} = \Proj_M\,\widehat{\Bellman}_N \hat{v}^{(k)}.
$$

Unlike deterministic quadrature, which introduces a fixed bias at each iteration, Monte Carlo introduces random error with zero mean but nonzero variance. However, combining Monte Carlo with the maximization in the Bellman operator creates a systematic problem: while the estimate of the expected return for any individual action is unbiased, taking the maximum over these noisy estimates introduces upward bias. This overestimation compounds through value iteration and degrades the resulting policies.

### Amortizing Action Selection via Q-Functions

Monte Carlo integration enables model-free approximate dynamic programming: we no longer need explicit transition probabilities $p(s'|s,a)$, only the ability to sample next states. However, one computational challenge remains. The standard formulation of an optimal decision rule is

$$
\pi(s) = \arg\max_{a \in \mathcal{A}} \left\{r(s,a) + \gamma \int v(s')p(ds'|s,a)\right\}.
$$

Even with an optimal value function $v^*$ in hand, extracting an action at state $s$ requires evaluating the transition expectation for each candidate action. In the model-free setting, this means we must draw Monte Carlo samples from each action's transition distribution every time we select an action. This repeated sampling "at inference time" wastes computation, especially when the same state is visited multiple times.

We can amortize this computation by working at a different level of representation. Define the **state-action value function** (or **Q-function**)

$$
q(s,a) = r(s,a) + \gamma \int v(s')p(ds'|s,a).
$$

The Q-function caches the result of evaluating each action at each state. Once we have $q$, action selection reduces to a finite maximization:

$$
\pi(s) = \arg\max_{a \in \mathcal{A}(s)} q(s,a).
$$

No integration appears in this expression. The transition expectation has been precomputed and stored in $q$ itself.

The optimal Q-function $q^*$ satisfies its own Bellman equation. Substituting the definition of $v^*(s) = \max_a q^*(s,a)$ into the expression for $q$:

$$
q^*(s,a) = r(s,a) + \gamma \int p(ds'|s,a) \max_{a' \in \mathcal{A}(s')} q^*(s', a').
$$

This defines a Bellman operator on Q-functions:

$$
(\Bellman q)(s,a) = r(s,a) + \gamma \int p(ds'|s,a)\max_{a' \in \mathcal{A}(s')} q(s', a').
$$

Like the Bellman operator on value functions, $\Bellman$ is a $\gamma$-contraction in the sup-norm, guaranteeing a unique fixed point $q^*$. We can thus apply the same projection and Monte Carlo techniques developed for value functions to Q-functions. The computational cost shifts from action selection (repeated sampling at decision time) to training (evaluating expectations during the iterations of approximate value iteration). Once $q$ is learned, acting is cheap.

```{prf:algorithm} Parametric Q-Value Iteration
:label: simadp-parametric-q-value-iteration

**Input** Given an MDP $(S, A, P, R, \gamma)$, base points $\mathcal{B} \subset S$, function approximator class $q(s,a; \boldsymbol{\theta})$, maximum iterations $N$, tolerance $\varepsilon > 0$

**Output** Parameters $\boldsymbol{\theta}$ for Q-function approximation

1. Initialize $\boldsymbol{\theta}_0$ (e.g., for zero initialization)
2. $n \leftarrow 0$
3. **repeat**
    1. $\mathcal{D} \leftarrow \emptyset$
    2. For each $(s,a) \in \mathcal{B} \times A$:
        1. $y_{s,a} \leftarrow r(s,a) + \gamma \int p(ds'|s,a)\max_{a' \in A} q(s',a'; \boldsymbol{\theta}_n)$
        2. $\mathcal{D} \leftarrow \mathcal{D} \cup \{((s,a), y_{s,a})\}$
    3. $\boldsymbol{\theta}_{n+1} \leftarrow \texttt{fit}(\mathcal{D})$
    4. $\delta \leftarrow \frac{1}{|\mathcal{D}||A|}\sum_{(s,a) \in \mathcal{D} \times A} (q(s,a; \boldsymbol{\theta}_{n+1}) - q(s,a; \boldsymbol{\theta}_n))^2$
    5. $n \leftarrow n + 1$
4. **until** ($\delta < \varepsilon$ or $n \geq N$)
5. **return** $\boldsymbol{\theta}_n$
```

## Overestimation Bias and Mitigation Strategies

The previous sections established that Monte Carlo integration enables simulation-based approximate dynamic programming. However, combining Monte Carlo with the maximization in the Bellman operator introduces a systematic problem. While individual expectations are unbiased, taking the maximum over noisy estimates creates upward bias. This section examines the sources of this bias and two approaches for mitigating it.

### Sources of Overestimation Bias

When we apply Monte Carlo integration to the Bellman operator, each individual expectation is unbiased. The problem arises when we maximize over these noisy estimates. To make this precise, consider a given state-action pair $(s,a)$ and value function $v$. Define the true continuation value

$$
\mu(s,a) \equiv \int v(s')\,p(ds' \mid s,a),
$$

and its Monte Carlo approximation with $N$ samples:

$$
\hat{\mu}_N(s,a) \equiv \frac{1}{N}\sum_{i=1}^N v(s'_i), \quad s'_i \sim p(\cdot|s,a).
$$

Each estimator is unbiased: $\mathbb{E}[\hat{\mu}_N(s,a)] = \mu(s,a)$. The Monte Carlo Bellman operator is

$$
(\widehat{\Bellman}_N v)(s) = \max_{a \in \mathcal{A}_s} \left\{ r(s,a) + \gamma \hat{\mu}_N(s,a)\right\}.
$$

While each $\hat{\mu}_N(s,a)$ is unbiased, the maximization introduces systematic overestimation. To see why, let $a^*$ be the truly optimal action. The maximum of any collection is at least as large as any particular element:

$$
\mathbb{E}\big[\max_a \{r(s,a) + \gamma \hat{\mu}_N(s,a)\}\big] \ge \mathbb{E}\big[r(s,a^*) + \gamma \hat{\mu}_N(s,a^*)\big] = r(s,a^*) + \gamma \mu(s,a^*) = (\Bellman v)(s).
$$

The inequality is strict whenever multiple actions have nonzero variance. The maximization selects whichever action happens to receive a positive noise realization, and that inflated estimate contributes to the target value. This is Jensen's inequality for the max operator: $\mathbb{E}[\max_a Y_a] \ge \max_a \mathbb{E}[Y_a]$ for random variables $\{Y_a\}$, since the max is convex. Even though we start with unbiased estimators, taking the maximum breaks unbiasedness.

Operationally, this means that repeatedly sampling fresh states and taking the max will not, on average, converge to the true value—it will systematically land above it. Unlike noise that averages out, this bias persists no matter how many samples we draw. Worse, it compounds across iterations as overestimates feed into future target computations.

```{prf:remark} Connection to deterministic policies
This inequality also underlies why deterministic policies are optimal in MDPs (Theorem {prf:ref}`stoch-policy-reduction`): $\max_a w(a) \ge \sum_a q(a) w(a)$ shows randomization cannot improve expected returns. The same mathematical principle appears in two contexts: (1) deterministic policies suffice, (2) maximization over noisy estimates creates upward bias.
```

Monte Carlo value iteration applies $v_{k+1} = \widehat{\Bellman}_N v_k$ repeatedly. The overestimation bias does not stay confined to a single iteration: it accumulates through the recursion. To see why, note that each iteration's output becomes the next iteration's input. At iteration 2, we compute Monte Carlo estimates $\hat{\mu}_N(s,a) = \frac{1}{N}\sum_{i=1}^N v_1(s'_i)$, but $v_1$ is already biased upward from iteration 1. We are now averaging an overestimated function before we even apply the maximization, which adds another layer of bias on top of what was inherited. This pattern repeats at each iteration, creating a feedback loop where $\mathbb{E}[v_k] \ge \mathbb{E}[v_{k-1}] \ge \cdots \ge v^*$. Favorable noise at iteration $k$ becomes embedded in the value function and treated as truth by iteration $k+1$.

### Learning the Bias Correction

One approach, developed by Keane and Wolpin {cite}`Keane1994` in the context of dynamic discrete choice models, treats the bias itself as a quantity to be learned and subtracted. For a given value function $v$ and Monte Carlo sample size $N$, define the bias at state $s$ as

$$
\delta(s) = \mathbb{E}\big[(\widehat{\Bellman}_N v)(s)\big] - (\Bellman v)(s) \ge 0.
$$

While we cannot compute $\delta(s)$ directly (we lack both the expectation and the exact Bellman application), the bias has structure. It depends on observable quantities: the number of actions $|\mathcal{A}_s|$, the sample size $N$, and the spread of action values. When one action dominates, $\delta(s)$ is small. When several actions have similar values, noise is more likely to flip the maximizer, increasing $\delta(s)$.

Rather than deriving $\delta(s)$ analytically, Keane and Wolpin proposed learning it empirically. The strategy follows a "simulate on a subset, interpolate everywhere" template common in econometric dynamic programming. At a carefully chosen set of states, we run both high-fidelity simulation (many samples or exact integration) and the low-fidelity $N$-sample estimate used in value iteration. The gap between these estimates provides training data for the bias. We then fit a regression model $g_\eta$ that predicts $\delta(s)$ from features of the state and action-value distribution. During value iteration, we subtract the predicted bias from the raw Monte Carlo maximum.

Useful features for predicting the bias include the spread of action values (from a separate high-fidelity simulation), the number of actions, and gaps to the best action. These are cheap to compute and track regimes where maximization bias is large. The procedure can be summarized as:

```{prf:algorithm} Keane-Wolpin Bias-Corrected Value Iteration
:label: keane-wolpin-bias-correction

**Input:** MDP, current value $v_k$, sample size $N$, learned bias correction $g_\eta$

**Output:** Bias-corrected next value $v_{k+1}$

1. **For** each state $s$ **do**:
   - **For** each action $a$ **do**:
     - Draw $N$ samples: $s'_1, \ldots, s'_N \sim p(\cdot|s,a)$
     - Compute $\hat{\mu}_N(s,a) = \frac{1}{N}\sum_{i=1}^N v_k(s'_i)$
   - Compute raw max: $M(s) = \max_a \{r(s,a) + \gamma \hat{\mu}_N(s,a)\}$
   - Construct features $\phi(s)$ from action-value spread (using separate high-fidelity estimate)
   - Bias-corrected value: $v_{k+1}(s) = M(s) - g_\eta(\phi(s))$
```

The regression model $g_\eta$ is trained offline by comparing high and low-fidelity simulations at a grid of states, then applied during each value iteration step. This approach has been influential in econometrics for structural estimation problems. However, it has seen limited adoption in reinforcement learning. The computational overhead is substantial: it requires high-fidelity simulation at training states, careful feature engineering, and maintaining the regression model throughout learning. More critically, the circular dependency between the bias estimate and the value function can amplify errors if $g_\eta$ is misspecified. We now turn to a simpler alternative that avoids explicit bias modeling.

### Decoupling Selection and Evaluation

An alternative approach modifies the estimator itself to break the coupling that creates bias. In the standard Monte Carlo update $\max_a \{r(s,a) + \gamma \hat{\mu}_N(s,a)\}$, the same noisy estimate both selects which action looks best and provides the value assigned to that action. To see the problem, decompose the estimator into its mean and noise:

$$
\hat{\mu}_N(s,a) = \mu(s,a) + \varepsilon_a,
$$

where $\varepsilon_a$ is zero-mean noise. Whichever action happens to have the largest $\varepsilon_a$ gets selected, and that same positive noise inflates the target value:

$$
Y = r(s,a^\star) + \gamma \mu(s,a^\star) + \gamma \varepsilon_{a^\star},
$$

where $a^\star = \arg\max_a \{r(s,a) + \gamma \mu(s,a) + \gamma \varepsilon_a\}$. This coupling (using the same random variable $\varepsilon_{a^\star}$ for both selection and evaluation) produces $\mathbb{E}[Y] \ge \max_a \{r(s,a) + \gamma \mu(s,a)\}$.

Double Q-learning {cite}`van2016deep` breaks this coupling by maintaining two independent Monte Carlo estimates. Conceptually, for each action $a$ at state $s$, we would draw two separate sets of samples from $p(\cdot|s,a)$:

$$
\hat{\mu}^{(1)}_N(s,a) = \mu(s,a) + \varepsilon^{(1)}_a, \quad \hat{\mu}^{(2)}_N(s,a) = \mu(s,a) + \varepsilon^{(2)}_a,
$$

where $\varepsilon^{(1)}_a$ and $\varepsilon^{(2)}_a$ are **independent zero-mean noise terms**. We use the first estimate to select the action but the second to evaluate it:

$$
a^\star = \arg\max_{a} \left\{r(s,a) + \gamma \hat{\mu}^{(1)}_N(s,a)\right\}, \quad Y = r(s,a^\star) + \gamma \hat{\mu}^{(2)}_N(s,a^\star).
$$

Note that $a^\star$ is a random variable—it's the result of maximizing over noisy estimates $\hat{\mu}^{(1)}_N(s,a)$, and therefore depends on the noise $\varepsilon^{(1)}$. The target value $Y$ uses the second estimator $\hat{\mu}^{(2)}_N(s,a^\star)$, evaluating it at the randomly selected action $a^\star$. Expanding the target value to make the dependencies explicit:

$$
Y = r(s,a^\star) + \gamma \hat{\mu}^{(2)}_N(s,a^\star) = r(s,a^\star) + \gamma \mu(s,a^\star) + \gamma \varepsilon^{(2)}_{a^\star},
$$

where $a^\star$ itself depends on $\varepsilon^{(1)}$. To see why this eliminates evaluation bias, write the expectation as a nested expectation:

$$
\mathbb{E}[Y] = \mathbb{E}_{\varepsilon^{(1)}}\Big[\mathbb{E}_{\varepsilon^{(2)}}\big[r(s,a^\star) + \gamma \mu(s,a^\star) + \gamma \varepsilon^{(2)}_{a^\star} \mid a^\star\big]\Big].
$$

The inner expectation, conditioned on the selected action $a^\star$, equals:

$$
\mathbb{E}_{\varepsilon^{(2)}}\big[r(s,a^\star) + \gamma \mu(s,a^\star) + \gamma \varepsilon^{(2)}_{a^\star} \mid a^\star\big] = r(s,a^\star) + \gamma \mu(s,a^\star),
$$

because $\varepsilon^{(2)}_{a^\star}$ is independent of $a^\star$ (the selection was made using $\varepsilon^{(1)}$, not $\varepsilon^{(2)}$). Thus $\mathbb{E}[\varepsilon^{(2)}_{a^\star} \mid a^\star] = 0$. Taking the outer expectation:

$$
\mathbb{E}[Y] = \mathbb{E}_{\varepsilon^{(1)}}\big[r(s,a^\star) + \gamma \mu(s,a^\star)\big].
$$

The evaluation noise contributes zero on average because it is independent of the selection. However, double Q-learning does not eliminate all bias. Selection bias remains: $a^\star$ still depends on $\varepsilon^{(1)}$, so in general $\mathbb{E}[Y] \ne \max_a \{r(s,a) + \gamma \mu(s,a)\}$. The method eliminates evaluation bias but not selection bias. In the standard estimator, both sources compound. In double Q-learning, only the selection noise contributes, yielding substantially smaller total bias.

```{prf:remark} Implementation via different Q-functions
:class: dropdown

The conceptual framework above assumes we can draw multiple independent samples from $p(\cdot|s,a)$ for each state-action pair. In practice, this would require resetting a simulator to the same state and sampling multiple times, which is often infeasible. The practical implementation achieves the same effect differently: maintain two Q-functions that are trained on different data or updated at different times (e.g., one is a slowly-updating target network). Since the two Q-functions experience different noise realizations during training, their errors remain less correlated than if we used a single Q-function for both selection and evaluation. This is how Double DQN works, as we'll see later in the online learning section.
```

The following algorithm implements this principle with Monte Carlo integration. We maintain two Q-functions and alternate which one selects actions and which one evaluates them. The bias reduction mechanism applies whether we store Q-values in a table or use function approximation.

```{prf:algorithm} Double Q Value Iteration with Monte Carlo
:label: double-q-value-iteration

**Input:** MDP, state sample $\mathcal{S}$, Monte Carlo sample size $N$, maximum iterations $K$

**Output:** Two Q-functions $q^{(1)}$, $q^{(2)}$

1. Initialize $q^{(1)}_0(s,a)$ and $q^{(2)}_0(s,a)$ for all $s,a$
2. $k \leftarrow 0$
3. **repeat**
4. $\quad$ **for** each $s \in \mathcal{S}$ **do**
5. $\quad\quad$ **for** each $a \in \mathcal{A}(s)$ **do**
6. $\quad\quad\quad$ Draw $N$ next states: $s'_1, \ldots, s'_N \sim p(\cdot \mid s,a)$
7. $\quad\quad\quad$ **// Compute targets using decoupled selection-evaluation**
8. $\quad\quad\quad$ **for** $i = 1, \ldots, N$ **do**
9. $\quad\quad\quad\quad$ $a^{(1)}_i \leftarrow \arg\max_{a'} q^{(1)}_k(s'_i, a')$
10. $\quad\quad\quad\quad$ $a^{(2)}_i \leftarrow \arg\max_{a'} q^{(2)}_k(s'_i, a')$
11. $\quad\quad\quad$ **end for**
12. $\quad\quad\quad$ $q^{(1)}_{k+1}(s,a) \leftarrow r(s,a) + \frac{\gamma}{N}\sum_{i=1}^N q^{(2)}_k(s'_i, a^{(1)}_i)$
13. $\quad\quad\quad$ $q^{(2)}_{k+1}(s,a) \leftarrow r(s,a) + \frac{\gamma}{N}\sum_{i=1}^N q^{(1)}_k(s'_i, a^{(2)}_i)$
14. $\quad\quad$ **end for**
15. $\quad$ **end for**
16. $\quad$ $k \leftarrow k+1$
17. **until** convergence or $k \geq K$
18. **return** $q^{(1)}_k$, $q^{(2)}_k$
```

At lines 9 and 12, $q^{(1)}$ selects the action but $q^{(2)}$ evaluates it. The noise in $q^{(1)}$ influences which action is chosen, but the independent noise in $q^{(2)}$ does not inflate the evaluation. Lines 10 and 13 apply the same principle symmetrically for updating $q^{(2)}$.

## A Unified View of Reinforcement Learning Algorithms

We have developed two foundational components for simulation-based approximate dynamic programming. From the [previous chapter on projection methods](projdp.md), we have the fitted-value iteration framework: given a current approximation $\hat{v}^{(k)}$, we evaluate the Bellman operator to obtain targets, then fit a new approximation $\hat{v}^{(k+1)}$ to these targets using a chosen function class. Earlier in this chapter, we introduced Q-functions $q(s,a)$ as a representation that amortizes the cost of action selection by precomputing and storing the value of each state-action pair. We also showed how Monte Carlo integration replaces exact expectations with sample averages, enabling learning from simulated or real experience rather than requiring explicit knowledge of transition probabilities.

These three elements combine into a template that generates most modern reinforcement learning algorithms. The iteration $\hat{q}^{(k+1)} = \text{fit}(\{(s_i, a_i, y_i)\})$, where targets $y_i$ are computed from the Bellman operator applied to $\hat{q}^{(k)}$ and expectations are approximated via Monte Carlo, admits many instantiations. Different choices of function approximator, optimization procedure, data collection strategy, and operator structure yield different algorithms, each with distinct computational properties and empirical characteristics.

This section maps the design space systematically. We show how classical algorithms like fitted Q-iteration, neural fitted Q-iteration, DQN, and Q-learning arise as specific configurations of design choices. We then extend to smooth Bellman operators (connecting to the regularized MDP framework from the [chapter on smooth Bellman optimality equations](regmdp.md)) and continuous action spaces where policy networks amortize the greedy action computation. The result is a unified perspective that reveals the common structure underlying apparently diverse methods.

### Design Choices and Algorithm Space

The template for simulation-based Q-iteration involves several independent design choices. Each choice affects computational cost, sample efficiency, convergence properties, and implementation complexity.

**Function approximator class.** The Q-function $q(s,a; \theta)$ can be represented using linear combinations of fixed basis functions, decision trees, neural networks, or kernel methods. Linear approximators and trees offer interpretability and theoretical guarantees but limited expressiveness. Neural networks provide flexibility for high-dimensional problems but require careful tuning. The choice determines both the representational capacity and the optimization landscape.

**Bellman operator structure.** The standard Bellman operator uses $\max_a$ to select the greedy action. The smooth Bellman operator replaces this with $\frac{1}{\beta} \log \sum_a \exp(\beta \cdot)$, yielding a differentiable relaxation that corresponds to entropy-regularized optimization. This choice affects the policy class (deterministic versus stochastic) and optimization properties (non-smooth versus smooth).

**Inner loop optimization.** At each iteration $k$, we solve $\min_\theta \sum_i (q(s_i, a_i; \theta) - y_i)^2$ for targets computed from $\hat{q}^{(k)}$. We can run this optimization to full convergence, perform a fixed number of gradient steps, or take just a single step. Full convergence gives the best fit but costs more computation per iteration. Single-step updates (stochastic gradient descent) are cheap but introduce bias from underoptimization.

**Inner loop initialization.** At each outer iteration, we solve a regression problem to fit the Q-function to new targets. We can initialize this optimization from scratch (cold start, using fixed $\theta_0$) or from the previous iteration's solution (warm start, using $\theta^{(k)}$). Cold starting treats each iteration independently, ensuring the function approximator adapts fully to the new targets. Warm starting leverages the similarity between consecutive value functions to reduce optimization time, but requires careful interaction with partial optimization—if we take only K gradient steps from a warm start, we must decide whether to recompute targets at each step or keep them fixed. This interaction leads naturally to target networks, as we will see.

**Data collection mode.** In offline (batch) mode, we collect a fixed dataset $\mathcal{D} = \{(s_i, a_i, r_i, s'_i)\}$ once and reuse it across all iterations. In online mode, we interleave data collection with learning: act in the environment using the current approximation, observe new transitions, and update. A middle ground uses a replay buffer: collect data online but store it in a buffer $\mathcal{D}$ from which we sample mini-batches for training.

**Overestimation mitigation.** As shown earlier, the maximization in the Bellman operator combined with Monte Carlo sampling creates upward bias. We can address this with double Q-learning, maintaining two independent estimators and using one to select actions while the other evaluates them. Alternatively, we can accept the bias or use learned corrections.

The following tables organize these choices:

| **Design Choice** | **Options** |
|:------------------|:------------|
| Function approximator | Linear basis, Decision trees, Neural networks, Kernel methods |
| Bellman operator | Hard max, Smooth logsumexp |
| Inner optimization | Full convergence, K steps, Single step (K=1) |
| Inner loop initialization | Cold start (from $\theta_0$), Warm start (from $\theta^{(k)}$) |
| Data collection | Offline (fixed batch), Online, Online with replay buffer |
| Bias mitigation | None, Double Q-learning, Learned correction |

Different combinations of these choices yield different algorithms. The table below shows how several well-known methods correspond to specific configurations:

| **Algorithm** | **Approximator** | **Bellman** | **Inner Loop** | **Initialization** | **Data** | **Bias Fix** |
|:--------------|:-----------------|:------------|:---------------|:-------------------|:---------|:-------------|
| FQI {cite}`ernst2005tree` | Extra Trees | Hard | Full | Cold | Offline | None |
| NFQI {cite}`riedmiller2005neural` | Neural Net | Hard | Full | Cold | Offline | None |
| DQN {cite}`mnih2013atari` | Deep NN | Hard | K steps | Warm | Replay | None |
| Double DQN {cite}`van2016deep` | Deep NN | Hard | K steps | Warm | Replay | Double Q |
| Q-learning {cite}`SuttonBarto2018` | Tabular/Linear | Hard | K=1 | Warm | Online | None |
| Soft Q {cite}`haarnoja2017reinforcement` | Neural Net | Smooth | K steps | Warm | Replay | None |

This table omits continuous action methods (NFQCA, DDPG, SAC), which introduce an additional design dimension. We address those after establishing the discrete action case. The initialization choice becomes particularly important when moving from batch to online algorithms, as we will see.

### Batch Algorithms: FQI and NFQI

The offline (batch) setting begins with a fixed dataset $\mathcal{D} = \{(s_i, a_i, r_i, s'_i)\}_{i=1}^N$ collected once before learning. This data might come from a previous controller, from human demonstrations, or from exploratory interactions. The task is to extract the best Q-function approximation from this data without additional environment interactions.

Fitted Q-iteration applies the template directly. At iteration $k$, we have Q-function parameters $\theta^{(k)}$. For each transition $(s_i, a_i, r_i, s'_i)$ in $\mathcal{D}$, we compute a target value:

$$
y_i^{(k)} = r_i + \gamma \max_{a' \in \mathcal{A}} q(s'_i, a'; \theta^{(k)}).
$$

This evaluates the Bellman operator at the sampled next state $s'_i$. We then solve the regression problem:

$$
\theta^{(k+1)} = \arg\min_\theta \sum_{i=1}^N \left(q(s_i, a_i; \theta) - y_i^{(k)}\right)^2.
$$

The choice of function approximator determines how we solve this problem. Ernst et al. {cite}`ernst2005tree` used extremely randomized trees, an ensemble method that partitions the state-action space into regions and fits piecewise constant Q-values. Trees handle high-dimensional inputs naturally and the ensemble reduces overfitting. The resulting method, simply called FQI (Fitted Q-Iteration), demonstrated that batch reinforcement learning could work with complex function approximators on continuous-state problems.

Riedmiller {cite}`riedmiller2005neural` replaced the tree ensemble with a neural network, yielding Neural Fitted Q-Iteration (NFQI). The neural network $q(s,a; \theta)$ provides smooth interpolation and can leverage modern optimization techniques like RProp (resilient backpropagation). NFQI runs the inner optimization to convergence at each iteration: train the network until the loss stops decreasing, then compute new targets using the converged Q-function. This full optimization ensures the network accurately represents the projected Bellman operator before moving to the next iteration.

The following algorithm shows the general structure:

```{prf:algorithm} Fitted Q-Iteration (Batch)
:label: fitted-q-iteration-batch

**Input:** Dataset $\mathcal{D} = \{(s_i, a_i, r_i, s'_i)\}_{i=1}^N$, function approximator class $q(s,a; \theta)$, discount factor $\gamma$, maximum iterations $K$, convergence tolerance $\varepsilon$

**Output:** Learned Q-function parameters $\theta$

1. Initialize $\theta_0$ (e.g., random initialization)
2. $k \leftarrow 0$
3. **repeat**
4. $\quad$ **// Compute targets from current Q-function**
5. $\quad$ **for** each transition $(s_i, a_i, r_i, s'_i) \in \mathcal{D}$ **do**
6. $\quad\quad$ $y_i \leftarrow r_i + \gamma \max_{a' \in \mathcal{A}} q(s'_i, a'; \theta_k)$
7. $\quad$ **end for**
8. $\quad$ **// Fit Q-function to targets**
9. $\quad$ $\theta_{k+1} \leftarrow \arg\min_\theta \sum_{i=1}^N (q(s_i, a_i; \theta) - y_i)^2$
10. $\quad$ $k \leftarrow k+1$
11. **until** $\|\theta_k - \theta_{k-1}\| < \varepsilon$ or $k \geq K$
12. **return** $\theta_k$
```

At line 9, the optimization is run to convergence (for FQI and NFQI) or for a fixed number of gradient steps (variants). The algorithm reuses the same offline dataset $\mathcal{D}$ at every iteration, making it sample-efficient when data collection is expensive but environment simulation is unavailable.

### From Nested to Flattened Q-Iteration

To understand modern deep RL algorithms like DQN, we need to see how they emerge from the basic fitted Q-iteration template through a series of natural modifications. We'll build this up step by step, starting with the basic offline setting and gradually introducing warmstarting, partial optimization, and online data collection.

#### Basic Offline Neural Fitted Q-Iteration

We start with the simplest case: a fixed dataset $\mathcal{T}$ of transitions, where we use a neural network as our function approximator and fit it to convergence at each iteration:

```{prf:algorithm} Basic Offline Neural Fitted Q-Value Iteration
:label: basic-nfqi

**Input**: MDP $(S, A, P, R, \gamma)$, dataset $\mathcal{T}$, neural network $q(s,a; \boldsymbol{\theta})$, initialization $\boldsymbol{\theta}_0$

**Output**: Parameters $\boldsymbol{\theta}$ for Q-function approximation

1. Initialize $\boldsymbol{\theta}_0$ randomly
2. $n \leftarrow 0$
3. **repeat**
    1. $\mathcal{D}_n \leftarrow \emptyset$
    2. For each $(s,a,r,s') \in \mathcal{T}$:
        1. $y_{s,a} \leftarrow r + \gamma \max_{a' \in A} q(s',a'; \boldsymbol{\theta}_n)$
        2. $\mathcal{D}_n \leftarrow \mathcal{D}_n \cup \{((s,a), y_{s,a})\}$
    3. $\boldsymbol{\theta}_{n+1} \leftarrow \texttt{fit}(\mathcal{D}_n, \boldsymbol{\theta}_0)$
    4. $n \leftarrow n + 1$
4. **until** training complete
5. **return** $\boldsymbol{\theta}_n$
```

This algorithm has a nested structure: the outer loop (indexed by $n$) computes targets using the current Q-function, while `fit` performs an inner optimization to minimize the regression loss. Importantly, `fit` starts from the initial parameters $\boldsymbol{\theta}_0$ each time (cold start), not from $\boldsymbol{\theta}_n$.

#### Opening Up the Inner Loop

To make the structure explicit, let's expand the `fit` procedure to show the inner gradient descent loop:

```{prf:algorithm} Fitted Q-Value Iteration with Explicit Inner Loop
:label: nfqi-explicit-inner

**Input**: MDP $(S, A, P, R, \gamma)$, dataset $\mathcal{T}$, neural network $q(s,a; \boldsymbol{\theta})$, learning rate $\alpha$, convergence test $\texttt{has\_converged}(\cdot)$, initialization $\boldsymbol{\theta}_0$, loss $\mathcal{L}$

**Output**: Parameters $\boldsymbol{\theta}$ for Q-function approximation

1. Initialize $\boldsymbol{\theta}_0$ randomly
2. $n \leftarrow 0$
3. **repeat**
    1. $\mathcal{D}_n \leftarrow \emptyset$
    2. For each $(s,a,r,s') \in \mathcal{T}$:
        1. $y_{s,a} \leftarrow r + \gamma \max_{a' \in A} q(s',a'; \boldsymbol{\theta}_n)$
        2. $\mathcal{D}_n \leftarrow \mathcal{D}_n \cup \{((s,a), y_{s,a})\}$
    3. // Inner optimization loop
    4. $\boldsymbol{\theta}^{(0)} \leftarrow \boldsymbol{\theta}_0$
    5. $k \leftarrow 0$
    6. **repeat**
        1. $\boldsymbol{\theta}^{(k+1)} \leftarrow \boldsymbol{\theta}^{(k)} - \alpha \nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta}^{(k)}; \mathcal{D}_n)$
        2. $k \leftarrow k + 1$
    7. **until** $\texttt{has\_converged}(\boldsymbol{\theta}^{(0)}, ..., \boldsymbol{\theta}^{(k)}, \mathcal{D}_n)$
    8. $\boldsymbol{\theta}_{n+1} \leftarrow \boldsymbol{\theta}^{(k)}$
    9. $n \leftarrow n + 1$
4. **until** training complete
5. **return** $\boldsymbol{\theta}_n$
```

Now the two-level structure is explicit: targets are computed using $\boldsymbol{\theta}_n$ (step 3.2.1) and remain fixed throughout the inner loop. Each inner optimization starts fresh from $\boldsymbol{\theta}_0$ (step 3.4) and runs until convergence.

#### Warmstarting and Partial Optimization

Two natural modifications suggest themselves. First, rather than starting each inner loop from scratch, we can warmstart by initializing with the previous iteration's parameters. Second, similar to modified policy iteration, we can perform partial optimization—taking only $K$ gradient steps rather than running to convergence:

```{prf:algorithm} Neural Fitted Q-Iteration with Warmstarting and Partial Optimization
:label: nfqi-warmstart-partial

**Input**: MDP $(S, A, P, R, \gamma)$, dataset $\mathcal{T}$, neural network $q(s,a; \boldsymbol{\theta})$, learning rate $\alpha$, number of steps $K$

**Output**: Parameters $\boldsymbol{\theta}$ for Q-function approximation

1. Initialize $\boldsymbol{\theta}_0$ randomly
2. $n \leftarrow 0$
3. **repeat**
    1. $\mathcal{D}_n \leftarrow \emptyset$
    2. For each $(s,a,r,s') \in \mathcal{T}$:
        1. $y_{s,a} \leftarrow r + \gamma \max_{a' \in A} q(s',a'; \boldsymbol{\theta}_n)$
        2. $\mathcal{D}_n \leftarrow \mathcal{D}_n \cup \{((s,a), y_{s,a})\}$
    3. // Inner optimization with warmstart and fixed steps
    4. $\boldsymbol{\theta}^{(0)} \leftarrow \boldsymbol{\theta}_n$
    5. For $k = 0$ to $K-1$:
        1. $\boldsymbol{\theta}^{(k+1)} \leftarrow \boldsymbol{\theta}^{(k)} - \alpha \nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta}^{(k)}; \mathcal{D}_n)$
    6. $\boldsymbol{\theta}_{n+1} \leftarrow \boldsymbol{\theta}^{(K)}$
    7. $n \leftarrow n + 1$
4. **until** training complete
5. **return** $\boldsymbol{\theta}_n$
```

This is the key template that captures the essence of modern fitted Q-iteration: Monte Carlo integration (using observed next states $s'$), partial inner optimization ($K$ steps), and warmstarting (initialize from $\boldsymbol{\theta}_n$).

#### Flattening the Nested Structure

The nested loop structure is conceptually clear but algorithmically cumbersome. We can flatten it into a single loop using a target network $\boldsymbol{\theta}_{target}$ that gets updated every $K$ steps. This is algorithmically equivalent but removes the explicit nesting:

```{prf:algorithm} Flattened Neural Fitted Q-Iteration
:label: nfqi-flattened

**Input**: MDP $(S, A, P, R, \gamma)$, dataset $\mathcal{T}$, neural network $q(s,a; \boldsymbol{\theta})$, learning rate $\alpha$, target update frequency $K$

**Output**: Parameters $\boldsymbol{\theta}$ for Q-function approximation

1. Initialize $\boldsymbol{\theta}_0$ randomly
2. $\boldsymbol{\theta}_{target} \leftarrow \boldsymbol{\theta}_0$
3. $t \leftarrow 0$
4. **while** training:
    1. $\mathcal{D}_t \leftarrow \emptyset$
    2. For each $(s,a,r,s') \in \mathcal{T}$:
        1. $y_{s,a} \leftarrow r + \gamma \max_{a' \in A} q(s',a'; \boldsymbol{\theta}_{target})$
        2. $\mathcal{D}_t \leftarrow \mathcal{D}_t \cup \{((s,a), y_{s,a})\}$
    3. $\boldsymbol{\theta}_{t+1} \leftarrow \boldsymbol{\theta}_t - \alpha \nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta}_t; \mathcal{D}_t)$
    4. If $t \bmod K = 0$:
        1. $\boldsymbol{\theta}_{target} \leftarrow \boldsymbol{\theta}_t$
    5. $t \leftarrow t + 1$
5. **return** $\boldsymbol{\theta}_t$
```

The target network $\boldsymbol{\theta}_{target}$ plays exactly the role that $\boldsymbol{\theta}_n$ played in the nested version: it provides fixed targets for $K$ gradient steps. The periodic synchronization $\boldsymbol{\theta}_{target} \leftarrow \boldsymbol{\theta}_t$ marks what would have been the boundary of the outer loop. This flattened structure is more suitable for online learning, where we continuously collect data.

The target network is thus not a magical stabilization trick—it's simply the natural consequence of flattening NFQI's nested structure while preserving the key property that targets remain fixed for multiple gradient steps.

An alternative to periodic updates is exponential moving average (EMA), which updates the target network smoothly at every step:

```{prf:algorithm} Flattened Neural Fitted Q-Iteration with EMA
:label: nfqi-flattened-ema

**Input**: MDP $(S, A, P, R, \gamma)$, dataset $\mathcal{T}$, neural network $q(s,a; \boldsymbol{\theta})$, learning rate $\alpha$, EMA rate $\tau$

**Output**: Parameters $\boldsymbol{\theta}$ for Q-function approximation

1. Initialize $\boldsymbol{\theta}_0$ randomly
2. $\boldsymbol{\theta}_{target} \leftarrow \boldsymbol{\theta}_0$
3. $n \leftarrow 0$
4. **while** training:
    1. $\mathcal{D}_n \leftarrow \emptyset$
    2. For each $(s,a,r,s') \in \mathcal{T}$:
        1. $y_{s,a} \leftarrow r + \gamma \max_{a' \in A} q(s',a'; \boldsymbol{\theta}_{target})$
        2. $\mathcal{D}_n \leftarrow \mathcal{D}_n \cup \{((s,a), y_{s,a})\}$
    3. $\boldsymbol{\theta}_{n+1} \leftarrow \boldsymbol{\theta}_n - \alpha \nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta}_n; \mathcal{D}_n)$
    4. $\boldsymbol{\theta}_{target} \leftarrow \tau\boldsymbol{\theta}_{n+1} + (1-\tau)\boldsymbol{\theta}_{target}$
    5. $n \leftarrow n + 1$
5. **return** $\boldsymbol{\theta}_n$
```

EMA targets (also called Polyak averaging) became popular with DDPG {cite}`lillicrap2015continuous` and are now standard in continuous control algorithms like TD3 {cite}`fujimoto2018addressing` and SAC {cite}`haarnoja2018soft`, typically using small $\tau$ values like 0.001.

#### Online Data Collection and Replay Buffers

So far we've assumed a fixed dataset $\mathcal{T}$. In practice, we often want to collect data online while learning. This requires three modifications:

1. **Online data collection**: Use the current Q-function to act in the environment with an exploration strategy (e.g., $\varepsilon$-greedy)
2. **Replay buffer**: Store transitions in a finite-capacity buffer $\mathcal{R}$ rather than keeping all data
3. **Mini-batch sampling**: Sample a small batch from $\mathcal{R}$ at each step rather than using the full dataset

The replay buffer has a natural interpretation as a nonparametric model of the environment. The buffer $\mathcal{R} = \{(s_i, a_i, r_i, s'_i)\}$ represents an empirical approximation $\hat{p}(s', r | s, a)$ of the true transition distribution. When we sample from $\mathcal{R}$ to compute targets, we're performing Monte Carlo integration using this empirical distribution instead of the true one. This connects replay buffer methods to kernel-based RL {cite}`Ormoneit2002`, which makes the nonparametric modeling explicit.

However, the replay distribution differs from the current policy's state-action visitation. Early transitions come from an exploratory policy; later we sample these alongside new transitions from the improved policy. The buffer contains a mixture that doesn't match any single policy—this is what makes replay buffer learning off-policy.

Putting these pieces together yields the Deep Q-Network algorithm:

```{prf:algorithm} Deep Q-Network (DQN)
:label: dqn

**Input**: MDP $(S, A, P, R, \gamma)$, neural network $q(s,a; \boldsymbol{\theta})$, learning rate $\alpha$, target update frequency $K$, replay buffer size $B$, mini-batch size $b$

**Output**: Parameters $\boldsymbol{\theta}$ for Q-function approximation

1. Initialize $\boldsymbol{\theta}_0$ randomly
2. $\boldsymbol{\theta}_{target} \leftarrow \boldsymbol{\theta}_0$
3. Initialize replay buffer $\mathcal{R}$ with capacity $B$
4. $n \leftarrow 0$
5. **while** training:
    1. Observe current state $s$
    2. Select action $a$ using $\varepsilon$-greedy policy: $a = \begin{cases} \arg\max_{a'} q(s,a';\boldsymbol{\theta}_n) & \text{with probability } 1-\varepsilon \\ \text{random action} & \text{with probability } \varepsilon \end{cases}$
    3. Execute $a$, observe reward $r$ and next state $s'$
    4. Store $(s,a,r,s')$ in $\mathcal{R}$, replacing oldest if full
    5. Sample mini-batch of $b$ transitions $(s_i,a_i,r_i,s_i')$ from $\mathcal{R}$
    6. $\mathcal{D}_n \leftarrow \emptyset$
    7. For each sampled $(s_i,a_i,r_i,s_i')$:
        1. $y_i \leftarrow r_i + \gamma \max_{a' \in A} q(s_i',a'; \boldsymbol{\theta}_{target})$
        2. $\mathcal{D}_n \leftarrow \mathcal{D}_n \cup \{((s_i,a_i), y_i)\}$
    8. $\boldsymbol{\theta}_{n+1} \leftarrow \boldsymbol{\theta}_n - \alpha \nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta}_n; \mathcal{D}_n)$
    9. If $n \bmod K = 0$:
        1. $\boldsymbol{\theta}_{target} \leftarrow \boldsymbol{\theta}_n$
    10. $n \leftarrow n + 1$
6. **return** $\boldsymbol{\theta}_n$
```

Note the replay ratio implicit in this algorithm: for each new transition we collect (step 5.4), we sample a mini-batch of size $b$ (step 5.5) and perform one update. We're reusing past experiences at a ratio of $b$:1. This ratio is a tunable hyperparameter—higher ratios mean more computation per environment step but better data efficiency.

#### Decoupling Selection and Evaluation

As discussed earlier in this chapter, the max operator in target computation can lead to overestimation: $y_i = r_i + \gamma \max_{a'} q(s_i',a'; \boldsymbol{\theta}_{target})$. We use the same network to both select the best-looking action and evaluate it, potentially compounding estimation errors.

Double DQN {cite}`van2016deep` addresses this by using the current network to select actions but the target network to evaluate them:

```{prf:algorithm} Double Deep Q-Network (Double DQN)
:label: double-dqn

**Input**: MDP $(S, A, P, R, \gamma)$, neural network $q(s,a; \boldsymbol{\theta})$, learning rate $\alpha$, target update frequency $K$, replay buffer size $B$, mini-batch size $b$

**Output**: Parameters $\boldsymbol{\theta}$ for Q-function approximation

1. Initialize $\boldsymbol{\theta}_0$ randomly
2. $\boldsymbol{\theta}_{target} \leftarrow \boldsymbol{\theta}_0$
3. Initialize replay buffer $\mathcal{R}$ with capacity $B$
4. $n \leftarrow 0$
5. **while** training:
    1. Observe current state $s$
    2. Select action $a$ using $\varepsilon$-greedy policy based on $q(s,\cdot;\boldsymbol{\theta}_n)$
    3. Execute $a$, observe reward $r$ and next state $s'$
    4. Store $(s,a,r,s')$ in $\mathcal{R}$, replacing oldest if full
    5. Sample mini-batch of $b$ transitions $(s_i,a_i,r_i,s_i')$ from $\mathcal{R}$
    6. $\mathcal{D}_n \leftarrow \emptyset$
    7. For each sampled $(s_i,a_i,r_i,s_i')$:
        1. $a^*_i \leftarrow \arg\max_{a' \in A} q(s_i',a'; \boldsymbol{\theta}_n)$
        2. $y_i \leftarrow r_i + \gamma q(s_i',a^*_i; \boldsymbol{\theta}_{target})$
        3. $\mathcal{D}_n \leftarrow \mathcal{D}_n \cup \{((s_i,a_i), y_i)\}$
    8. $\boldsymbol{\theta}_{n+1} \leftarrow \boldsymbol{\theta}_n - \alpha \nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta}_n; \mathcal{D}_n)$
    9. If $n \bmod K = 0$:
        1. $\boldsymbol{\theta}_{target} \leftarrow \boldsymbol{\theta}_n$
    10. $n \leftarrow n + 1$
6. **return** $\boldsymbol{\theta}_n$
```

The key change is in step 5.7: we first select the action using our current network $\boldsymbol{\theta}_n$ (step 5.7.1), then evaluate that specific action using the target network $\boldsymbol{\theta}_{target}$ (step 5.7.2). Since the two networks differ (the target updates less frequently), their estimation errors are less correlated, mitigating the overestimation bias.

### Smooth Bellman Operators and Entropy Regularization

The [chapter on smooth Bellman optimality equations](regmdp.md) showed that replacing $\max_a$ with $\frac{1}{\beta} \log \sum_a \exp(\beta \cdot)$ yields a smooth operator corresponding to entropy-regularized optimization. Applying the fitted Q-iteration framework with this smooth operator requires only one change: replace the target computation

$$
y_i = r_i + \gamma \max_{a'} q(s'_i, a'; \theta^{(k)})
$$

with the smooth version

$$
y_i = r_i + \gamma \frac{1}{\beta} \log \sum_{a'} \exp(\beta q(s'_i, a'; \theta^{(k)})).
$$

The regression problem $\min_\theta \sum_i (q(s_i, a_i; \theta) - y_i)^2$ remains unchanged. The learned policy is the softmax $\pi(a|s) = \exp(\beta q(s,a))/\sum_{a'} \exp(\beta q(s,a'))$ rather than the greedy $\arg\max_a q(s,a)$.

For discrete action spaces, this modification is straightforward. The logsumexp can be computed stably using the log-sum-exp trick (subtract the maximum before exponentiating to prevent overflow), and gradient computation through the softmax is standard in neural network frameworks. Soft Q-learning applies this to the DQN architecture: use smooth targets, sample from the softmax policy for exploration, and maintain replay buffers and target networks as in DQN.

For continuous action spaces, evaluating the logsumexp requires integration or sampling over the continuous action set, which is expensive. This motivated the development of soft actor-critic {cite}`haarnoja2018soft`, which we discuss in the continuous action section below. The soft actor-critic approach uses the reparameterization trick and analytically tractable policy distributions (typically Gaussian) to compute the entropy term and its gradients efficiently.

### From Sample Average Approximation to Stochastic Approximation

The algorithms we have examined so far (FQI, NFQI, DQN) all solve optimization problems of the form $\min_\theta \frac{1}{N}\sum_{i=1}^N \ell_i(\theta)$, where $\ell_i(\theta) = (q(s_i, a_i; \theta) - y_i)^2$ is the loss on the $i$-th sample. We compute the empirical average over a dataset or mini-batch, then minimize this average. This is sample average approximation (SAA): approximate an expectation with a sample average, then solve the resulting deterministic optimization problem.

Classical Q-learning {cite}`SuttonBarto2018` takes a different approach. Instead of accumulating samples and solving $\min_\theta \mathbb{E}[\ell(\theta)]$, it performs incremental updates:

$$
\theta \leftarrow \theta - \alpha \nabla_\theta \ell(\theta),
$$

using a single sample at a time. This is stochastic approximation (SA): directly update parameters using noisy gradient estimates without explicitly forming sample averages.

For Q-learning with a linear approximation $q(s,a; \theta) = \theta^\top \varphi(s,a)$, the update at transition $(s, a, r, s')$ is:

$$
\theta \leftarrow \theta + \alpha \left[r + \gamma \max_{a'} \theta^\top \varphi(s', a') - \theta^\top \varphi(s,a)\right] \varphi(s,a).
$$

This is a single gradient step on the squared TD error $(r + \gamma \max_{a'} q(s',a') - q(s,a))^2$ with step size $\alpha$. Each transition yields one update, and the parameter vector evolves continuously as new data arrives.

Comparing the approaches: SAA (FQI, NFQI, DQN) accumulates samples, computes targets, and solves $\min_\theta \sum_i (q(s_i, a_i; \theta) - y_i)^2$ via multiple gradient steps. SA (Q-learning) takes one gradient step per sample. The design choice is the number of inner-loop gradient steps: full convergence (SAA), many steps (DQN with $K$ steps per sample), or single step (SA). As $K$ decreases from full convergence to one, we transition from sample average approximation to stochastic approximation.

Stochastic approximation has a rich convergence theory. Under appropriate conditions on the step size sequence $\{\alpha_k\}$ (typically $\sum_k \alpha_k = \infty$ and $\sum_k \alpha_k^2 < \infty$, satisfied by $\alpha_k = 1/k$) and the sampling distribution, SA iterates converge almost surely to a local minimum. The ODE method provides a framework for analyzing these algorithms by relating the discrete stochastic updates to a continuous-time ordinary differential equation that describes the limiting behavior. We defer detailed convergence analysis to later treatment, but note that the distinction between SAA and SA is fundamental: SAA separates sampling from optimization while SA interleaves them completely.

Q-learning is thus the limiting case of our template: $K=1$ inner optimization step, online data collection (no replay buffer), and typically tabular or linear function approximation. Modern deep RL usually employs intermediate designs (DQN with $K=1$ to $4$ steps, replay buffers, neural networks) that blend SAA and SA characteristics.

## Synthesis and Connections

This chapter combined projection methods from the [previous chapter](projdp.md) with Monte Carlo integration, enabling approximate dynamic programming from sampled experience rather than exact model knowledge. Q-function representations amortize action selection, and different design choices (function approximators, optimization strategies, data collection modes) generate the major value-based algorithms in reinforcement learning.

The methods developed here search in the space of value functions. The [previous chapter](projdp.md) established the projection framework with exact operator evaluations. This chapter extended it to simulation-based settings where we approximate expectations via Monte Carlo sampling. The [next chapter](cadp.md) takes a complementary approach: rather than searching for value functions and extracting policies through maximization, we directly parameterize and optimize the policy itself. Just as value-based methods progressed from exact evaluation to Monte Carlo sampling, policy parametrization methods face the same challenge of evaluating expectations, leading to similar Monte Carlo techniques applied to policy gradients rather than Bellman operators.
