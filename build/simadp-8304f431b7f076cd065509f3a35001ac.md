---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.16.3
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# Simulation-Based Approximate Dynamic Programming 

The projection methods from the previous chapter showed how to transform the infinite-dimensional fixed-point problem $\Bellman v = v$ into a finite-dimensional one by choosing basis functions $\{\varphi_i\}$ and imposing conditions that make the residual $R(s) = \Bellman v(s) - v(s)$ small. Different projection conditions (Galerkin orthogonality, collocation at points, least squares minimization) yield different finite-dimensional systems to solve.

However, we left unresolved the question of how to evaluate the Bellman operator itself. Applying $\Bellman$ at any state requires computing an expectation:

$$
(\Bellman v)(s) = \max_{a \in \mathcal{A}_s} \left\{r(s,a) + \gamma \int v(s')p(ds'|s,a)\right\}
$$

For discrete state spaces with a manageable number of states, this expectation is a finite sum we can compute exactly. For continuous or high-dimensional state spaces, we need numerical integration. The projection methods framework is compatible with any quadrature scheme, but leaves the choice of integration method unspecified.

This chapter addresses the **integration subproblem** that arises at two levels in approximate dynamic programming. First, we must evaluate the transition expectation $\int v(s')p(ds'|s,a)$ within the Bellman operator itself. Second, when using projection methods like Galerkin or least squares, we encounter outer integrations for enforcing orthogonality conditions or minimizing residuals over distributions of states. Both require numerical approximation in continuous or high-dimensional spaces.

We begin by examining deterministic numerical integration methods: quadrature rules that approximate integrals by evaluating integrands at carefully chosen points with associated weights. We discuss how to coordinate the choice of quadrature with the choice of basis functions to balance approximation accuracy and computational cost. Then we turn to **Monte Carlo integration**, which approximates expectations using random samples rather than deterministic quadrature points. This shift from deterministic to stochastic integration is what brings us into machine learning territory. When we replace exact transition probabilities with samples drawn from simulations or real interactions, projection methods combined with Monte Carlo integration become what the operations research community calls **simulation-based approximate dynamic programming** and what the machine learning community calls **reinforcement learning**. By relying on samples rather than explicit probability functions, we move from model-based planning to data-driven learning.

## Evaluating the Bellman Operator with Numerical Quadrature

Before turning to Monte Carlo methods, we examine the structure of the numerical integration problem. When we apply the Bellman operator to an approximate value function $\hat{v}(s; \theta) = \sum_i \theta_i \varphi_i(s)$, we must evaluate integrals of the form:

$$ \int \hat{v}(s'; \theta) \, p(s'|s,a) \, ds' = \int \left(\sum_i \theta_i \varphi_i(s')\right) p(s'|s,a) \, ds' = \sum_i \theta_i \int \varphi_i(s') \, p(s'|s,a) \, ds' $$

This shows two independent approximations:

1. **Value function approximation**: We represent $v$ using basis functions $\{\varphi_i\}$ and coefficients $\theta$
2. **Quadrature approximation**: We approximate each integral $\int \varphi_i(s') p(s'|s,a) ds'$ numerically

These choices are independent but should be coordinated. To see why, consider what happens in projection methods when we iterate $\hat{v}^{(k+1)} = \Proj \Bellman \hat{v}^{(k)}$. In practice, we cannot evaluate $\Bellman$ exactly due to the integrals, so we compute $\hat{v}^{(k+1)} = \Proj \BellmanQuad \hat{v}^{(k)}$ instead, where $\BellmanQuad$ denotes the Bellman operator with numerical quadrature.

The error in a single iteration can be bounded by the triangle inequality. Let $v$ denote the true fixed point $v = \Bellman v$. Then:

$$
\begin{aligned}
\|v - \hat{v}^{(k+1)}\| &= \|v - \Proj \BellmanQuad \hat{v}^{(k)}\| \\
&\le \|v - \Proj \Bellman v\| + \|\Proj \Bellman v - \Proj \Bellman \hat{v}^{(k)}\| + \|\Proj \Bellman \hat{v}^{(k)} - \Proj \BellmanQuad \hat{v}^{(k)}\| \\
&= \underbrace{\|v - \Proj \Bellman v\|}_{\text{best approximation error}} + \underbrace{\|\Proj \Bellman v - \Proj \Bellman \hat{v}^{(k)}\|}_{\text{contraction of iterate error}} + \underbrace{\|\Proj \Bellman \hat{v}^{(k)} - \Proj \BellmanQuad \hat{v}^{(k)}\|}_{\text{quadrature error}}
\end{aligned}
$$

The first term is the best we can do with our basis (how well $\Proj$ approximates the true solution). The second term decreases with iterations when $\Proj \Bellman$ is a contraction. The third term is the error from replacing exact integrals with quadrature, and it does not vanish as iterations proceed.

To make this concrete, consider evaluating $(\Bellman \hat{v})(s)$ for our current approximation $\hat{v}(s; \theta) = \sum_i \theta_i \varphi_i(s)$. We want:

$$ (\Bellman \hat{v})(s) = \max_{a \in \mathcal{A}_s} \left\{r(s,a) + \gamma \sum_i \theta_i \int \varphi_i(s') p(s'|s,a) ds' \right\} $$

But we compute instead:

$$ (\BellmanQuad \hat{v})(s) = \max_{a \in \mathcal{A}_s} \left\{r(s,a) + \gamma \sum_i \theta_i \sum_j w_j \varphi_i(s'_j) \right\} $$

where $\{(s'_j, w_j)\}$ are quadrature nodes and weights. If the quadrature error $\|(\Bellman \hat{v})(s) - (\BellmanQuad \hat{v})(s)\|$ is large relative to the basis approximation quality, we cannot exploit the expressive power of the basis. For instance, degree-10 Chebyshev polynomials represent smooth functions to $O(10^{-8})$ accuracy, but combined with rectangle-rule quadrature (error $O(h^2) \approx 10^{-2}$), the quadrature term dominates the error bound. We pay the cost of storing and manipulating 10 coefficients but achieve only $O(h^2)$ convergence in the quadrature mesh size.

This echoes the coordination principle from continuous optimal control (Chapter on trajectory optimization): when transcribing a continuous-time problem, we use the same quadrature nodes for both the running cost integral and the dynamics integral. There, coordination ensures that "where we pay" aligns with "where we enforce" the dynamics. Here, coordination ensures that integration accuracy matches approximation accuracy. Both are instances of balancing multiple sources of error in numerical methods.

Standard basis-quadrature pairings achieve this balance:

- Piecewise constant or linear elements with midpoint or trapezoidal rules
- Chebyshev polynomials with Gauss-Chebyshev quadrature
- Legendre polynomials with Gauss-Legendre quadrature
- Hermite polynomials with Gauss-Hermite quadrature (for Gaussian shocks)

To make this concrete, we examine what these pairings look like in practice for collocation and Galerkin projection.

### Orthogonal Collocation with Chebyshev Polynomials

Consider approximating the value function using Chebyshev polynomials of degree $n-1$:

$$
\hat{v}(s; \theta) = \sum_{i=0}^{n-1} \theta_i T_i(s)
$$

For orthogonal collocation, we place collocation points at the zeros of $T_n(s)$, denoted $\{s_j\}_{j=1}^n$. At each collocation point, we require the Bellman equation to hold exactly:

$$
\hat{v}(s_j; \theta) = \max_{a \in \mathcal{A}} \left\{r(s_j,a) + \gamma \int \hat{v}(s'; \theta) p(ds'|s_j,a)\right\}
$$

The integral on the right must be approximated using quadrature. With Chebyshev-Gauss quadrature using the same nodes $\{s_k\}_{k=1}^n$ and weights $\{w_k\}_{k=1}^n$, this becomes:

$$
\hat{v}(s_j; \theta) = \max_{a \in \mathcal{A}} \left\{r(s_j,a) + \gamma \sum_{k=1}^n w_k \hat{v}(s_k; \theta) p(s_k|s_j,a)\right\}
$$

Substituting the basis representation $\hat{v}(s_k; \theta) = \sum_{i=0}^{n-1} \theta_i T_i(s_k)$:

$$
\sum_{i=0}^{n-1} \theta_i T_i(s_j) = \max_{a \in \mathcal{A}} \left\{r(s_j,a) + \gamma \sum_{k=1}^n w_k p(s_k|s_j,a) \sum_{i=0}^{n-1} \theta_i T_i(s_k)\right\}
$$

Rearranging:

$$
\sum_{i=0}^{n-1} \theta_i T_i(s_j) = \max_{a \in \mathcal{A}} \left\{r(s_j,a) + \gamma \sum_{i=0}^{n-1} \theta_i \underbrace{\sum_{k=1}^n w_k T_i(s_k) p(s_k|s_j,a)}_{B_{ji}^a}\right\}
$$

This yields a system of $n$ nonlinear equations (one per collocation point):

$$
\sum_{i=0}^{n-1} T_i(s_j) \theta_i = \max_{a \in \mathcal{A}} \left\{r(s_j,a) + \gamma \sum_{i=0}^{n-1} B_{ji}^a \theta_i\right\}, \quad j=1,\ldots,n
$$

The matrix elements $B_{ji}^a$ can be precomputed once the quadrature nodes, weights, and transition probabilities are known. Solving this system gives the coefficient vector $\theta$.

### Galerkin Projection with Hermite Polynomials

For a problem with Gaussian shocks, we might use Hermite polynomials $\{H_i(s)\}_{i=0}^{n-1}$ weighted by the Gaussian density $\phi(s)$. The Galerkin condition requires:

$$
\int \left(\Bellman \hat{v}(s; \theta) - \hat{v}(s; \theta)\right) H_j(s) \phi(s) ds = 0, \quad j=0,\ldots,n-1
$$

Expanding the Bellman operator:

$$
\int \left[\max_{a \in \mathcal{A}} \left\{r(s,a) + \gamma \int \hat{v}(s'; \theta) p(ds'|s,a)\right\} - \hat{v}(s; \theta)\right] H_j(s) \phi(s) ds = 0
$$

We approximate this outer integral using Gauss-Hermite quadrature with nodes $\{s_\ell\}_{\ell=1}^m$ and weights $\{w_\ell\}_{\ell=1}^m$:

$$
\sum_{\ell=1}^m w_\ell \left[\max_{a \in \mathcal{A}} \left\{r(s_\ell,a) + \gamma \int \hat{v}(s'; \theta) p(ds'|s_\ell,a)\right\} - \hat{v}(s_\ell; \theta)\right] H_j(s_\ell) = 0
$$

The inner integral (transition expectation) is also approximated using Gauss-Hermite quadrature:

$$
\int \hat{v}(s'; \theta) p(ds'|s_\ell,a) \approx \sum_{k=1}^m w_k \hat{v}(s_k; \theta) p(s_k|s_\ell,a)
$$

Substituting the basis representation and collecting terms:

$$
\sum_{\ell=1}^m w_\ell H_j(s_\ell) \max_{a \in \mathcal{A}} \left\{r(s_\ell,a) + \gamma \sum_{i=0}^{n-1} \theta_i \sum_{k=1}^m w_k H_i(s_k) p(s_k|s_\ell,a)\right\} = \sum_{\ell=1}^m w_\ell H_j(s_\ell) \sum_{i=0}^{n-1} \theta_i H_i(s_\ell)
$$

This gives $n$ nonlinear equations in $n$ unknowns. The right-hand side simplifies using orthogonality: when using the same quadrature nodes for projection and integration, $\sum_{\ell=1}^m w_\ell H_j(s_\ell) H_i(s_\ell) = \delta_{ij} \|H_j\|^2$.

In both cases, the basis-quadrature pairing ensures that:
1. The quadrature nodes appear in both the transition expectation and the outer projection
2. The quadrature accuracy matches the polynomial approximation order
3. Precomputed matrices capture the dynamics, making iterations efficient

## Monte Carlo Integration 

Deterministic quadrature rules work well when the state space has low dimension, the transition density is smooth, and we can evaluate it cheaply at arbitrary points. In many stochastic control problems none of these conditions truly hold. The state may be high dimensional, the dynamics may be given by a simulator rather than an explicit density, and the cost of each call to the model may be large. In that regime, deterministic quadrature becomes brittle. Monte Carlo methods offer a different way to approximate expectations, one that relies only on the ability to **sample** from the relevant distributions.

### Monte Carlo as randomized quadrature

Consider a single expectation of the form

$$
J = \int f(x)\,p(dx) = \mathbb{E}[f(X)],
$$

where $X \sim p$ and $f$ is some integrable function. Monte Carlo integration approximates $J$ by drawing independent samples $X^{(1)},\ldots,X^{(N)} \sim p$ and forming the sample average

$$
\hat{J}_N \equiv \frac{1}{N}\sum_{n=1}^N f\bigl(X^{(n)}\bigr).
$$

This estimator has two basic properties that will matter throughout this chapter.

First, it is **unbiased**:

$$
\mathbb{E}[\hat{J}_N]
= \mathbb{E}\left[\frac{1}{N}\sum_{n=1}^N f(X^{(n)})\right]
= \frac{1}{N}\sum_{n=1}^N \mathbb{E}[f(X^{(n)})]
= \mathbb{E}[f(X)]
= J.
$$

Second, its variance scales as $1/N$. If we write

$$
\sigma^2 \equiv \mathrm{Var}[f(X)],
$$

then independence of the samples gives

$$
\mathrm{Var}(\hat{J}_N)
= \frac{1}{N^2}\sum_{n=1}^N \mathrm{Var}\bigl(f(X^{(n)})\bigr)
= \frac{\sigma^2}{N}.
$$

The central limit theorem then says that for large $N$,

$$
\sqrt{N}\,(\hat{J}_N - J) \Longrightarrow \mathcal{N}(0,\sigma^2),
$$

so the integration error decays at rate $O(N^{-1/2})$. This rate is slow compared to high-order quadrature in low dimension, but it has one crucial advantage: it does not explicitly depend on the dimension of $x$. Monte Carlo integration pays in variance, not in an exponential growth in the number of nodes.

It is often helpful to view Monte Carlo as **randomized quadrature**. A deterministic quadrature rule selects nodes $x_j$ and weights $w_j$ in advance and computes

$$
\sum_j w_j f(x_j).
$$

Monte Carlo can be written in the same form: if we draw $X^{(n)}$ from density $p$, the sample average

$$
\hat{J}_N = \frac{1}{N}\sum_{n=1}^N f(X^{(n)})
$$

is just a quadrature rule with random nodes and equal weights. More advanced Monte Carlo schemes, such as importance sampling, change both the sampling distribution and the weights, but the basic idea remains the same.

### Monte Carlo evaluation of the Bellman operator

We now apply this to the Bellman operator. For a fixed value function $v$ and a given state-action pair $(s,a)$, the transition part of the Bellman operator is

$$
\int v(s')\,p(ds' \mid s,a)
= \mathbb{E}\left[v(S') \mid S = s, A = a\right].
$$

If we can simulate next states $S'^{(1)},\ldots,S'^{(N)}$ from the transition kernel $p(\cdot \mid s,a)$, either by calling a simulator or by interacting with the environment, we can approximate this expectation by

$$
\widehat{\mathbb{E}}_N\bigl[v(S') \mid s,a\bigr]
\equiv
\frac{1}{N}\sum_{n=1}^N v\bigl(S'^{(n)}\bigr).
$$

If the immediate reward is also random, say

$$
r = r(S,A,S') \quad \text{with} \quad (S', r) \sim p(\cdot \mid s,a),
$$

we can approximate the full one-step return

$$
\mathbb{E}\bigl[r + \gamma v(S') \mid S = s, A = a\bigr]
$$

by

$$
\widehat{G}_N(s,a)
\equiv
\frac{1}{N}\sum_{n=1}^N \bigl[r^{(n)} + \gamma v(S'^{(n)})\bigr],
$$

where $(r^{(n)}, S'^{(n)})$ are independent samples given $(s,a)$. Again, this is an unbiased estimator of the Bellman expectation for fixed $v$.

Plugging this into the Bellman operator gives a **Monte Carlo Bellman operator**:

$$
(\widehat{\Bellman}_N v)(s)
\equiv
\max_{a \in \mathcal{A}_s}
\left\{
\widehat{G}_N(s,a)
\right\}.
$$

The expectation inside the braces is now replaced by a random sample average. In model-based settings, we implement this by simulating many next states for each candidate action $a$ at state $s$. In model-free settings, we obtain samples from real interactions and re-use them to estimate the expectation.

At this stage nothing about approximation or projection has entered yet. For a fixed value function $v$, Monte Carlo provides unbiased, noisy evaluations of $(\Bellman v)(s)$. The approximation question arises once we couple this stochastic evaluation with basis functions and projections.

### Sampling the outer expectations

Projection methods introduce a second layer of integration. In Galerkin and least squares schemes, we choose a distribution $\mu$ over states (and sometimes actions) and enforce conditions of the form

$$
\int R(s; \theta)\,p_i(s)\, \mu(ds) = 0
\quad \text{or} \quad
\int R(s; \theta)^2\,\mu(ds) \text{ is minimized}.
$$

Here $R(s; \theta)$ is the residual function, such as

$$
R(s; \theta) = (\Bellman \hat{v})(s;\theta) - \hat{v}(s;\theta),
$$

and $p_i$ are test functions or derivatives of the residual with respect to parameters.

Note a subtle but important shift in perspective from the previous chapter. There, the weight function $w(s)$ in the inner product $\langle f, g \rangle_w = \int f(s) g(s) w(s) ds$ could be any positive weight function (not necessarily normalized). For Monte Carlo integration, however, we need a probability distribution we can sample from. We write $\mu$ for this sampling distribution and express projection conditions as integrals with respect to $\mu$: $\int R(s; \theta) p_i(s) \mu(ds)$. If a problem was originally formulated with an unnormalized weight $w(s)$, we must either (i) normalize it to define $\mu$, or (ii) use importance sampling with a different $\mu$ and reweight samples by $w(s)/\mu(s)$. In reinforcement learning, $\mu$ is typically the empirical state visitation distribution from collected trajectories.

These outer integrals over $s$ are generally not easier to compute than the inner transition expectations. Monte Carlo gives a way to approximate them as well. If we can draw states $S^{(1)},\ldots,S^{(M)} \sim \mu$, we can approximate, for example, the Galerkin orthogonality conditions by

$$
\int R(s; \theta)\,p_i(s)\,\mu(ds)
\approx
\frac{1}{M}\sum_{m=1}^M R\bigl(S^{(m)}; \theta\bigr)\,p_i\bigl(S^{(m)}\bigr).
$$

Similarly, a least squares objective

$$
\int R(s; \theta)^2\,\mu(ds)
$$

is approximated by the empirical risk

$$
\frac{1}{M}\sum_{m=1}^M R\bigl(S^{(m)}; \theta\bigr)^2.
$$

If we now substitute Monte Carlo estimates for both the inner transition expectations and the outer projection conditions, we obtain fully simulation-based schemes. We no longer need explicit access to the transition kernel $p(\cdot \mid s,a)$ or the state distribution $\mu$. It is enough to be able to **sample** from them, either through a simulator or by interacting with the real system. 

### Simulation-Based Projection Methods

Monte Carlo integration replaces both levels of integration in approximate dynamic programming. The transition expectation in the Bellman operator becomes $\frac{1}{N}\sum_{n=1}^N v(S'^{(n)})$, and the outer integrals for projection become empirical averages over sampled states. Writing $\Proj_M$ for projection using $M$ state samples and $\widehat{\Bellman}_N$ for the Monte Carlo Bellman operator with $N$ transition samples, the iteration becomes

$$
\hat{v}^{(k+1)} = \Proj_M\,\widehat{\Bellman}_N \hat{v}^{(k)}.
$$

Unlike deterministic quadrature, which introduces a fixed bias at each iteration, Monte Carlo introduces random error with zero mean but nonzero variance. However, combining Monte Carlo with the maximization in the Bellman operator creates a systematic problem: while the estimate of the expected return for any individual action is unbiased, taking the maximum over these noisy estimates introduces upward bias. This overestimation compounds through value iteration and degrades the resulting policies.


## Amortizing Action Selection via Q-Functions

Monte Carlo integration enables model-free approximate dynamic programming: we no longer need explicit transition probabilities $p(s'|s,a)$, only the ability to sample next states. However, one computational challenge remains. The standard formulation of an optimal decision rule is

$$
\pi(s) = \arg\max_{a \in \mathcal{A}} \left\{r(s,a) + \gamma \int v(s')p(ds'|s,a)\right\}.
$$

Even with an optimal value function $v^*$ in hand, extracting an action at state $s$ requires evaluating the transition expectation for each candidate action. In the model-free setting, this means we must draw Monte Carlo samples from each action's transition distribution every time we select an action. This repeated sampling "at inference time" wastes computation, especially when the same state is visited multiple times.

We can amortize this computation by working at a different level of representation. Define the **state-action value function** (or **Q-function**)

$$
q(s,a) = r(s,a) + \gamma \int v(s')p(ds'|s,a).
$$

The Q-function caches the result of evaluating each action at each state. Once we have $q$, action selection reduces to a finite maximization:

$$
\pi(s) = \arg\max_{a \in \mathcal{A}(s)} q(s,a).
$$

No integration appears in this expression. The transition expectation has been precomputed and stored in $q$ itself.

The optimal Q-function $q^*$ satisfies its own Bellman equation. Substituting the definition of $v^*(s) = \max_a q^*(s,a)$ into the expression for $q$:

$$
q^*(s,a) = r(s,a) + \gamma \int p(ds'|s,a) \max_{a' \in \mathcal{A}(s')} q^*(s', a').
$$

This defines a Bellman operator on Q-functions:

$$
(\Bellman q)(s,a) = r(s,a) + \gamma \int p(ds'|s,a)\max_{a' \in \mathcal{A}(s')} q(s', a').
$$

Like the Bellman operator on value functions, $\Bellman$ is a $\gamma$-contraction in the sup-norm, guaranteeing a unique fixed point $q^*$. We can thus apply the same projection and Monte Carlo techniques developed for value functions to Q-functions. The computational cost shifts from action selection (repeated sampling at decision time) to training (evaluating expectations during the iterations of approximate value iteration). Once $q$ is learned, acting is cheap.

```{prf:algorithm} Parametric Q-Value Iteration
:label: simadp-parametric-q-value-iteration

**Input** Given an MDP $(S, A, P, R, \gamma)$, base points $\mathcal{B} \subset S$, function approximator class $q(s,a; \boldsymbol{\theta})$, maximum iterations $N$, tolerance $\varepsilon > 0$

**Output** Parameters $\boldsymbol{\theta}$ for Q-function approximation

1. Initialize $\boldsymbol{\theta}_0$ (e.g., for zero initialization)
2. $n \leftarrow 0$
3. **repeat**
    1. $\mathcal{D} \leftarrow \emptyset$
    2. For each $(s,a) \in \mathcal{B} \times A$:
        1. $y_{s,a} \leftarrow r(s,a) + \gamma \int p(ds'|s,a)\max_{a' \in A} q(s',a'; \boldsymbol{\theta}_n)$
        2. $\mathcal{D} \leftarrow \mathcal{D} \cup \{((s,a), y_{s,a})\}$
    3. $\boldsymbol{\theta}_{n+1} \leftarrow \texttt{fit}(\mathcal{D})$
    4. $\delta \leftarrow \frac{1}{|\mathcal{D}||A|}\sum_{(s,a) \in \mathcal{D} \times A} (q(s,a; \boldsymbol{\theta}_{n+1}) - q(s,a; \boldsymbol{\theta}_n))^2$
    5. $n \leftarrow n + 1$
4. **until** ($\delta < \varepsilon$ or $n \geq N$)
5. **return** $\boldsymbol{\theta}_n$
```

## Upward Bias from Maximizing Noisy Estimates

When we apply Monte Carlo integration to the Bellman operator, each individual expectation $\mathbb{E}[\hat{\mu}_N(s,a)] = \mu(s,a)$ is unbiased. The problem arises when we maximize over these noisy estimates. Write the Monte Carlo Bellman operator as

$$
(\widehat{\Bellman}_N v)(s) = \max_{a \in \mathcal{A}_s} \left\{ r(s,a) + \gamma \hat{\mu}_N(s,a)\right\},
$$

where $\hat{\mu}_N(s,a) = \frac{1}{N}\sum_{i=1}^N v(s'_i)$ with $s'_i \sim p(\cdot|s,a)$. The maximization introduces systematic overestimation. To see why, let $a^*$ be the truly optimal action. Since the maximum of any collection is at least as large as any element:

$$
\mathbb{E}\big[\max_a \{r(s,a) + \gamma \hat{\mu}_N(s,a)\}\big] \ge \mathbb{E}\big[r(s,a^*) + \gamma \hat{\mu}_N(s,a^*)\big] = r(s,a^*) + \gamma \mu(s,a^*) = (\Bellman v)(s).
$$

The inequality is strict whenever multiple actions have nonzero variance. Actions with positive noise realizations are more likely to be selected, and their inflated values contribute to the maximum. This is Jensen's inequality: $\mathbb{E}[\max_a Y_a] \ge \max_a \mathbb{E}[Y_a]$ for random variables $\{Y_a\}$, since the max is convex.

```{prf:remark} Connection to deterministic policies
This inequality also underlies why deterministic policies are optimal in MDPs (Theorem {prf:ref}`stoch-policy-reduction`): $\max_a w(a) \ge \sum_a q(a) w(a)$ shows randomization cannot improve expected returns. The same mathematical principle appears in two contexts: (1) deterministic policies suffice, (2) maximization over noisy estimates creates upward bias.
```

### Bias Accumulation Under Iteration

Value iteration applies $v_{k+1} = \widehat{\Bellman}_N v_k$ repeatedly. The bias accumulates because each iteration's output becomes the next iteration's input. At iteration 2, we compute $\hat{\mu}_N(s,a) = \frac{1}{N}\sum v_1(s'_i)$, but $v_1$ is already inflated from iteration 1. We average an overestimated function before applying the maximization, which adds another layer of bias. This feedback loop propagates and compounds: $\mathbb{E}[v_k] \ge \mathbb{E}[v_{k-1}] \ge \cdots \ge v^*$. Favorable noise at iteration $k$ becomes part of the "ground truth" for iteration $k+1$.

## Learning the Bias Correction

One approach to overestimation bias, developed by Keane and Wolpin {cite}`Keane1994`, treats the bias as a quantity to be learned and subtracted. Define the bias at state $s$ as $\delta(s) = \mathbb{E}[(\widehat{\Bellman}_N v)(s)] - (\Bellman v)(s) \ge 0$. While we cannot compute this directly, it depends on observable quantities: the number of actions, the sample size $N$, and the spread of action values. When several actions have similar values, noise is more likely to flip the maximizer, increasing $\delta(s)$.

The method estimates $\delta(s)$ empirically at a subset of states by comparing high-fidelity simulation (many samples or exact integration) against the low-fidelity $N$-sample estimate used in value iteration. A regression model $g_\eta$ is fit to predict the bias from features like the spread of action values and number of actions. During value iteration, the correction is subtracted: $r(s,a^\star) + \gamma(\max_a \hat{\mu}_N(s,a) - g_\eta(s))$.

This approach has been influential in econometrics but has seen limited adoption in reinforcement learning. The computational overhead is significant: it requires high-fidelity simulation at training states, feature engineering, and maintaining the regression model. More critically, the circular dependency between the bias estimate and the value function can amplify errors if $g_\eta$ is misspecified. We now turn to a simpler alternative.

## Decoupling Selection and Evaluation

The Keane-Wolpin approach treated maximization bias as a quantity to be learned and subtracted. An alternative strategy is to modify the estimator itself to break the coupling that causes the bias. Recall from the previous section that the bias arises when we take $\max_a \{r(s,a) + \gamma \hat{\mu}_N(s,a)\}$: the maximization both selects an action and uses the noise in that same estimate to compute the value.

To see the mechanism precisely, decompose the Monte Carlo estimate into its mean and noise:

$$
\hat{\mu}_N(s,a) = \mu(s,a) + \varepsilon_a,
$$

where $\mu(s,a) = \mathbb{E}_{s' \sim p(\cdot|s,a)}[v(s')]$ is the true continuation value and $\varepsilon_a$ is zero-mean noise from the Monte Carlo sampling. The standard value iteration update computes

$$
\max_a \left\{r(s,a) + \gamma \hat{\mu}_N(s,a)\right\} = \max_a \left\{r(s,a) + \gamma \mu(s,a) + \gamma \varepsilon_a\right\}.
$$

Whichever action $a$ has the largest noise realization $\varepsilon_a$ becomes more likely to be selected, and the final value includes that same positive noise. This is the source of the coupling: the same random variable $\varepsilon_a$ influences both which action is selected and what value we assign to it. Writing $a^\star = \arg\max_a \{r(s,a) + \gamma \mu(s,a) + \gamma \varepsilon_a\}$ for the selected action, the target is

$$
Y = r(s,a^\star) + \gamma \mu(s,a^\star) + \gamma \varepsilon_{a^\star}.
$$

Taking expectations gives $\mathbb{E}[Y] \ge \max_a \{r(s,a) + \gamma \mu(s,a)\}$ because the maximization systematically selects actions with positive noise and includes that noise in the evaluation.

Double Q-learning {cite}`van2016deep` breaks this coupling by maintaining two independent Monte Carlo estimates. For each action $a$ at state $s$, we draw two separate sets of samples:
- Draw $N$ next states $s'_1, \ldots, s'_N \sim p(\cdot|s,a)$ to compute $\hat{\mu}^{(1)}_N(s,a) = \frac{1}{N}\sum_{i=1}^N v(s'_i)$
- Draw $N$ different next states $s''_1, \ldots, s''_N \sim p(\cdot|s,a)$ to compute $\hat{\mu}^{(2)}_N(s,a) = \frac{1}{N}\sum_{i=1}^N v(s''_i)$

The key is that these are physically different samples: the $s'_i$ and $s''_i$ are drawn independently. Decompose both estimates as

$$
\hat{\mu}^{(1)}_N(s,a) = \mu(s,a) + \varepsilon^{(1)}_a, \quad \hat{\mu}^{(2)}_N(s,a) = \mu(s,a) + \varepsilon^{(2)}_a,
$$

where $\varepsilon^{(1)}_a$ and $\varepsilon^{(2)}_a$ are zero-mean random variables. Because the two sample sets $\{s'_i\}$ and $\{s''_i\}$ are drawn independently and we treat $v$ as a fixed function when computing the averages, the noise terms are independent:

$$
\varepsilon^{(1)}_a \perp\!\!\!\perp \varepsilon^{(2)}_{a'} \quad \text{for all } a, a'.
$$

This independence holds conditional on the current value function $v$. Even though $v$ may have been estimated from data in previous iterations, at the current iteration we treat it as a deterministic function and draw fresh independent samples to evaluate expectations involving $v$.

Use the first estimate to select the action but the second to evaluate it:

$$
a^\star = \arg\max_{a} \left\{r(s,a) + \gamma \hat{\mu}^{(1)}_N(s,a)\right\}, \quad Y = r(s,a^\star) + \gamma \hat{\mu}^{(2)}_N(s,a^\star).
$$

Now the selected action depends on $\varepsilon^{(1)}$:

$$
a^\star = \arg\max_{a} \left\{r(s,a) + \gamma \mu(s,a) + \gamma \varepsilon^{(1)}_a\right\},
$$

but the target value uses $\varepsilon^{(2)}_{a^\star}$:

$$
Y = r(s,a^\star) + \gamma \mu(s,a^\star) + \gamma \varepsilon^{(2)}_{a^\star}.
$$

To see why this eliminates the evaluation bias, write the expectation as a nested expectation:

$$
\mathbb{E}[Y] = \mathbb{E}_{\varepsilon^{(1)}}\Big[\mathbb{E}_{\varepsilon^{(2)}}\big[r(s,a^\star) + \gamma \mu(s,a^\star) + \gamma \varepsilon^{(2)}_{a^\star} \mid a^\star\big]\Big].
$$

The inner expectation, conditional on the selected action $a^\star$, equals:

$$
\mathbb{E}_{\varepsilon^{(2)}}\big[r(s,a^\star) + \gamma \mu(s,a^\star) + \gamma \varepsilon^{(2)}_{a^\star} \mid a^\star\big] = r(s,a^\star) + \gamma \mu(s,a^\star),
$$

because $\varepsilon^{(2)}_{a^\star}$ is independent of $a^\star$ (the selection was made using $\varepsilon^{(1)}$, not $\varepsilon^{(2)}$), so $\mathbb{E}[\varepsilon^{(2)}_{a^\star} \mid a^\star] = 0$. Taking the outer expectation over $a^\star$ gives:

$$
\mathbb{E}[Y] = \mathbb{E}_{\varepsilon^{(1)}}\big[r(s,a^\star) + \gamma \mu(s,a^\star)\big].
$$

The evaluation noise $\varepsilon^{(2)}_{a^\star}$ contributes zero on average because it is independent of the selection. The coupling is broken: the noise that influences which action is selected ($\varepsilon^{(1)}$) is different from the noise used to evaluate that action ($\varepsilon^{(2)}$).

However, double Q-learning does not eliminate all bias. The selected action $a^\star$ still depends on the selection noise $\varepsilon^{(1)}$. If an action has a large positive realization of $\varepsilon^{(1)}_a$, it becomes more likely to be selected. This means

$$
\mathbb{E}\big[r(s,a^\star) + \gamma \mu(s,a^\star)\big] \ne \max_a \left\{r(s,a) + \gamma \mu(s,a)\right\}
$$

in general. The selection bias remains, but the evaluation bias is eliminated. In the standard estimator, both sources of noise compound to produce overestimation. In double Q-learning, only the selection noise contributes, typically yielding a much smaller bias. The algorithm updates the second Q-function symmetrically, using $\hat{\mu}^{(2)}_N(s,a)$ for selection and $\hat{\mu}^{(1)}_N(s,a)$ for evaluation.

The following algorithm implements this principle with Monte Carlo integration. We maintain two Q-functions and alternate which one selects actions and which one evaluates them. The bias reduction mechanism applies whether we store Q-values in a table or use function approximation.

```{prf:algorithm} Double Q Value Iteration with Monte Carlo
:label: double-q-value-iteration

**Input:** MDP, state sample $\mathcal{S}$, Monte Carlo sample size $N$, maximum iterations $K$

**Output:** Two Q-functions $q^{(1)}$, $q^{(2)}$

1. Initialize $q^{(1)}_0(s,a)$ and $q^{(2)}_0(s,a)$ for all $s,a$
2. $k \leftarrow 0$
3. **repeat**
4. $\quad$ **for** each $s \in \mathcal{S}$ **do**
5. $\quad\quad$ **for** each $a \in \mathcal{A}(s)$ **do**
6. $\quad\quad\quad$ Draw $N$ next states: $s'_1, \ldots, s'_N \sim p(\cdot \mid s,a)$
7. $\quad\quad\quad$ **// Compute targets using decoupled selection-evaluation**
8. $\quad\quad\quad$ **for** $i = 1, \ldots, N$ **do**
9. $\quad\quad\quad\quad$ $a^{(1)}_i \leftarrow \arg\max_{a'} q^{(1)}_k(s'_i, a')$
10. $\quad\quad\quad\quad$ $a^{(2)}_i \leftarrow \arg\max_{a'} q^{(2)}_k(s'_i, a')$
11. $\quad\quad\quad$ **end for**
12. $\quad\quad\quad$ $q^{(1)}_{k+1}(s,a) \leftarrow r(s,a) + \frac{\gamma}{N}\sum_{i=1}^N q^{(2)}_k(s'_i, a^{(1)}_i)$
13. $\quad\quad\quad$ $q^{(2)}_{k+1}(s,a) \leftarrow r(s,a) + \frac{\gamma}{N}\sum_{i=1}^N q^{(1)}_k(s'_i, a^{(2)}_i)$
14. $\quad\quad$ **end for**
15. $\quad$ **end for**
16. $\quad$ $k \leftarrow k+1$
17. **until** convergence or $k \geq K$
18. **return** $q^{(1)}_k$, $q^{(2)}_k$
```

At lines 9 and 12, we use $q^{(1)}$ to select the action but $q^{(2)}$ to evaluate it. The selection at line 9 is influenced by the noise in $q^{(1)}$, but the evaluation at line 12 uses the independent noise in $q^{(2)}$. This breaks the coupling: the evaluation noise does not contribute to overestimation bias because it was not used in the selection. The same principle applies symmetrically at lines 10 and 13 for updating $q^{(2)}$. Bias from selection noise remains, but is typically much smaller than the compounded bias from coupling both selection and evaluation to the same noise source.
