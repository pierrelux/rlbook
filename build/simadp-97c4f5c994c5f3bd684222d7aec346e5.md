---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.16.3
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# Simulation-Based Approximate Dynamic Programming 

The projection methods from the previous chapter showed how to transform the infinite-dimensional fixed-point problem $\Bellman v = v$ into a finite-dimensional one by choosing basis functions $\{\varphi_i\}$ and imposing conditions that make the residual $R(s) = \Bellman v(s) - v(s)$ small. Different projection conditions (Galerkin orthogonality, collocation at points, least squares minimization) yield different finite-dimensional systems to solve.

However, we left unresolved the question of how to evaluate the Bellman operator itself. Applying $\Bellman$ at any state requires computing an expectation:

$$
(\Bellman v)(s) = \max_{a \in \mathcal{A}_s} \left\{r(s,a) + \gamma \int v(s')p(ds'|s,a)\right\}
$$

For discrete state spaces with a manageable number of states, this expectation is a finite sum we can compute exactly. For continuous or high-dimensional state spaces, we need numerical integration. The projection methods framework is compatible with any quadrature scheme, but leaves the choice of integration method unspecified.

This chapter addresses the **integration subproblem** that arises at two levels in approximate dynamic programming. First, we must evaluate the transition expectation $\int v(s')p(ds'|s,a)$ within the Bellman operator itself. Second, when using projection methods like Galerkin or least squares, we encounter outer integrations for enforcing orthogonality conditions or minimizing residuals over distributions of states. Both require numerical approximation in continuous or high-dimensional spaces.

We begin by examining deterministic numerical integration methods: quadrature rules that approximate integrals by evaluating integrands at carefully chosen points with associated weights. We discuss how to coordinate the choice of quadrature with the choice of basis functions to balance approximation accuracy and computational cost. Then we turn to **Monte Carlo integration**, which approximates expectations using random samples rather than deterministic quadrature points. This shift from deterministic to stochastic integration is what brings us into machine learning territory. When we replace exact transition probabilities with samples drawn from simulations or real interactions, projection methods combined with Monte Carlo integration become what the operations research community calls **simulation-based approximate dynamic programming** and what the machine learning community calls **reinforcement learning**. By relying on samples rather than explicit probability functions, we move from model-based planning to data-driven learning.

## Evaluating the Bellman Operator with Numerical Quadrature

Before turning to Monte Carlo methods, we examine the structure of the numerical integration problem. When we apply the Bellman operator to an approximate value function $\hat{v}(s; \theta) = \sum_i \theta_i \varphi_i(s)$, we must evaluate integrals of the form:

$$ \int \hat{v}(s'; \theta) \, p(s'|s,a) \, ds' = \int \left(\sum_i \theta_i \varphi_i(s')\right) p(s'|s,a) \, ds' = \sum_i \theta_i \int \varphi_i(s') \, p(s'|s,a) \, ds' $$

This shows two independent approximations:

1. **Value function approximation**: We represent $v$ using basis functions $\{\varphi_i\}$ and coefficients $\theta$
2. **Quadrature approximation**: We approximate each integral $\int \varphi_i(s') p(s'|s,a) ds'$ numerically

These choices are independent but should be coordinated. To see why, consider what happens in projection methods when we iterate $\hat{v}^{(k+1)} = \Proj \Bellman \hat{v}^{(k)}$. In practice, we cannot evaluate $\Bellman$ exactly due to the integrals, so we compute $\hat{v}^{(k+1)} = \Proj \BellmanQuad \hat{v}^{(k)}$ instead, where $\BellmanQuad$ denotes the Bellman operator with numerical quadrature.

The error in a single iteration can be bounded by the triangle inequality. Let $v$ denote the true fixed point $v = \Bellman v$. Then:

$$
\begin{aligned}
\|v - \hat{v}^{(k+1)}\| &= \|v - \Proj \BellmanQuad \hat{v}^{(k)}\| \\
&\le \|v - \Proj \Bellman v\| + \|\Proj \Bellman v - \Proj \Bellman \hat{v}^{(k)}\| + \|\Proj \Bellman \hat{v}^{(k)} - \Proj \BellmanQuad \hat{v}^{(k)}\| \\
&= \underbrace{\|v - \Proj \Bellman v\|}_{\text{best approximation error}} + \underbrace{\|\Proj \Bellman v - \Proj \Bellman \hat{v}^{(k)}\|}_{\text{contraction of iterate error}} + \underbrace{\|\Proj \Bellman \hat{v}^{(k)} - \Proj \BellmanQuad \hat{v}^{(k)}\|}_{\text{quadrature error}}
\end{aligned}
$$

The first term is the best we can do with our basis (how well $\Proj$ approximates the true solution). The second term decreases with iterations when $\Proj \Bellman$ is a contraction. The third term is the error from replacing exact integrals with quadrature, and it does not vanish as iterations proceed.

To make this concrete, consider evaluating $(\Bellman \hat{v})(s)$ for our current approximation $\hat{v}(s; \theta) = \sum_i \theta_i \varphi_i(s)$. We want:

$$ (\Bellman \hat{v})(s) = \max_{a \in \mathcal{A}_s} \left\{r(s,a) + \gamma \sum_i \theta_i \int \varphi_i(s') p(s'|s,a) ds' \right\} $$

But we compute instead:

$$ (\BellmanQuad \hat{v})(s) = \max_{a \in \mathcal{A}_s} \left\{r(s,a) + \gamma \sum_i \theta_i \sum_j w_j \varphi_i(s'_j) \right\} $$

where $\{(s'_j, w_j)\}$ are quadrature nodes and weights. If the quadrature error $\|(\Bellman \hat{v})(s) - (\BellmanQuad \hat{v})(s)\|$ is large relative to the basis approximation quality, we cannot exploit the expressive power of the basis. For instance, degree-10 Chebyshev polynomials represent smooth functions to $O(10^{-8})$ accuracy, but combined with rectangle-rule quadrature (error $O(h^2) \approx 10^{-2}$), the quadrature term dominates the error bound. We pay the cost of storing and manipulating 10 coefficients but achieve only $O(h^2)$ convergence in the quadrature mesh size.

This echoes the coordination principle from continuous optimal control (Chapter on trajectory optimization): when transcribing a continuous-time problem, we use the same quadrature nodes for both the running cost integral and the dynamics integral. There, coordination ensures that "where we pay" aligns with "where we enforce" the dynamics. Here, coordination ensures that integration accuracy matches approximation accuracy. Both are instances of balancing multiple sources of error in numerical methods.

Standard basis-quadrature pairings achieve this balance:

- Piecewise constant or linear elements with midpoint or trapezoidal rules
- Chebyshev polynomials with Gauss-Chebyshev quadrature
- Legendre polynomials with Gauss-Legendre quadrature
- Hermite polynomials with Gauss-Hermite quadrature (for Gaussian shocks)

To make this concrete, we examine what these pairings look like in practice for collocation and Galerkin projection.

### Orthogonal Collocation with Chebyshev Polynomials

Consider approximating the value function using Chebyshev polynomials of degree $n-1$:

$$
\hat{v}(s; \theta) = \sum_{i=0}^{n-1} \theta_i T_i(s)
$$

For orthogonal collocation, we place collocation points at the zeros of $T_n(s)$, denoted $\{s_j\}_{j=1}^n$. At each collocation point, we require the Bellman equation to hold exactly:

$$
\hat{v}(s_j; \theta) = \max_{a \in \mathcal{A}} \left\{r(s_j,a) + \gamma \int \hat{v}(s'; \theta) p(ds'|s_j,a)\right\}
$$

The integral on the right must be approximated using quadrature. With Chebyshev-Gauss quadrature using the same nodes $\{s_k\}_{k=1}^n$ and weights $\{w_k\}_{k=1}^n$, this becomes:

$$
\hat{v}(s_j; \theta) = \max_{a \in \mathcal{A}} \left\{r(s_j,a) + \gamma \sum_{k=1}^n w_k \hat{v}(s_k; \theta) p(s_k|s_j,a)\right\}
$$

Substituting the basis representation $\hat{v}(s_k; \theta) = \sum_{i=0}^{n-1} \theta_i T_i(s_k)$:

$$
\sum_{i=0}^{n-1} \theta_i T_i(s_j) = \max_{a \in \mathcal{A}} \left\{r(s_j,a) + \gamma \sum_{k=1}^n w_k p(s_k|s_j,a) \sum_{i=0}^{n-1} \theta_i T_i(s_k)\right\}
$$

Rearranging:

$$
\sum_{i=0}^{n-1} \theta_i T_i(s_j) = \max_{a \in \mathcal{A}} \left\{r(s_j,a) + \gamma \sum_{i=0}^{n-1} \theta_i \underbrace{\sum_{k=1}^n w_k T_i(s_k) p(s_k|s_j,a)}_{B_{ji}^a}\right\}
$$

This yields a system of $n$ nonlinear equations (one per collocation point):

$$
\sum_{i=0}^{n-1} T_i(s_j) \theta_i = \max_{a \in \mathcal{A}} \left\{r(s_j,a) + \gamma \sum_{i=0}^{n-1} B_{ji}^a \theta_i\right\}, \quad j=1,\ldots,n
$$

The matrix elements $B_{ji}^a$ can be precomputed once the quadrature nodes, weights, and transition probabilities are known. Solving this system gives the coefficient vector $\theta$.

### Galerkin Projection with Hermite Polynomials

For a problem with Gaussian shocks, we might use Hermite polynomials $\{H_i(s)\}_{i=0}^{n-1}$ weighted by the Gaussian density $\phi(s)$. The Galerkin condition requires:

$$
\int \left(\Bellman \hat{v}(s; \theta) - \hat{v}(s; \theta)\right) H_j(s) \phi(s) ds = 0, \quad j=0,\ldots,n-1
$$

Expanding the Bellman operator:

$$
\int \left[\max_{a \in \mathcal{A}} \left\{r(s,a) + \gamma \int \hat{v}(s'; \theta) p(ds'|s,a)\right\} - \hat{v}(s; \theta)\right] H_j(s) \phi(s) ds = 0
$$

We approximate this outer integral using Gauss-Hermite quadrature with nodes $\{s_\ell\}_{\ell=1}^m$ and weights $\{w_\ell\}_{\ell=1}^m$:

$$
\sum_{\ell=1}^m w_\ell \left[\max_{a \in \mathcal{A}} \left\{r(s_\ell,a) + \gamma \int \hat{v}(s'; \theta) p(ds'|s_\ell,a)\right\} - \hat{v}(s_\ell; \theta)\right] H_j(s_\ell) = 0
$$

The inner integral (transition expectation) is also approximated using Gauss-Hermite quadrature:

$$
\int \hat{v}(s'; \theta) p(ds'|s_\ell,a) \approx \sum_{k=1}^m w_k \hat{v}(s_k; \theta) p(s_k|s_\ell,a)
$$

Substituting the basis representation and collecting terms:

$$
\sum_{\ell=1}^m w_\ell H_j(s_\ell) \max_{a \in \mathcal{A}} \left\{r(s_\ell,a) + \gamma \sum_{i=0}^{n-1} \theta_i \sum_{k=1}^m w_k H_i(s_k) p(s_k|s_\ell,a)\right\} = \sum_{\ell=1}^m w_\ell H_j(s_\ell) \sum_{i=0}^{n-1} \theta_i H_i(s_\ell)
$$

This gives $n$ nonlinear equations in $n$ unknowns. The right-hand side simplifies using orthogonality: when using the same quadrature nodes for projection and integration, $\sum_{\ell=1}^m w_\ell H_j(s_\ell) H_i(s_\ell) = \delta_{ij} \|H_j\|^2$.

In both cases, the basis-quadrature pairing ensures that:
1. The quadrature nodes appear in both the transition expectation and the outer projection
2. The quadrature accuracy matches the polynomial approximation order
3. Precomputed matrices capture the dynamics, making iterations efficient

## Monte Carlo Integration 

Deterministic quadrature rules work well when the state space has low dimension, the transition density is smooth, and we can evaluate it cheaply at arbitrary points. In many stochastic control problems none of these conditions truly hold. The state may be high dimensional, the dynamics may be given by a simulator rather than an explicit density, and the cost of each call to the model may be large. In that regime, deterministic quadrature becomes brittle. Monte Carlo methods offer a different way to approximate expectations, one that relies only on the ability to **sample** from the relevant distributions.

### Monte Carlo as randomized quadrature

Consider a single expectation of the form

$$
J = \int f(x)\,p(dx) = \mathbb{E}[f(X)],
$$

where $X \sim p$ and $f$ is some integrable function. Monte Carlo integration approximates $J$ by drawing independent samples $X^{(1)},\ldots,X^{(N)} \sim p$ and forming the sample average

$$
\hat{J}_N \equiv \frac{1}{N}\sum_{n=1}^N f\bigl(X^{(n)}\bigr).
$$

This estimator has two basic properties that will matter throughout this chapter.

First, it is **unbiased**:

$$
\mathbb{E}[\hat{J}_N]
= \mathbb{E}\left[\frac{1}{N}\sum_{n=1}^N f(X^{(n)})\right]
= \frac{1}{N}\sum_{n=1}^N \mathbb{E}[f(X^{(n)})]
= \mathbb{E}[f(X)]
= J.
$$

Second, its variance scales as $1/N$. If we write

$$
\sigma^2 \equiv \mathrm{Var}[f(X)],
$$

then independence of the samples gives

$$
\mathrm{Var}(\hat{J}_N)
= \frac{1}{N^2}\sum_{n=1}^N \mathrm{Var}\bigl(f(X^{(n)})\bigr)
= \frac{\sigma^2}{N}.
$$

The central limit theorem then says that for large $N$,

$$
\sqrt{N}\,(\hat{J}_N - J) \Longrightarrow \mathcal{N}(0,\sigma^2),
$$

so the integration error decays at rate $O(N^{-1/2})$. This rate is slow compared to high-order quadrature in low dimension, but it has one crucial advantage: it does not explicitly depend on the dimension of $x$. Monte Carlo integration pays in variance, not in an exponential growth in the number of nodes.

It is often helpful to view Monte Carlo as **randomized quadrature**. A deterministic quadrature rule selects nodes $x_j$ and weights $w_j$ in advance and computes

$$
\sum_j w_j f(x_j).
$$

Monte Carlo can be written in the same form: if we draw $X^{(n)}$ from density $p$, the sample average

$$
\hat{J}_N = \frac{1}{N}\sum_{n=1}^N f(X^{(n)})
$$

is just a quadrature rule with random nodes and equal weights. More advanced Monte Carlo schemes, such as importance sampling, change both the sampling distribution and the weights, but the basic idea remains the same.

### Monte Carlo evaluation of the Bellman operator

We now apply this to the Bellman operator. For a fixed value function $v$ and a given state-action pair $(s,a)$, the transition part of the Bellman operator is

$$
\int v(s')\,p(ds' \mid s,a)
= \mathbb{E}\left[v(S') \mid S = s, A = a\right].
$$

If we can simulate next states $S'^{(1)},\ldots,S'^{(N)}$ from the transition kernel $p(\cdot \mid s,a)$, either by calling a simulator or by interacting with the environment, we can approximate this expectation by

$$
\widehat{\mathbb{E}}_N\bigl[v(S') \mid s,a\bigr]
\equiv
\frac{1}{N}\sum_{n=1}^N v\bigl(S'^{(n)}\bigr).
$$

If the immediate reward is also random, say

$$
r = r(S,A,S') \quad \text{with} \quad (S', r) \sim p(\cdot \mid s,a),
$$

we can approximate the full one-step return

$$
\mathbb{E}\bigl[r + \gamma v(S') \mid S = s, A = a\bigr]
$$

by

$$
\widehat{G}_N(s,a)
\equiv
\frac{1}{N}\sum_{n=1}^N \bigl[r^{(n)} + \gamma v(S'^{(n)})\bigr],
$$

where $(r^{(n)}, S'^{(n)})$ are independent samples given $(s,a)$. Again, this is an unbiased estimator of the Bellman expectation for fixed $v$.

Plugging this into the Bellman operator gives a **Monte Carlo Bellman operator**:

$$
(\widehat{\Bellman}_N v)(s)
\equiv
\max_{a \in \mathcal{A}_s}
\left\{
\widehat{G}_N(s,a)
\right\}.
$$

The expectation inside the braces is now replaced by a random sample average. In model-based settings, we implement this by simulating many next states for each candidate action $a$ at state $s$. In model-free settings, we obtain samples from real interactions and re-use them to estimate the expectation.

At this stage nothing about approximation or projection has entered yet. For a fixed value function $v$, Monte Carlo provides unbiased, noisy evaluations of $(\Bellman v)(s)$. The approximation question arises once we couple this stochastic evaluation with basis functions and projections.

### Sampling the outer expectations

Projection methods introduce a second layer of integration. In Galerkin and least squares schemes, we choose a distribution $\mu$ over states (and sometimes actions) and enforce conditions of the form

$$
\int R(s; \theta)\,p_i(s)\, \mu(ds) = 0
\quad \text{or} \quad
\int R(s; \theta)^2\,\mu(ds) \text{ is minimized}.
$$

Here $R(s; \theta)$ is the residual function, such as

$$
R(s; \theta) = (\Bellman \hat{v})(s;\theta) - \hat{v}(s;\theta),
$$

and $p_i$ are test functions or derivatives of the residual with respect to parameters.

Note a subtle but important shift in perspective from the previous chapter. There, the weight function $w(s)$ in the inner product $\langle f, g \rangle_w = \int f(s) g(s) w(s) ds$ could be any positive weight function (not necessarily normalized). For Monte Carlo integration, however, we need a probability distribution we can sample from. We write $\mu$ for this sampling distribution and express projection conditions as integrals with respect to $\mu$: $\int R(s; \theta) p_i(s) \mu(ds)$. If a problem was originally formulated with an unnormalized weight $w(s)$, we must either (i) normalize it to define $\mu$, or (ii) use importance sampling with a different $\mu$ and reweight samples by $w(s)/\mu(s)$. In reinforcement learning, $\mu$ is typically the empirical state visitation distribution from collected trajectories.

These outer integrals over $s$ are generally not easier to compute than the inner transition expectations. Monte Carlo gives a way to approximate them as well. If we can draw states $S^{(1)},\ldots,S^{(M)} \sim \mu$, we can approximate, for example, the Galerkin orthogonality conditions by

$$
\int R(s; \theta)\,p_i(s)\,\mu(ds)
\approx
\frac{1}{M}\sum_{m=1}^M R\bigl(S^{(m)}; \theta\bigr)\,p_i\bigl(S^{(m)}\bigr).
$$

Similarly, a least squares objective

$$
\int R(s; \theta)^2\,\mu(ds)
$$

is approximated by the empirical risk

$$
\frac{1}{M}\sum_{m=1}^M R\bigl(S^{(m)}; \theta\bigr)^2.
$$

If we now substitute Monte Carlo estimates for both the inner transition expectations and the outer projection conditions, we obtain fully simulation-based schemes. We no longer need explicit access to the transition kernel $p(\cdot \mid s,a)$ or the state distribution $\mu$. It is enough to be able to **sample** from them, either through a simulator or by interacting with the real system. 

### Simulation-Based Projection Methods

Monte Carlo integration replaces both levels of integration in approximate dynamic programming. The transition expectation in the Bellman operator becomes $\frac{1}{N}\sum_{n=1}^N v(S'^{(n)})$, and the outer integrals for projection become empirical averages over sampled states. Writing $\Proj_M$ for projection using $M$ state samples and $\widehat{\Bellman}_N$ for the Monte Carlo Bellman operator with $N$ transition samples, the iteration becomes

$$
\hat{v}^{(k+1)} = \Proj_M\,\widehat{\Bellman}_N \hat{v}^{(k)}.
$$

Unlike deterministic quadrature, which introduces a fixed bias at each iteration, Monte Carlo introduces random error with zero mean but nonzero variance. However, combining Monte Carlo with the maximization in the Bellman operator creates a systematic problem: while the estimate of the expected return for any individual action is unbiased, taking the maximum over these noisy estimates introduces upward bias. This overestimation compounds through value iteration and degrades the resulting policies.

### Amortizing Action Selection via Q-Functions

Monte Carlo integration enables model-free approximate dynamic programming: we no longer need explicit transition probabilities $p(s'|s,a)$, only the ability to sample next states. However, one computational challenge remains. The standard formulation of an optimal decision rule is

$$
\pi(s) = \arg\max_{a \in \mathcal{A}} \left\{r(s,a) + \gamma \int v(s')p(ds'|s,a)\right\}.
$$

Even with an optimal value function $v^*$ in hand, extracting an action at state $s$ requires evaluating the transition expectation for each candidate action. In the model-free setting, this means we must draw Monte Carlo samples from each action's transition distribution every time we select an action. This repeated sampling "at inference time" wastes computation, especially when the same state is visited multiple times.

We can amortize this computation by working at a different level of representation. Define the **state-action value function** (or **Q-function**)

$$
q(s,a) = r(s,a) + \gamma \int v(s')p(ds'|s,a).
$$

The Q-function caches the result of evaluating each action at each state. Once we have $q$, action selection reduces to a finite maximization:

$$
\pi(s) = \arg\max_{a \in \mathcal{A}(s)} q(s,a).
$$

No integration appears in this expression. The transition expectation has been precomputed and stored in $q$ itself.

The optimal Q-function $q^*$ satisfies its own Bellman equation. Substituting the definition of $v^*(s) = \max_a q^*(s,a)$ into the expression for $q$:

$$
q^*(s,a) = r(s,a) + \gamma \int p(ds'|s,a) \max_{a' \in \mathcal{A}(s')} q^*(s', a').
$$

This defines a Bellman operator on Q-functions:

$$
(\Bellman q)(s,a) = r(s,a) + \gamma \int p(ds'|s,a)\max_{a' \in \mathcal{A}(s')} q(s', a').
$$

Like the Bellman operator on value functions, $\Bellman$ is a $\gamma$-contraction in the sup-norm, guaranteeing a unique fixed point $q^*$. We can thus apply the same projection and Monte Carlo techniques developed for value functions to Q-functions. The computational cost shifts from action selection (repeated sampling at decision time) to training (evaluating expectations during the iterations of approximate value iteration). Once $q$ is learned, acting is cheap.

```{prf:algorithm} Parametric Q-Value Iteration
:label: simadp-parametric-q-value-iteration

**Input** Given an MDP $(S, A, P, R, \gamma)$, base points $\mathcal{B} \subset S$, function approximator class $q(s,a; \boldsymbol{\theta})$, maximum iterations $N$, tolerance $\varepsilon > 0$

**Output** Parameters $\boldsymbol{\theta}$ for Q-function approximation

1. Initialize $\boldsymbol{\theta}_0$ (e.g., for zero initialization)
2. $n \leftarrow 0$
3. **repeat**
    1. $\mathcal{D} \leftarrow \emptyset$
    2. For each $(s,a) \in \mathcal{B} \times A$:
        1. $y_{s,a} \leftarrow r(s,a) + \gamma \int p(ds'|s,a)\max_{a' \in A} q(s',a'; \boldsymbol{\theta}_n)$
        2. $\mathcal{D} \leftarrow \mathcal{D} \cup \{((s,a), y_{s,a})\}$
    3. $\boldsymbol{\theta}_{n+1} \leftarrow \texttt{fit}(\mathcal{D})$
    4. $\delta \leftarrow \frac{1}{|\mathcal{D}||A|}\sum_{(s,a) \in \mathcal{D} \times A} (q(s,a; \boldsymbol{\theta}_{n+1}) - q(s,a; \boldsymbol{\theta}_n))^2$
    5. $n \leftarrow n + 1$
4. **until** ($\delta < \varepsilon$ or $n \geq N$)
5. **return** $\boldsymbol{\theta}_n$
```

## Overestimation Bias and Mitigation Strategies

The previous sections established that Monte Carlo integration enables simulation-based approximate dynamic programming. However, combining Monte Carlo with the maximization in the Bellman operator introduces a systematic problem. While individual expectations are unbiased, taking the maximum over noisy estimates creates upward bias. This section examines the sources of this bias and two approaches for mitigating it.

### Sources of Overestimation Bias

When we apply Monte Carlo integration to the Bellman operator, each individual expectation is unbiased. The problem arises when we maximize over these noisy estimates. To make this precise, consider a given state-action pair $(s,a)$ and value function $v$. Define the true continuation value

$$
\mu(s,a) \equiv \int v(s')\,p(ds' \mid s,a),
$$

and its Monte Carlo approximation with $N$ samples:

$$
\hat{\mu}_N(s,a) \equiv \frac{1}{N}\sum_{i=1}^N v(s'_i), \quad s'_i \sim p(\cdot|s,a).
$$

Each estimator is unbiased: $\mathbb{E}[\hat{\mu}_N(s,a)] = \mu(s,a)$. The Monte Carlo Bellman operator is

$$
(\widehat{\Bellman}_N v)(s) = \max_{a \in \mathcal{A}_s} \left\{ r(s,a) + \gamma \hat{\mu}_N(s,a)\right\}.
$$

While each $\hat{\mu}_N(s,a)$ is unbiased, the maximization introduces systematic overestimation. To see why, let $a^*$ be the truly optimal action. The maximum of any collection is at least as large as any particular element:

$$
\mathbb{E}\big[\max_a \{r(s,a) + \gamma \hat{\mu}_N(s,a)\}\big] \ge \mathbb{E}\big[r(s,a^*) + \gamma \hat{\mu}_N(s,a^*)\big] = r(s,a^*) + \gamma \mu(s,a^*) = (\Bellman v)(s).
$$

The inequality is strict whenever multiple actions have nonzero variance. The maximization selects whichever action happens to receive a positive noise realization, and that inflated estimate contributes to the target value. This is Jensen's inequality for the max operator: $\mathbb{E}[\max_a Y_a] \ge \max_a \mathbb{E}[Y_a]$ for random variables $\{Y_a\}$, since the max is convex. Even though we start with unbiased estimators, taking the maximum breaks unbiasedness.

```{prf:remark} Connection to deterministic policies
This inequality also underlies why deterministic policies are optimal in MDPs (Theorem {prf:ref}`stoch-policy-reduction`): $\max_a w(a) \ge \sum_a q(a) w(a)$ shows randomization cannot improve expected returns. The same mathematical principle appears in two contexts: (1) deterministic policies suffice, (2) maximization over noisy estimates creates upward bias.
```

Monte Carlo value iteration applies $v_{k+1} = \widehat{\Bellman}_N v_k$ repeatedly. The overestimation bias does not stay confined to a single iteration: it accumulates through the recursion. To see why, note that each iteration's output becomes the next iteration's input. At iteration 2, we compute Monte Carlo estimates $\hat{\mu}_N(s,a) = \frac{1}{N}\sum_{i=1}^N v_1(s'_i)$, but $v_1$ is already biased upward from iteration 1. We are now averaging an overestimated function before we even apply the maximization, which adds another layer of bias on top of what was inherited. This pattern repeats at each iteration, creating a feedback loop where $\mathbb{E}[v_k] \ge \mathbb{E}[v_{k-1}] \ge \cdots \ge v^*$. Favorable noise at iteration $k$ becomes embedded in the value function and treated as truth by iteration $k+1$.

### Learning the Bias Correction

One approach, developed by Keane and Wolpin {cite}`Keane1994` in the context of dynamic discrete choice models, treats the bias itself as a quantity to be learned and subtracted. For a given value function $v$ and Monte Carlo sample size $N$, define the bias at state $s$ as

$$
\delta(s) = \mathbb{E}\big[(\widehat{\Bellman}_N v)(s)\big] - (\Bellman v)(s) \ge 0.
$$

While we cannot compute $\delta(s)$ directly (we lack both the expectation and the exact Bellman application), the bias has structure. It depends on observable quantities: the number of actions $|\mathcal{A}_s|$, the sample size $N$, and the spread of action values. When one action dominates, $\delta(s)$ is small. When several actions have similar values, noise is more likely to flip the maximizer, increasing $\delta(s)$.

Rather than deriving $\delta(s)$ analytically, Keane and Wolpin proposed learning it empirically. The strategy follows a "simulate on a subset, interpolate everywhere" template common in econometric dynamic programming. At a carefully chosen set of states, we run both high-fidelity simulation (many samples or exact integration) and the low-fidelity $N$-sample estimate used in value iteration. The gap between these estimates provides training data for the bias. We then fit a regression model $g_\eta$ that predicts $\delta(s)$ from features of the state and action-value distribution. During value iteration, we subtract the predicted bias from the raw Monte Carlo maximum.

Useful features for predicting the bias include the spread of action values (from a separate high-fidelity simulation), the number of actions, and gaps to the best action. These are cheap to compute and track regimes where maximization bias is large. The procedure can be summarized as:

```{prf:algorithm} Keane-Wolpin Bias-Corrected Value Iteration
:label: keane-wolpin-bias-correction

**Input:** MDP, current value $v_k$, sample size $N$, learned bias correction $g_\eta$

**Output:** Bias-corrected next value $v_{k+1}$

1. **For** each state $s$ **do**:
   - **For** each action $a$ **do**:
     - Draw $N$ samples: $s'_1, \ldots, s'_N \sim p(\cdot|s,a)$
     - Compute $\hat{\mu}_N(s,a) = \frac{1}{N}\sum_{i=1}^N v_k(s'_i)$
   - Compute raw max: $M(s) = \max_a \{r(s,a) + \gamma \hat{\mu}_N(s,a)\}$
   - Construct features $\phi(s)$ from action-value spread (using separate high-fidelity estimate)
   - Bias-corrected value: $v_{k+1}(s) = M(s) - g_\eta(\phi(s))$
```

The regression model $g_\eta$ is trained offline by comparing high and low-fidelity simulations at a grid of states, then applied during each value iteration step. This approach has been influential in econometrics for structural estimation problems. However, it has seen limited adoption in reinforcement learning. The computational overhead is substantial: it requires high-fidelity simulation at training states, careful feature engineering, and maintaining the regression model throughout learning. More critically, the circular dependency between the bias estimate and the value function can amplify errors if $g_\eta$ is misspecified. We now turn to a simpler alternative that avoids explicit bias modeling.

### Decoupling Selection and Evaluation

An alternative approach modifies the estimator itself to break the coupling that creates bias. In the standard Monte Carlo update $\max_a \{r(s,a) + \gamma \hat{\mu}_N(s,a)\}$, the same noisy estimate both selects which action looks best and provides the value assigned to that action. To see the problem, decompose the estimator into its mean and noise:

$$
\hat{\mu}_N(s,a) = \mu(s,a) + \varepsilon_a,
$$

where $\varepsilon_a$ is zero-mean noise. Whichever action happens to have the largest $\varepsilon_a$ gets selected, and that same positive noise inflates the target value:

$$
Y = r(s,a^\star) + \gamma \mu(s,a^\star) + \gamma \varepsilon_{a^\star},
$$

where $a^\star = \arg\max_a \{r(s,a) + \gamma \mu(s,a) + \gamma \varepsilon_a\}$. This coupling (using the same random variable $\varepsilon_{a^\star}$ for both selection and evaluation) produces $\mathbb{E}[Y] \ge \max_a \{r(s,a) + \gamma \mu(s,a)\}$.

Double Q-learning {cite}`van2016deep` breaks this coupling by maintaining two independent Monte Carlo estimates. For each action $a$ at state $s$, we draw two separate sets of samples from $p(\cdot|s,a)$:

$$
\hat{\mu}^{(1)}_N(s,a) = \mu(s,a) + \varepsilon^{(1)}_a, \quad \hat{\mu}^{(2)}_N(s,a) = \mu(s,a) + \varepsilon^{(2)}_a,
$$

where $\varepsilon^{(1)}_a$ and $\varepsilon^{(2)}_a$ are independent zero-mean noise terms. We use the first estimate to select the action but the second to evaluate it:

$$
a^\star = \arg\max_{a} \left\{r(s,a) + \gamma \hat{\mu}^{(1)}_N(s,a)\right\}, \quad Y = r(s,a^\star) + \gamma \hat{\mu}^{(2)}_N(s,a^\star).
$$

Now $a^\star$ depends on $\varepsilon^{(1)}$, but the target value uses $\varepsilon^{(2)}_{a^\star}$. To see why this eliminates evaluation bias, write the expectation as a nested expectation:

$$
\mathbb{E}[Y] = \mathbb{E}_{\varepsilon^{(1)}}\Big[\mathbb{E}_{\varepsilon^{(2)}}\big[r(s,a^\star) + \gamma \mu(s,a^\star) + \gamma \varepsilon^{(2)}_{a^\star} \mid a^\star\big]\Big].
$$

The inner expectation, conditioned on the selected action $a^\star$, equals:

$$
\mathbb{E}_{\varepsilon^{(2)}}\big[r(s,a^\star) + \gamma \mu(s,a^\star) + \gamma \varepsilon^{(2)}_{a^\star} \mid a^\star\big] = r(s,a^\star) + \gamma \mu(s,a^\star),
$$

because $\varepsilon^{(2)}_{a^\star}$ is independent of $a^\star$ (the selection was made using $\varepsilon^{(1)}$, not $\varepsilon^{(2)}$). Thus $\mathbb{E}[\varepsilon^{(2)}_{a^\star} \mid a^\star] = 0$. Taking the outer expectation:

$$
\mathbb{E}[Y] = \mathbb{E}_{\varepsilon^{(1)}}\big[r(s,a^\star) + \gamma \mu(s,a^\star)\big].
$$

The evaluation noise contributes zero on average because it is independent of the selection. However, double Q-learning does not eliminate all bias. Selection bias remains: $a^\star$ still depends on $\varepsilon^{(1)}$, so in general $\mathbb{E}[Y] \ne \max_a \{r(s,a) + \gamma \mu(s,a)\}$. The method eliminates evaluation bias but not selection bias. In the standard estimator, both sources compound. In double Q-learning, only the selection noise contributes, yielding substantially smaller total bias.

The following algorithm implements this principle with Monte Carlo integration. We maintain two Q-functions and alternate which one selects actions and which one evaluates them. The bias reduction mechanism applies whether we store Q-values in a table or use function approximation.

```{prf:algorithm} Double Q Value Iteration with Monte Carlo
:label: double-q-value-iteration

**Input:** MDP, state sample $\mathcal{S}$, Monte Carlo sample size $N$, maximum iterations $K$

**Output:** Two Q-functions $q^{(1)}$, $q^{(2)}$

1. Initialize $q^{(1)}_0(s,a)$ and $q^{(2)}_0(s,a)$ for all $s,a$
2. $k \leftarrow 0$
3. **repeat**
4. $\quad$ **for** each $s \in \mathcal{S}$ **do**
5. $\quad\quad$ **for** each $a \in \mathcal{A}(s)$ **do**
6. $\quad\quad\quad$ Draw $N$ next states: $s'_1, \ldots, s'_N \sim p(\cdot \mid s,a)$
7. $\quad\quad\quad$ **// Compute targets using decoupled selection-evaluation**
8. $\quad\quad\quad$ **for** $i = 1, \ldots, N$ **do**
9. $\quad\quad\quad\quad$ $a^{(1)}_i \leftarrow \arg\max_{a'} q^{(1)}_k(s'_i, a')$
10. $\quad\quad\quad\quad$ $a^{(2)}_i \leftarrow \arg\max_{a'} q^{(2)}_k(s'_i, a')$
11. $\quad\quad\quad$ **end for**
12. $\quad\quad\quad$ $q^{(1)}_{k+1}(s,a) \leftarrow r(s,a) + \frac{\gamma}{N}\sum_{i=1}^N q^{(2)}_k(s'_i, a^{(1)}_i)$
13. $\quad\quad\quad$ $q^{(2)}_{k+1}(s,a) \leftarrow r(s,a) + \frac{\gamma}{N}\sum_{i=1}^N q^{(1)}_k(s'_i, a^{(2)}_i)$
14. $\quad\quad$ **end for**
15. $\quad$ **end for**
16. $\quad$ $k \leftarrow k+1$
17. **until** convergence or $k \geq K$
18. **return** $q^{(1)}_k$, $q^{(2)}_k$
```

At lines 9 and 12, $q^{(1)}$ selects the action but $q^{(2)}$ evaluates it. The noise in $q^{(1)}$ influences which action is chosen, but the independent noise in $q^{(2)}$ does not inflate the evaluation. Lines 10 and 13 apply the same principle symmetrically for updating $q^{(2)}$.

## A Unified View of Reinforcement Learning Algorithms

We have developed two foundational components for simulation-based approximate dynamic programming. From the previous chapter on projection methods, we have the fitted-value iteration framework: given a current approximation $\hat{v}^{(k)}$, we evaluate the Bellman operator to obtain targets, then fit a new approximation $\hat{v}^{(k+1)}$ to these targets using a chosen function class. Earlier in this chapter, we introduced Q-functions $q(s,a)$ as a representation that amortizes the cost of action selection by precomputing and storing the value of each state-action pair. We also showed how Monte Carlo integration replaces exact expectations with sample averages, enabling learning from simulated or real experience rather than requiring explicit knowledge of transition probabilities.

These three elements combine into a template that generates most modern reinforcement learning algorithms. The iteration $\hat{q}^{(k+1)} = \text{fit}(\{(s_i, a_i, y_i)\})$, where targets $y_i$ are computed from the Bellman operator applied to $\hat{q}^{(k)}$ and expectations are approximated via Monte Carlo, admits many instantiations. Different choices of function approximator, optimization procedure, data collection strategy, and operator structure yield different algorithms, each with distinct computational properties and empirical characteristics.

This section maps the design space systematically. We show how classical algorithms like fitted Q-iteration, neural fitted Q-iteration, DQN, and Q-learning arise as specific configurations of design choices. We then extend to smooth Bellman operators (connecting to the regularized MDP framework from an earlier chapter) and continuous action spaces where policy networks amortize the greedy action computation. The result is a unified perspective that reveals the common structure underlying apparently diverse methods.

### Design Choices and Algorithm Space

The template for simulation-based Q-iteration involves several independent design choices. Each choice affects computational cost, sample efficiency, convergence properties, and implementation complexity.

**Function approximator class.** The Q-function $q(s,a; \theta)$ can be represented using linear combinations of fixed basis functions, decision trees, neural networks, or kernel methods. Linear approximators and trees offer interpretability and theoretical guarantees but limited expressiveness. Neural networks provide flexibility for high-dimensional problems but require careful tuning. The choice determines both the representational capacity and the optimization landscape.

**Bellman operator structure.** The standard Bellman operator uses $\max_a$ to select the greedy action. The smooth Bellman operator replaces this with $\frac{1}{\beta} \log \sum_a \exp(\beta \cdot)$, yielding a differentiable relaxation that corresponds to entropy-regularized optimization. This choice affects the policy class (deterministic versus stochastic) and optimization properties (non-smooth versus smooth).

**Inner loop optimization.** At each iteration $k$, we solve $\min_\theta \sum_i (q(s_i, a_i; \theta) - y_i)^2$ for targets computed from $\hat{q}^{(k)}$. We can run this optimization to full convergence, perform a fixed number of gradient steps, or take just a single step. Full convergence gives the best fit but costs more computation per iteration. Single-step updates (stochastic gradient descent) are cheap but introduce bias from underoptimization.

**Warm starting.** We can initialize $\theta^{(k+1)}$ from scratch at each iteration, or warm-start from the previous parameters $\theta^{(k)}$. Warm starting leverages the fact that consecutive iterates are usually similar, reducing optimization cost. In the extreme case of single-step updates with warm starting, we maintain a continuously updated approximation.

**Data collection mode.** In offline (batch) mode, we collect a fixed dataset $\mathcal{D} = \{(s_i, a_i, r_i, s'_i)\}$ once and reuse it across all iterations. In online mode, we interleave data collection with learning: act in the environment using the current approximation, observe new transitions, and update. A middle ground uses a replay buffer: collect data online but store it in a buffer $\mathcal{D}$ from which we sample mini-batches for training.

**Overestimation mitigation.** As shown earlier, the maximization in the Bellman operator combined with Monte Carlo sampling creates upward bias. We can address this with double Q-learning, maintaining two independent estimators and using one to select actions while the other evaluates them. Alternatively, we can accept the bias or use learned corrections.

The following tables organize these choices:

| **Design Choice** | **Options** |
|:------------------|:------------|
| Function approximator | Linear basis, Decision trees, Neural networks, Kernel methods |
| Bellman operator | Hard max, Smooth logsumexp |
| Inner optimization | Full convergence, K steps, Single step (K=1) |
| Inner loop initialization | Cold start (from $\theta_0$), Warm start (from $\theta^{(k)}$) |
| Data collection | Offline (fixed batch), Online, Online with replay buffer |
| Bias mitigation | None, Double Q-learning, Learned correction |

Different combinations of these choices yield different algorithms. The table below shows how several well-known methods correspond to specific configurations:

| **Algorithm** | **Approximator** | **Bellman** | **Inner Loop** | **Initialization** | **Data** | **Bias Fix** |
|:--------------|:-----------------|:------------|:---------------|:-------------------|:---------|:-------------|
| FQI {cite}`ernst2005tree` | Extra Trees | Hard | Full | Cold | Offline | None |
| NFQI {cite}`riedmiller2005neural` | Neural Net | Hard | Full | Cold | Offline | None |
| DQN {cite}`mnih2013atari` | Deep NN | Hard | K steps | Warm | Replay | None |
| Double DQN {cite}`van2016deep` | Deep NN | Hard | K steps | Warm | Replay | Double Q |
| Q-learning {cite}`SuttonBarto2018` | Tabular/Linear | Hard | K=1 | Warm | Online | None |
| Soft Q {cite}`haarnoja2017reinforcement` | Neural Net | Smooth | K steps | Warm | Replay | None |

This table omits continuous action methods (NFQCA, DDPG, SAC), which introduce an additional design dimension. We address those after establishing the discrete action case. The initialization choice becomes particularly important when moving from batch to online algorithms, as we will see.

### Batch Algorithms: FQI and NFQI

The offline (batch) setting begins with a fixed dataset $\mathcal{D} = \{(s_i, a_i, r_i, s'_i)\}_{i=1}^N$ collected once before learning. This data might come from a previous controller, from human demonstrations, or from exploratory interactions. The task is to extract the best Q-function approximation from this data without additional environment interactions.

Fitted Q-iteration applies the template directly. At iteration $k$, we have Q-function parameters $\theta^{(k)}$. For each transition $(s_i, a_i, r_i, s'_i)$ in $\mathcal{D}$, we compute a target value:

$$
y_i^{(k)} = r_i + \gamma \max_{a' \in \mathcal{A}} q(s'_i, a'; \theta^{(k)}).
$$

This evaluates the Bellman operator at the sampled next state $s'_i$. We then solve the regression problem:

$$
\theta^{(k+1)} = \arg\min_\theta \sum_{i=1}^N \left(q(s_i, a_i; \theta) - y_i^{(k)}\right)^2.
$$

The choice of function approximator determines how we solve this problem. Ernst et al. {cite}`ernst2005tree` used extremely randomized trees, an ensemble method that partitions the state-action space into regions and fits piecewise constant Q-values. Trees handle high-dimensional inputs naturally and the ensemble reduces overfitting. The resulting method, simply called FQI (Fitted Q-Iteration), demonstrated that batch reinforcement learning could work with complex function approximators on continuous-state problems.

Riedmiller {cite}`riedmiller2005neural` replaced the tree ensemble with a neural network, yielding Neural Fitted Q-Iteration (NFQI). The neural network $q(s,a; \theta)$ provides smooth interpolation and can leverage modern optimization techniques like RProp (resilient backpropagation). NFQI runs the inner optimization to convergence at each iteration: train the network until the loss stops decreasing, then compute new targets using the converged Q-function. This full optimization ensures the network accurately represents the projected Bellman operator before moving to the next iteration.

The following algorithm shows the general structure:

```{prf:algorithm} Fitted Q-Iteration (Batch)
:label: fitted-q-iteration-batch

**Input:** Dataset $\mathcal{D} = \{(s_i, a_i, r_i, s'_i)\}_{i=1}^N$, function approximator class $q(s,a; \theta)$, discount factor $\gamma$, maximum iterations $K$, convergence tolerance $\varepsilon$

**Output:** Learned Q-function parameters $\theta$

1. Initialize $\theta_0$ (e.g., random initialization)
2. $k \leftarrow 0$
3. **repeat**
4. $\quad$ **// Compute targets from current Q-function**
5. $\quad$ **for** each transition $(s_i, a_i, r_i, s'_i) \in \mathcal{D}$ **do**
6. $\quad\quad$ $y_i \leftarrow r_i + \gamma \max_{a' \in \mathcal{A}} q(s'_i, a'; \theta_k)$
7. $\quad$ **end for**
8. $\quad$ **// Fit Q-function to targets**
9. $\quad$ $\theta_{k+1} \leftarrow \arg\min_\theta \sum_{i=1}^N (q(s_i, a_i; \theta) - y_i)^2$
10. $\quad$ $k \leftarrow k+1$
11. **until** $\|\theta_k - \theta_{k-1}\| < \varepsilon$ or $k \geq K$
12. **return** $\theta_k$
```

At line 9, the optimization is run to convergence (for FQI and NFQI) or for a fixed number of gradient steps (variants). The algorithm reuses the same offline dataset $\mathcal{D}$ at every iteration, making it sample-efficient when data collection is expensive but environment simulation is unavailable.

### Online Learning with Replay Buffers

The batch setting assumes we have a fixed dataset collected before learning begins. In many applications, we can interact with the environment (or a simulator) during training. Online learning interleaves data collection with parameter updates: use the current Q-function to act in the environment, observe new transitions, and immediately incorporate them into learning.

Pure online learning uses each transition once, then discards it. When we compute $y_i = r_i + \gamma \max_{a'} q(s'_i, a'; \theta)$ for a new transition and perform a gradient step, that sample never contributes to future updates. This is wasteful. Monte Carlo integration produces noisy estimates, and each sample contains information about the transition distribution that we throw away after a single use.

The replay buffer addresses this by storing transitions in a dataset $\mathcal{D}$ that grows as we interact with the environment. At each learning step, we sample a mini-batch from $\mathcal{D}$ and perform gradient updates on this sample. Transitions observed early in training continue to contribute to learning much later, even as the Q-function evolves.

This mechanism has a natural interpretation as a nonparametric model of the environment dynamics. The buffer $\mathcal{D} = \{(s_i, a_i, r_i, s'_i)\}$ represents an empirical approximation $\hat{p}(s', r | s, a)$ of the true transition distribution $p(s', r | s, a)$. When we compute target values, we need to evaluate expectations:

$$
\mathbb{E}_{s' \sim p(\cdot | s, a)}[\max_{a'} q(s', a'; \theta)].
$$

Sampling from the buffer to compute targets is Monte Carlo integration using the empirical distribution $\hat{p}$ instead of the true distribution $p$. This places replay buffer methods in the broader context we developed earlier in this chapter: deterministic quadrature uses fixed integration nodes and weights (model-based with exact dynamics), direct Monte Carlo samples from the true transition distribution (model-free with access to simulator or real environment), and replay buffer Monte Carlo samples from an empirical distribution built from stored data (model-based with a nonparametric model).

The sample efficiency of replay buffers comes from refining this empirical distribution. Each new transition improves the empirical model $\hat{p}$, and all stored transitions contribute to estimating expectations under this model. The replay buffer is a form of model-based reinforcement learning where the model is nonparametric: we store samples rather than fitting a parametric function. This connects to kernel-based RL methods {cite}`Ormoneit2002`, which make the nonparametric modeling explicit through kernel density estimation.

However, the replay distribution differs from the current policy's state-action visitation distribution. Early in training, we store transitions from an exploratory policy. Later, we sample these old transitions alongside new ones from the improved policy. The buffer contains a mixture distribution that does not match any single policy. This distribution shift is what makes replay buffer learning off-policy: we learn about the optimal Q-function while sampling from a distribution that the current policy does not induce.

Deep Q-Networks (DQN) {cite}`mnih2013atari` combines the replay buffer with deep neural networks and several other design choices. The algorithm maintains a replay buffer with capacity $C$ (typically 1 million transitions), storing $(s, a, r, s')$ tuples from recent interactions. At each time step, the agent:

1. Acts using an $\varepsilon$-greedy policy based on the current Q-network
2. Stores the resulting transition in the buffer
3. Samples a mini-batch of size $B$ (typically 32) uniformly from the buffer
4. Computes targets using a separate target network $\theta^-$
5. Performs a single gradient step on the Q-network

The target network $\theta^-$ is a copy of the main Q-network $\theta$ that updates only every $N$ steps (typically every 10,000 steps). This is a form of warm starting that stabilizes learning. Without it, targets change at every step as $\theta$ updates, creating a moving target that can cause divergent oscillations. The target network freezes the Bellman operator for multiple gradient steps, giving the main network time to converge toward the current target before the target shifts.

DQN uses $K$ gradient steps per environment interaction, where $K$ is usually small (1 to 4). This partial inner-loop optimization balances computational cost with convergence quality. Unlike batch FQI, which fully converges the Q-network before computing new targets, DQN continuously updates both data and parameters. This makes it an online algorithm despite using a replay buffer.

Double DQN {cite}`van2016deep` adds the selection-evaluation decoupling to mitigate overestimation bias. Recall that standard Q-learning computes:

$$
y = r + \gamma \max_{a'} q(s', a'; \theta^-).
$$

The same network evaluates all actions and selects the maximum. Double DQN uses the main network to select the action but the target network to evaluate it:

$$
y = r + \gamma q(s', \arg\max_{a'} q(s', a'; \theta); \theta^-).
$$

The main network $q(\cdot; \theta)$ determines which action looks best, but the target network $q(\cdot; \theta^-)$ provides the value estimate for that action. Since $\theta$ and $\theta^-$ differ (the target network updates less frequently), their estimation errors are less correlated than in standard DQN. This breaks the coupling that creates overestimation, as we analyzed earlier in the chapter.

### Smooth Bellman Operators and Entropy Regularization

All algorithms discussed so far use the hard max operator in the Bellman equation: $v(s) = \max_a \{r(s,a) + \gamma \mathbb{E}[v(s')]\}$. This produces deterministic optimal policies. An earlier chapter introduced the smooth Bellman operator as an alternative:

$$
v(s) = \frac{1}{\beta} \log \sum_a \exp\left(\beta \left[r(s,a) + \gamma \mathbb{E}[v(s')]\right]\right),
$$

where $\beta > 0$ is an inverse temperature parameter. As $\beta \to \infty$, the logsumexp converges to the max. For finite $\beta$, the operator is smooth and corresponds to solving an entropy-regularized MDP where we maximize expected return plus the entropy of the policy.

The smooth operator produces stochastic policies via the softmax:

$$
\pi(a|s) = \frac{\exp(\beta q(s,a))}{\sum_{a'} \exp(\beta q(s,a'))}.
$$

This policy assigns positive probability to all actions, with probability mass concentrated on high-value actions when $\beta$ is large and spread more uniformly when $\beta$ is small.

Replacing the hard max with the smooth logsumexp in any of our algorithms yields an entropy-regularized variant. Soft Q-learning {cite}`haarnoja2017reinforcement` applies this modification to DQN. The algorithm maintains a neural network $q(s,a; \theta)$ and uses the smooth Bellman backup:

$$
y = r + \gamma \frac{1}{\beta} \log \sum_{a'} \exp(\beta q(s', a'; \theta^-)).
$$

Training proceeds exactly as in DQN: collect data with a stochastic policy sampled from the current softmax, store transitions in a replay buffer, sample mini-batches, and update the Q-network toward these smooth targets. The learned policy is the softmax over the final Q-values.

The smooth operator has several advantages. First, it is differentiable everywhere, which can improve optimization when using gradient-based methods. Second, the stochastic policy enables exploration naturally without needing $\varepsilon$-greedy heuristics. Third, the entropy regularization can improve robustness by preventing premature convergence to brittle deterministic policies. The cost is an additional hyperparameter $\beta$ that must be tuned: too large and we recover nearly-deterministic behavior with the disadvantages of hard max, too small and the policy becomes overly random.

The connection to entropy-regularized MDPs provides theoretical grounding. The value function $v$ solving the smooth Bellman equation is the optimal value for the entropy-regularized objective $\max_\pi \mathbb{E}_\pi[\sum_t \gamma^t r_t + \alpha H(\pi(\cdot|s_t))]$ with $\alpha = 1/\beta$. This equivalence, established in detail in the earlier chapter, means soft Q-learning computes an exact solution to a well-defined optimization problem, not merely an approximation to the hard max problem.

### From Sample Average Approximation to Stochastic Approximation

The algorithms we have examined so far (FQI, NFQI, DQN) all solve optimization problems of the form $\min_\theta \frac{1}{N}\sum_{i=1}^N \ell_i(\theta)$, where $\ell_i(\theta) = (q(s_i, a_i; \theta) - y_i)^2$ is the loss on the $i$-th sample. We compute the empirical average over a dataset or mini-batch, then minimize this average. This is sample average approximation (SAA): approximate an expectation with a sample average, then solve the resulting deterministic optimization problem.

Classical Q-learning {cite}`SuttonBarto2018` takes a different approach. Instead of accumulating samples and solving $\min_\theta \mathbb{E}[\ell(\theta)]$, it performs incremental updates:

$$
\theta \leftarrow \theta - \alpha \nabla_\theta \ell(\theta),
$$

using a single sample at a time. This is stochastic approximation (SA): directly update parameters using noisy gradient estimates without explicitly forming sample averages.

For Q-learning with a linear approximation $q(s,a; \theta) = \theta^\top \varphi(s,a)$, the update at transition $(s, a, r, s')$ is:

$$
\theta \leftarrow \theta + \alpha \left[r + \gamma \max_{a'} \theta^\top \varphi(s', a') - \theta^\top \varphi(s,a)\right] \varphi(s,a).
$$

This is a single gradient step on the squared TD error $(r + \gamma \max_{a'} q(s',a') - q(s,a))^2$ with step size $\alpha$. Each transition yields one update, and the parameter vector evolves continuously as new data arrives.

Comparing the approaches: SAA (FQI, NFQI, DQN) accumulates samples, computes targets, and solves $\min_\theta \sum_i (q(s_i, a_i; \theta) - y_i)^2$ via multiple gradient steps. SA (Q-learning) takes one gradient step per sample. The design choice is the number of inner-loop gradient steps: full convergence (SAA), many steps (DQN with $K$ steps per sample), or single step (SA). As $K$ decreases from full convergence to one, we transition from sample average approximation to stochastic approximation.

Stochastic approximation has a rich convergence theory. Under appropriate conditions on the step size sequence $\{\alpha_k\}$ (typically $\sum_k \alpha_k = \infty$ and $\sum_k \alpha_k^2 < \infty$, satisfied by $\alpha_k = 1/k$) and the sampling distribution, SA iterates converge almost surely to a local minimum. The ODE method provides a framework for analyzing these algorithms by relating the discrete stochastic updates to a continuous-time ordinary differential equation that describes the limiting behavior. We defer detailed convergence analysis to later treatment, but note that the distinction between SAA and SA is fundamental: SAA separates sampling from optimization while SA interleaves them completely.

Q-learning is thus the limiting case of our template: $K=1$ inner optimization step, online data collection (no replay buffer), and typically tabular or linear function approximation. Modern deep RL usually employs intermediate designs (DQN with $K=1$ to $4$ steps, replay buffers, neural networks) that blend SAA and SA characteristics.

## Continuous Action Spaces

The algorithms developed so far assume discrete action spaces where we can enumerate all actions and compute $\max_a q(s,a)$ by evaluation. When actions are continuous, $a \in \mathbb{R}^d$, the maximization becomes an optimization problem that must be solved at every decision point.

### The Amortization Problem

Consider a robot arm control task where the action is a $d$-dimensional torque vector. To act greedily given Q-function $q(s,a; \theta)$, we must solve:

$$
\pi(s) = \arg\max_{a \in \mathcal{A}} q(s, a; \theta),
$$

where $\mathcal{A} \subset \mathbb{R}^d$ is a continuous set (often a box or polytope). This requires running an optimization algorithm at every time step. For neural network Q-functions, this means solving a nonlinear program whose objective involves forward passes through the network.

This computation happens at inference time. After training converges, we deploy the agent and it must select actions in real-time. Running interior-point methods or gradient-based optimizers at every decision creates unacceptable latency, especially in high-frequency control where decisions occur at 100Hz or faster.

The solution is to amortize the optimization cost by learning a separate policy network $\pi(s; \phi)$ that directly outputs actions. During training, we optimize $\phi$ so that $\pi(s; \phi) \approx \arg\max_a q(s,a; \theta)$ for states we encounter. At deployment, action selection reduces to a single forward pass through the policy network: $a = \pi(s; \phi)$. The computational cost of optimization is paid during training (where time is less constrained) rather than at inference.

This introduces a second approximation beyond the Q-function. We now have two function approximators: a critic $q(s,a; \theta)$ that estimates values, and an actor $\pi(s; \phi)$ that selects actions. The critic is trained using Bellman targets as before. The actor is trained to maximize the critic:

$$
\phi \leftarrow \phi + \beta \mathbb{E}_s \left[\nabla_\phi q(s, \pi(s; \phi); \theta)\right],
$$

where the expectation is over states in the replay buffer or current dataset. This gradient ascent pushes the actor toward actions that the critic considers valuable.

### Neural Fitted Q with Continuous Actions

Hafner and Riedmiller {cite}`Hafner2011` introduced Neural Fitted Q-Iteration for Continuous Actions (NFQCA), which extends NFQI to continuous action spaces using the actor-critic architecture. The algorithm alternates between:

1. **Critic update**: Perform fitted Q-iteration using the actor to compute targets. For each transition $(s_i, a_i, r_i, s'_i)$, the target is $y_i = r_i + \gamma q(s'_i, \pi(s'_i; \phi); \theta)$, where the actor $\pi(\cdot; \phi)$ provides the next action.

2. **Actor update**: Optimize the actor to maximize the critic over sampled states. This uses $\nabla_\phi q(s, \pi(s; \phi); \theta)$, which by the chain rule equals $(\nabla_a q(s,a; \theta)|_{a=\pi(s;\phi)}) \cdot (\nabla_\phi \pi(s; \phi))$.

Both networks are neural networks trained via backpropagation. The algorithm operates in batch mode: collect a dataset, run multiple critic-actor update cycles, evaluate the policy, collect more data. The following algorithm shows the structure:

```{prf:algorithm} Neural Fitted Q-Iteration for Continuous Actions (NFQCA)
:label: nfqca

**Input:** Dataset $\mathcal{D} = \{(s_i, a_i, r_i, s'_i)\}$, critic network $q(s,a; \theta)$, actor network $\pi(s; \phi)$, discount $\gamma$, critic iterations $K_c$, actor iterations $K_a$

**Output:** Trained critic $\theta$ and actor $\phi$

1. Initialize $\theta_0$ and $\phi_0$
2. **repeat**
3. $\quad$ **// Critic update (fitted Q-iteration)**
4. $\quad$ **for** $k = 1$ to $K_c$ **do**
5. $\quad\quad$ **for** each $(s_i, a_i, r_i, s'_i) \in \mathcal{D}$ **do**
6. $\quad\quad\quad$ $y_i \leftarrow r_i + \gamma q(s'_i, \pi(s'_i; \phi); \theta)$
7. $\quad\quad$ **end for**
8. $\quad\quad$ $\theta \leftarrow \theta - \alpha_c \nabla_\theta \sum_i (q(s_i, a_i; \theta) - y_i)^2$
9. $\quad$ **end for**
10. $\quad$ **// Actor update (policy gradient on critic)**
11. $\quad$ **for** $k = 1$ to $K_a$ **do**
12. $\quad\quad$ Sample mini-batch of states $\{s_j\}$ from $\mathcal{D}$
13. $\quad\quad$ $\phi \leftarrow \phi + \alpha_a \nabla_\phi \sum_j q(s_j, \pi(s_j; \phi); \theta)$
14. $\quad$ **end for**
15. **until** convergence or maximum iterations
16. **return** $\theta$, $\phi$
```

Line 6 uses the actor to evaluate the greedy action at $s'_i$, replacing the $\max_{a'}$ that would be intractable in continuous action spaces. Line 13 improves the actor by ascending the gradient of the critic's value. The algorithm maintains the batch structure of NFQI: full convergence of both networks before generating new data.

### DDPG: Online Learning with Continuous Actions

Deep Deterministic Policy Gradient (DDPG) {cite}`lillicrap2015continuous` is to NFQCA what DQN is to NFQI: it moves from batch to online mode with a replay buffer. The algorithm maintains:

- An actor network $\pi(s; \phi)$ and critic network $q(s,a; \theta)$
- Target networks $\pi(s; \phi^-)$ and $q(s,a; \theta^-)$ that update slowly
- A replay buffer $\mathcal{D}$ storing transitions from online interactions

At each environment step, DDPG collects one transition using the current actor plus exploration noise (typically Ornstein-Uhlenbeck process), stores it in the buffer, samples a mini-batch, and performs one gradient step on both actor and critic. The target networks update via exponential moving average: $\theta^- \leftarrow \tau \theta + (1-\tau)\theta^-$ with $\tau \ll 1$ (typically 0.001).

This is exactly the same relationship as between batch NFQI and online DQN: replace offline dataset with replay buffer, replace full optimization with mini-batch gradient steps, add target networks for stability. The algorithmic template accommodates continuous actions by substituting the actor-critic architecture for explicit maximization, but the core structure (fitted Q-iteration plus data collection strategy) remains unchanged.

### Soft Actor-Critic

Soft Actor-Critic (SAC) {cite}`haarnoja2018soft` combines three design choices: continuous actions (actor-critic), smooth Bellman operator, and online learning with replay. This completes the design space we outlined earlier.

The critic uses the smooth Bellman backup from entropy-regularized MDPs:

$$
y = r + \gamma \left(q(s', \pi(s'; \phi); \theta^-) - \alpha \log \pi(\pi(s'; \phi) | s'; \phi)\right),
$$

where $\alpha$ is the temperature parameter (inverse of $\beta$ in earlier notation) and $\pi(a|s; \phi)$ is the probability density of the stochastic policy. This adds the entropy bonus $\alpha H(\pi(\cdot|s))$ to the value, making the policy stochastic rather than deterministic.

The actor maximizes the entropy-regularized objective:

$$
\phi \leftarrow \phi + \beta \mathbb{E}_s\left[\mathbb{E}_{a \sim \pi(\cdot|s; \phi)}\left[q(s,a; \theta) - \alpha \log \pi(a|s; \phi)\right]\right].
$$

This gradient ascent increases expected Q-value while maintaining policy entropy. The result is a stochastic policy that explores naturally without adding noise externally.

SAC brings together all the design axes: neural network function approximators, smooth Bellman operator (entropy regularization), partial inner-loop optimization (mini-batch gradient steps), warm starting (slowly updated target networks), replay buffer for off-policy learning, and actor-critic for continuous actions. It demonstrates that our template framework accommodates diverse algorithmic choices in a unified structure.

The design space is complete: we have covered discrete and continuous actions, hard and smooth Bellman operators, batch and online data collection, full and partial optimization, and various approaches to bias mitigation. Each algorithm is a configuration of these choices, and understanding the template reveals both the commonalities and the meaningful distinctions between methods.

## Synthesis and Connections

This chapter developed simulation-based approximate dynamic programming by combining three foundational elements: projection methods from the previous chapter (fitting value functions to targets using finite-dimensional approximations), Q-function representations (amortizing action selection by caching state-action values), and Monte Carlo integration (replacing expectations with sample averages). These elements yield a template that generates the major algorithms in reinforcement learning through different design choices.

The progression from deterministic quadrature to Monte Carlo to replay buffers reveals a conceptual arc. Deterministic quadrature uses model-based planning with exact dynamics. Direct Monte Carlo sampling enables model-free learning when we can simulate or interact with the environment. Replay buffers build nonparametric models from stored data, bridging model-based and model-free approaches. The replay buffer is a form of empirical modeling where we store samples rather than fitting parametric transition functions. This interpretation connects to kernel methods and other nonparametric statistical techniques.

The design choices examined here determine both computational properties and empirical performance. Function approximators determine expressiveness and optimization landscape. The Bellman operator structure (hard versus smooth) determines whether we obtain deterministic or stochastic policies and affects differentiability. Inner-loop optimization (full convergence, multiple steps, single step) trades off per-iteration cost against convergence quality, with the extreme cases corresponding to sample average approximation (SAA) and stochastic approximation (SA). Data collection mode (offline, online, replay buffer) determines sample efficiency and distribution shift. Overestimation mitigation strategies address the bias created by combining maximization with noisy estimates.

These design choices are largely orthogonal. We can combine smooth Bellman operators with replay buffers (soft Q-learning), use actor-critic with batch data (NFQCA) or online replay (DDPG, SAC), and apply double Q-learning to any architecture that maximizes over actions. The modularity of the template explains both the proliferation of algorithms in the deep RL literature and the underlying commonalities. Each new method typically modifies one or two design dimensions while leaving others fixed.

The methods developed here all belong to the value-based family: they approximate value functions and extract policies by maximization or softmax. An alternative approach learns policies directly through policy gradient methods, which we will explore in subsequent chapters. Policy gradients optimize parametric policies $\pi(a|s; \theta)$ by ascending gradients of the expected return, without explicitly representing value functions (though they often use critics for variance reduction). The actor-critic architectures we saw for continuous actions anticipate this connection: the actor resembles a policy gradient method while the critic provides value-based guidance. Understanding both perspectives is necessary for modern reinforcement learning, and the unified framework developed here provides the foundation for comparing and combining them.
