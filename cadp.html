
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Policy Parametrization Methods &#8212; Practical Reinforcement Learning: From Algorithms to Applications</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=b4b7a797" />
    <link rel="stylesheet" type="text/css" href="_static/graphviz.css?v=fd3f3429" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"bm": ["{\\boldsymbol #1}", 1]}, "processEscapes": true}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'cadp';</script>
    <script src="_static/iframe-modal.js?v=f72a1242"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Example COCPs" href="appendix_examples.html" />
    <link rel="prev" title="Approximate Dynamic Programming" href="adp.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Practical Reinforcement Learning: From Algorithms to Applications</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Why This Book?
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Modeling</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="modeling.html">Why Build a Model? For Whom?</a></li>

<li class="toctree-l1"><a class="reference internal" href="ssm.html">Dynamics Models for Decision Making</a></li>




<li class="toctree-l1"><a class="reference internal" href="simulation.html">Programs as Models</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Numerical Trajectory Optimization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ocp.html">Discrete-Time Trajectory Optimization</a></li>


<li class="toctree-l1"><a class="reference internal" href="cocp.html">Trajectory Optimization in Continuous Time</a></li>




</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">From Trajectories to Policies</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="mpc.html">Model Predictive Control</a></li>




<li class="toctree-l1"><a class="reference internal" href="dp.html">Dynamic Programming</a></li>



</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Learning from Data</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="adp.html">Approximate Dynamic Programming</a></li>





<li class="toctree-l1 current active"><a class="current reference internal" href="#">Policy Parametrization Methods</a></li>







</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="appendix_examples.html">Example COCPs</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix_ivps.html">Solving Initial Value Problems</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix_nlp.html">Nonlinear Programming</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/pierrelux/rlbook" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pierrelux/rlbook/edit/main/cadp.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pierrelux/rlbook/issues/new?title=Issue%20on%20page%20%2Fcadp.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/cadp.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Policy Parametrization Methods</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Policy Parametrization Methods</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#embedded-optimization">Embedded Optimization</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#amortized-optimization-approach">Amortized Optimization Approach</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#deterministic-parametrized-policies">Deterministic Parametrized Policies</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-fitted-q-iteration-for-continuous-actions-nfqca">Neural Fitted Q-iteration for Continuous Actions (NFQCA)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-deterministic-policy-gradient-ddpg">Deep Deterministic Policy Gradient (DDPG)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#twin-delayed-deep-deterministic-policy-gradient-td3">Twin Delayed Deep Deterministic Policy Gradient (TD3)</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#soft-actor-critic">Soft Actor-Critic</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitted-q-iteration-for-the-smooth-bellman-equations">Fitted Q-Iteration for the Smooth Bellman Equations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#approximating-boltzmann-policies-by-gaussians">Approximating Boltzmann Policies by Gaussians</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reparameterizating-the-objective">Reparameterizating the Objective</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#derivative-estimation-for-stochastic-optimization">Derivative Estimation for Stochastic Optimization</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#change-of-measure-the-likelihood-ratio-method">Change of Measure: The Likelihood Ratio Method</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-change-of-variables-approach-the-reparameterization-trick">A Change of Variables Approach: The Reparameterization Trick</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-examples-of-reparameterization">Common Examples of Reparameterization</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bounded-intervals-the-truncated-normal">Bounded Intervals: The Truncated Normal</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling-from-0-1-the-kumaraswamy-distribution">Sampling from [0,1]: The Kumaraswamy Distribution</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#discrete-actions-the-gumbel-softmax">Discrete Actions: The Gumbel-Softmax</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#demonstration-numerical-analysis-of-gradient-estimators">Demonstration: Numerical Analysis of Gradient Estimators</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#score-function-gradient-estimation-in-reinforcement-learning">Score Function Gradient Estimation in Reinforcement Learning</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#leveraging-conditional-independence">Leveraging Conditional Independence</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variance-reduction-via-control-variates">Variance Reduction via Control Variates</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generalized-advantage-estimator">Generalized Advantage Estimator</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-policy-gradient-theorem">The Policy Gradient Theorem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normalized-discounted-state-visitation-distribution">Normalized Discounted State Visitation Distribution</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-optimization-with-a-model">Policy Optimization with a Model</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation-policy-optimization">Backpropagation Policy Optimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-value-gradient-svg">Stochastic Value Gradient (SVG)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#noise-inference-in-svg">Noise Inference in SVG</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dpg-as-a-special-case-of-sac">DPG as a Special Case of SAC</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-optimization-with-a-trust-region">Policy Optimization with a Trust Region</a></li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="policy-parametrization-methods">
<h1>Policy Parametrization Methods<a class="headerlink" href="#policy-parametrization-methods" title="Link to this heading">#</a></h1>
<p>In the previous chapter, we explored various approaches to approximate dynamic programming, focusing on ways to handle large state spaces through function approximation. However, these methods still face significant challenges when dealing with large or continuous action spaces. The need to maximize over actions during the Bellman operator evaluation becomes computationally prohibitive as the action space grows.</p>
<p>This chapter explores a natural evolution of these ideas: rather than exhaustively searching over actions, we can parameterize and directly optimize the policy itself. We begin by examining how fitted Q methods, while powerful for handling large state spaces, still struggle with action space complexity.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="embedded-optimization">
<h1>Embedded Optimization<a class="headerlink" href="#embedded-optimization" title="Link to this heading">#</a></h1>
<p>Recall that in fitted Q methods, the main idea is to compute the Bellman operator only at a subset of all states, relying on function approximation to generalize to the remaining states. At each step of the successive approximation loop, we build a dataset of input state-action pairs mapped to their corresponding optimality operator evaluations:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{D}_n = \{((s, a), (Lq)(s, a; \boldsymbol{\theta}_n)) \mid (s,a) \in \mathcal{B}\}
\]</div>
<p>This dataset is then fed to our function approximator (neural network, random forest, linear model) to obtain the next set of parameters:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\theta}_{n+1} \leftarrow \texttt{fit}(\mathcal{D}_n)
\]</div>
<p>While this strategy allows us to handle very large or even infinite (continuous) state spaces, it still requires maximizing over actions (<span class="math notranslate nohighlight">\(\max_{a \in A}\)</span>) during the dataset creation when computing the operator <span class="math notranslate nohighlight">\(L\)</span> for each basepoint. This maximization becomes computationally expensive for large action spaces. A natural improvement is to add another level of optimization: for each sample added to our regression dataset, we can employ numerical optimization methods to find actions that maximize the Bellman operator for the given state.</p>
<div class="proof algorithm admonition" id="fitted-q-iteration-explicit">
<p class="admonition-title"><span class="caption-number">Algorithm 33 </span> (Fitted Q-Iteration with Explicit Optimization)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> Given an MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, base points <span class="math notranslate nohighlight">\(\mathcal{B}\)</span>, function approximator class <span class="math notranslate nohighlight">\(q(s,a; \boldsymbol{\theta})\)</span>, maximum iterations <span class="math notranslate nohighlight">\(N\)</span>, tolerance <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span></p>
<p><strong>Output</strong> Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> for Q-function approximation</p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span> (e.g., for zero initialization)</p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow 0\)</span></p></li>
<li><p><strong>repeat</strong></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{D} \leftarrow \emptyset\)</span> // Regression Dataset</p></li>
<li><p>For each <span class="math notranslate nohighlight">\((s,a,r,s') \in \mathcal{B}\)</span>: // Assumes Monte Carlo Integration with one sample</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(y_{s,a} \leftarrow r + \gamma \texttt{maximize}(q(s', \cdot; \boldsymbol{\theta}_n))\)</span> // <span class="math notranslate nohighlight">\(s'\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span> are kept fixed</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D} \leftarrow \mathcal{D} \cup \{((s,a), y_{s,a})\}\)</span></p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{n+1} \leftarrow \texttt{fit}(\mathcal{D})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\delta \leftarrow \frac{1}{|\mathcal{D}||A|}\sum_{(s,a) \in \mathcal{D} \times A} (q(s,a; \boldsymbol{\theta}_{n+1}) - q(s,a; \boldsymbol{\theta}_n))^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow n + 1\)</span></p></li>
</ol>
</li>
<li><p><strong>until</strong> (<span class="math notranslate nohighlight">\(\delta &lt; \varepsilon\)</span> or <span class="math notranslate nohighlight">\(n \geq N\)</span>)</p></li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span></p></li>
</ol>
</section>
</div><p>The above pseudocode introduces a generic <span class="math notranslate nohighlight">\(\texttt{maximize}\)</span> routine which represents any numerical optimization method that searches for an action maximizing the given function. This approach is versatile and can be adapted to different types of action spaces. For continuous action spaces, we can employ standard nonlinear optimization methods like gradient descent or L-BFGS (e.g., using scipy.optimize.minimize). For large discrete action spaces, we can use integer programming solvers - linear integer programming if the Q-function approximator is linear in actions, or mixed-integer nonlinear programming (MINLP) solvers for nonlinear Q-functions. The choice of solver depends on the structure of our Q-function approximator and the constraints on our action space.</p>
<section id="amortized-optimization-approach">
<h2>Amortized Optimization Approach<a class="headerlink" href="#amortized-optimization-approach" title="Link to this heading">#</a></h2>
<p>This process is computationally intensive. A natural question is whether we can “amortize” some of this computation by replacing the explicit optimization for each sample with a direct mapping that gives us an approximate maximizer directly.
For Q-functions, recall that the operator is given by:</p>
<div class="math notranslate nohighlight">
\[
(\mathrm{L}q)(s,a) = r(s,a) + \gamma \int p(ds'|s,a)\max_{a' \in \mathcal{A}(s')} q(s', a')
\]</div>
<p>If <span class="math notranslate nohighlight">\(q^*\)</span> is the optimal state-action value function, then <span class="math notranslate nohighlight">\(v^*(s) = \max_a q^*(s,a)\)</span>, and we can derive the optimal policy directly by computing the decision rule:</p>
<div class="math notranslate nohighlight">
\[
d^\star(s) = \arg\max_{a \in \mathcal{A}(s)} q^\star(s,a)
\]</div>
<p>Since <span class="math notranslate nohighlight">\(q^*\)</span> is a fixed point of <span class="math notranslate nohighlight">\(L\)</span>, we can write:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
q^\star(s,a) &amp;= (Lq^*)(s,a) \\
&amp;= r(s,a) + \gamma \int p(ds'|s,a) \max_{a' \in \mathcal{A}(s')} q^\star(s', a') \\
&amp;= r(s,a) + \gamma \int p(ds'|s,a) q^\star(s', d^\star(s'))
\end{align*}
\end{split}\]</div>
<p>Note that <span class="math notranslate nohighlight">\(d^\star\)</span> is implemented by our <span class="math notranslate nohighlight">\(\texttt{maximize}\)</span> numerical solver in the procedure above. A practical strategy would be to collect these maximizer values at each step and use them to train a function approximator that directly predicts these solutions. Due to computational constraints, we might want to compute these exact maximizer values only for a subset of states, based on some computational budget, and use the fitted decision rule to generalize to the remaining states. This leads to the following amortized version:</p>
<div class="proof algorithm admonition" id="fitted-q-iteration-amortized">
<p class="admonition-title"><span class="caption-number">Algorithm 34 </span> (Fitted Q-Iteration with Amortized Optimization)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> Given an MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, base points <span class="math notranslate nohighlight">\(\mathcal{B}\)</span>, subset for exact optimization <span class="math notranslate nohighlight">\(\mathcal{B}_{\text{opt}} \subset \mathcal{B}\)</span>, Q-function approximator <span class="math notranslate nohighlight">\(q(s,a; \boldsymbol{\theta})\)</span>, policy approximator <span class="math notranslate nohighlight">\(d(s; \boldsymbol{w})\)</span>, maximum iterations <span class="math notranslate nohighlight">\(N\)</span>, tolerance <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span></p>
<p><strong>Output</strong> Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> for Q-function, <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span> for policy</p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{w}_0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow 0\)</span></p></li>
<li><p><strong>repeat</strong></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}_q \leftarrow \emptyset\)</span> // Q-function regression dataset</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}_d \leftarrow \emptyset\)</span> // Policy regression dataset</p></li>
<li><p>For each <span class="math notranslate nohighlight">\((s,a,r,s') \in \mathcal{B}\)</span>:</p>
<ol class="arabic simple">
<li><p>// Determine next state’s action using either exact optimization or approximation</p></li>
<li><p><strong>if</strong> <span class="math notranslate nohighlight">\(s' \in \mathcal{B}_{\text{opt}}\)</span> <strong>then</strong></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(a^*_{s'} \leftarrow \texttt{maximize}(q(s', \cdot; \boldsymbol{\theta}_n))\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}_d \leftarrow \mathcal{D}_d \cup \{(s', a^*_{s'})\}\)</span></p></li>
</ol>
</li>
<li><p><strong>else</strong></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(a^*_{s'} \leftarrow d(s'; \boldsymbol{w}_n)\)</span></p></li>
</ol>
</li>
<li><p>// Compute Q-function target using chosen action</p></li>
<li><p><span class="math notranslate nohighlight">\(y_{s,a} \leftarrow r + \gamma q(s', a^*_{s'}; \boldsymbol{\theta}_n)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}_q \leftarrow \mathcal{D}_q \cup \{((s,a), y_{s,a})\}\)</span></p></li>
</ol>
</li>
<li><p>// Update both function approximators</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{n+1} \leftarrow \texttt{fit}(\mathcal{D}_q)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{w}_{n+1} \leftarrow \texttt{fit}(\mathcal{D}_d)\)</span></p></li>
<li><p>// Compute convergence criteria</p></li>
<li><p><span class="math notranslate nohighlight">\(\delta_q \leftarrow \frac{1}{|\mathcal{D}_q|}\sum_{(s,a) \in \mathcal{D}_q} (q(s,a; \boldsymbol{\theta}_{n+1}) - q(s,a; \boldsymbol{\theta}_n))^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\delta_d \leftarrow \frac{1}{|\mathcal{D}_d|}\sum_{(s,a^*) \in \mathcal{D}_d} \|a^* - d(s; \boldsymbol{w}_{n+1})\|^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow n + 1\)</span></p></li>
</ol>
</li>
<li><p><strong>until</strong> (<span class="math notranslate nohighlight">\(\max(\delta_q, \delta_d) \geq \varepsilon\)</span> or <span class="math notranslate nohighlight">\(n \geq N\)</span>)</p></li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{w}_n\)</span></p></li>
</ol>
</section>
</div><p>An important observation about this procedure is that the policy <span class="math notranslate nohighlight">\(d(s; \boldsymbol{w})\)</span> is being trained on a dataset <span class="math notranslate nohighlight">\(\mathcal{D}_d\)</span> containing optimal actions computed with respect to an evolving Q-function. Specifically, at iteration n, we collect pairs <span class="math notranslate nohighlight">\((s', a^*_{s'})\)</span> where <span class="math notranslate nohighlight">\(a^*_{s'} = \arg\max_a q(s', a; \boldsymbol{\theta}_n)\)</span>. However, after updating to <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{n+1}\)</span>, these actions may no longer be optimal with respect to the new Q-function.</p>
<p>A natural approach to handle this staleness would be to maintain only the most recent optimization data. We could modify our procedure to keep a sliding window of K iterations, where at iteration n, we only use data from iterations max(0, n-K) to n. This would be implemented by augmenting each entry in <span class="math notranslate nohighlight">\(\mathcal{D}_d\)</span> with a timestamp:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{D}_d^t = \{(s', a^*_{s'}, t) \mid t \in \{n-K,\ldots,n\}\}
\]</div>
<p>where t indicates the iteration at which the optimal action was computed. When fitting the policy network, we would then only use data points that are at most K iterations old:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{w}_{n+1} \leftarrow \texttt{fit}(\{(s', a^*_{s'}) \mid (s', a^*_{s'}, t) \in \mathcal{D}_d^t, n-K \leq t \leq n\})
\]</div>
<p>This introduces a trade-off between using more data (larger K) versus using more recent, accurate data (smaller K). The choice of K would depend on how quickly the Q-function evolves and the computational budget available for computing exact optimal actions.</p>
<p>Now the main issue with this approach, apart from the intrinsic out-of-distribution drift that we are trying to track, is that it requires “ground truth” - samples of optimal actions computed by the actual solver. This raises an intriguing question: how few samples do we actually need? Could we even envision eliminating the solver entirely? What seems impossible at first glance turns out to be achievable. The intuition is that as our policy improves at selecting actions, we can bootstrap from these increasingly better choices. As we continuously amortize these improving actions over time, it creates a virtuous cycle of self-improvement towards the optimal policy. But for this bootstrapping process to work, we need careful management - move too quickly and the process may become unstable. Let’s examine how this balance can be achieved.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="deterministic-parametrized-policies">
<h1>Deterministic Parametrized Policies<a class="headerlink" href="#deterministic-parametrized-policies" title="Link to this heading">#</a></h1>
<p>In this section, we consider deterministic parametrized policies of the form <span class="math notranslate nohighlight">\(d(s; \boldsymbol{w})\)</span> which directly output an action given a state. This approach differs from stochastic policies that output probability distributions over actions, making it particularly suitable for continuous control problems where the optimal policy is often deterministic. We’ll see how fitted Q-value methods can be naturally extended to simultaneously learn both the Q-function and such a deterministic policy.</p>
<section id="neural-fitted-q-iteration-for-continuous-actions-nfqca">
<h2>Neural Fitted Q-iteration for Continuous Actions (NFQCA)<a class="headerlink" href="#neural-fitted-q-iteration-for-continuous-actions-nfqca" title="Link to this heading">#</a></h2>
<p>To develop this approach, let’s first consider an idealized setting where we have access to <span class="math notranslate nohighlight">\(q^\star\)</span>, the optimal Q-function. Then we can state our goal as finding policy parameters <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span> that maximize <span class="math notranslate nohighlight">\(q^\star\)</span> with respect to the actions chosen by our policy across the state space:</p>
<div class="math notranslate nohighlight">
\[
\max_{\boldsymbol{w}} q^*(s, d(s; \boldsymbol{w})) \quad \text{for all } s
\]</div>
<p>However, it’s computationally infeasible to satisfy this condition for every possible state <span class="math notranslate nohighlight">\(s\)</span>, especially in large or continuous state spaces. To address this, we assume a distribution of states, denoted <span class="math notranslate nohighlight">\(\mu(s)\)</span>, and take the expectation, leading to the problem:</p>
<div class="math notranslate nohighlight">
\[
\max_{\boldsymbol{w}} \mathbb{E}_{s \sim \mu(s)}[q^*(s, d(s; \boldsymbol{w}))]
\]</div>
<p>However in practice, we do not have access to <span class="math notranslate nohighlight">\(q^*\)</span>. Instead, we need to approximate <span class="math notranslate nohighlight">\(q^*\)</span> with a Q-function <span class="math notranslate nohighlight">\(q(s, a; \boldsymbol{\theta})\)</span>, parameterized by <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>, which we will learn simultaneously with the policy function <span class="math notranslate nohighlight">\(d(s; \boldsymbol{w})\)</span>. Given a samples of initial states drawn from <span class="math notranslate nohighlight">\(\mu\)</span>, we then maximize this objective via a Monte Carlo surrogate problem:</p>
<div class="math notranslate nohighlight">
\[
\max_{\boldsymbol{w}} \mathbb{E}_{s \sim \mu(s)}[q(s, d(s; \boldsymbol{w}); \boldsymbol{\theta})] \approx
\max_{\boldsymbol{w}} \frac{1}{|\mathcal{B}|} \sum_{s \in \mathcal{B}}  q(s, d(s; \boldsymbol{w}); \boldsymbol{\theta})
\]</div>
<p>When using neural networks to parametrize <span class="math notranslate nohighlight">\(q\)</span> and <span class="math notranslate nohighlight">\(d\)</span>, we obtain the Neural Fitted Q-Iteration with Continuous Actions (NFQCA) algorithm proposed by <span id="id1">[<a class="reference internal" href="bibliography.html#id40" title="Roland Hafner and Martin Riedmiller. Reinforcement learning in feedback control: challenges and benchmarks from technical process control. Machine Learning, 84(1-2):137-169, feb 2011. URL: http://dx.doi.org/10.1007/s10994-011-5235-x, doi:10.1007/s10994-011-5235-x.">23</a>]</span>.</p>
<div class="proof algorithm admonition" id="nfqca">
<p class="admonition-title"><span class="caption-number">Algorithm 35 </span> (Neural Fitted Q-Iteration with Continuous Actions (NFQCA))</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, base points <span class="math notranslate nohighlight">\(\mathcal{B}\)</span>, Q-function <span class="math notranslate nohighlight">\(q(s,a; \boldsymbol{\theta})\)</span>, policy <span class="math notranslate nohighlight">\(d(s; \boldsymbol{w})\)</span></p>
<p><strong>Output</strong> Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> for Q-function, <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span> for policy</p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{w}_0\)</span></p></li>
<li><p><strong>for</strong> <span class="math notranslate nohighlight">\(n = 0,1,2,...\)</span> <strong>do</strong></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}_q \leftarrow \emptyset\)</span></p></li>
<li><p>For each <span class="math notranslate nohighlight">\((s,a,r,s') \in \mathcal{B}\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(a'_{s'} \leftarrow d(s'; \boldsymbol{w}_n)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(y_{s,a} \leftarrow r + \gamma q(s', a'_{s'}; \boldsymbol{\theta}_n)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}_q \leftarrow \mathcal{D}_q \cup \{((s,a), y_{s,a})\}\)</span></p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{n+1} \leftarrow \texttt{fit}(\mathcal{D}_q)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{w}_{n+1} \leftarrow \texttt{minimize}_{\boldsymbol{w}} -\frac{1}{|\mathcal{B}|} \sum_{(s,a,r,s') \in \mathcal{B}} q(s, d(s; \boldsymbol{w}); \boldsymbol{\theta}_{n+1})\)</span></p></li>
</ol>
</li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{w}_n\)</span></p></li>
</ol>
</section>
</div><p>In practice, both the <code class="docutils literal notranslate"><span class="pre">fit</span></code> and <code class="docutils literal notranslate"><span class="pre">minimize</span></code> operations above are implemented using gradient descent. For the Q-function, the <code class="docutils literal notranslate"><span class="pre">fit</span></code> operation minimizes the mean squared error between the network’s predictions and the target values:</p>
<div class="math notranslate nohighlight">
\[
\texttt{fit}(\mathcal{D}_q) = \arg\min_{\boldsymbol{\theta}} \frac{1}{|\mathcal{D}_q|} \sum_{((s,a), y) \in \mathcal{D}_q} (q(s,a; \boldsymbol{\theta}) - y)^2
\]</div>
<p>For the policy update, the <code class="docutils literal notranslate"><span class="pre">minimize</span></code> operation uses gradient descent on the composition of the “critic” network <span class="math notranslate nohighlight">\(q\)</span> and the “actor” network <span class="math notranslate nohighlight">\(d\)</span>. This results in the following update rule:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{w}_{n+1} = \boldsymbol{w}_n + \alpha \nabla_{\boldsymbol{w}} \left(\frac{1}{|\mathcal{B}|} \sum_{(s,a,r,s') \in \mathcal{B}} q(s, d(s; \boldsymbol{w}); \boldsymbol{\theta}_{n+1})\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> is the learning rate. Both operations can be efficiently implemented using modern automatic differentiation libraries and stochastic gradient descent variants like Adam or RMSProp.</p>
</section>
<section id="deep-deterministic-policy-gradient-ddpg">
<h2>Deep Deterministic Policy Gradient (DDPG)<a class="headerlink" href="#deep-deterministic-policy-gradient-ddpg" title="Link to this heading">#</a></h2>
<p>Just as DQN adapted Neural Fitted Q-Iteration to the online setting, DDPG <span id="id2">[<a class="reference internal" href="bibliography.html#id42" title="Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.">28</a>]</span> extends NFQCA to learn from data collected online. Like NFQCA, DDPG simultaneously learns a Q-function and a deterministic policy that maximizes it, but differs in how it collects and processes data.</p>
<p>Instead of maintaining a fixed set of basepoints, DDPG uses a replay buffer that continuously stores new transitions as the agent interacts with the environment. Since the policy is deterministic, exploration becomes challenging. DDPG addresses this by adding noise to the policy’s actions during data collection:</p>
<div class="math notranslate nohighlight">
\[
a = d(s; \boldsymbol{w}) + \mathcal{N}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{N}\)</span> represents exploration noise drawn from an Ornstein-Uhlenbeck (OU) process. The OU process is particularly well-suited for control tasks as it generates temporally correlated noise, leading to smoother exploration trajectories compared to independent random noise. It is defined by the stochastic differential equation:</p>
<div class="math notranslate nohighlight">
\[
d\mathcal{N}_t = \theta(\mu - \mathcal{N}_t)dt + \sigma dW_t
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu\)</span> is the long-term mean value (typically set to 0), <span class="math notranslate nohighlight">\(\theta\)</span> determines how strongly the noise is pulled toward this mean, <span class="math notranslate nohighlight">\(\sigma\)</span> scales the random fluctuations, and <span class="math notranslate nohighlight">\(dW_t\)</span> is a Wiener process (continuous-time random walk). For implementation, we discretize this continuous-time process using the Euler-Maruyama method:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{N}_{t+1} = \mathcal{N}_t + \theta(\mu - \mathcal{N}_t)\Delta t + \sigma\sqrt{\Delta t}\epsilon_t
\]</div>
<p>where <span class="math notranslate nohighlight">\(\Delta t\)</span> is the time step and <span class="math notranslate nohighlight">\(\epsilon_t \sim \mathcal{N}(0,1)\)</span> is standard Gaussian noise. Think of this process like a spring mechanism: when the noise value <span class="math notranslate nohighlight">\(\mathcal{N}_t\)</span> deviates from <span class="math notranslate nohighlight">\(\mu\)</span>, the term <span class="math notranslate nohighlight">\(\theta(\mu - \mathcal{N}_t)\Delta t\)</span> acts like a spring force, continuously pulling it back. Unlike a spring, however, this return to <span class="math notranslate nohighlight">\(\mu\)</span> is not oscillatory - it’s more like motion through a viscous fluid, where the force simply decreases as the noise gets closer to <span class="math notranslate nohighlight">\(\mu\)</span>. The random term <span class="math notranslate nohighlight">\(\sigma\sqrt{\Delta t}\epsilon_t\)</span> then adds perturbations to this smooth return trajectory. This creates noise that wanders away from <span class="math notranslate nohighlight">\(\mu\)</span> (enabling exploration) but is always gently pulled back (preventing the actions from wandering too far), with <span class="math notranslate nohighlight">\(\theta\)</span> controlling the strength of this pulling force.</p>
<p>The policy gradient update follows the same principle as NFQCA:</p>
<div class="math notranslate nohighlight">
\[
\nabla_{\boldsymbol{w}} \mathbb{E}_{s \sim \mu(s)}[q(s, d(s; \boldsymbol{w}); \boldsymbol{\theta})]
\]</div>
<p>We then embed this exploration mechanism into the data collection procedure and use the same flattened FQI structure that we adopted in DQN. Similar to DQN, flattening the outer-inner optimization structure leads to the need for target networks - both for the Q-function and the policy.</p>
<div class="proof algorithm admonition" id="ddpg">
<p class="admonition-title"><span class="caption-number">Algorithm 36 </span> (Deep Deterministic Policy Gradient (DDPG))</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, Q-network <span class="math notranslate nohighlight">\(q(s,a; \boldsymbol{\theta})\)</span>, policy network <span class="math notranslate nohighlight">\(d(s; \boldsymbol{w})\)</span>, learning rates <span class="math notranslate nohighlight">\(\alpha_q, \alpha_d\)</span>, replay buffer size <span class="math notranslate nohighlight">\(B\)</span>, mini-batch size <span class="math notranslate nohighlight">\(b\)</span>, target update frequency <span class="math notranslate nohighlight">\(K\)</span></p>
<p><strong>Initialize</strong></p>
<ol class="arabic simple">
<li><p>Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{w}_0\)</span> randomly</p></li>
<li><p>Target parameters: <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{target} \leftarrow \boldsymbol{\theta}_0\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{w}_{target} \leftarrow \boldsymbol{w}_0\)</span></p></li>
<li><p>Initialize replay buffer <span class="math notranslate nohighlight">\(\mathcal{R}\)</span> with capacity <span class="math notranslate nohighlight">\(B\)</span></p></li>
<li><p>Initialize exploration noise process <span class="math notranslate nohighlight">\(\mathcal{N}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow 0\)</span></p></li>
<li><p><strong>while</strong> training:</p>
<ol class="arabic simple">
<li><p>Observe current state <span class="math notranslate nohighlight">\(s\)</span></p></li>
<li><p>Select action with noise: <span class="math notranslate nohighlight">\(a = d(s; \boldsymbol{w}_n) + \mathcal{N}\)</span></p></li>
<li><p>Execute <span class="math notranslate nohighlight">\(a\)</span>, observe reward <span class="math notranslate nohighlight">\(r\)</span> and next state <span class="math notranslate nohighlight">\(s'\)</span></p></li>
<li><p>Store <span class="math notranslate nohighlight">\((s,a,r,s')\)</span> in <span class="math notranslate nohighlight">\(\mathcal{R}\)</span>, replacing oldest if full</p></li>
<li><p>Sample mini-batch of <span class="math notranslate nohighlight">\(b\)</span> transitions <span class="math notranslate nohighlight">\((s_i,a_i,r_i,s'_i)\)</span> from <span class="math notranslate nohighlight">\(\mathcal{R}\)</span></p></li>
<li><p>For each sampled transition:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(y_i \leftarrow r_i + \gamma q(s'_i, d(s'_i; \boldsymbol{w}_{target}); \boldsymbol{\theta}_{target})\)</span></p></li>
</ol>
</li>
<li><p>Update Q-network: <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{n+1} \leftarrow \boldsymbol{\theta}_n - \alpha_q \nabla_{\boldsymbol{\theta}} \frac{1}{b}\sum_i(y_i - q(s_i,a_i;\boldsymbol{\theta}_n))^2\)</span></p></li>
<li><p>Update policy: <span class="math notranslate nohighlight">\(\boldsymbol{w}_{n+1} \leftarrow \boldsymbol{w}_n + \alpha_d \frac{1}{b}\sum_i \nabla_a q(s_i,a;\boldsymbol{\theta}_{n+1})|_{a=d(s_i;\boldsymbol{w}_n)} \nabla_{\boldsymbol{w}} d(s_i;\boldsymbol{w}_n)\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(n \bmod K = 0\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{target} \leftarrow \boldsymbol{\theta}_n\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{w}_{target} \leftarrow \boldsymbol{w}_n\)</span></p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow n + 1\)</span></p></li>
</ol>
</li>
</ol>
<p><strong>return</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{w}_n\)</span></p>
</section>
</div></section>
<section id="twin-delayed-deep-deterministic-policy-gradient-td3">
<h2>Twin Delayed Deep Deterministic Policy Gradient (TD3)<a class="headerlink" href="#twin-delayed-deep-deterministic-policy-gradient-td3" title="Link to this heading">#</a></h2>
<p>While DDPG provided a foundation for continuous control with deep RL, it suffers from similar overestimation issues as DQN. TD3 <span id="id3">[<a class="reference internal" href="bibliography.html#id43" title="Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In International Conference on Machine Learning (ICML), 1587–1596. 2018.">14</a>]</span> addresses these challenges through three key modifications: double Q-learning to reduce overestimation bias, delayed policy updates to reduce per-update error, and target policy smoothing to prevent exploitation of Q-function errors.</p>
<div class="proof algorithm admonition" id="td3">
<p class="admonition-title"><span class="caption-number">Algorithm 37 </span> (Twin Delayed Deep Deterministic Policy Gradient (TD3))</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, twin Q-networks <span class="math notranslate nohighlight">\(q^A(s,a; \boldsymbol{\theta}^A)\)</span>, <span class="math notranslate nohighlight">\(q^B(s,a; \boldsymbol{\theta}^B)\)</span>, policy network <span class="math notranslate nohighlight">\(d(s; \boldsymbol{w})\)</span>, learning rates <span class="math notranslate nohighlight">\(\alpha_q, \alpha_d\)</span>, replay buffer size <span class="math notranslate nohighlight">\(B\)</span>, mini-batch size <span class="math notranslate nohighlight">\(b\)</span>, policy delay <span class="math notranslate nohighlight">\(d\)</span>, noise scale <span class="math notranslate nohighlight">\(\sigma\)</span>, noise clip <span class="math notranslate nohighlight">\(c\)</span>, exploration noise std <span class="math notranslate nohighlight">\(\sigma_{explore}\)</span></p>
<p><strong>Initialize</strong></p>
<ol class="arabic simple">
<li><p>Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^A_0\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^B_0\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{w}_0\)</span> randomly</p></li>
<li><p>Target parameters: <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^A_{target} \leftarrow \boldsymbol{\theta}^A_0\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^B_{target} \leftarrow \boldsymbol{\theta}^B_0\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{w}_{target} \leftarrow \boldsymbol{w}_0\)</span></p></li>
<li><p>Initialize replay buffer <span class="math notranslate nohighlight">\(\mathcal{R}\)</span> with capacity <span class="math notranslate nohighlight">\(B\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow 0\)</span></p></li>
<li><p><strong>while</strong> training:</p>
<ol class="arabic simple">
<li><p>Observe current state <span class="math notranslate nohighlight">\(s\)</span></p></li>
<li><p>Select action with Gaussian noise: <span class="math notranslate nohighlight">\(a = d(s; \boldsymbol{w}_n) + \epsilon\)</span>, where <span class="math notranslate nohighlight">\(\epsilon \sim \mathcal{N}(0, \sigma_{explore})\)</span></p></li>
<li><p>Execute <span class="math notranslate nohighlight">\(a\)</span>, observe reward <span class="math notranslate nohighlight">\(r\)</span> and next state <span class="math notranslate nohighlight">\(s'\)</span></p></li>
<li><p>Store <span class="math notranslate nohighlight">\((s,a,r,s')\)</span> in <span class="math notranslate nohighlight">\(\mathcal{R}\)</span>, replacing oldest if full</p></li>
<li><p>Sample mini-batch of <span class="math notranslate nohighlight">\(b\)</span> transitions <span class="math notranslate nohighlight">\((s_i,a_i,r_i,s'_i)\)</span> from <span class="math notranslate nohighlight">\(\mathcal{R}\)</span></p></li>
<li><p>For each sampled transition:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\tilde{a}_i \leftarrow d(s'_i; \boldsymbol{w}_{target}) + \text{clip}(\mathcal{N}(0, \sigma), -c, c)\)</span>  // Add clipped noise</p></li>
<li><p><span class="math notranslate nohighlight">\(q_{target} \leftarrow \min(q^A(s'_i, \tilde{a}_i; \boldsymbol{\theta}^A_{target}), q^B(s'_i, \tilde{a}_i; \boldsymbol{\theta}^B_{target}))\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(y_i \leftarrow r_i + \gamma q_{target}\)</span></p></li>
</ol>
</li>
<li><p>Update Q-networks:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}^A_{n+1} \leftarrow \boldsymbol{\theta}^A_n - \alpha_q \nabla_{\boldsymbol{\theta}} \frac{1}{b}\sum_i(y_i - q^A(s_i,a_i;\boldsymbol{\theta}^A_n))^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}^B_{n+1} \leftarrow \boldsymbol{\theta}^B_n - \alpha_q \nabla_{\boldsymbol{\theta}} \frac{1}{b}\sum_i(y_i - q^B(s_i,a_i;\boldsymbol{\theta}^B_n))^2\)</span></p></li>
</ol>
</li>
<li><p>If <span class="math notranslate nohighlight">\(n \bmod d = 0\)</span>:  // Delayed policy update</p>
<ol class="arabic simple">
<li><p>Update policy: <span class="math notranslate nohighlight">\(\boldsymbol{w}_{n+1} \leftarrow \boldsymbol{w}_n + \alpha_d \frac{1}{b}\sum_i \nabla_a q^A(s_i,a;\boldsymbol{\theta}^A_{n+1})|_{a=d(s_i;\boldsymbol{w}_n)} \nabla_{\boldsymbol{w}} d(s_i;\boldsymbol{w}_n)\)</span></p></li>
<li><p>Soft update of target networks:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}^A_{target} \leftarrow \tau\boldsymbol{\theta}^A_{n+1} + (1-\tau)\boldsymbol{\theta}^A_{target}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}^B_{target} \leftarrow \tau\boldsymbol{\theta}^B_{n+1} + (1-\tau)\boldsymbol{\theta}^B_{target}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{w}_{target} \leftarrow \tau\boldsymbol{w}_{n+1} + (1-\tau)\boldsymbol{w}_{target}\)</span></p></li>
</ol>
</li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow n + 1\)</span></p></li>
</ol>
</li>
</ol>
<p><strong>return</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^A_n\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^B_n\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{w}_n\)</span></p>
</section>
</div><p>Similar to Double Q-learning, TD3 decouples selection from evaluation when forming the targets. However, instead of intertwining the two existing online and target networks, TD3 suggests learning two Q-functions simultaneously and uses their minimum when computing target values to help combat the overestimation bias further.</p>
<p>Furthermore, when computing target Q-values, TD3 adds small random noise to the target policy’s actions and clips it to keep the perturbations bounded. This regularization technique essentially implements a form of “policy smoothing” that prevents the policy from exploiting areas where the Q-function may have erroneously high values:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$$\tilde{a} = d(s&#39;; \boldsymbol{w}_{target}) + \text{clip}(\mathcal{N}(0, \sigma), -c, c)$$
</pre></div>
</div>
<p>While DDPG used the OU process which generates temporally correlated noise, TD3’s authors found that simple uncorrelated Gaussian noise works just as well for exploration. It is also easier to implement and tune since you only need to set a single parameter (<span class="math notranslate nohighlight">\(\sigma_{explore}\)</span>) for exploration rather than the multiple parameters required by the OU process (<span class="math notranslate nohighlight">\(\theta\)</span>, <span class="math notranslate nohighlight">\(\mu\)</span>, <span class="math notranslate nohighlight">\(\sigma\)</span>).</p>
<p>Finally, TD3 updates the policy network (and target networks) less frequently than the Q-networks, typically once every <span class="math notranslate nohighlight">\(d\)</span> Q-function updates. This helps reduce the per-update error and gives the Q-functions time to become more accurate before they are used to update the policy.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="soft-actor-critic">
<h1>Soft Actor-Critic<a class="headerlink" href="#soft-actor-critic" title="Link to this heading">#</a></h1>
<p>Adapting the intuition of NFQCA to the smooth Bellman optimality equations leads us to the soft actor-critic algorithm <span id="id4">[<a class="reference internal" href="bibliography.html#id30" title="Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the 35th International Conference on Machine Learning (ICML), 1861–1870. PMLR, 2018.">22</a>]</span>. To understand this connection, let’s first examine how the smooth Bellman equations emerge naturally from entropy regularization.</p>
<p>Consider the standard Bellman operator augmented with an entropy term. The smooth Bellman operator <span class="math notranslate nohighlight">\(\mathrm{L}_\beta\)</span> takes the form:</p>
<div class="math notranslate nohighlight">
\[
(\mathrm{L}_\beta v)(s) = \max_{d \in D^{MR}}\{\mathbb{E}_{a \sim d}[r(s,a) + \gamma v(s')] + \beta\mathcal{H}(d)\}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{H}(d) = -\mathbb{E}_{a \sim d}[\log d(a|s)]\)</span> represents the entropy of the policy. To find the solution to the optimization problem embedded in the operator <span class="math notranslate nohighlight">\(\mathrm{L}_\beta\)</span>, we set the functional derivative of the objective with respect to the decision rule to zero:</p>
<div class="math notranslate nohighlight">
\[
\frac{\delta}{\delta d(a|s)} \left[\int_A d(a|s)(r(s,a) + \gamma v(s'))da - \beta\int_A d(a|s)\log d(a|s)da \right] = 0
\]</div>
<p>Enforcing that <span class="math notranslate nohighlight">\(\int_A d(a|s)da = 1\)</span> leads to the following Lagrangian:</p>
<div class="math notranslate nohighlight">
\[
r(s,a) + \gamma v(s') - \beta(1 + \log d(a|s)) - \lambda(s) = 0
\]</div>
<p>Solving for <span class="math notranslate nohighlight">\(d\)</span> shows that the optimal policy is a Boltzmann distribution</p>
<div class="math notranslate nohighlight">
\[
d^*(a|s) = \frac{\exp(\frac{1}{\beta}(r(s,a) + \gamma \mathbb{E}_{s'}[v(s')]))}{Z(s)}
\]</div>
<p>When we substitute this optimal policy back into the entropy-regularized objective, we obtain:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
v(s) &amp;= \mathbb{E}_{a \sim d^*}[r(s,a) + \gamma v(s')] + \beta\mathcal{H}(d^*) \\
&amp;= \beta \log \int_A \exp(\frac{1}{\beta}(r(s,a) + \gamma \mathbb{E}_{s'}[v(s')]))da
\end{align*}
\end{split}\]</div>
<p>As we saw at the beginning of this chapter, the smooth Bellman optimality operator for Q-factors is defined as:</p>
<div class="math notranslate nohighlight">
\[
(\mathrm{L}_\beta q)(s,a) = r(s,a) + \gamma \mathbb{E}_{s'}\left[\beta \log \int_A \exp(\frac{1}{\beta}q(s',a'))da'\right]
\]</div>
<p>This operator maintains the contraction property of its standard counterpart, guaranteeing a unique fixed point <span class="math notranslate nohighlight">\(q^*\)</span>. The optimal policy takes the form:</p>
<div class="math notranslate nohighlight">
\[
d^*(a|s) = \frac{\exp(\frac{1}{\beta}q^*(s,a))}{Z(s)}
\]</div>
<p>where <span class="math notranslate nohighlight">\(Z(s) = \int_A \exp(\frac{1}{\beta}q^*(s,a))da\)</span>. The optimal value function can be recovered as:</p>
<div class="math notranslate nohighlight">
\[
v^*(s) = \beta \log \int_A \exp(\frac{1}{\beta}q^*(s,a))da
\]</div>
<section id="fitted-q-iteration-for-the-smooth-bellman-equations">
<h2>Fitted Q-Iteration for the Smooth Bellman Equations<a class="headerlink" href="#fitted-q-iteration-for-the-smooth-bellman-equations" title="Link to this heading">#</a></h2>
<p>Following the principles of fitted value iteration, we can approximate approximate the effect of the smooth Bellman operator by computing it exactly at a number of basepoints and generalizing elsewhere using function approximation. Concretely, given a collection of states <span class="math notranslate nohighlight">\(s_i\)</span> and actions <span class="math notranslate nohighlight">\(a_i\)</span>, we would compute regression target values:</p>
<div class="math notranslate nohighlight">
\[
y_i = r(s_i,a_i) + \gamma \mathbb{E}_{s'}\left[\beta \log \int_A \exp(\frac{1}{\beta}q_\theta(s',a'))da'\right]
\]</div>
<p>and fit our Q-function approximator by minimizing:</p>
<div class="math notranslate nohighlight">
\[
\min_\theta \sum_i (q_\theta(s_i,a_i) - y_i)^2
\]</div>
<p>The expectation over next states can be handled through Monte Carlo estimation using samples from the environment: given a transition <span class="math notranslate nohighlight">\((s_i,a_i,s'_i)\)</span>, we can approximate:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}_{s'}\left[\beta \log \int_A \exp(\frac{1}{\beta}q_\theta(s',a'))da'\right] \approx \beta \log \int_A \exp(\frac{1}{\beta}q_\theta(s'_i,a'))da'
\]</div>
<p>However, we still face the challenge of computing the integral over actions. This motivates maintaining separate function approximators for both Q and V, using samples from the current policy to estimate the value function:</p>
<div class="math notranslate nohighlight">
\[
v_\psi(s) \approx \mathbb{E}_{a \sim d(\cdot|s;\phi)}\left[q_\theta(s,a) - \beta \log d(a|s;\phi)\right]
\]</div>
<p>By maintaining both approximators, we can estimate targets using sampled actions from our policy. Specifically, if we have a transition <span class="math notranslate nohighlight">\((s_i,a_i,s'_i)\)</span> and sample <span class="math notranslate nohighlight">\(a'_i \sim d(\cdot|s'_i;\phi)\)</span>, our target becomes:</p>
<div class="math notranslate nohighlight">
\[
y_i = r(s_i,a_i) + \gamma\left(q_\theta(s'_i,a'_i) - \beta \log d(a'_i|s'_i;\phi)\right)
\]</div>
<p>This is a remarkable idea! One that exists only due to the dual representation of the smooth Bellman equations as an entropy-regularized problem which transforms the intractable log-sum-exp into a form we can estimate efficiently through sampling.</p>
</section>
<section id="approximating-boltzmann-policies-by-gaussians">
<h2>Approximating Boltzmann Policies by Gaussians<a class="headerlink" href="#approximating-boltzmann-policies-by-gaussians" title="Link to this heading">#</a></h2>
<p>The entropy-regularized objective and the smooth Bellman equation are mathematically equivalent. However, both formulations face a practical challenge: they require evaluating an intractable integral due to the Boltzmann distribution. Soft Actor-Critic (SAC) addresses this problem by approximating the optimal policy with a simpler, more tractable Gaussian distribution. Given the optimal soft policy:</p>
<div class="math notranslate nohighlight">
\[
d^*(a|s) = \frac{\exp(\frac{1}{\beta}q^*(s,a))}{Z(s)}
\]</div>
<p>we seek to approximate it with a Gaussian policy:</p>
<div class="math notranslate nohighlight">
\[
d(a|s;\phi) = \mathcal{N}(\mu_\phi(s), \sigma_\phi(s))
\]</div>
<p>This approximation task naturally raises the question of how to measure the “closeness” between the target Boltzmann distribution and a candidate Gaussian approximation. Following common practice in deep learning, we employ the Kullback-Leibler (KL) divergence as our measure of distributional distance. To find the best approximation, we minimize the KL divergence between our policy and the optimal policy, using our current estimate <span class="math notranslate nohighlight">\(q_\theta\)</span> of <span class="math notranslate nohighlight">\(q^*\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\operatorname{minimize}_{\phi} \mathbb{E}_{s \sim \mu(s)}\left[D_{KL}\left(d(\cdot|s;\phi) \| \frac{\exp(\frac{1}{\beta}q_\theta(s,\cdot))}{Z(s)}\right)\right]
\]</div>
<p>However, an important question remains: how can we solve this optimization problem when it involves the intractable partition function <span class="math notranslate nohighlight">\(Z(s)\)</span>? To see this, recall that for two distributions p and q, the KL divergence takes the form <span class="math notranslate nohighlight">\(D_{KL}(p\|q) = \mathbb{E}_{x \sim p}[\log p(x) - \log q(x)]\)</span>. Let’s denote the target Boltzmann distribution based on our current Q-estimate as:</p>
<div class="math notranslate nohighlight">
\[
d_\theta(a|s) = \frac{\exp(\frac{1}{\beta}q_\theta(s,a))}{Z_\theta(s)}
\]</div>
<p>Then the KL minimization becomes:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
D_{KL}(d(\cdot|s;\phi)\|d_\theta) &amp;= \mathbb{E}_{a \sim d(\cdot|s;\phi)}[\log d(a|s;\phi) - \log d_\theta(a|s)] \\
&amp;= \mathbb{E}_{a \sim d(\cdot|s;\phi)}\left[\log d(a|s;\phi) - \log \left(\frac{\exp(\frac{1}{\beta}q_\theta(s,a))}{Z_\theta(s)}\right)\right] \\
&amp;= \mathbb{E}_{a \sim d(\cdot|s;\phi)}\left[\log d(a|s;\phi) - \frac{1}{\beta}q_\theta(s,a) + \log Z_\theta(s)\right]
\end{align*}
\end{split}\]</div>
<p>Since <span class="math notranslate nohighlight">\(\log Z(s)\)</span> is constant with respect to <span class="math notranslate nohighlight">\(\phi\)</span>, minimizing this KL divergence is equivalent to:</p>
<div class="math notranslate nohighlight">
\[
\operatorname{minimize}_{\phi} \mathbb{E}_{s \sim \mu(s)}\mathbb{E}_{a \sim d(\cdot|s;\phi)}[\log d(a|s;\phi) - \frac{1}{\beta}q_\theta(s,a)]
\]</div>
</section>
<section id="reparameterizating-the-objective">
<h2>Reparameterizating the Objective<a class="headerlink" href="#reparameterizating-the-objective" title="Link to this heading">#</a></h2>
<p>One last challenge remains: <span class="math notranslate nohighlight">\(\phi\)</span> appears in the distribution underlying the inner expectation, not just in the integrand. This setting departs from standard empirical risk minimization (ERM) in supervised learning where the distribution of the data (e.g., cats and dogs in image classification) remains fixed regardless of model parameters. Here, however, the “data” - our sampled actions - depends directly on the parameters <span class="math notranslate nohighlight">\(\phi\)</span> we’re trying to optimize.</p>
<p>This dependence prevents us from simply using sample average estimators and differentiating through them, as we typically do in supervised learning. The challenge of correctly and efficiently estimating such derivatives has been extensively studied in the simulation literature under the umbrella of “derivative estimation.” SAC adopts a particular solution known as the reparameterization trick in deep learning (or the IPA estimator in simulation literature). This approach transforms the problem by pushing <span class="math notranslate nohighlight">\(\phi\)</span> inside the expectation through a change of variables.</p>
<p>To address this, we can express our Gaussian policy through a deterministic function <span class="math notranslate nohighlight">\(f_\phi\)</span> that transforms noise samples to actions:</p>
<div class="math notranslate nohighlight">
\[
f_\phi(s,\epsilon) = \mu_\phi(s) + \sigma_\phi(s)\epsilon, \quad \epsilon \sim \mathcal{N}(0,1)
\]</div>
<p>This transformation allows us to rewrite our objective using an expectation over the fixed noise distribution:</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
&amp;\mathbb{E}_{s \sim \mu(s)}\mathbb{E}_{\epsilon \sim \mathcal{N}(0,1)}[\log d(f_\phi(s,\epsilon)|s;\phi) - \frac{1}{\beta}q_\theta(s,f_\phi(s,\epsilon))]
\end{align*}
\]</div>
<p>Now <span class="math notranslate nohighlight">\(\phi\)</span> appears only in the integrand through the function <span class="math notranslate nohighlight">\(f_\phi\)</span>, not in the sampling distribution. The objective involves two terms. First, the log-probability of our Gaussian policy has a simple closed form:</p>
<div class="math notranslate nohighlight">
\[
\log d(f_\phi(s,\epsilon)|s;\phi) = -\frac{1}{2}\log(2\pi\sigma_\phi(s)^2) - \frac{(f_\phi(s,\epsilon)-\mu_\phi(s))^2}{2\sigma_\phi(s)^2}
\]</div>
<p>Second, <span class="math notranslate nohighlight">\(\phi\)</span> enters through the composition of <span class="math notranslate nohighlight">\(q^\star\)</span> with <span class="math notranslate nohighlight">\(f_\phi\)</span>: <span class="math notranslate nohighlight">\(q^\star(s,f_\phi(s,\epsilon))\)</span>. The chain rule for this composition would involve derivatives of both functions. While this might be problematic if the Q-factors were to come from outside of our control (ie. not in the computational graph), but since SAC learns it simultaneously with the policy, then we can simply compute all required derivatives through automatic differentiation.</p>
<p>This composition of policy and value functions - where <span class="math notranslate nohighlight">\(f_\phi\)</span> enters as input to <span class="math notranslate nohighlight">\(q_\theta\)</span> - directly parallels the structure we encountered in deterministic policy methods like NFQCA and DDPG. In those methods, we optimized:</p>
<div class="math notranslate nohighlight">
\[
\max_{\phi} \mathbb{E}_{s \sim \mu(s)}[q_\theta(s, f_\phi(s))]
\]</div>
<p>where <span class="math notranslate nohighlight">\(f_\phi(s)\)</span> was a deterministic policy. SAC extends this idea to stochastic policies by having <span class="math notranslate nohighlight">\(f_\phi\)</span> transform both state and noise:</p>
<div class="math notranslate nohighlight">
\[
\max_{\phi} \mathbb{E}_{s \sim \mu(s)}\mathbb{E}_{\epsilon \sim \mathcal{N}(0,1)}[q_\theta(s,f_\phi(s,\epsilon))]
\]</div>
<p>Thus, rather than learning a single action for each state as in DDPG, we learn a function that transforms random noise into actions, explicitly parameterizing a distribution over actions while maintaining the same underlying principle of differentiating through composed policy and value functions.</p>
<div class="proof algorithm admonition" id="sac">
<p class="admonition-title"><span class="caption-number">Algorithm 38 </span> (Soft Actor-Critic)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, Q-networks <span class="math notranslate nohighlight">\(q^1(s,a; \boldsymbol{\theta}^1)\)</span>, <span class="math notranslate nohighlight">\(q^2(s,a; \boldsymbol{\theta}^2)\)</span>, value network <span class="math notranslate nohighlight">\(v(s; \boldsymbol{\psi})\)</span>, policy network <span class="math notranslate nohighlight">\(d(a|s; \boldsymbol{\phi})\)</span>, learning rates <span class="math notranslate nohighlight">\(\alpha_q, \alpha_v, \alpha_\pi\)</span>, replay buffer size <span class="math notranslate nohighlight">\(B\)</span>, mini-batch size <span class="math notranslate nohighlight">\(b\)</span>, target smoothing coefficient <span class="math notranslate nohighlight">\(\tau\)</span></p>
<p><strong>Initialize</strong></p>
<ol class="arabic simple">
<li><p>Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^1_0\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^2_0\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\psi}_0\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_0\)</span> randomly</p></li>
<li><p>Target parameters: <span class="math notranslate nohighlight">\(\boldsymbol{\bar{\psi}}_0 \leftarrow \boldsymbol{\psi}_0\)</span></p></li>
<li><p>Initialize replay buffer <span class="math notranslate nohighlight">\(\mathcal{R}\)</span> with capacity <span class="math notranslate nohighlight">\(B\)</span></p></li>
</ol>
<p><strong>while</strong> training:</p>
<ol class="arabic simple">
<li><p>Observe current state <span class="math notranslate nohighlight">\(s\)</span></p></li>
<li><p>Sample action from policy: <span class="math notranslate nohighlight">\(a \sim d(a|s; \boldsymbol{\phi})\)</span></p></li>
<li><p>Execute <span class="math notranslate nohighlight">\(a\)</span>, observe reward <span class="math notranslate nohighlight">\(r\)</span> and next state <span class="math notranslate nohighlight">\(s'\)</span></p></li>
<li><p>Store <span class="math notranslate nohighlight">\((s, a, r, s')\)</span> in <span class="math notranslate nohighlight">\(\mathcal{R}\)</span>, replacing oldest if full</p></li>
<li><p>Sample mini-batch of <span class="math notranslate nohighlight">\(b\)</span> transitions <span class="math notranslate nohighlight">\((s_i, a_i, r_i, s'_i)\)</span> from <span class="math notranslate nohighlight">\(\mathcal{R}\)</span></p></li>
</ol>
<p><strong>Update Value Network:</strong></p>
<ol class="arabic">
<li><p>Compute target for value network:</p>
<div class="math notranslate nohighlight">
\[
   y_v = \mathbb{E}_{a' \sim d(\cdot|s'; \boldsymbol{\phi})} \left[ \min \left( q^1(s', a'; \boldsymbol{\theta}^1), q^2(s', a'; \boldsymbol{\theta}^2) \right) - \alpha \log d(a'|s'; \boldsymbol{\phi}) \right]
   \]</div>
</li>
<li><p>Update <span class="math notranslate nohighlight">\(\boldsymbol{\psi}\)</span> via gradient descent:</p>
<div class="math notranslate nohighlight">
\[
   \boldsymbol{\psi} \leftarrow \boldsymbol{\psi} - \alpha_v \nabla_{\boldsymbol{\psi}} \frac{1}{b} \sum_i (v(s_i; \boldsymbol{\psi}) - y_v)^2
   \]</div>
</li>
</ol>
<p><strong>Update Q-Networks:</strong></p>
<ol class="arabic">
<li><p>Compute targets for Q-networks:</p>
<div class="math notranslate nohighlight">
\[
   y_q = r_i + \gamma \cdot v(s'_i; \boldsymbol{\bar{\psi}})
   \]</div>
</li>
<li><p>Update <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^1\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^2\)</span> via gradient descent:</p>
<div class="math notranslate nohighlight">
\[
   \boldsymbol{\theta}^j \leftarrow \boldsymbol{\theta}^j - \alpha_q \nabla_{\boldsymbol{\theta}^j} \frac{1}{b} \sum_i (q^j(s_i, a_i; \boldsymbol{\theta}^j) - y_q)^2, \quad j \in \{1, 2\}
   \]</div>
</li>
</ol>
<p><strong>Update Policy Network:</strong></p>
<ol class="arabic">
<li><p>Sample actions <span class="math notranslate nohighlight">\(a \sim d(\cdot|s_i; \boldsymbol{\phi})\)</span> for each <span class="math notranslate nohighlight">\(s_i\)</span> in the mini-batch</p></li>
<li><p>Update <span class="math notranslate nohighlight">\(\boldsymbol{\phi}\)</span> via gradient ascent:</p>
<div class="math notranslate nohighlight">
\[
   \boldsymbol{\phi} \leftarrow \boldsymbol{\phi} + \alpha_\pi \nabla_{\boldsymbol{\phi}} \frac{1}{b} \sum_i \left[ \alpha \log d(a|s_i; \boldsymbol{\phi}) - q^1(s_i, a; \boldsymbol{\theta}^1) \right]
   \]</div>
</li>
</ol>
<p><strong>Update Target Value Network:</strong></p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\bar{\psi}} \leftarrow \tau \boldsymbol{\psi} + (1 - \tau) \boldsymbol{\bar{\psi}}
\]</div>
<p><strong>return</strong> Learned parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^1\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^2\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\psi}\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\phi}\)</span></p>
</section>
</div></section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="derivative-estimation-for-stochastic-optimization">
<h1>Derivative Estimation for Stochastic Optimization<a class="headerlink" href="#derivative-estimation-for-stochastic-optimization" title="Link to this heading">#</a></h1>
<p>Consider optimizing an objective that involves an expectation:</p>
<div class="math notranslate nohighlight">
\[
J(\theta) = \mathbb{E}_{x \sim p(x;\theta)}[f(x,\theta)]
\]</div>
<p>For concreteness, let’s examine a simple example where <span class="math notranslate nohighlight">\(x \sim \mathcal{N}(\theta,1)\)</span> and <span class="math notranslate nohighlight">\(f(x,\theta) = x^2\theta\)</span>. The derivative we seek is:</p>
<div class="math notranslate nohighlight">
\[
\frac{d}{d\theta}J(\theta) = \frac{d}{d\theta}\int x^2\theta p(x;\theta)dx
\]</div>
<p>While we can compute this exactly for the Gaussian example, this is often impossible for more general problems. We might then be tempted to approximate our objective using samples:</p>
<div class="math notranslate nohighlight">
\[
J(\theta) \approx \frac{1}{N}\sum_{i=1}^N f(x_i,\theta), \quad x_i \sim p(x;\theta)
\]</div>
<p>Then differentiate this approximation:</p>
<div class="math notranslate nohighlight">
\[
\frac{d}{d\theta}J(\theta) \approx \frac{1}{N}\sum_{i=1}^N \frac{\partial}{\partial \theta}f(x_i,\theta)
\]</div>
<p>However, this naive approach ignores that the samples themselves depend on <span class="math notranslate nohighlight">\(\theta\)</span>. The correct derivative requires the product rule:</p>
<div class="math notranslate nohighlight">
\[
\frac{d}{d\theta}J(\theta) = \int \frac{\partial}{\partial \theta}[f(x,\theta)p(x;\theta)]dx = \int \left[\frac{\partial f}{\partial \theta}p(x;\theta) + f(x,\theta)\frac{\partial p(x;\theta)}{\partial \theta}\right]dx
\]</div>
<p>The issue here is that while the first term could be numerically integrated using the Monte Carlo, the second one can’t as it’s not an expectation.</p>
<p>Would there be a way to transform our objective in such a way that the Monte Carlo estimator for the objective could be differentiated directly while ensuring that the resulting derivative is unbiased? We will see that there are two main solutions to that problem: by doing a change of measure, or a change of variables.</p>
<section id="change-of-measure-the-likelihood-ratio-method">
<h2>Change of Measure: The Likelihood Ratio Method<a class="headerlink" href="#change-of-measure-the-likelihood-ratio-method" title="Link to this heading">#</a></h2>
<p>One solution comes from rewriting our objective using any distribution <span class="math notranslate nohighlight">\(q(x)\)</span>:</p>
<div class="math notranslate nohighlight">
\[
J(\theta) = \int f(x,\theta)\frac{p(x;\theta)}{q(x)}q(x)dx = \mathbb{E}_{x \sim q(x)}\left[f(x,\theta)\frac{p(x;\theta)}{q(x)}\right]
\]</div>
<p>Let’s write this more functionally by defining:</p>
<div class="math notranslate nohighlight">
\[
J(\theta) = \mathbb{E}_{x \sim q(x)}[h(x,\theta)]
, \enspace h(x,\theta) \equiv f(x,\theta)\frac{p(x;\theta)}{q(x)}
\]</div>
<p>Now when we differentiate <span class="math notranslate nohighlight">\(J\)</span>, it’s clear that we must take the partial derivative of <span class="math notranslate nohighlight">\(h\)</span> with respect to its second argument:</p>
<div class="math notranslate nohighlight">
\[
\frac{d}{d\theta}J(\theta) = \mathbb{E}_{x \sim q(x)}\left[\frac{\partial h}{\partial \theta}(x,\theta)\right] = \mathbb{E}_{x \sim q(x)}\left[f(x,\theta)\frac{\partial}{\partial \theta}\frac{p(x;\theta)}{q(x)} + \frac{p(x;\theta)}{q(x)}\frac{\partial f}{\partial \theta}(x,\theta)\right]
\]</div>
<p>The so-called “score function” derivative estimator is obtained for the choice of <span class="math notranslate nohighlight">\(q(x) = p(x;\theta)\)</span>, where the ratio simplifies to <span class="math notranslate nohighlight">\(1\)</span> and its derivative becomes the score function:</p>
<div class="math notranslate nohighlight">
\[
\frac{d}{d\theta}J(\theta) = \mathbb{E}_{x \sim p(x;\theta)}\left[f(x,\theta)\frac{\partial \log p(x,\theta)}{\partial \theta} + \frac{\partial f(x,\theta)}{\partial \theta}\right]
\]</div>
</section>
<section id="a-change-of-variables-approach-the-reparameterization-trick">
<h2>A Change of Variables Approach: The Reparameterization Trick<a class="headerlink" href="#a-change-of-variables-approach-the-reparameterization-trick" title="Link to this heading">#</a></h2>
<p>An alternative approach eliminates the <span class="math notranslate nohighlight">\(\theta\)</span>-dependence in the sampling distribution by expressing <span class="math notranslate nohighlight">\(x\)</span> through a deterministic transformation of the noise:</p>
<div class="math notranslate nohighlight">
\[
x = g(\epsilon,\theta), \quad \epsilon \sim q(\epsilon)
\]</div>
<p>Therefore if we want to sample from some target distribution <span class="math notranslate nohighlight">\(p(x;\theta)\)</span>, we can do so by first sampling from a simple base distribution <span class="math notranslate nohighlight">\(q(\epsilon)\)</span> (like a standard normal) and then transforming those samples through a carefully chosen function <span class="math notranslate nohighlight">\(g\)</span>. If <span class="math notranslate nohighlight">\(g(\cdot,\theta)\)</span> is invertible, the change of variables formula tells us how these distributions relate:</p>
<div class="math notranslate nohighlight">
\[
p(x;\theta) = q(g^{-1}(x,\theta))\left|\det\frac{\partial g^{-1}(x,\theta)}{\partial x}\right| = q(\epsilon)\left|\det\frac{\partial g(\epsilon,\theta)}{\partial \epsilon}\right|^{-1}
\]</div>
<p>For example, if we want to sample from any multivariate Gaussian distributions with covariance matrix <span class="math notranslate nohighlight">\(\Sigma\)</span> and mean <span class="math notranslate nohighlight">\(\mu\)</span>, it suffices to be able to sample from a standard normal noise and compute the linear transformation:</p>
<div class="math notranslate nohighlight">
\[
x = \mu + \Sigma^{1/2}\epsilon, \quad \epsilon \sim \mathcal{N}(0,I)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\Sigma^{1/2}\)</span> is the matrix square root obtained via Cholesky decomposition. In the univariate case, this transformation is simply:</p>
<div class="math notranslate nohighlight">
\[
x = \mu + \sigma \epsilon, \quad \epsilon \sim \mathcal{N}(0,1)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma = \sqrt{\sigma^2}\)</span> is the standard deviation (square root of the variance).</p>
<section id="common-examples-of-reparameterization">
<h3>Common Examples of Reparameterization<a class="headerlink" href="#common-examples-of-reparameterization" title="Link to this heading">#</a></h3>
<section id="bounded-intervals-the-truncated-normal">
<h4>Bounded Intervals: The Truncated Normal<a class="headerlink" href="#bounded-intervals-the-truncated-normal" title="Link to this heading">#</a></h4>
<p>When we need samples constrained to an interval <span class="math notranslate nohighlight">\([a,b]\)</span>, we can use the truncated normal distribution. To sample from it, we transform uniform noise through the inverse cumulative distribution function (CDF) of the standard normal:</p>
<div class="math notranslate nohighlight">
\[
x = \Phi^{-1}(u\Phi(b) + (1-u)\Phi(a)), \quad u \sim \text{Uniform}(0,1)
\]</div>
<p>Here:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\Phi(z) = \frac{1}{2}\left[1 + \text{erf}\left(\frac{z}{\sqrt{2}}\right)\right]\)</span> is the CDF of the standard normal distribution</p></li>
<li><p><span class="math notranslate nohighlight">\(\Phi^{-1}\)</span> is its inverse (the quantile function)</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{erf}(z) = \frac{2}{\sqrt{\pi}}\int_0^z e^{-t^2}dt\)</span> is the error function</p></li>
</ul>
<p>The resulting samples follow a normal distribution restricted to <span class="math notranslate nohighlight">\([a,b]\)</span>, with the density properly normalized over this interval.</p>
</section>
<section id="sampling-from-0-1-the-kumaraswamy-distribution">
<h4>Sampling from [0,1]: The Kumaraswamy Distribution<a class="headerlink" href="#sampling-from-0-1-the-kumaraswamy-distribution" title="Link to this heading">#</a></h4>
<p>When we need samples in the unit interval [0,1], a natural choice might be the Beta distribution. However, its inverse CDF doesn’t have a closed form. Instead, we can use the Kumaraswamy distribution as a convenient approximation, which allows for a simple reparameterization:</p>
<div class="math notranslate nohighlight">
\[
x = (1-(1-u^{\alpha})^{1/\beta}), \quad u \sim \text{Uniform}(0,1)
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\alpha, \beta &gt; 0\)</span> are shape parameters that control the distribution</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span> determines the concentration around 0</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta\)</span> determines the concentration around 1</p></li>
<li><p>The distribution is similar to Beta(α,β) but with analytically tractable CDF and inverse CDF</p></li>
</ul>
<p>The Kumaraswamy distribution has density:</p>
<div class="math notranslate nohighlight">
\[
f(x; \alpha, \beta) = \alpha\beta x^{\alpha-1}(1-x^{\alpha})^{\beta-1}, \quad x \in [0,1]
\]</div>
</section>
<section id="discrete-actions-the-gumbel-softmax">
<h4>Discrete Actions: The Gumbel-Softmax<a class="headerlink" href="#discrete-actions-the-gumbel-softmax" title="Link to this heading">#</a></h4>
<p>When sampling from a categorical distribution with probabilities <span class="math notranslate nohighlight">\(\{\pi_i\}\)</span>, one approach uses <span class="math notranslate nohighlight">\(\text{Gumbel}(0,1)\)</span> noise combined with the argmax of log-perturbed probabilities:</p>
<div class="math notranslate nohighlight">
\[
\text{argmax}_i(\log \pi_i + g_i), \quad g_i \sim \text{Gumbel}(0,1)
\]</div>
<p>This approach, known in machine learning as the Gumbel-Max trick, relies on sampling Gumbel noise from uniform random variables through the transformation <span class="math notranslate nohighlight">\(g_i = -\log(-\log(u_i))\)</span> where <span class="math notranslate nohighlight">\(u_i \sim \text{Uniform}(0,1)\)</span>. To see why this gives us samples from the categorical distribution, consider the probability of selecting category <span class="math notranslate nohighlight">\(i\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
P(\text{argmax}_j(\log \pi_j + g_j) = i) &amp;= P(\log \pi_i + g_i &gt; \log \pi_j + g_j \text{ for all } j \neq i) \\
&amp;= P(g_i - g_j &gt; \log \pi_j - \log \pi_i \text{ for all } j \neq i)
\end{align*}
\end{split}\]</div>
<p>Since the difference of two Gumbel random variables follows a logistic distribution, <span class="math notranslate nohighlight">\(g_i - g_j \sim \text{Logistic}(0,1)\)</span>, and these differences are independent for different <span class="math notranslate nohighlight">\(j\)</span> (due to the independence of the original Gumbel variables), we can write:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
P(\text{argmax}_j(\log \pi_j + g_j) = i) &amp;= \prod_{j \neq i} P(g_i - g_j &gt; \log \pi_j - \log \pi_i) \\
&amp;= \prod_{j \neq i} \frac{\pi_i}{\pi_i + \pi_j} = \pi_i
\end{align*}
\end{split}\]</div>
<p>The last equality requires some additional algebra to show, but follows from the fact that these probabilities must sum to 1 over all <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p>While we have shown that the Gumbel-Max trick gives us exact samples from a categorical distribution, the argmax operation isn’t differentiable. For stochastic optimization problems of the form:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}_{x \sim p(x;\theta)}[f(x)] = \mathbb{E}_{\epsilon \sim \text{Gumbel}(0,1)}[f(g(\epsilon,\theta))]
\]</div>
<p>we need <span class="math notranslate nohighlight">\(g\)</span> to be differentiable with respect to <span class="math notranslate nohighlight">\(\theta\)</span>. This leads us to consider a continuous relaxation where we replace the hard argmax with a temperature-controlled softmax:</p>
<div class="math notranslate nohighlight">
\[
z_i = \frac{\exp((\log \pi_i + g_i)/\tau)}{\sum_j \exp((\log \pi_j + g_j)/\tau)}
\]</div>
<p>As <span class="math notranslate nohighlight">\(\tau \to 0\)</span>, this approximation approaches the argmax:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\lim_{\tau \to 0} \frac{\exp(x_i/\tau)}{\sum_j \exp(x_j/\tau)} = \begin{cases} 1 &amp; \text{if } x_i = \max_j x_j \\ 0 &amp; \text{otherwise} \end{cases}
\end{split}\]</div>
<p>The resulting distribution over the probability simplex is called the Gumbel-Softmax (or Concrete) distribution. The temperature parameter <span class="math notranslate nohighlight">\(\tau\)</span> controls the discreteness of our samples: smaller values give samples closer to one-hot vectors but with less stable gradients, while larger values give smoother gradients but more diffuse samples.</p>
</section>
</section>
</section>
<section id="demonstration-numerical-analysis-of-gradient-estimators">
<h2>Demonstration: Numerical Analysis of Gradient Estimators<a class="headerlink" href="#demonstration-numerical-analysis-of-gradient-estimators" title="Link to this heading">#</a></h2>
<p>Let us examine the behavior of our three gradient estimators for the stochastic optimization objective:</p>
<div class="math notranslate nohighlight">
\[J(\theta) = \mathbb{E}_{x \sim \mathcal{N}(\theta,1)}[x^2\theta]\]</div>
<p>To get an analytical expression for the derivative, first note that we can factor out <span class="math notranslate nohighlight">\(\theta\)</span> to obtain <span class="math notranslate nohighlight">\(J(\theta) = \theta\mathbb{E}[x^2]\)</span> where <span class="math notranslate nohighlight">\(x \sim \mathcal{N}(\theta,1)\)</span>. By definition of the variance, we know that <span class="math notranslate nohighlight">\(\text{Var}(x) = \mathbb{E}[x^2] - (\mathbb{E}[x])^2\)</span>, which we can rearrange to <span class="math notranslate nohighlight">\(\mathbb{E}[x^2] = \text{Var}(x) + (\mathbb{E}[x])^2\)</span>. Since <span class="math notranslate nohighlight">\(x \sim \mathcal{N}(\theta,1)\)</span>, we have <span class="math notranslate nohighlight">\(\text{Var}(x) = 1\)</span> and <span class="math notranslate nohighlight">\(\mathbb{E}[x] = \theta\)</span>, therefore <span class="math notranslate nohighlight">\(\mathbb{E}[x^2] = 1 + \theta^2\)</span>. This gives us:</p>
<div class="math notranslate nohighlight">
\[J(\theta) = \theta(1 + \theta^2)\]</div>
<p>Now differentiating with respect to <span class="math notranslate nohighlight">\(\theta\)</span> using the product rule yields:</p>
<div class="math notranslate nohighlight">
\[\frac{d}{d\theta}J(\theta) = 1 + 3\theta^2\]</div>
<p>For concreteness, we fix <span class="math notranslate nohighlight">\(\theta = 1.0\)</span> and analyze samples drawn using Monte Carlo estimation with batch size 1000 and 1000 independent trials. Evaluating at <span class="math notranslate nohighlight">\(\theta = 1\)</span> gives us <span class="math notranslate nohighlight">\(\frac{d}{d\theta}J(\theta)\big|_{\theta=1} = 1 + 3(1)^2 = 4\)</span>, which serves as our ground truth against which we compare our estimators:</p>
<ol class="arabic">
<li><p>First, we consider the naive estimator that incorrectly differentiates the Monte Carlo approximation:</p>
<div class="math notranslate nohighlight">
\[\hat{g}_{\text{naive}}(\theta) = \frac{1}{N}\sum_{i=1}^N x_i^2\]</div>
<p>For <span class="math notranslate nohighlight">\(x \sim \mathcal{N}(1,1)\)</span>, we have <span class="math notranslate nohighlight">\(\mathbb{E}[x^2] = \theta^2 + 1 = 2.0\)</span> and <span class="math notranslate nohighlight">\(\mathbb{E}[\hat{g}_{\text{naive}}] = 2.0\)</span>. We should therefore expect a bias of about <span class="math notranslate nohighlight">\(-2\)</span> in our experiment.</p>
</li>
<li><p>Then we compute the score function estimator:</p>
<div class="math notranslate nohighlight">
\[\hat{g}_{\text{SF}}(\theta) = \frac{1}{N}\sum_{i=1}^N \left[x_i^2\theta(x_i - \theta) + x_i^2\right]\]</div>
<p>This estimator is unbiased with <span class="math notranslate nohighlight">\(\mathbb{E}[\hat{g}_{\text{SF}}] = 4\)</span></p>
</li>
<li><p>Finally, through the reparameterization <span class="math notranslate nohighlight">\(x = \theta + \epsilon\)</span> where <span class="math notranslate nohighlight">\(\epsilon \sim \mathcal{N}(0,1)\)</span>, we obtain:</p>
<div class="math notranslate nohighlight">
\[\hat{g}_{\text{RT}}(\theta) = \frac{1}{N}\sum_{i=1}^N \left[2\theta(\theta + \epsilon_i) + (\theta + \epsilon_i)^2\right]\]</div>
<p>This estimator is also unbiased with <span class="math notranslate nohighlight">\(\mathbb{E}[\hat{g}_{\text{RT}}] = 4\)</span>.</p>
</li>
</ol>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Define the objective function f(x,θ) = x²θ where x ~ N(θ, 1)</span>
<span class="k">def</span> <span class="nf">objective</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">theta</span>

<span class="c1"># Naive Monte Carlo gradient estimation</span>
<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">naive_gradient_batch</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="mi">1000</span><span class="p">,))</span> <span class="o">+</span> <span class="n">theta</span>
    <span class="c1"># Use jax.grad on the objective with respect to theta</span>
    <span class="n">grad_fn</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">objective</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">t</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

<span class="c1"># Score function estimator (REINFORCE)</span>
<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">score_function_batch</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="mi">1000</span><span class="p">,))</span> <span class="o">+</span> <span class="n">theta</span>
    <span class="c1"># f(x,θ) * ∂logp(x|θ)/∂θ + ∂f(x,θ)/∂θ</span>
    <span class="c1"># score function for N(θ,1) is (x-θ)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">samples</span> <span class="o">-</span> <span class="n">theta</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">objective</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span> <span class="o">*</span> <span class="n">score</span> <span class="o">+</span> <span class="n">samples</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Reparameterization gradient</span>
<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">reparam_gradient_batch</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="mi">1000</span><span class="p">,))</span>
    <span class="c1"># Use reparameterization x = θ + ε, ε ~ N(0,1)</span>
    <span class="n">grad_fn</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">objective</span><span class="p">(</span><span class="n">t</span> <span class="o">+</span> <span class="n">eps</span><span class="p">,</span> <span class="n">t</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

<span class="c1"># Run trials</span>
<span class="n">n_trials</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">theta</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">true_grad</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">theta</span><span class="o">**</span><span class="mi">2</span>

<span class="n">keys</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">n_trials</span><span class="p">)</span>
<span class="n">naive_estimates</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">naive_gradient_batch</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">])</span>
<span class="n">score_estimates</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">score_function_batch</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">])</span>
<span class="n">reparam_estimates</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">reparam_gradient_batch</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">])</span>

<span class="c1"># Create violin plots with individual points</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">naive_estimates</span><span class="p">,</span> <span class="n">score_estimates</span><span class="p">,</span> <span class="n">reparam_estimates</span><span class="p">]</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;#ff9999&#39;</span><span class="p">,</span> <span class="s1">&#39;#66b3ff&#39;</span><span class="p">,</span> <span class="s1">&#39;#99ff99&#39;</span><span class="p">]</span>

<span class="n">parts</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">violinplot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">showextrema</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">pc</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">parts</span><span class="p">[</span><span class="s1">&#39;bodies&#39;</span><span class="p">]):</span>
    <span class="n">pc</span><span class="o">.</span><span class="n">set_facecolor</span><span class="p">(</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">pc</span><span class="o">.</span><span class="n">set_alpha</span><span class="p">(</span><span class="mf">0.7</span><span class="p">)</span>

<span class="c1"># Add box plots</span>
<span class="n">plt</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">notch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">showfliers</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Add true gradient line</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">true_grad</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;True Gradient&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;Naive&#39;</span><span class="p">,</span> <span class="s1">&#39;Score Function&#39;</span><span class="p">,</span> <span class="s1">&#39;Reparam&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Gradient Estimate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Gradient Estimators (θ=</span><span class="si">{</span><span class="n">theta</span><span class="si">}</span><span class="s1">, true grad=</span><span class="si">{</span><span class="n">true_grad</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># Print statistics</span>
<span class="n">methods</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;Naive&#39;</span><span class="p">:</span> <span class="n">naive_estimates</span><span class="p">,</span>
    <span class="s1">&#39;Score Function&#39;</span><span class="p">:</span> <span class="n">score_estimates</span><span class="p">,</span> 
    <span class="s1">&#39;Reparameterization&#39;</span><span class="p">:</span> <span class="n">reparam_estimates</span>
<span class="p">}</span>

<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">estimates</span> <span class="ow">in</span> <span class="n">methods</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">bias</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">estimates</span><span class="p">)</span> <span class="o">-</span> <span class="n">true_grad</span>
    <span class="n">variance</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">estimates</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean: </span><span class="si">{</span><span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">estimates</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Bias: </span><span class="si">{</span><span class="n">bias</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Variance: </span><span class="si">{</span><span class="n">variance</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MSE: </span><span class="si">{</span><span class="n">bias</span><span class="o">**</span><span class="mi">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">variance</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Naive:
Mean: 1.999266
Bias: -2.000734
Variance: 0.005756
MSE: 4.008693

Score Function:
Mean: 3.995299
Bias: -0.004701
Variance: 0.058130
MSE: 0.058152

Reparameterization:
Mean: 3.999579
Bias: -0.000421
Variance: 0.017229
MSE: 0.017230
</pre></div>
</div>
<img alt="_images/b677d4ec7b3037a0b09766778585e8e3a29edfa41b29272dbe50a34064534a33.png" src="_images/b677d4ec7b3037a0b09766778585e8e3a29edfa41b29272dbe50a34064534a33.png" />
</div>
</div>
<p>The numerical experiments coroborate our theory. The naive estimator consistently underestimates the true gradient by 2.0, though it maintains a relatively small variance. This systematic bias would make it unsuitable for optimization despite its low variance. The score function estimator corrects this bias but introduces substantial variance. While unbiased, this estimator would require many samples to achieve reliable gradient estimates. Finally, the reparameterization trick achieves a much lower variance while remaining unbiased. While this experiment is for didactic purposes only, it reproduces what is commonly found in practice: that when applicable, the reparameterization estimator tends to perform better than the score function counterpart.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="score-function-gradient-estimation-in-reinforcement-learning">
<h1>Score Function Gradient Estimation in Reinforcement Learning<a class="headerlink" href="#score-function-gradient-estimation-in-reinforcement-learning" title="Link to this heading">#</a></h1>
<p>Let <span class="math notranslate nohighlight">\(G(\tau) \equiv \sum_{t=0}^T r(s_t, a_t)\)</span> be the sum of undiscounted rewards in a trajectory <span class="math notranslate nohighlight">\(\tau\)</span>. The stochastic optimization problem we face is to maximize:</p>
<div class="math notranslate nohighlight">
\[
J(\boldsymbol{w}) = \mathbb{E}_{\tau \sim p(\tau;\boldsymbol{w})}[G(\tau)]
\]</div>
<p>where <span class="math notranslate nohighlight">\(\tau = (s_0,a_0,s_1,a_1,...)\)</span> is a trajectory and <span class="math notranslate nohighlight">\(G(\tau)\)</span> is the total return.
Applying the score function estimator, we get:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\nabla_{\boldsymbol{w}}J(\boldsymbol{w}) &amp;= \nabla_{\boldsymbol{w}}\mathbb{E}_{\tau}[G(\tau)] \\
&amp;= \mathbb{E}_{\tau}\left[G(\tau)\nabla_{\boldsymbol{w}}\log p(\tau;\boldsymbol{w})\right] \\
&amp;= \mathbb{E}_{\tau}\left[G(\tau)\nabla_{\boldsymbol{w}}\sum_{t=0}^T\log d(a_t|s_t;\boldsymbol{w})\right] \\
&amp;= \mathbb{E}_{\tau}\left[G(\tau)\sum_{t=0}^T\nabla_{\boldsymbol{w}}\log d(a_t|s_t;\boldsymbol{w})\right]
\end{align*}
\end{split}\]</div>
<p>We have eliminated the need to know the transition probabilities in this estimator since the probability of a trajectory factorizes as:</p>
<div class="math notranslate nohighlight">
\[
p(\tau;\boldsymbol{w}) = p(s_0)\prod_{t=0}^T d(a_t|s_t;\boldsymbol{w})p(s_{t+1}|s_t,a_t)
\]</div>
<p>Therefore, only the policy depends on <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>. When taking the logarithm of this product, we get a sum where all the <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>-independent terms vanish. The final estimator samples trajectories under the distribution <span class="math notranslate nohighlight">\(p(\tau; \boldsymbol{w})\)</span> and computes:</p>
<div class="math notranslate nohighlight">
\[
\nabla_{\boldsymbol{w}}J(\boldsymbol{w}) \approx \frac{1}{N}\sum_{i=1}^N\left[G(\tau^{(i)})\sum_{t=0}^T\nabla_{\boldsymbol{w}}\log d(a_t^{(i)}|s_t^{(i)};\boldsymbol{w})\right]
\]</div>
<p>This is a direct application of the score function estimator. However, we rarely use this form in practice and instead make several improvements to further reduce the variance.</p>
<section id="leveraging-conditional-independence">
<h2>Leveraging Conditional Independence<a class="headerlink" href="#leveraging-conditional-independence" title="Link to this heading">#</a></h2>
<p>Given the Markov property of the MDP, rewards <span class="math notranslate nohighlight">\(r_k\)</span> for <span class="math notranslate nohighlight">\(k &lt; t\)</span> are conditionally independent of action <span class="math notranslate nohighlight">\(a_t\)</span> given the history <span class="math notranslate nohighlight">\(h_t = (s_0,a_0,...,s_{t-1},a_{t-1},s_t)\)</span>. This allows us to only need to consider future rewards when computing policy gradients.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\nabla_{\boldsymbol{w}}J(\boldsymbol{w}) &amp;= \mathbb{E}_{\tau}\left[\sum_{t=0}^T\nabla_{\boldsymbol{w}}\log d(a_t|s_t;\boldsymbol{w})\sum_{k=0}^T r_k\right] \\
&amp;= \mathbb{E}_{\tau}\left[\sum_{t=0}^T\nabla_{\boldsymbol{w}}\log d(a_t|s_t;\boldsymbol{w})\left(\sum_{k=0}^{t-1} r_k + \sum_{k=t}^T r_k\right)\right] \\
&amp;= \mathbb{E}_{\tau}\left[\sum_{t=0}^T\nabla_{\boldsymbol{w}}\log d(a_t|s_t;\boldsymbol{w})\sum_{k=t}^T r_k\right]
\end{align*}
\end{split}\]</div>
<p>The condition independence assumption means that the term <span class="math notranslate nohighlight">\(\mathbb{E}_{\tau}\left[\sum_{t=0}^T\nabla_{\boldsymbol{w}}\log d(a_t|s_t;\boldsymbol{w})\sum_{k=0}^{t-1} r_k \right]\)</span> vanishes. To see this, let’s factor the trajectory distribution as:</p>
<div class="math notranslate nohighlight">
\[
p(\tau) = p(s_0,...,s_t,a_0,...,a_{t-1})\cdot d(a_t|s_t;\boldsymbol{w})\cdot p(s_{t+1},...,s_T,a_{t+1},...,a_T|s_t,a_t)
\]</div>
<p>We can now re-write a single term of this summation as:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}_{\tau}\left[\nabla_{\boldsymbol{w}}\log d(a_t|s_t;\boldsymbol{w})\sum_{k=0}^{t-1} r_k\right] = \mathbb{E}_{s_{0:t},a_{0:t-1}}\left[\sum_{k=0}^{t-1} r_k \cdot \mathbb{E}_{a_t}\left[\nabla_{\boldsymbol{w}}\log d(a_t|s_t;\boldsymbol{w})\right]\right]
\]</div>
<p>The inner expectation is zero because</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathbb{E}_{a_t}\left[\nabla_{\boldsymbol{w}}\log d(a_t|s_t;\boldsymbol{w})\right] &amp;= \int \nabla_{\boldsymbol{w}}\log d(a_t|s_t;\boldsymbol{w})d(a_t|s_t;\boldsymbol{w})da_t \\
&amp;= \int \frac{\nabla_{\boldsymbol{w}}d(a_t|s_t;\boldsymbol{w})}{d(a_t|s_t;\boldsymbol{w})}d(a_t|s_t;\boldsymbol{w})da_t \\
&amp;= \int \nabla_{\boldsymbol{w}}d(a_t|s_t;\boldsymbol{w})da_t \\
&amp;= \nabla_{\boldsymbol{w}}\int d(a_t|s_t;\boldsymbol{w})da_t \\
&amp;= \nabla_{\boldsymbol{w}}1 = 0
\end{align*}
\end{split}\]</div>
<p>The Monte Carlo estimator becomes:</p>
<div class="math notranslate nohighlight">
\[
\nabla_{\boldsymbol{w}}J(\boldsymbol{w}) \approx \frac{1}{N}\sum_{i=1}^N\left[\sum_{t=0}^T\nabla_{\boldsymbol{w}}\log d(a_t^{(i)}|s_t^{(i)};\boldsymbol{w})\sum_{k=t}^T r_k^{(i)}\right]
\]</div>
<p>The benefit of this estimator compared to the naive one is that it generally has less variance. More formally, we can show that this estimator arises from the application of a variance reduction technique known as the Extended Conditional Monte Carlo Method.</p>
</section>
<section id="variance-reduction-via-control-variates">
<h2>Variance Reduction via Control Variates<a class="headerlink" href="#variance-reduction-via-control-variates" title="Link to this heading">#</a></h2>
<p>A control variate is a zero-mean random variable that we subtract from our estimator to reduce variance. Given an estimator <span class="math notranslate nohighlight">\(Z\)</span> and a control variate <span class="math notranslate nohighlight">\(C\)</span> with <span class="math notranslate nohighlight">\(\mathbb{E}[C]=0\)</span>, we can construct a new unbiased estimator:</p>
<div class="math notranslate nohighlight">
\[
Z_{\text{cv}} = Z - \alpha C
\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> is a coefficient we can choose. The variance of this new estimator is:</p>
<div class="math notranslate nohighlight">
\[
\text{Var}(Z_{\text{cv}}) = \text{Var}(Z) + \alpha^2\text{Var}(C) - 2\alpha\text{Cov}(Z,C)
\]</div>
<p>The optimal <span class="math notranslate nohighlight">\(\alpha\)</span> that minimizes this variance is:</p>
<div class="math notranslate nohighlight">
\[
\alpha^* = \frac{\text{Cov}(Z,C)}{\text{Var}(C)}
\]</div>
<p>In the reinforcement learning setting, we usually choose <span class="math notranslate nohighlight">\(C_t = \nabla_{\boldsymbol{w}}\log d(a_t|s_t;\boldsymbol{w})\)</span> as our control variate at each timestep. For a given state <span class="math notranslate nohighlight">\(s_t\)</span>, our estimator at time <span class="math notranslate nohighlight">\(t\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
Z_t = \nabla_{\boldsymbol{w}}\log d(a_t|s_t;\boldsymbol{w})\sum_{k=t}^T r_k
\]</div>
<p>Our control variate estimator becomes:</p>
<div class="math notranslate nohighlight">
\[
Z_{t,\text{cv}} = Z_t - \alpha_t^* C_t = \nabla_{\boldsymbol{w}}\log d(a_t|s_t;\boldsymbol{w})(\sum_{k=t}^T r_k - \alpha_t^*)
\]</div>
<p>Following the general theory, and using the fact that <span class="math notranslate nohighlight">\(\mathbb{E}[C_t|s_t] = 0\)</span>  the optimal coefficient is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\alpha^*_t = \frac{\text{Cov}(Z_t,C_t|s_t)}{\text{Var}(C_t|s_t)} &amp;= \frac{\mathbb{E}[Z_tC_t^T|s_t] - \mathbb{E}[Z_t|s_t]\mathbb{E}[C_t^T|s_t]}{\mathbb{E}[C_tC_t^T|s_t] - \mathbb{E}[C_t|s_t]\mathbb{E}[C_t^T|s_t]} \\
&amp;= \frac{\mathbb{E}[\nabla_{\boldsymbol{w}}\log d(a_t|s_t;\boldsymbol{w})\nabla_{\boldsymbol{w}}\log d(a_t|s_t;\boldsymbol{w})^T\sum_{k=t}^T r_k|s_t] - 0}{\mathbb{E}[\nabla_{\boldsymbol{w}}\log d(a_t|s_t;\boldsymbol{w})\nabla_{\boldsymbol{w}}\log d(a_t|s_t;\boldsymbol{w})^T|s_t] - 0} \\
&amp;= \frac{\mathbb{E}[\|\nabla_{\boldsymbol{w}}\log d(a_t|s_t;\boldsymbol{w})\|^2\sum_{k=t}^T r_k|s_t]}{\mathbb{E}[\|\nabla_{\boldsymbol{w}}\log d(a_t|s_t;\boldsymbol{w})\|^2|s_t]} \\
&amp;= \frac{\mathbb{E}_{a_t|s_t}[\|\nabla_{\boldsymbol{w}}\log d(a_t|s_t;\boldsymbol{w})\|^2]\mathbb{E}[\sum_{k=t}^T r_k|s_t]}{\mathbb{E}_{a_t|s_t}[\|\nabla_{\boldsymbol{w}}\log d(a_t|s_t;\boldsymbol{w})\|^2]} \\
&amp;= \mathbb{E}[\sum_{k=t}^T r_k|s_t] = v^{d_{\boldsymbol{w}}}(s_t)
\end{align*}
\end{split}\]</div>
<p>Therefore, our variance-reduced estimator becomes:</p>
<div class="math notranslate nohighlight">
\[
Z_{\text{cv},t} = \nabla_{\boldsymbol{w}}\log d(a_t|s_t;\boldsymbol{w})\left(\sum_{k=t}^T r_k - v^{d_{\boldsymbol{w}}}(s_t)\right)
\]</div>
<p>In practice when implementing this estimator, we won’t have access to the true value function. So as we did earlier for NFQCA or SAC, we commonly learn that value function simultaneously with the policy. Do do so, we could either using a fitted value approach, or even more simply just regress from states to sum of rewards to learn what Williams 1992 called a “baseline”:</p>
<div class="proof algorithm admonition" id="policy-grad-baseline">
<p class="admonition-title"><span class="caption-number">Algorithm 39 </span> (Policy Gradient with Simple Baseline)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> Policy parameterization <span class="math notranslate nohighlight">\(d(a|s;\boldsymbol{w})\)</span>, baseline function <span class="math notranslate nohighlight">\(b(s;\boldsymbol{\theta})\)</span><br />
<strong>Output:</strong> Updated policy parameters <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span><br />
<strong>Hyperparameters:</strong> Learning rates <span class="math notranslate nohighlight">\(\alpha_w\)</span>, <span class="math notranslate nohighlight">\(\alpha_\theta\)</span>, number of episodes <span class="math notranslate nohighlight">\(N\)</span>, episode length <span class="math notranslate nohighlight">\(T\)</span></p>
<ol class="arabic simple">
<li><p>Initialize parameters <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span></p></li>
<li><p>For episode = 1, …, <span class="math notranslate nohighlight">\(N\)</span> do:</p>
<ol class="arabic simple">
<li><p>Collect trajectory <span class="math notranslate nohighlight">\(\tau = (s_0, a_0, r_0, ..., s_T, a_T, r_T)\)</span> using policy <span class="math notranslate nohighlight">\(d(a|s;\boldsymbol{w})\)</span></p></li>
<li><p>Compute returns for each timestep: <span class="math notranslate nohighlight">\(R_t = \sum_{k=t}^T r_k\)</span></p></li>
<li><p>Update baseline: <span class="math notranslate nohighlight">\(\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} + \alpha_\theta \nabla_{\boldsymbol{\theta}}\sum_{t=0}^T (R_t - b(s_t;\boldsymbol{\theta}))^2\)</span></p></li>
<li><p>For <span class="math notranslate nohighlight">\(t = 0, ..., T\)</span> do:</p>
<ol class="arabic simple">
<li><p>Update policy: <span class="math notranslate nohighlight">\(\boldsymbol{w} \leftarrow \boldsymbol{w} + \alpha_w \nabla_{\boldsymbol{w}}\log d(a_t|s_t;\boldsymbol{w})(R_t - b(s_t;\boldsymbol{\theta}))\)</span></p></li>
</ol>
</li>
</ol>
</li>
<li><p>Return <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span></p></li>
</ol>
</section>
</div><p>When implementing this algorithm nowadays, we always using mini-batching to make full use of our GPUs. Therefore, a more representative variant for this algorithm would be:</p>
<div class="proof algorithm admonition" id="policy-grad-cv-batch">
<p class="admonition-title"><span class="caption-number">Algorithm 40 </span> (Policy Gradient with Optimal Control Variate and Mini-batches)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> Policy parameterization <span class="math notranslate nohighlight">\(d(a|s;\boldsymbol{w})\)</span>, value function <span class="math notranslate nohighlight">\(v(s;\boldsymbol{\theta})\)</span><br />
<strong>Output:</strong> Updated policy parameters <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span><br />
<strong>Hyperparameters:</strong> Learning rates <span class="math notranslate nohighlight">\(\alpha_w\)</span>, <span class="math notranslate nohighlight">\(\alpha_\theta\)</span>, number of iterations <span class="math notranslate nohighlight">\(N\)</span>, episode length <span class="math notranslate nohighlight">\(T\)</span>, batch size <span class="math notranslate nohighlight">\(B\)</span>, mini-batch size <span class="math notranslate nohighlight">\(M\)</span></p>
<ol class="arabic simple">
<li><p>Initialize parameters <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span></p></li>
<li><p>For iteration = 1, …, N:</p>
<ol class="arabic simple">
<li><p>Initialize empty buffer <span class="math notranslate nohighlight">\(\mathcal{D}\)</span></p></li>
<li><p>For b = 1, …, B:</p>
<ol class="arabic simple">
<li><p>Collect trajectory <span class="math notranslate nohighlight">\(\tau_b = (s_0, a_0, r_0, ..., s_T, a_T, r_T)\)</span> using policy <span class="math notranslate nohighlight">\(d(a|s;\boldsymbol{w})\)</span></p></li>
<li><p>Compute returns: <span class="math notranslate nohighlight">\(R_t = \sum_{k=t}^T r_k\)</span> for all t</p></li>
<li><p>Store tuple <span class="math notranslate nohighlight">\((s_t, a_t, R_t)_{t=0}^T\)</span> in <span class="math notranslate nohighlight">\(\mathcal{D}\)</span></p></li>
</ol>
</li>
<li><p>Compute value targets: <span class="math notranslate nohighlight">\(v_{\text{target}}(s) = \frac{1}{|\mathcal{D}_s|}\sum_{(s,\cdot,R) \in \mathcal{D}_s} R\)</span></p></li>
<li><p>For value_epoch = 1, …, K:</p>
<ol class="arabic simple">
<li><p>Sample mini-batch <span class="math notranslate nohighlight">\(\mathcal{B}_v\)</span> of size <span class="math notranslate nohighlight">\(M\)</span> from <span class="math notranslate nohighlight">\(\mathcal{D}\)</span></p></li>
<li><p>Compute value loss: <span class="math notranslate nohighlight">\(L_v = \frac{1}{M}\sum_{(s,\cdot,R) \in \mathcal{B}_v} (v(s;\boldsymbol{\theta}) - v_{\text{target}}(s))^2\)</span></p></li>
<li><p>Update value function: <span class="math notranslate nohighlight">\(\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} - \alpha_\theta \nabla_{\boldsymbol{\theta}}L_v\)</span></p></li>
</ol>
</li>
<li><p>Compute advantages: <span class="math notranslate nohighlight">\(A(s,a) = R - v(s;\boldsymbol{\theta})\)</span> for all <span class="math notranslate nohighlight">\((s,a,R) \in \mathcal{D}\)</span></p></li>
<li><p>Normalize advantages: <span class="math notranslate nohighlight">\(A \leftarrow \frac{A - \mu_A}{\sigma_A}\)</span></p></li>
<li><p>For policy_epoch = 1, …, J:</p>
<ol class="arabic simple">
<li><p>Sample mini-batch <span class="math notranslate nohighlight">\(\mathcal{B}_\pi\)</span> of size <span class="math notranslate nohighlight">\(M\)</span> from <span class="math notranslate nohighlight">\(\mathcal{D}\)</span></p></li>
<li><p>Compute policy loss: <span class="math notranslate nohighlight">\(L_\pi = -\frac{1}{M}\sum_{(s,a,\cdot) \in \mathcal{B}_\pi} \log d(a|s;\boldsymbol{w})A(s,a)\)</span></p></li>
<li><p>Update policy: <span class="math notranslate nohighlight">\(\boldsymbol{w} \leftarrow \boldsymbol{w} - \alpha_w \nabla_{\boldsymbol{w}}L_\pi\)</span></p></li>
</ol>
</li>
</ol>
</li>
<li><p>Return <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span></p></li>
</ol>
</section>
</div></section>
<section id="generalized-advantage-estimator">
<h2>Generalized Advantage Estimator<a class="headerlink" href="#generalized-advantage-estimator" title="Link to this heading">#</a></h2>
<p>Given our control variate estimator with baseline <span class="math notranslate nohighlight">\(v(s)\)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[
\nabla_{\boldsymbol{w}}\log d(a_t|s_t;\boldsymbol{w})(G_t - v(s_t))
\]</div>
<p>where <span class="math notranslate nohighlight">\(G_t\)</span> is the return <span class="math notranslate nohighlight">\(\sum_{k=t}^T r_k\)</span>. We can improve this estimator by considering how it relates to the advantage function, defined as:</p>
<div class="math notranslate nohighlight">
\[
A(s_t,a_t) = q(s_t,a_t) - v(s_t)
\]</div>
<p>where <span class="math notranslate nohighlight">\(q(s_t,a_t)\)</span> is the action-value function. Due to the Bellman equation:</p>
<div class="math notranslate nohighlight">
\[
q(s_t,a_t) = \mathbb{E}_{s_{t+1},r_t}[r_t + \gamma v(s_{t+1})|s_t,a_t]
\]</div>
<p>This leads to the one-step TD error:</p>
<div class="math notranslate nohighlight">
\[
\delta_t = r_t + \gamma v(s_{t+1}) - v(s_t)
\]</div>
<p>Now, let’s decompose our original term:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
G_t - v(s_t) &amp;= r_t + \gamma G_{t+1} - v(s_t) \\
&amp;= r_t + \gamma v(s_{t+1}) - v(s_t) + \gamma(G_{t+1} - v(s_{t+1})) \\
&amp;= \delta_t + \gamma(G_{t+1} - v(s_{t+1}))
\end{align*}
\end{split}\]</div>
<p>Expanding recursively:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
G_t - v(s_t) &amp;= \delta_t + \gamma(G_{t+1} - v(s_{t+1})) \\
&amp;= \delta_t + \gamma[\delta_{t+1} + \gamma(G_{t+2} - v(s_{t+2}))] \\
&amp;= \delta_t + \gamma\delta_{t+1} + \gamma^2\delta_{t+2} + ... + \gamma^{T-t}\delta_T
\end{align*}
\end{split}\]</div>
<p>GAE generalizes this by introducing a weighted version with parameter <span class="math notranslate nohighlight">\(\lambda\)</span>:</p>
<div class="math notranslate nohighlight">
\[
A^{\text{GAE}(\gamma,\lambda)}(s_t,a_t) = (1-\lambda)\sum_{k=0}^{\infty}\lambda^k\sum_{l=0}^k \gamma^l\delta_{t+l}
\]</div>
<p>Which simplifies to:</p>
<div class="math notranslate nohighlight">
\[
A^{\text{GAE}(\gamma,\lambda)}(s_t,a_t) = \sum_{l=0}^{\infty}(\gamma\lambda)^l\delta_{t+l}
\]</div>
<p>This formulation allows us to trade off bias and variance through <span class="math notranslate nohighlight">\(\lambda\)</span>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\lambda=0\)</span>: one-step TD error (low variance, high bias)</p></li>
<li><p><span class="math notranslate nohighlight">\(\lambda=1\)</span>: Monte Carlo estimate (high variance, low bias)</p></li>
</ul>
<p>The general GAE algorithm with mini-batches is the following:</p>
<div class="proof algorithm admonition" id="policy-grad-gae-batch">
<p class="admonition-title"><span class="caption-number">Algorithm 41 </span> (Policy Gradient with GAE and Mini-batches)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> Policy parameterization <span class="math notranslate nohighlight">\(d(a|s;\boldsymbol{w})\)</span>, value function <span class="math notranslate nohighlight">\(v(s;\boldsymbol{\theta})\)</span><br />
<strong>Output:</strong> Updated policy parameters <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span><br />
<strong>Hyperparameters:</strong> Learning rates <span class="math notranslate nohighlight">\(\alpha_w\)</span>, <span class="math notranslate nohighlight">\(\alpha_\theta\)</span>, number of iterations <span class="math notranslate nohighlight">\(N\)</span>, episode length <span class="math notranslate nohighlight">\(T\)</span>, batch size <span class="math notranslate nohighlight">\(B\)</span>, mini-batch size <span class="math notranslate nohighlight">\(M\)</span>, discount <span class="math notranslate nohighlight">\(\gamma\)</span>, GAE parameter <span class="math notranslate nohighlight">\(\lambda\)</span></p>
<ol class="arabic simple">
<li><p>Initialize parameters <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span></p></li>
<li><p>For iteration = 1, …, N:</p>
<ol class="arabic simple">
<li><p>Initialize empty buffer <span class="math notranslate nohighlight">\(\mathcal{D}\)</span></p></li>
<li><p>For b = 1, …, B:</p>
<ol class="arabic simple">
<li><p>Collect trajectory <span class="math notranslate nohighlight">\(\tau_b = (s_0, a_0, r_0, ..., s_T, a_T, r_T)\)</span> using policy <span class="math notranslate nohighlight">\(d(a|s;\boldsymbol{w})\)</span></p></li>
<li><p>Compute TD errors: <span class="math notranslate nohighlight">\(\delta_t = r_t + \gamma v(s_{t+1};\boldsymbol{\theta}) - v(s_t;\boldsymbol{\theta})\)</span> for all t</p></li>
<li><p>Compute GAE advantages:</p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(A_T = 0\)</span></p></li>
<li><p>For t = T-1, …, 0:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(A_t = \delta_t + (\gamma\lambda)A_{t+1}\)</span></p></li>
</ol>
</li>
</ol>
</li>
<li><p>Store tuple <span class="math notranslate nohighlight">\((s_t, a_t, A_t, v(s_t;\boldsymbol{\theta}))_{t=0}^T\)</span> in <span class="math notranslate nohighlight">\(\mathcal{D}\)</span></p></li>
</ol>
</li>
<li><p>For value_epoch = 1, …, K:</p>
<ol class="arabic simple">
<li><p>Sample mini-batch <span class="math notranslate nohighlight">\(\mathcal{B}_v\)</span> of size <span class="math notranslate nohighlight">\(M\)</span> from <span class="math notranslate nohighlight">\(\mathcal{D}\)</span></p></li>
<li><p>Compute value loss: <span class="math notranslate nohighlight">\(L_v = \frac{1}{M}\sum_{(s,\cdot,\cdot,v_{\text{old}}) \in \mathcal{B}_v} (v(s;\boldsymbol{\theta}) - v_{\text{old}})^2\)</span></p></li>
<li><p>Update value function: <span class="math notranslate nohighlight">\(\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} - \alpha_\theta \nabla_{\boldsymbol{\theta}}L_v\)</span></p></li>
</ol>
</li>
<li><p>Normalize advantages: <span class="math notranslate nohighlight">\(A \leftarrow \frac{A - \mu_A}{\sigma_A}\)</span></p></li>
<li><p>For policy_epoch = 1, …, J:</p>
<ol class="arabic simple">
<li><p>Sample mini-batch <span class="math notranslate nohighlight">\(\mathcal{B}_\pi\)</span> of size <span class="math notranslate nohighlight">\(M\)</span> from <span class="math notranslate nohighlight">\(\mathcal{D}\)</span></p></li>
<li><p>Compute policy loss: <span class="math notranslate nohighlight">\(L_\pi = -\frac{1}{M}\sum_{(s,a,A,\cdot) \in \mathcal{B}_\pi} \log d(a|s;\boldsymbol{w})A\)</span></p></li>
<li><p>Update policy: <span class="math notranslate nohighlight">\(\boldsymbol{w} \leftarrow \boldsymbol{w} - \alpha_w \nabla_{\boldsymbol{w}}L_\pi\)</span></p></li>
</ol>
</li>
</ol>
</li>
<li><p>Return <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span></p></li>
</ol>
</section>
</div><p>With <span class="math notranslate nohighlight">\(\lambda = 0\)</span>, the GAE advantage estimator becomes just the one-step TD error:</p>
<div class="math notranslate nohighlight">
\[
A^{\text{GAE}(\gamma,0)}(s_t,a_t) = \delta_t = r_t + \gamma v(s_{t+1}) - v(s_t)
\]</div>
<p>The non-batched, purely online, GAE(0) algorithm then becomes:</p>
<div class="proof algorithm admonition" id="actor-critic-td0">
<p class="admonition-title"><span class="caption-number">Algorithm 42 </span> (Actor-Critic with TD(0))</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> Policy parameterization <span class="math notranslate nohighlight">\(d(a|s;\boldsymbol{w})\)</span>, value function <span class="math notranslate nohighlight">\(v(s;\boldsymbol{\theta})\)</span><br />
<strong>Output:</strong> Updated policy parameters <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span><br />
<strong>Hyperparameters:</strong> Learning rates <span class="math notranslate nohighlight">\(\alpha_w\)</span>, <span class="math notranslate nohighlight">\(\alpha_\theta\)</span>, number of episodes <span class="math notranslate nohighlight">\(N\)</span>, episode length <span class="math notranslate nohighlight">\(T\)</span>, discount <span class="math notranslate nohighlight">\(\gamma\)</span></p>
<ol class="arabic simple">
<li><p>Initialize parameters <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span></p></li>
<li><p>For episode = 1, …, <span class="math notranslate nohighlight">\(N\)</span> do:</p>
<ol class="arabic simple">
<li><p>Initialize state <span class="math notranslate nohighlight">\(s_0\)</span></p></li>
<li><p>For <span class="math notranslate nohighlight">\(t = 0, ..., T\)</span> do:</p>
<ol class="arabic simple">
<li><p>Sample action: <span class="math notranslate nohighlight">\(a_t \sim d(\cdot|s_t;\boldsymbol{w})\)</span></p></li>
<li><p>Execute <span class="math notranslate nohighlight">\(a_t\)</span>, observe <span class="math notranslate nohighlight">\(r_t\)</span>, <span class="math notranslate nohighlight">\(s_{t+1}\)</span></p></li>
<li><p>Compute TD error: <span class="math notranslate nohighlight">\(\delta_t = r_t + \gamma v(s_{t+1};\boldsymbol{\theta}) - v(s_t;\boldsymbol{\theta})\)</span></p></li>
<li><p>Update value function: <span class="math notranslate nohighlight">\(\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} + \alpha_\theta \delta_t \nabla_{\boldsymbol{\theta}}v(s_t;\boldsymbol{\theta})\)</span></p></li>
<li><p>Update policy: <span class="math notranslate nohighlight">\(\boldsymbol{w} \leftarrow \boldsymbol{w} + \alpha_w \nabla_{\boldsymbol{w}}\log d(a_t|s_t;\boldsymbol{w})\delta_t\)</span></p></li>
</ol>
</li>
</ol>
</li>
<li><p>Return <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span></p></li>
</ol>
</section>
</div><p>Interestingly, this version was first derived by Richard Sutton in his 1984 PhD thesis. He called it the Adaptive Heuristic Actor-Critic algorithm. As far as I know, it was not derived using the score function method outlined here, but rather through intuitive reasoning (great intuition!).</p>
</section>
<section id="the-policy-gradient-theorem">
<h2>The Policy Gradient Theorem<a class="headerlink" href="#the-policy-gradient-theorem" title="Link to this heading">#</a></h2>
<p>Sutton 1999 provided an expression for the gradient of the infinite discounted return with respect to the parameters of a parameterized policy. Here is an alternative derivation by considering a bilevel optimization problem:</p>
<div class="math notranslate nohighlight">
\[
\max_{\mathbf{w}} \alpha^\top \mathbf{v}_\gamma^{d^\infty}
\]</div>
<p>subject to:</p>
<div class="math notranslate nohighlight">
\[
(\mathbf{I} - \gamma \mathbf{P}_d) \mathbf{v}_\gamma^{d^\infty} = \mathbf{r}_d
\]</div>
<p>The Implicit Function Theorem states that if there is a solution to the problem <span class="math notranslate nohighlight">\(F(\mathbf{v}, \mathbf{w}) = 0\)</span>, then we can “reparameterize” our problem as <span class="math notranslate nohighlight">\(F(\mathbf{v}(\mathbf{w}), \mathbf{w})\)</span> where <span class="math notranslate nohighlight">\(\mathbf{v}(\mathbf{w})\)</span> is an implicit function of <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>. If the Jacobian <span class="math notranslate nohighlight">\(\frac{\partial F}{\partial \mathbf{v}}\)</span> is invertible, then:</p>
<div class="math notranslate nohighlight">
\[
\frac{d\mathbf{v}(\mathbf{w})}{d\mathbf{w}} = -\left(\frac{\partial F(\mathbf{v}(\mathbf{w}), \mathbf{w})}{\partial \mathbf{x}}\right)^{-1}\frac{\partial F(\mathbf{v}(\mathbf{w}), \mathbf{w})}{\partial \mathbf{w}}
\]</div>
<p>Here we made it clear in our notation that the derivative must be evaluated at root <span class="math notranslate nohighlight">\((\mathbf{v}(\mathbf{w}), \mathbf{w})\)</span> of <span class="math notranslate nohighlight">\(F\)</span>. For the remaining of this derivation, we will drop this dependence to make notation more compact.</p>
<p>Applying this to our case with <span class="math notranslate nohighlight">\(F(\mathbf{v}, \mathbf{w}) = (\mathbf{I} - \gamma \mathbf{P}_d)\mathbf{v} - \mathbf{r}_d\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathbf{v}_\gamma^{d^\infty}}{\partial \mathbf{w}} = (\mathbf{I} - \gamma \mathbf{P}_d)^{-1}\left(\frac{\partial \mathbf{r}_d}{\partial \mathbf{w}} + \gamma \frac{\partial \mathbf{P}_d}{\partial \mathbf{w}}\mathbf{v}_\gamma^{d^\infty}\right)
\]</div>
<p>Then:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\nabla_{\mathbf{w}}J(\mathbf{w}) &amp;= \alpha^\top \frac{\partial \mathbf{v}_\gamma^{d^\infty}}{\partial \mathbf{w}} \\
&amp;= \mathbf{x}_\alpha^\top\left(\frac{\partial \mathbf{r}_d}{\partial \mathbf{w}} + \gamma \frac{\partial \mathbf{P}_d}{\partial \mathbf{w}}\mathbf{v}_\gamma^{d^\infty}\right)
\end{align*}
\end{split}\]</div>
<p>where we have defined the discounted state visitation distribution:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}_\alpha^\top \equiv \alpha^\top(\mathbf{I} - \gamma \mathbf{P}_d)^{-1}.
\]</div>
<p>Remember the vector notation for MDPs:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
[\mathbf{r}_d]_s &amp;= \sum_a d(a|s)r(s,a) \\
[\mathbf{P}_d]_{ss'} &amp;= \sum_a d(a|s)P(s'|s,a)
\end{align*}
\end{split}\]</div>
<p>Then taking the derivatives gives us:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\left[\frac{\partial \mathbf{r}_d}{\partial \mathbf{w}}\right]_s &amp;= \sum_a \nabla_{\mathbf{w}}d(a|s)r(s,a) \\
\left[\frac{\partial \mathbf{P}_d}{\partial \mathbf{w}}\mathbf{v}_\gamma^{d^\infty}\right]_s &amp;= \sum_a \nabla_{\mathbf{w}}d(a|s)\sum_{s'} P(s'|s,a)v_\gamma^{d^\infty}(s')
\end{align*}
\end{split}\]</div>
<p>Substituting back:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\nabla_{\mathbf{w}}J(\mathbf{w}) &amp;= \sum_s x_\alpha(s)\left(\sum_a \nabla_{\mathbf{w}}d(a|s)r(s,a) + \gamma\sum_a \nabla_{\mathbf{w}}d(a|s)\sum_{s'} P(s'|s,a)v_\gamma^{d^\infty}(s')\right) \\
&amp;= \sum_s x_\alpha(s)\sum_a \nabla_{\mathbf{w}}d(a|s)\left(r(s,a) + \gamma \sum_{s'} P(s'|s,a)v_\gamma^{d^\infty}(s')\right)
\end{align*}
\end{split}\]</div>
<p>This is the policy gradient theorem, where <span class="math notranslate nohighlight">\(x_\alpha(s)\)</span> is the discounted state visitation distribution and the term in parentheses is the state-action value function <span class="math notranslate nohighlight">\(q(s,a)\)</span>.</p>
</section>
<section id="normalized-discounted-state-visitation-distribution">
<h2>Normalized Discounted State Visitation Distribution<a class="headerlink" href="#normalized-discounted-state-visitation-distribution" title="Link to this heading">#</a></h2>
<p>The discounted state visitation <span class="math notranslate nohighlight">\(x_\alpha(s)\)</span> is not normalized. Therefore the expression we obtained above is not an expectation. However, we can tranform it into one by normalizing by <span class="math notranslate nohighlight">\(1-  \gamma\)</span>. Note that for any initial distribution <span class="math notranslate nohighlight">\(\alpha\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\sum_s x_\alpha(s) = \alpha^\top(\mathbf{I} - \gamma \mathbf{P}_d)^{-1}\mathbf{1} = \frac{\alpha^\top\mathbf{1}}{1-\gamma} = \frac{1}{1-\gamma}
\]</div>
<p>Therefore, defining the normalized state distribution <span class="math notranslate nohighlight">\(\mu_\alpha(s) = (1-\gamma)x_\alpha(s)\)</span>, we can write:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\nabla_{\mathbf{w}}J(\mathbf{w}) &amp;= \frac{1}{1-\gamma}\sum_s \mu_\alpha(s)\sum_a \nabla_{\mathbf{w}}d(a|s)\left(r(s,a) + \gamma \sum_{s'} P(s'|s,a)v_\gamma^{d^\infty}(s')\right) \\
&amp;= \frac{1}{1-\gamma}\mathbb{E}_{s\sim\mu_\alpha}\left[\sum_a \nabla_{\mathbf{w}}d(a|s)Q(s,a)\right]
\end{align*}
\end{split}\]</div>
<p>Now we have expressed the policy gradient theorem in terms of expectations under the normalized discounted state visitation distribution. But what does sampling from <span class="math notranslate nohighlight">\(\mu_\alpha\)</span> means? Recall that <span class="math notranslate nohighlight">\(\mathbf{x}_\alpha^\top = \alpha^\top(\mathbf{I} - \gamma \mathbf{P}_d)^{-1}\)</span>. Using the Neumann series expansion (valid when <span class="math notranslate nohighlight">\(\|\gamma \mathbf{P}_d\| &lt; 1\)</span>, which holds for <span class="math notranslate nohighlight">\(\gamma &lt; 1\)</span> since <span class="math notranslate nohighlight">\(\mathbf{P}_d\)</span> is a stochastic matrix) we have:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\mu}_\alpha^\top = (1-\gamma)\alpha^\top\sum_{k=0}^{\infty} (\gamma \mathbf{P}_d)^k
\]</div>
<p>We can then factor out the first term from this summation to obtain:
s</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\boldsymbol{\mu}_\alpha^\top &amp;= (1-\gamma)\alpha^\top\sum_{k=0}^{\infty} (\gamma \mathbf{P}_d)^k \\
&amp;= (1-\gamma)\alpha^\top + (1-\gamma)\alpha^\top\sum_{k=1}^{\infty} (\gamma \mathbf{P}_d)^k \\
&amp;= (1-\gamma)\alpha^\top + (1-\gamma)\alpha^\top\gamma\mathbf{P}_d\sum_{k=0}^{\infty} (\gamma \mathbf{P}_d)^k \\
&amp;= (1-\gamma)\alpha^\top + \gamma\boldsymbol{\mu}_\alpha^\top \mathbf{P}_d
\end{align*}\]</div>
<p>The balance equation :</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\mu}_\alpha^\top = (1-\gamma)\alpha^\top + \gamma\boldsymbol{\mu}_\alpha^\top \mathbf{P}_d
\]</div>
<p>shows that <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_\alpha\)</span> is a mixture distribution: with probability <span class="math notranslate nohighlight">\(1-\gamma\)</span> you draw a state from the initial distribution <span class="math notranslate nohighlight">\(\alpha\)</span> (reset), and with probability <span class="math notranslate nohighlight">\(\gamma\)</span> you follow the policy dynamics <span class="math notranslate nohighlight">\(\mathbf{P}_d\)</span> from the current state (continue). This interpretation directly connects to the geometric process: at each step you either terminate and resample from <span class="math notranslate nohighlight">\(\alpha\)</span> (with probability <span class="math notranslate nohighlight">\(1-\gamma\)</span>) or continue following the policy (with probability <span class="math notranslate nohighlight">\(\gamma\)</span>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">sample_from_discounted_visitation</span><span class="p">(</span>
    <span class="n">alpha</span><span class="p">,</span> 
    <span class="n">policy</span><span class="p">,</span> 
    <span class="n">transition_model</span><span class="p">,</span> 
    <span class="n">gamma</span><span class="p">,</span> 
    <span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sample states from the discounted visitation distribution.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        alpha: Initial state distribution (vector of probabilities)</span>
<span class="sd">        policy: Function (state -&gt; action probabilities)</span>
<span class="sd">        transition_model: Function (state, action -&gt; next state probabilities)</span>
<span class="sd">        gamma: Discount factor</span>
<span class="sd">        n_samples: Number of states to sample</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        Array of sampled states</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">n_states</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
    
    <span class="c1"># Initialize state from alpha</span>
    <span class="n">current_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_states</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
        <span class="n">samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_state</span><span class="p">)</span>
        
        <span class="c1"># With probability (1-gamma): reset</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">gamma</span><span class="p">:</span>
            <span class="n">current_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_states</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
        <span class="c1"># With probability gamma: continue</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Sample action from policy</span>
            <span class="n">action_probs</span> <span class="o">=</span> <span class="n">policy</span><span class="p">(</span><span class="n">current_state</span><span class="p">)</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">action_probs</span><span class="p">),</span> <span class="n">p</span><span class="o">=</span><span class="n">action_probs</span><span class="p">)</span>
            
            <span class="c1"># Sample next state from transition model</span>
            <span class="n">next_state_probs</span> <span class="o">=</span> <span class="n">transition_model</span><span class="p">(</span><span class="n">current_state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
            <span class="n">current_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_states</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">next_state_probs</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

<span class="c1"># Example usage for a simple 2-state MDP</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">])</span>  <span class="c1"># Initial distribution</span>
<span class="n">policy</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">])</span>  <span class="c1"># Dummy policy</span>
<span class="n">transition_model</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">])</span>  <span class="c1"># Dummy transitions</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span>

<span class="n">samples</span> <span class="o">=</span> <span class="n">sample_from_discounted_visitation</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">transition_model</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>

<span class="c1"># Check empirical distribution</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Empirical state distribution:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Empirical state distribution:
[0.857 0.143]
</pre></div>
</div>
</div>
</div>
<p>While the math shows that sampling from the discounted visitation distribution <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_\alpha\)</span> would give us unbiased policy gradient estimates, Thomas (2014) demonstrated that this implementation can be detrimental to performance in practice. The issue arises because terminating trajectories early (with probability <span class="math notranslate nohighlight">\(1-\gamma\)</span>) reduces the effective amount of data we collect from each trajectory. This early termination weakens the learning signal, as many trajectories don’t reach meaningful terminal states or rewards.</p>
<p>Therefore, in practice, we typically sample complete trajectories from the undiscounted process (i.e., run the policy until natural termination or a fixed horizon) while still using <span class="math notranslate nohighlight">\(\gamma\)</span> in the advantage estimation. This approach preserves the full learning signal from each trajectory
and has been empirically shown to lead to better performance.</p>
<p>This is one of several cases in RL where the theoretically optimal procedure differs from the best practical implementation.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="policy-optimization-with-a-model">
<h1>Policy Optimization with a Model<a class="headerlink" href="#policy-optimization-with-a-model" title="Link to this heading">#</a></h1>
<p>In this section, we’ll explore how incorporating a model of the dynamics can help us design better policy gradient estimators. Let’s begin with a pure model-free approach that uses a critic to maximize a deterministic policy:</p>
<div class="math notranslate nohighlight">
\[
J(\boldsymbol{w}) = \mathbb{E}_{s\sim\rho}[Q(s,d(s;\boldsymbol{w}))], \enspace \nabla_{\boldsymbol{w}} J(\boldsymbol{w}) = \mathbb{E}_{s\sim\rho}[\nabla_a Q(s,a)|_{a=d(s;\boldsymbol{w})} \nabla_{\boldsymbol{w}} d(s;\boldsymbol{w})]
\]</div>
<p>Using the recursive structure of the Bellman equations, we can unroll our objective one step ahead:</p>
<div class="math notranslate nohighlight">
\[
J(\boldsymbol{w}) = \mathbb{E}_{s\sim\rho}[r(s,d(s;\boldsymbol{w})) + \gamma\mathbb{E}_{s'\sim p(\cdot|s,d(s;\boldsymbol{w}))}[Q(s',d(s';\boldsymbol{w}))]],
\]</div>
<p>To differentiate this objective, we need access to both a model of the dynamics and the reward function, as shown in the following expression:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla_{\boldsymbol{w}} J(\boldsymbol{w}) = \mathbb{E}_{s\sim\rho}[\nabla_a r(s,a)|_{a=d(s;\boldsymbol{w})}\nabla_{\boldsymbol{w}} d(s;\boldsymbol{w}) + 
\gamma(\mathbb{E}_{s'\sim p(\cdot|s,d(s;\boldsymbol{w}))}[\nabla_a Q(s',a)|_{a=d(s';\boldsymbol{w})}\nabla_{\boldsymbol{w}} d(s';\boldsymbol{w})] + \\\nabla_{\boldsymbol{w}} d(s;\boldsymbol{w})\int_{s'} \nabla_a p(s'|s,a)|_{a=d(s;\boldsymbol{w})}Q(s',d(s';\boldsymbol{w}))ds')]
\end{split}\]</div>
<p>While <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span> doesn’t appear in the outer expectation over initial states, it affects the inner expectation over next states—a distributional dependency. As a result, the product rule of calculus yields two terms: the first being an expectation, and the second being problematic for sample average estimation. However, we have tools to address this challenge: we can either apply the reparameterization trick to the dynamics or use score function estimators.</p>
<p>For the reparameterization approach, assuming <span class="math notranslate nohighlight">\(s' = f(s,a,\xi;\boldsymbol{w})\)</span> where <span class="math notranslate nohighlight">\(\xi\)</span> is the noise variable:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
J^{\text{DPMB-R}}(\boldsymbol{w}) &amp;= \mathbb{E}_{s\sim\rho,\xi}[r(s,d(s;\boldsymbol{w})) + \gamma Q(f(s,d(s;\boldsymbol{w}),\xi),d(f(s,d(s;\boldsymbol{w}),\xi);\boldsymbol{w}))]\\
\nabla_{\boldsymbol{w}} J^{\text{DPMB-R}}(\boldsymbol{w}) &amp;= \mathbb{E}_{s\sim\rho,\xi}[\nabla_a r(s,a)|_{a=d(s;\boldsymbol{w})}\nabla_{\boldsymbol{w}} d(s;\boldsymbol{w}) + \\
&amp;\gamma(\nabla_a Q(s',a)|_{a=d(s';\boldsymbol{w})}\nabla_{\boldsymbol{w}} d(s';\boldsymbol{w}) + 
\nabla_{s'} Q(s',d(s';\boldsymbol{w}))\nabla_{\boldsymbol{w}} f(s,d(s;\boldsymbol{w}),\xi)\nabla_{\boldsymbol{w}} d(s;\boldsymbol{w}))]
\end{align*}
\end{split}\]</div>
<p>As for the score function approach:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\nabla_{\boldsymbol{w}} J^{\text{DPMB-SF}}(\boldsymbol{w}) = \mathbb{E}_{s\sim\rho}[&amp;\nabla_a r(s,a)|_{a=d(s;\boldsymbol{w})}\nabla_{\boldsymbol{w}} d(s;\boldsymbol{w}) + \\
&amp;\gamma\mathbb{E}_{s'\sim p(\cdot|s,d(s;\boldsymbol{w}))}[\nabla_{\boldsymbol{w}} \log p(s'|s,d(s;\boldsymbol{w}))Q(s',d(s';\boldsymbol{w})) + \nabla_a Q(s',a)|_{a=d(s';\boldsymbol{w})}\nabla_{\boldsymbol{w}} d(s';\boldsymbol{w})]]
\end{align*}
\end{split}\]</div>
<p>We’ve now developed a hybrid algorithm that combines model-based and model-free approaches while integrating derivative estimators for stochastic dynamics with a deterministic policy parameterization. While this hybrid estimator remains relatively unexplored in practice, it could prove valuable for systems with specific structural properties.</p>
<p>Consider a robotics scenario with hybrid continuous-discrete dynamics: a robotic arm operates in continuous space but interacts with discrete object states. While the arm’s policy remains differentiable (<span class="math notranslate nohighlight">\(\nabla_{\boldsymbol{w}} d\)</span>), the object state transitions follow categorical distributions. In this case, reparameterization becomes impractical, but the score function approach is viable if we can compute <span class="math notranslate nohighlight">\(\nabla_{\boldsymbol{w}} \log p(s'|s,d(s;\boldsymbol{w}))\)</span> from the known transition model. Similar structures arise in manufacturing processes, where machine actions might be continuous and differentiable, but material state transitions often follow discrete steps with known probabilities. Note that both approaches require knowledge of transition probabilities and won’t work with pure black-box simulators or systems where we can only sample transitions without probability estimates.</p>
<p>Another dimension to explore in our algorithm design is the number of steps we wish to unroll our model. This allows us to better understand and control the bias-variance tradeoffs in our methods.</p>
<section id="backpropagation-policy-optimization">
<h2>Backpropagation Policy Optimization<a class="headerlink" href="#backpropagation-policy-optimization" title="Link to this heading">#</a></h2>
<p>The questions of derivative estimators only arise with stochastic dynamics. For systems with deterministic dynamics and a deterministic policy, the one-step gradient unroll simplifies to:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\nabla_{\boldsymbol{w}} J(\boldsymbol{w}) = &amp;\mathbb{E}_{s\sim\rho}[\nabla_a r(s,a)|_{a=d(s;\boldsymbol{w})}\nabla_{\boldsymbol{w}} d(s;\boldsymbol{w}) + \\
&amp;\gamma(\nabla_a Q(s',a)|_{a=d(s';\boldsymbol{w})}\nabla_{\boldsymbol{w}} d(s';\boldsymbol{w}) + 
\nabla_{\boldsymbol{w}} d(s;\boldsymbol{w})\nabla_a f(s,a)|_{a=d(s;\boldsymbol{w})}\nabla_{s'} Q(s',d(s';\boldsymbol{w})))]
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(s' = f(s,d(s;\boldsymbol{w}))\)</span> is the deterministic next state. Notice the resemblance between this expression and that obtained for <span class="math notranslate nohighlight">\(\nabla_{\boldsymbol{w}} J^{\text{DPMB-R}}\)</span> above. They are essentially the same except that in the reparameterized case, the dynamics have made the dependence on the noise variable explicit and the outer expectation has been updated accordingly. This similarity arises because differentiation through reparameterized dynamics models is, in essence, backpropagation: it tracks the propagation of perturbations through a computation graph—which we refer to as a stochastic computation graph in this setting.</p>
<p>Still under this simplified setting with deterministic policies and dynamics, we can extend the expression for the gradient through n-steps of model unroll, leading to:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\nabla_{\boldsymbol{w}} J(\boldsymbol{w}) = &amp;\mathbb{E}_{s\sim\rho}[\sum_{t=0}^{n-1} \gamma^t(\nabla_a r(s_t,a_t)|_{a_t=d(s_t;\boldsymbol{w})}\nabla_{\boldsymbol{w}} d(s_t;\boldsymbol{w}) + \nabla_{s_t} r(s_t,d(s_t;\boldsymbol{w}))\nabla_{\boldsymbol{w}} s_t) + \\
&amp;\gamma^n(\nabla_a Q(s_n,a)|_{a=d(s_n;\boldsymbol{w})}\nabla_{\boldsymbol{w}} d(s_n;\boldsymbol{w}) + \nabla_{s_n} Q(s_n,d(s_n;\boldsymbol{w}))\nabla_{\boldsymbol{w}} s_n)]
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(s_{t+1} = f(s_t,d(s_t;\boldsymbol{w}))\)</span> for <span class="math notranslate nohighlight">\(t=0,\ldots,n-1\)</span>, <span class="math notranslate nohighlight">\(s_0=s\)</span>, and <span class="math notranslate nohighlight">\(\nabla_{\boldsymbol{w}} s_t\)</span> follows the recursive relationship:</p>
<div class="math notranslate nohighlight">
\[
\nabla_{\boldsymbol{w}} s_{t+1} = \nabla_a f(s_t,a)|_{a=d(s_t;\boldsymbol{w})}\nabla_{\boldsymbol{w}} d(s_t;\boldsymbol{w}) + \nabla_s f(s_t,d(s_t;\boldsymbol{w}))\nabla_{\boldsymbol{w}} s_t
\]</div>
<p>with base case <span class="math notranslate nohighlight">\(\nabla_{\boldsymbol{w}} s_0 = 0\)</span> since the initial state does not depend on the policy parameters.</p>
<p>At both ends of the spectrum, we have that for n=0, we fall back to the pure critic NFQCA-like approach, and for <span class="math notranslate nohighlight">\(n=\infty\)</span>, we don’t bootstrap at all and are fully model-based without a critic. The pure model-based critic-free approach to optimization is what we may refer to as a backpropagation-based policy optimization (BPO).</p>
<p>But just as backpropagation through RNNs or very deep networks can be challenging due to exploding and vanishing gradients, “vanilla” Backpropagation Policy Optimization (BPO) without a critic can severely suffer from the curse of horizon. This is because it essentially implements single shooting optimization. Using a critic can be an effective remedy to this problem, allowing us to better control the bias-variance tradeoff while preserving gradient information that would be lost with a more drastic truncated backpropagation approach.</p>
</section>
<section id="stochastic-value-gradient-svg">
<h2>Stochastic Value Gradient (SVG)<a class="headerlink" href="#stochastic-value-gradient-svg" title="Link to this heading">#</a></h2>
<p>The stochastic value gradient framework of Heess (2015) applies the recipe for policy optimization with a model using reparameterized dynamics and action selection via randomized policies. In this setting, the stochastic policy model based reparameterized estimator over n steps is</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
J^{\text{SPMB-R-N}}(\boldsymbol{w}) &amp;= \mathbb{E}_{s\sim\rho,\{\epsilon_i\}_{i=0}^{n-1},\{\xi_i\}_{i=0}^{n-1}}[\sum_{i=0}^{n-1} \gamma^i r(s_i,d(s_i,\epsilon_i;\boldsymbol{w})) + \gamma^n Q(s_n,d(s_n,\epsilon_n;\boldsymbol{w}))]
\end{align*}
\]</div>
<p>where <span class="math notranslate nohighlight">\(s_0 = s\)</span> and for <span class="math notranslate nohighlight">\(i \geq 0\)</span>, <span class="math notranslate nohighlight">\(s_{i+1} = f(s_i,d(s_i,\epsilon_i;\boldsymbol{w}),\xi_i)\)</span>. The gradient becomes:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\nabla_{\boldsymbol{w}} J^{\text{SPMB-R-N}}(\boldsymbol{w}) &amp;= \mathbb{E}_{s\sim\rho,\{\epsilon_i\}_{i=0}^{n-1},\{\xi_i\}_{i=0}^{n-1}}[\sum_{i=0}^{n-1} \gamma^i \left(\nabla_a r(s_i,a)|_{a=d(s_i,\epsilon_i;\boldsymbol{w})}\nabla_{\boldsymbol{w}} d(s_i,\epsilon_i;\boldsymbol{w}) + \nabla_{s_i} r(s_i,d(s_i,\epsilon_i;\boldsymbol{w}))\nabla_{\boldsymbol{w}} s_i\right) \enspace + \\
&amp;\qquad \gamma^n(\nabla_a Q(s_n,a)|_{a=d(s_n,\epsilon_n;\boldsymbol{w})}\nabla_{\boldsymbol{w}} d(s_n,\epsilon_n;\boldsymbol{w}) + \nabla_{s_n} Q(s_n,d(s_n,\epsilon_n;\boldsymbol{w}))\nabla_{\boldsymbol{w}} s_n)]
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\nabla_{\boldsymbol{w}} s_0 = 0\)</span> and for <span class="math notranslate nohighlight">\(i \geq 0\)</span>:</p>
<div class="math notranslate nohighlight">
\[\nabla_{\boldsymbol{w}} s_{i+1} = \nabla_a f(s_i,a,\xi_i)|_{a=d(s_i,\epsilon_i;\boldsymbol{w})}\nabla_{\boldsymbol{w}} d(s_i,\epsilon_i;\boldsymbol{w}) + \nabla_{s_i} f(s_i,d(s_i,\epsilon_i;\boldsymbol{w}),\xi_i)\nabla_{\boldsymbol{w}} s_i\]</div>
<p>While we could implement this expression for the gradient ourselves, this approach is much easier, less error-prone, and most likely better optimized for performance when using automatic differentiation. Given  a set of rollouts (for which we know the primitive noise variables), then we can compute the monte carlo objective:</p>
<div class="math notranslate nohighlight">
\[\hat{J}^{\text{SPMB-R-N}}(\boldsymbol{w}) = \frac{1}{M}\sum_{m=1}^M [\sum_{i=0}^{n-1} \gamma^i r(s_i^m,d(s_i^m,\epsilon_i^m;\boldsymbol{w})) + \gamma^n Q(s_n^m,d(s_n^m,\epsilon_n^m;\boldsymbol{w}))]\]</div>
<p>where the states are generated recursively using the known noise variables: starting with initial state <span class="math notranslate nohighlight">\(s_0^m\)</span>, each subsequent state is computed as <span class="math notranslate nohighlight">\(s_{i+1}^m = f(s_i^m,d(s_i^m,\epsilon_i^m;\boldsymbol{w}),\xi_i^m)\)</span>. Thus,  a trajectory is completely determined by just the sequence of noise variables:<span class="math notranslate nohighlight">\((\epsilon_0^m, \xi_0^m, \epsilon_1^m, \xi_1^m, ..., \epsilon_{n-1}^m, \xi_{n-1}^m, \epsilon_n^m)\)</span> where <span class="math notranslate nohighlight">\(\epsilon_i^m\)</span> are the action noise variables and <span class="math notranslate nohighlight">\(\xi_i^m\)</span> are the dynamics noise variables.</p>
<p>The choice of unroll steps <span class="math notranslate nohighlight">\(n\)</span> gives us precise control over the balance between model-based and critic-based components in our gradient estimator. At one extreme, setting <span class="math notranslate nohighlight">\(n = 0\)</span> yields a purely model-free algorithm known as SVG(0) (Heess et al., 2015), which relies entirely on the critic for value estimation:</p>
<div class="math notranslate nohighlight">
\[
\hat{J}^{\text{SPMB-R-0}}(\boldsymbol{w}) = \frac{1}{M}\sum_{m=1}^M Q(s_0^m,d(s_0^m,\epsilon_0^m;\boldsymbol{w}))
\]</div>
<p>At the other extreme, as <span class="math notranslate nohighlight">\(n \to \infty\)</span>, we can drop the critic term (since <span class="math notranslate nohighlight">\(\gamma^n Q \to 0\)</span>) to obtain a purely model-based algorithm, SVG(∞):</p>
<div class="math notranslate nohighlight">
\[
\hat{J}^{\text{SPMB-R-$\infty$}}(\boldsymbol{w}) = \frac{1}{M}\sum_{m=1}^M \sum_{i=0}^{\infty} \gamma^i r(s_i^m,d(s_i^m,\epsilon_i^m;\boldsymbol{w}))
\]</div>
<p>In the 2015 paper, the authors make a specific choice to reparameterize both the dynamics and action selections using normal distributions. For the policy, they use:</p>
<div class="math notranslate nohighlight">
\[
a_t = d(s_t;\boldsymbol{w}) + \sigma(s_t;\boldsymbol{w})\epsilon_t, \quad \epsilon_t \sim \mathcal{N}(0,I)
\]</div>
<p>where <span class="math notranslate nohighlight">\(d(s_t;\boldsymbol{w})\)</span> predicts the mean action and <span class="math notranslate nohighlight">\(\sigma(s_t;\boldsymbol{w})\)</span> predicts the standard deviation. For the dynamics:</p>
<div class="math notranslate nohighlight">
\[
s_{t+1} = \mu(s_t,a_t;\boldsymbol{\phi}) + \Sigma(s_t,a_t;\boldsymbol{\phi})\xi_t, \quad \xi_t \sim \mathcal{N}(0,I)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu(s_t,a_t;\boldsymbol{\phi})\)</span> predicts the mean next state and <span class="math notranslate nohighlight">\(\Sigma(s_t,a_t;\boldsymbol{\phi})\)</span> predicts the covariance matrix.</p>
<p>Under this reparameterization, the n-step surrogate loss becomes:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\hat{J}^{\text{SPMB-R-n}}(\boldsymbol{w}) = \frac{1}{M}&amp;\sum_{m=1}^M [\sum_{t=0}^{n-1} \gamma^t r(s_t^m(\boldsymbol{w}),d(s_t^m(\boldsymbol{w});\boldsymbol{w}) + \sigma(s_t^m(\boldsymbol{w});\boldsymbol{w})\epsilon_t^m) + \\
&amp;\gamma^n Q(s_n^m(\boldsymbol{w}),d(s_n^m(\boldsymbol{w});\boldsymbol{w}) + \sigma(s_n^m(\boldsymbol{w});\boldsymbol{w})\epsilon_n^m)]
\end{align*}
\end{split}\]</div>
<p>where:
$<span class="math notranslate nohighlight">\(
s_{t+1}^m(\boldsymbol{w}) = \mu(s_t^m(\boldsymbol{w}),d(s_t^m(\boldsymbol{w});\boldsymbol{w}) + \sigma(s_t^m(\boldsymbol{w});\boldsymbol{w})\epsilon_t^m;\boldsymbol{\phi}) + \Sigma(s_t^m(\boldsymbol{w}),a_t^m(\boldsymbol{w});\boldsymbol{\phi})\xi_t^m
\)</span>$</p>
<div class="proof algorithm admonition" id="svg_autodiff">
<p class="admonition-title"><span class="caption-number">Algorithm 43 </span> (Stochastic Value Gradients (SVG) Infinity (automatic differentiation))</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> Initial state distribution <span class="math notranslate nohighlight">\(\rho_0(s)\)</span>, policy networks <span class="math notranslate nohighlight">\(d(s;\boldsymbol{w})\)</span> and <span class="math notranslate nohighlight">\(\sigma(s;\boldsymbol{w})\)</span>, dynamics model networks <span class="math notranslate nohighlight">\(\mu(s,a;\boldsymbol{\phi})\)</span> and <span class="math notranslate nohighlight">\(\Sigma(s,a;\boldsymbol{\phi})\)</span>, reward function <span class="math notranslate nohighlight">\(r(s,a)\)</span>, rollout horizon <span class="math notranslate nohighlight">\(T\)</span>, learning rate <span class="math notranslate nohighlight">\(\alpha_w\)</span>, batch size <span class="math notranslate nohighlight">\(N\)</span></p>
<p><strong>Initialize</strong></p>
<ol class="arabic simple">
<li><p>Policy parameters <span class="math notranslate nohighlight">\(\boldsymbol{w}_0\)</span> randomly</p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow 0\)</span></p></li>
<li><p><strong>while</strong> not converged:</p>
<ol class="arabic simple">
<li><p>Sample batch of <span class="math notranslate nohighlight">\(N\)</span> initial states <span class="math notranslate nohighlight">\(s_0^i \sim \rho_0(s)\)</span></p></li>
<li><p>Sample noise sequences <span class="math notranslate nohighlight">\(\epsilon_{0:T}^i, \xi_{0:T}^i \sim \mathcal{N}(0,I)\)</span> for <span class="math notranslate nohighlight">\(i=1,\ldots,N\)</span></p></li>
<li><p>Compute objective using autodiff-enabled computation graph:</p>
<ol class="arabic simple">
<li><p>For each <span class="math notranslate nohighlight">\(i=1,\ldots,N\)</span>:</p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(s_0^i(\boldsymbol{w}) = s_0^i\)</span></p></li>
<li><p>For <span class="math notranslate nohighlight">\(t=0\)</span> to <span class="math notranslate nohighlight">\(T\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(a_t^i(\boldsymbol{w}) = d(s_t^i(\boldsymbol{w});\boldsymbol{w}) + \sigma(s_t^i(\boldsymbol{w});\boldsymbol{w})\epsilon_t^i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(s_{t+1}^i(\boldsymbol{w}) = \mu(s_t^i(\boldsymbol{w}),a_t^i(\boldsymbol{w});\boldsymbol{\phi}) + \Sigma(s_t^i(\boldsymbol{w}),a_t^i(\boldsymbol{w});\boldsymbol{\phi})\xi_t^i\)</span></p></li>
</ol>
</li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(J(\boldsymbol{w}) = \frac{1}{N}\sum_{i=1}^N \sum_{t=0}^T \gamma^t r(s_t^i(\boldsymbol{w}),a_t^i(\boldsymbol{w}))\)</span></p></li>
</ol>
</li>
<li><p>Compute gradient using autodiff: <span class="math notranslate nohighlight">\(\nabla_{\boldsymbol{w}}J\)</span></p></li>
<li><p>Update policy: <span class="math notranslate nohighlight">\(\boldsymbol{w}_{n+1} \leftarrow \boldsymbol{w}_n + \alpha_w \nabla_{\boldsymbol{w}}J\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow n + 1\)</span></p></li>
</ol>
</li>
</ol>
<p><strong>return</strong> <span class="math notranslate nohighlight">\(\boldsymbol{w}_n\)</span></p>
</section>
</div></section>
<section id="noise-inference-in-svg">
<h2>Noise Inference in SVG<a class="headerlink" href="#noise-inference-in-svg" title="Link to this heading">#</a></h2>
<p>The method we’ve presented so far assumes we have direct access to the noise variables <span class="math notranslate nohighlight">\(\epsilon\)</span> and <span class="math notranslate nohighlight">\(\xi\)</span> used to generate trajectories. This works well in the on-policy setting where we generate our own data. However, in off-policy scenarios where we receive externally generated trajectories, we need to infer these noise variables—a process the authors call noise inference.</p>
<p>For the Gaussian case discussed above, this inference is straightforward. Given an observed scalar action <span class="math notranslate nohighlight">\(a_t\)</span> and the current policy parameters <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>, we can recover the action noise <span class="math notranslate nohighlight">\(\epsilon_t\)</span> through inverse reparameterization:</p>
<div class="math notranslate nohighlight">
\[
\epsilon_t = \frac{a_t - d(s_t;\boldsymbol{w})}{\sigma(s_t;\boldsymbol{w})}
\]</div>
<p>Similarly, for the dynamics noise where states are typically vector-valued:</p>
<div class="math notranslate nohighlight">
\[
\xi_t = \Sigma(s_t,a_t;\boldsymbol{\phi})^{-1}(s_{t+1} - \mu(s_t,a_t;\boldsymbol{\phi}))
\]</div>
<p>This simple inversion is possible because the Gaussian reparameterization is an affine transformation, which is invertible as long as <span class="math notranslate nohighlight">\(\sigma(s_t;\boldsymbol{w})\)</span> is non-zero for scalar actions and <span class="math notranslate nohighlight">\(\Sigma(s_t,a_t;\boldsymbol{\phi})\)</span> is full rank for vector-valued states. The same principle extends naturally to vector-valued actions, where <span class="math notranslate nohighlight">\(\sigma\)</span> would be replaced by a full covariance matrix.</p>
<p>More generally, this idea of invertible transformations can be extended far beyond simple Gaussian reparameterization. We could consider a sequence of invertible transformations:</p>
<div class="math notranslate nohighlight">
\[
z_0 \sim \mathcal{N}(0,I) \xrightarrow{f_1} z_1 \xrightarrow{f_2} z_2 \xrightarrow{f_3} \cdots \xrightarrow{f_K} z_K = a_t
\]</div>
<p>where each <span class="math notranslate nohighlight">\(f_k\)</span> is an invertible neural network layer. The forward process can be written compactly as:</p>
<div class="math notranslate nohighlight">
\[
a_t = (f_K \circ f_{K-1} \circ \cdots \circ f_1)(z_0;\boldsymbol{w})
\]</div>
<p>This is precisely the idea behind normalizing flows: a series of invertible transformations that can transform a simple base distribution (like a standard normal) into a complex target distribution while maintaining exact invertibility.</p>
<p>The noise inference in this case would involve applying the inverse transformations:</p>
<div class="math notranslate nohighlight">
\[
z_0 = (f_1^{-1} \circ \cdots \circ f_K^{-1})(a_t;\boldsymbol{w})
\]</div>
<p>This approach offers several advantages:</p>
<ol class="arabic simple">
<li><p>More expressive policies and dynamics models capable of capturing multimodal distributions</p></li>
<li><p>Exact likelihood computation through the change of variables formula (can be useful for computing the log prob terms in entropy regularization for example)</p></li>
<li><p>Precise noise inference through the guaranteed invertibility of the flow</p></li>
</ol>
<p>As far as I know, this approach has not been explored in the literature.</p>
</section>
<section id="dpg-as-a-special-case-of-sac">
<h2>DPG as a Special Case of SAC<a class="headerlink" href="#dpg-as-a-special-case-of-sac" title="Link to this heading">#</a></h2>
<p>At first glance, SAC and DPG might appear to be fundamentally different approaches to policy optimization. SAC begins with the principle of entropy maximization and policy distribution matching through KL divergence minimization, while DPG directly optimizes a deterministic policy to maximize expected Q-values. However, we can show that DPG emerges as a special case of SAC as we take the temperature parameter to zero.</p>
<div class="proof proposition admonition" id="proposition-11">
<p class="admonition-title"><span class="caption-number">Proposition 5 </span> (Convergence of SAC to DPG)</p>
<section class="proposition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(d(\cdot|s;\boldsymbol{w}_\alpha)\)</span> be the optimal stochastic policy for SAC with temperature <span class="math notranslate nohighlight">\(\alpha\)</span>, and <span class="math notranslate nohighlight">\(d(s;\boldsymbol{w}_{DPG})\)</span> be the optimal deterministic policy gradient solution. Under appropriate assumptions, as <span class="math notranslate nohighlight">\(\alpha \to 0\)</span>:</p>
<div class="math notranslate nohighlight">
\[
d(a|s;\boldsymbol{w}_\alpha) \to \delta(a - d(s;\boldsymbol{w}_{DPG}))
\]</div>
<p><strong>Assumptions:</strong></p>
<ol class="arabic">
<li><p>The stochastic policy class is Gaussian with learnable mean and standard deviation:</p>
<div class="math notranslate nohighlight">
\[
   d(a|s;\boldsymbol{w}) = \mathcal{N}(\mu_w(s), \sigma_w(s)^2)
   \]</div>
</li>
<li><p>The SAC objective for policy improvement uses the soft Q-function:</p>
<div class="math notranslate nohighlight">
\[
   \boldsymbol{w}^*_\alpha = \arg\min_w \mathbb{E}_{s\sim\rho}\left[D_{KL}\left(d(\cdot|s;\boldsymbol{w}) \| \frac{\exp(Q_{soft}(s,\cdot)/\alpha)}{\int \exp(Q_{soft}(s,b)/\alpha)db}\right)\right]
   \]</div>
<p>where <span class="math notranslate nohighlight">\(Q_{soft}\)</span> follows the soft Bellman equation:</p>
<div class="math notranslate nohighlight">
\[
   Q_{soft}(s,a) = r(s,a) + \gamma \mathbb{E}_{s' \sim P}\left[\mathbb{E}_{a' \sim d(\cdot|s')}\left[Q_{soft}(s',a') - \alpha \log d(a'|s')\right]\right]
   \]</div>
</li>
<li><p>The DPG objective with a deterministic policy uses the standard Q-function:</p>
<div class="math notranslate nohighlight">
\[
   \boldsymbol{w}^*_{DPG} = \arg\max_w \mathbb{E}_{s\sim\rho}\left[Q(s,d(s;\boldsymbol{w}))\right]
   \]</div>
<p>where <span class="math notranslate nohighlight">\(Q\)</span> follows the standard Bellman equation:</p>
<div class="math notranslate nohighlight">
\[
   Q(s,a) = r(s,a) + \gamma \mathbb{E}_{s' \sim P}[Q(s',d(s';\boldsymbol{w}))]
   \]</div>
</li>
<li><p><span class="math notranslate nohighlight">\(Q_{soft}(s,a)\)</span> and <span class="math notranslate nohighlight">\(Q(s,a)\)</span> are continuous and achieve their maxima for each state <span class="math notranslate nohighlight">\(s\)</span>.</p></li>
</ol>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. At the fixed point of the soft Bellman equation, as <span class="math notranslate nohighlight">\(\alpha \to 0\)</span>, the entropy term <span class="math notranslate nohighlight">\(-\alpha \log d(a|s)\)</span> vanishes, and <span class="math notranslate nohighlight">\(Q_{soft} \to Q\)</span>. This implies that the SAC target distribution, which is proportional to <span class="math notranslate nohighlight">\(\exp(Q_{soft}(s,a)/\alpha)\)</span>, becomes:</p>
<div class="math notranslate nohighlight">
\[
\lim_{\alpha \to 0} \frac{\exp(Q(s,a)/\alpha)}{\int \exp(Q(s,b)/\alpha) db} = \delta(a - \arg\max_a Q(s,a)),
\]</div>
<p>by Laplace’s method. The target distribution thus collapses to a delta function centered at the deterministic optimal action <span class="math notranslate nohighlight">\(\arg\max_a Q(s,a)\)</span>.</p>
<p>The KL divergence term in the SAC objective measures the divergence between the stochastic policy <span class="math notranslate nohighlight">\(d(a|s;\boldsymbol{w})\)</span> (Gaussian) and this target distribution. For a Gaussian <span class="math notranslate nohighlight">\(\mathcal{N}(\mu, \sigma^2)\)</span> and a delta function <span class="math notranslate nohighlight">\(\delta(a - a^*)\)</span>, we derive:</p>
<div class="math notranslate nohighlight">
\[
D_{KL}(\mathcal{N}(\mu, \sigma^2) \| \delta(a - a^*)) = \lim_{\epsilon \to 0} D_{KL}(\mathcal{N}(\mu, \sigma^2) \| \mathcal{N}(a^*, \epsilon^2)),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{N}(a^*, \epsilon^2)\)</span> is a Gaussian approximation of the delta. Using the KL formula:</p>
<div class="math notranslate nohighlight">
\[
D_{KL} = \frac{1}{2}\left[\log\left(\frac{\epsilon^2}{\sigma^2}\right) + \frac{\sigma^2}{\epsilon^2} + \frac{(\mu - a^*)^2}{\epsilon^2} - 1\right].
\]</div>
<p>Taking the limit <span class="math notranslate nohighlight">\(\epsilon \to 0\)</span>, the divergence diverges unless <span class="math notranslate nohighlight">\(\mu = a^*\)</span> and <span class="math notranslate nohighlight">\(\sigma = 0\)</span>, where it becomes zero. Thus, minimizing the SAC objective drives <span class="math notranslate nohighlight">\(\mu_w(s) \to \arg\max_a Q(s,a)\)</span> and <span class="math notranslate nohighlight">\(\sigma_w(s) \to 0\)</span>.</p>
<p>Consequently, the stochastic policy converges to a delta function:</p>
<div class="math notranslate nohighlight">
\[
\lim_{\alpha \to 0} d(a|s;\boldsymbol{w}^*_\alpha) = \delta(a - \arg\max_a Q(s,a)) = \delta(a - d(s;\boldsymbol{w}^*_{DPG})).
\]</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="policy-optimization-with-a-trust-region">
<h1>Policy Optimization with a Trust Region<a class="headerlink" href="#policy-optimization-with-a-trust-region" title="Link to this heading">#</a></h1>
<p>Trust region methods in optimization approximate the objective function with a simpler local model within a region where we “trust” this approximation to be good. This brings about the need to define what we mean by a local region, and therefore to pick a geometry which suits our problem.</p>
<p>In standard optimization in the Euclidean space on <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>, at each iteration <span class="math notranslate nohighlight">\(k\)</span>, we create a quadratic approximation around the current point <span class="math notranslate nohighlight">\(x_k\)</span>:</p>
<div class="math notranslate nohighlight">
\[
m_k(p) = f(x_k) + g_k^T p + \frac{1}{2}p^T B_k p
\]</div>
<p>where <span class="math notranslate nohighlight">\(g_k = \nabla f(x_k)\)</span> is the gradient and <span class="math notranslate nohighlight">\(B_k\)</span> approximates the Hessian. The trust region constrains updates using Euclidean distance:</p>
<div class="math notranslate nohighlight">
\[
\min_p m_k(p) \text{ subject to } \|p\| \leq \Delta_k
\]</div>
<p>However, when optimizing over probability distributions <span class="math notranslate nohighlight">\(p(x;\theta)\)</span>, the Euclidean geometry becomes unnatural. Instead, the Kullback-Leibler divergence provides a more natural mean of measuring proximity:</p>
<div class="math notranslate nohighlight">
\[
D_{KL}(p(x;\theta) || p(x;\theta_k)) = \int p(x;\theta) \log\left(\frac{p(x;\theta)}{p(x;\theta_k)}\right)dx
\]</div>
<p>This leads to the following trust region subproblem:</p>
<div class="math notranslate nohighlight">
\[
\min_\theta m_k(\theta) \text{ subject to } D_{KL}(p(x;\theta) || p(x;\theta_k)) \leq \Delta_k
\]</div>
<p>For exponential families, the KL divergence locally reduces to a quadratic form involving the Fisher Information Matrix <span class="math notranslate nohighlight">\(I(\theta_k)\)</span>:</p>
<div class="math notranslate nohighlight">
\[
D_{KL}(p(x;\theta) || p(x;\theta_k)) \approx \frac{1}{2}(\theta - \theta_k)^T I(\theta_k)(\theta - \theta_k)
\]</div>
<p>In both cases, after solving for the step, we evaluate the actual versus predicted reduction ratio:</p>
<div class="math notranslate nohighlight">
\[
\rho_k = \frac{f(x_k) - f(x_k + p)}{m_k(0) - m_k(p)}
\]</div>
<p>This ratio determines both step acceptance and trust region adjustment:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\Delta_{k+1} = \begin{cases}
\alpha_1 \Delta_k &amp; \text{if } \rho_k &lt; \eta_1 \text{ (poor prediction)} \\
\Delta_k &amp; \text{if } \eta_1 \leq \rho_k &lt; \eta_2 \text{ (acceptable)} \\
\alpha_2 \Delta_k &amp; \text{if } \rho_k \geq \eta_2 \text{ (very good)}
\end{cases}
\end{split}\]</div>
<p>The method accepts steps when the model prediction is sufficiently accurate:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
x_{k+1} = \begin{cases}
x_k + p &amp; \text{if } \rho_k &gt; \eta_1 \\
x_k &amp; \text{otherwise}
\end{cases}
\end{split}\]</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="adp.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Approximate Dynamic Programming</p>
      </div>
    </a>
    <a class="right-next"
       href="appendix_examples.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Example COCPs</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Policy Parametrization Methods</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#embedded-optimization">Embedded Optimization</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#amortized-optimization-approach">Amortized Optimization Approach</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#deterministic-parametrized-policies">Deterministic Parametrized Policies</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-fitted-q-iteration-for-continuous-actions-nfqca">Neural Fitted Q-iteration for Continuous Actions (NFQCA)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-deterministic-policy-gradient-ddpg">Deep Deterministic Policy Gradient (DDPG)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#twin-delayed-deep-deterministic-policy-gradient-td3">Twin Delayed Deep Deterministic Policy Gradient (TD3)</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#soft-actor-critic">Soft Actor-Critic</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitted-q-iteration-for-the-smooth-bellman-equations">Fitted Q-Iteration for the Smooth Bellman Equations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#approximating-boltzmann-policies-by-gaussians">Approximating Boltzmann Policies by Gaussians</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reparameterizating-the-objective">Reparameterizating the Objective</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#derivative-estimation-for-stochastic-optimization">Derivative Estimation for Stochastic Optimization</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#change-of-measure-the-likelihood-ratio-method">Change of Measure: The Likelihood Ratio Method</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-change-of-variables-approach-the-reparameterization-trick">A Change of Variables Approach: The Reparameterization Trick</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-examples-of-reparameterization">Common Examples of Reparameterization</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bounded-intervals-the-truncated-normal">Bounded Intervals: The Truncated Normal</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling-from-0-1-the-kumaraswamy-distribution">Sampling from [0,1]: The Kumaraswamy Distribution</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#discrete-actions-the-gumbel-softmax">Discrete Actions: The Gumbel-Softmax</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#demonstration-numerical-analysis-of-gradient-estimators">Demonstration: Numerical Analysis of Gradient Estimators</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#score-function-gradient-estimation-in-reinforcement-learning">Score Function Gradient Estimation in Reinforcement Learning</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#leveraging-conditional-independence">Leveraging Conditional Independence</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variance-reduction-via-control-variates">Variance Reduction via Control Variates</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generalized-advantage-estimator">Generalized Advantage Estimator</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-policy-gradient-theorem">The Policy Gradient Theorem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normalized-discounted-state-visitation-distribution">Normalized Discounted State Visitation Distribution</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-optimization-with-a-model">Policy Optimization with a Model</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation-policy-optimization">Backpropagation Policy Optimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-value-gradient-svg">Stochastic Value Gradient (SVG)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#noise-inference-in-svg">Noise Inference in SVG</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dpg-as-a-special-case-of-sac">DPG as a Special Case of SAC</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-optimization-with-a-trust-region">Policy Optimization with a Trust Region</a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Pierre-Luc Bacon
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>