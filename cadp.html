
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>18. Policy Parametrization Methods &#8212; Pragmatic Reinforcement Learning: Algorithms and Applications</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="_static/graphviz.css?v=fd3f3429" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=87e54e7c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"bm": ["{\\boldsymbol #1}", 1]}, "processEscapes": true}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'cadp';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="22. Bibliography" href="bibliography.html" />
    <link rel="prev" title="13. Approximate Dynamic Programming" href="adp.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Pragmatic Reinforcement Learning: Algorithms and Applications</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ocp.html">1. Discrete-Time Trajectory Optimization</a></li>



<li class="toctree-l1"><a class="reference internal" href="cocp.html">5. Continuous-Time Trajectory Optimization</a></li>

<li class="toctree-l1"><a class="reference internal" href="mpc.html">7. From Trajectories to Policies</a></li>



<li class="toctree-l1"><a class="reference internal" href="dp.html">11. Dynamic Programming</a></li>

<li class="toctree-l1"><a class="reference internal" href="adp.html">13. Approximate Dynamic Programming</a></li>




<li class="toctree-l1 current active"><a class="current reference internal" href="#">18. Policy Parametrization Methods</a></li>



<li class="toctree-l1"><a class="reference internal" href="bibliography.html">22. Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/pierrelux/rlbook" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pierrelux/rlbook/edit/main/cadp.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pierrelux/rlbook/issues/new?title=Issue%20on%20page%20%2Fcadp.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/cadp.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Policy Parametrization Methods</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">18. Policy Parametrization Methods</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#embedded-optimization">19. Embedded Optimization</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#amortized-optimization-approach">19.1. Amortized Optimization Approach</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#deterministic-parametrized-policies">20. Deterministic Parametrized Policies</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-fitted-q-iteration-for-continuous-actions-hafner-et-al-2011">20.1. Neural Fitted Q-iteration for Continuous Actions (Hafner et al. 2011)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-deterministic-policy-gradient-ddpg">20.2. Deep Deterministic Policy Gradient (DDPG)</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#from-deterministic-to-stochastic-policies-soft-actor-critic">21. From Deterministic to Stochastic Policies: Soft Actor-Critic</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitted-q-iteration-for-the-smooth-bellman-equations">21.1. Fitted Q-Iteration for the Smooth Bellman Equations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#approximating-boltzmann-policies-by-gaussians">21.2. Approximating Boltzmann Policies by Gaussians</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reparameterizating-the-objective">21.3. Reparameterizating the Objective</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="policy-parametrization-methods">
<h1><span class="section-number">18. </span>Policy Parametrization Methods<a class="headerlink" href="#policy-parametrization-methods" title="Link to this heading">#</a></h1>
<p>In the previous chapter, we explored various approaches to approximate dynamic programming, focusing on ways to handle large state spaces through function approximation. However, these methods still face significant challenges when dealing with large or continuous action spaces. The need to maximize over actions during the Bellman operator evaluation becomes computationally prohibitive as the action space grows.</p>
<p>This chapter explores a natural evolution of these ideas: rather than exhaustively searching over actions, we can parameterize and directly optimize the policy itself. We begin by examining how fitted Q methods, while powerful for handling large state spaces, still struggle with action space complexity.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="embedded-optimization">
<h1><span class="section-number">19. </span>Embedded Optimization<a class="headerlink" href="#embedded-optimization" title="Link to this heading">#</a></h1>
<p>Recall that in fitted Q methods, the main idea is to compute the Bellman operator only at a subset of all states, relying on function approximation to generalize to the remaining states. At each step of the successive approximation loop, we build a dataset of input state-action pairs mapped to their corresponding optimality operator evaluations:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{D}_n = \{((s, a), (Lq)(s, a; \boldsymbol{\theta}_n)) \mid (s,a) \in \mathcal{B}\}
\]</div>
<p>This dataset is then fed to our function approximator (neural network, random forest, linear model) to obtain the next set of parameters:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\theta}_{n+1} \leftarrow \texttt{fit}(\mathcal{D}_n)
\]</div>
<p>While this strategy allows us to handle very large or even infinite (continuous) state spaces, it still requires maximizing over actions (<span class="math notranslate nohighlight">\(\max_{a \in A}\)</span>) during the dataset creation when computing the operator <span class="math notranslate nohighlight">\(L\)</span> for each basepoint. This maximization becomes computationally expensive for large action spaces. A natural improvement is to add another level of optimization: for each sample added to our regression dataset, we can employ numerical optimization methods to find actions that maximize the Bellman operator for the given state.</p>
<div class="proof algorithm admonition" id="fitted-q-iteration-explicit">
<p class="admonition-title"><span class="caption-number">Algorithm 19.1 </span> (Fitted Q-Iteration with Explicit Optimization)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> Given an MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, base points <span class="math notranslate nohighlight">\(\mathcal{B}\)</span>, function approximator class <span class="math notranslate nohighlight">\(q(s,a; \boldsymbol{\theta})\)</span>, maximum iterations <span class="math notranslate nohighlight">\(N\)</span>, tolerance <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span></p>
<p><strong>Output</strong> Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> for Q-function approximation</p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span> (e.g., for zero initialization)</p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow 0\)</span></p></li>
<li><p><strong>repeat</strong></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{D} \leftarrow \emptyset\)</span> // Regression Dataset</p></li>
<li><p>For each <span class="math notranslate nohighlight">\((s,a,r,s') \in \mathcal{B}\)</span>: // Assumes Monte Carlo Integration with one sample</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(y_{s,a} \leftarrow r + \gamma \texttt{maximize}(q(s', \cdot; \boldsymbol{\theta}_n))\)</span> // <span class="math notranslate nohighlight">\(s'\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span> are kept fixed</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D} \leftarrow \mathcal{D} \cup \{((s,a), y_{s,a})\}\)</span></p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{n+1} \leftarrow \texttt{fit}(\mathcal{D})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\delta \leftarrow \frac{1}{|\mathcal{D}||A|}\sum_{(s,a) \in \mathcal{D} \times A} (q(s,a; \boldsymbol{\theta}_{n+1}) - q(s,a; \boldsymbol{\theta}_n))^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow n + 1\)</span></p></li>
</ol>
</li>
<li><p><strong>until</strong> (<span class="math notranslate nohighlight">\(\delta &lt; \varepsilon\)</span> or <span class="math notranslate nohighlight">\(n \geq N\)</span>)</p></li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span></p></li>
</ol>
</section>
</div><p>The above pseudocode introduces a generic <span class="math notranslate nohighlight">\(\texttt{maximize}\)</span> routine which represents any numerical optimization method that searches for an action maximizing the given function. This approach is versatile and can be adapted to different types of action spaces. For continuous action spaces, we can employ standard nonlinear optimization methods like gradient descent or L-BFGS (e.g., using scipy.optimize.minimize). For large discrete action spaces, we can use integer programming solvers - linear integer programming if the Q-function approximator is linear in actions, or mixed-integer nonlinear programming (MINLP) solvers for nonlinear Q-functions. The choice of solver depends on the structure of our Q-function approximator and the constraints on our action space.</p>
<section id="amortized-optimization-approach">
<h2><span class="section-number">19.1. </span>Amortized Optimization Approach<a class="headerlink" href="#amortized-optimization-approach" title="Link to this heading">#</a></h2>
<p>This process is computationally intensive. A natural question is whether we can “amortize” some of this computation by replacing the explicit optimization for each sample with a direct mapping that gives us an approximate maximizer directly.
For Q-functions, recall that the operator is given by:</p>
<div class="math notranslate nohighlight">
\[
(\mathrm{L}q)(s,a) = r(s,a) + \gamma \int p(ds'|s,a)\max_{a' \in \mathcal{A}(s')} q(s', a')
\]</div>
<p>If <span class="math notranslate nohighlight">\(q^*\)</span> is the optimal state-action value function, then <span class="math notranslate nohighlight">\(v^*(s) = \max_a q^*(s,a)\)</span>, and we can derive the optimal policy directly by computing the decision rule:</p>
<div class="math notranslate nohighlight">
\[
d^\star(s) = \arg\max_{a \in \mathcal{A}(s)} q^\star(s,a)
\]</div>
<p>Since <span class="math notranslate nohighlight">\(q^*\)</span> is a fixed point of <span class="math notranslate nohighlight">\(L\)</span>, we can write:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
q^\star(s,a) &amp;= (Lq^*)(s,a) \\
&amp;= r(s,a) + \gamma \int p(ds'|s,a) \max_{a' \in \mathcal{A}(s')} q^\star(s', a') \\
&amp;= r(s,a) + \gamma \int p(ds'|s,a) q^\star(s', d^\star(s'))
\end{align*}
\end{split}\]</div>
<p>Note that <span class="math notranslate nohighlight">\(d^\star\)</span> is implemented by our <span class="math notranslate nohighlight">\(\texttt{maximize}\)</span> numerical solver in the procedure above. A practical strategy would be to collect these maximizer values at each step and use them to train a function approximator that directly predicts these solutions. Due to computational constraints, we might want to compute these exact maximizer values only for a subset of states, based on some computational budget, and use the fitted decision rule to generalize to the remaining states. This leads to the following amortized version:</p>
<div class="proof algorithm admonition" id="fitted-q-iteration-amortized">
<p class="admonition-title"><span class="caption-number">Algorithm 19.2 </span> (Fitted Q-Iteration with Amortized Optimization)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> Given an MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, base points <span class="math notranslate nohighlight">\(\mathcal{B}\)</span>, subset for exact optimization <span class="math notranslate nohighlight">\(\mathcal{B}_{\text{opt}} \subset \mathcal{B}\)</span>, Q-function approximator <span class="math notranslate nohighlight">\(q(s,a; \boldsymbol{\theta})\)</span>, policy approximator <span class="math notranslate nohighlight">\(d(s; \boldsymbol{w})\)</span>, maximum iterations <span class="math notranslate nohighlight">\(N\)</span>, tolerance <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span></p>
<p><strong>Output</strong> Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> for Q-function, <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span> for policy</p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{w}_0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow 0\)</span></p></li>
<li><p><strong>repeat</strong></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}_q \leftarrow \emptyset\)</span> // Q-function regression dataset</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}_d \leftarrow \emptyset\)</span> // Policy regression dataset</p></li>
<li><p>For each <span class="math notranslate nohighlight">\((s,a,r,s') \in \mathcal{B}\)</span>:</p>
<ol class="arabic simple">
<li><p>// Determine next state’s action using either exact optimization or approximation</p></li>
<li><p><strong>if</strong> <span class="math notranslate nohighlight">\(s' \in \mathcal{B}_{\text{opt}}\)</span> <strong>then</strong></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(a^*_{s'} \leftarrow \texttt{maximize}(q(s', \cdot; \boldsymbol{\theta}_n))\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}_d \leftarrow \mathcal{D}_d \cup \{(s', a^*_{s'})\}\)</span></p></li>
</ol>
</li>
<li><p><strong>else</strong></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(a^*_{s'} \leftarrow d(s'; \boldsymbol{w}_n)\)</span></p></li>
</ol>
</li>
<li><p>// Compute Q-function target using chosen action</p></li>
<li><p><span class="math notranslate nohighlight">\(y_{s,a} \leftarrow r + \gamma q(s', a^*_{s'}; \boldsymbol{\theta}_n)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}_q \leftarrow \mathcal{D}_q \cup \{((s,a), y_{s,a})\}\)</span></p></li>
</ol>
</li>
<li><p>// Update both function approximators</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{n+1} \leftarrow \texttt{fit}(\mathcal{D}_q)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{w}_{n+1} \leftarrow \texttt{fit}(\mathcal{D}_d)\)</span></p></li>
<li><p>// Compute convergence criteria</p></li>
<li><p><span class="math notranslate nohighlight">\(\delta_q \leftarrow \frac{1}{|\mathcal{D}_q|}\sum_{(s,a) \in \mathcal{D}_q} (q(s,a; \boldsymbol{\theta}_{n+1}) - q(s,a; \boldsymbol{\theta}_n))^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\delta_d \leftarrow \frac{1}{|\mathcal{D}_d|}\sum_{(s,a^*) \in \mathcal{D}_d} \|a^* - d(s; \boldsymbol{w}_{n+1})\|^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow n + 1\)</span></p></li>
</ol>
</li>
<li><p><strong>until</strong> (<span class="math notranslate nohighlight">\(\max(\delta_q, \delta_d) &lt; \varepsilon\)</span> or <span class="math notranslate nohighlight">\(n \geq N\)</span>)</p></li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{w}_n\)</span></p></li>
</ol>
</section>
</div><p>An important observation about this procedure is that the policy <span class="math notranslate nohighlight">\(d(s; \boldsymbol{w})\)</span> is being trained on a dataset <span class="math notranslate nohighlight">\(\mathcal{D}_d\)</span> containing optimal actions computed with respect to an evolving Q-function. Specifically, at iteration n, we collect pairs <span class="math notranslate nohighlight">\((s', a^*_{s'})\)</span> where <span class="math notranslate nohighlight">\(a^*_{s'} = \arg\max_a q(s', a; \boldsymbol{\theta}_n)\)</span>. However, after updating to <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{n+1}\)</span>, these actions may no longer be optimal with respect to the new Q-function.</p>
<p>A natural approach to handle this staleness would be to maintain only the most recent optimization data. We could modify our procedure to keep a sliding window of K iterations, where at iteration n, we only use data from iterations max(0, n-K) to n. This would be implemented by augmenting each entry in <span class="math notranslate nohighlight">\(\mathcal{D}_d\)</span> with a timestamp:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{D}_d^t = \{(s', a^*_{s'}, t) \mid t \in \{n-K,\ldots,n\}\}
\]</div>
<p>where t indicates the iteration at which the optimal action was computed. When fitting the policy network, we would then only use data points that are at most K iterations old:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{w}_{n+1} \leftarrow \texttt{fit}(\{(s', a^*_{s'}) \mid (s', a^*_{s'}, t) \in \mathcal{D}_d^t, n-K \leq t \leq n\})
\]</div>
<p>This introduces a trade-off between using more data (larger K) versus using more recent, accurate data (smaller K). The choice of K would depend on how quickly the Q-function evolves and the computational budget available for computing exact optimal actions.</p>
<p>Now the main issue with this approach, apart from the intrinsic out-of-distribution drift that we are trying to track, is that it requires “ground truth” - samples of optimal actions computed by the actual solver. This raises an intriguing question: how few samples do we actually need? Could we even envision eliminating the solver entirely? What seems impossible at first glance turns out to be achievable. The intuition is that as our policy improves at selecting actions, we can bootstrap from these increasingly better choices. As we continuously amortize these improving actions over time, it creates a virtuous cycle of self-improvement towards the optimal policy. But for this bootstrapping process to work, we need careful management - move too quickly and the process may become unstable. Let’s examine how this balance can be achieved.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="deterministic-parametrized-policies">
<h1><span class="section-number">20. </span>Deterministic Parametrized Policies<a class="headerlink" href="#deterministic-parametrized-policies" title="Link to this heading">#</a></h1>
<p>In this section, we consider deterministic parametrized policies of the form <span class="math notranslate nohighlight">\(d(s; \boldsymbol{w})\)</span> which directly output an action given a state. This approach differs from stochastic policies that output probability distributions over actions, making it particularly suitable for continuous control problems where the optimal policy is often deterministic. We’ll see how fitted Q-value methods can be naturally extended to simultaneously learn both the Q-function and such a deterministic policy.</p>
<section id="neural-fitted-q-iteration-for-continuous-actions-hafner-et-al-2011">
<h2><span class="section-number">20.1. </span>Neural Fitted Q-iteration for Continuous Actions (Hafner et al. 2011)<a class="headerlink" href="#neural-fitted-q-iteration-for-continuous-actions-hafner-et-al-2011" title="Link to this heading">#</a></h2>
<p>To develop this approach, let’s first consider an idealized setting where we have access to <span class="math notranslate nohighlight">\(q^\star\)</span>, the optimal Q-function. Then we can state our goal as finding policy parameters <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span> that maximize <span class="math notranslate nohighlight">\(q^\star\)</span> with respect to the actions chosen by our policy across the state space:</p>
<div class="math notranslate nohighlight">
\[
\max_{\boldsymbol{w}} q^*(s, d(s; \boldsymbol{w})) \quad \text{for all } s
\]</div>
<p>However, it’s computationally infeasible to satisfy this condition for every possible state <span class="math notranslate nohighlight">\(s\)</span>, especially in large or continuous state spaces. To address this, we assume a distribution of states, denoted <span class="math notranslate nohighlight">\(\mu(s)\)</span>, and take the expectation, leading to the problem:</p>
<div class="math notranslate nohighlight">
\[
\max_{\boldsymbol{w}} \mathbb{E}_{s \sim \mu(s)}[q^*(s, d(s; \boldsymbol{w}))]
\]</div>
<p>However in practice, we do not have access to <span class="math notranslate nohighlight">\(q^*\)</span>. Instead, we need to approximate <span class="math notranslate nohighlight">\(q^*\)</span> with a Q-function <span class="math notranslate nohighlight">\(q(s, a; \boldsymbol{\theta})\)</span>, parameterized by <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>, which we will learn simultaneously with the policy function <span class="math notranslate nohighlight">\(d(s; \boldsymbol{w})\)</span>. Given a samples of initial states drawn from <span class="math notranslate nohighlight">\(\mu\)</span>, we then maximize this objective via a Monte Carlo surrogate problem:</p>
<div class="math notranslate nohighlight">
\[
\max_{\boldsymbol{w}} \mathbb{E}_{s \sim \mu(s)}[q(s, d(s; \boldsymbol{w}); \boldsymbol{\theta})] \approx
\max_{\boldsymbol{w}} \frac{1}{|\mathcal{B}|} \sum_{s \in \mathcal{B}}  q(s, d(s; \boldsymbol{w}); \boldsymbol{\theta})
\]</div>
<p>When using neural networks to parametrize <span class="math notranslate nohighlight">\(q\)</span> and <span class="math notranslate nohighlight">\(d\)</span>, we obtain the Neural Fitted Q-Iteration with Continuous Actions (NFQCA) algorithm proposed by <span id="id1">[<a class="reference internal" href="bibliography.html#id40" title="Roland Hafner and Martin Riedmiller. Reinforcement learning in feedback control: challenges and benchmarks from technical process control. Machine Learning, 84(1-2):137-169, feb 2011. URL: http://dx.doi.org/10.1007/s10994-011-5235-x, doi:10.1007/s10994-011-5235-x.">19</a>]</span>.</p>
<div class="proof algorithm admonition" id="nfqca">
<p class="admonition-title"><span class="caption-number">Algorithm 20.1 </span> (Neural Fitted Q-Iteration with Continuous Actions (NFQCA))</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, base points <span class="math notranslate nohighlight">\(\mathcal{B}\)</span>, Q-function <span class="math notranslate nohighlight">\(q(s,a; \boldsymbol{\theta})\)</span>, policy <span class="math notranslate nohighlight">\(d(s; \boldsymbol{w})\)</span></p>
<p><strong>Output</strong> Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> for Q-function, <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span> for policy</p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{w}_0\)</span></p></li>
<li><p><strong>for</strong> <span class="math notranslate nohighlight">\(n = 0,1,2,...\)</span> <strong>do</strong></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}_q \leftarrow \emptyset\)</span></p></li>
<li><p>For each <span class="math notranslate nohighlight">\((s,a,r,s') \in \mathcal{B}\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(a'_{s'} \leftarrow d(s'; \boldsymbol{w}_n)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(y_{s,a} \leftarrow r + \gamma q(s', a'_{s'}; \boldsymbol{\theta}_n)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}_q \leftarrow \mathcal{D}_q \cup \{((s,a), y_{s,a})\}\)</span></p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{n+1} \leftarrow \texttt{fit}(\mathcal{D}_q)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{w}_{n+1} \leftarrow \texttt{minimize}_{\boldsymbol{w}} -\frac{1}{|\mathcal{B}|} \sum_{(s,a,r,s') \in \mathcal{B}} q(s, d(s; \boldsymbol{w}); \boldsymbol{\theta}_{n+1})\)</span></p></li>
</ol>
</li>
<li><p><strong>return</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{w}_n\)</span></p></li>
</ol>
</section>
</div><p>In practice, both the <code class="docutils literal notranslate"><span class="pre">fit</span></code> and <code class="docutils literal notranslate"><span class="pre">minimize</span></code> operations above are implemented using gradient descent. For the Q-function, the <code class="docutils literal notranslate"><span class="pre">fit</span></code> operation minimizes the mean squared error between the network’s predictions and the target values:</p>
<div class="math notranslate nohighlight">
\[
\texttt{fit}(\mathcal{D}_q) = \arg\min_{\boldsymbol{\theta}} \frac{1}{|\mathcal{D}_q|} \sum_{((s,a), y) \in \mathcal{D}_q} (q(s,a; \boldsymbol{\theta}) - y)^2
\]</div>
<p>For the policy update, the <code class="docutils literal notranslate"><span class="pre">minimize</span></code> operation uses gradient descent on the composition of the “critic” network <span class="math notranslate nohighlight">\(q\)</span> and the “actor” network <span class="math notranslate nohighlight">\(d\)</span>. This results in the following update rule:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{w}_{n+1} = \boldsymbol{w}_n + \alpha \nabla_{\boldsymbol{w}} \left(\frac{1}{|\mathcal{B}|} \sum_{(s,a,r,s') \in \mathcal{B}} q(s, d(s; \boldsymbol{w}); \boldsymbol{\theta}_{n+1})\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> is the learning rate. Both operations can be efficiently implemented using modern automatic differentiation libraries and stochastic gradient descent variants like Adam or RMSProp.</p>
</section>
<section id="deep-deterministic-policy-gradient-ddpg">
<h2><span class="section-number">20.2. </span>Deep Deterministic Policy Gradient (DDPG)<a class="headerlink" href="#deep-deterministic-policy-gradient-ddpg" title="Link to this heading">#</a></h2>
<p>Just as DQN adapted Neural Fitted Q-Iteration to the online setting, DDPG <span id="id2">[<a class="reference internal" href="bibliography.html#id42" title="Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.">28</a>]</span> extends NFQCA to learn from data collected online. Like NFQCA, DDPG simultaneously learns a Q-function and a deterministic policy that maximizes it, but differs in how it collects and processes data.</p>
<p>Instead of maintaining a fixed set of basepoints, DDPG uses a replay buffer that continuously stores new transitions as the agent interacts with the environment. Since the policy is deterministic, exploration becomes challenging. DDPG addresses this by adding noise to the policy’s actions during data collection:</p>
<div class="math notranslate nohighlight">
\[
a = d(s; \boldsymbol{w}) + \mathcal{N}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{N}\)</span> represents exploration noise, typically drawn from an Ornstein-Uhlenbeck process to generate temporally correlated exploration.</p>
<p>The policy gradient update follows the same principle as NFQCA:</p>
<div class="math notranslate nohighlight">
\[
\nabla_{\boldsymbol{w}} \mathbb{E}_{s \sim \mu(s)}[q(s, d(s; \boldsymbol{w}); \boldsymbol{\theta})]
\]</div>
<p>We then embed this exploration mechanism into the data collection procedure and use the same flattened FQI structure that we adopted in DQN. Similar to DQN, flattening the outer-inner optimization structure leads to the need for target networks - both for the Q-function and the policy.</p>
<div class="proof algorithm admonition" id="ddpg">
<p class="admonition-title"><span class="caption-number">Algorithm 20.2 </span> (Deep Deterministic Policy Gradient (DDPG))</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input</strong> MDP <span class="math notranslate nohighlight">\((S, A, P, R, \gamma)\)</span>, Q-network <span class="math notranslate nohighlight">\(q(s,a; \boldsymbol{\theta})\)</span>, policy network <span class="math notranslate nohighlight">\(d(s; \boldsymbol{w})\)</span>, learning rates <span class="math notranslate nohighlight">\(\alpha_q, \alpha_d\)</span>, replay buffer size <span class="math notranslate nohighlight">\(B\)</span>, mini-batch size <span class="math notranslate nohighlight">\(b\)</span>, target update frequency <span class="math notranslate nohighlight">\(K\)</span></p>
<p><strong>Initialize</strong></p>
<ol class="arabic simple">
<li><p>Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{w}_0\)</span> randomly</p></li>
<li><p>Target parameters: <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{target} \leftarrow \boldsymbol{\theta}_0\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{w}_{target} \leftarrow \boldsymbol{w}_0\)</span></p></li>
<li><p>Initialize replay buffer <span class="math notranslate nohighlight">\(\mathcal{R}\)</span> with capacity <span class="math notranslate nohighlight">\(B\)</span></p></li>
<li><p>Initialize exploration noise process <span class="math notranslate nohighlight">\(\mathcal{N}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow 0\)</span></p></li>
<li><p><strong>while</strong> training:</p>
<ol class="arabic simple">
<li><p>Observe current state <span class="math notranslate nohighlight">\(s\)</span></p></li>
<li><p>Select action with noise: <span class="math notranslate nohighlight">\(a = d(s; \boldsymbol{w}_n) + \mathcal{N}\)</span></p></li>
<li><p>Execute <span class="math notranslate nohighlight">\(a\)</span>, observe reward <span class="math notranslate nohighlight">\(r\)</span> and next state <span class="math notranslate nohighlight">\(s'\)</span></p></li>
<li><p>Store <span class="math notranslate nohighlight">\((s,a,r,s')\)</span> in <span class="math notranslate nohighlight">\(\mathcal{R}\)</span>, replacing oldest if full</p></li>
<li><p>Sample mini-batch of <span class="math notranslate nohighlight">\(b\)</span> transitions <span class="math notranslate nohighlight">\((s_i,a_i,r_i,s'_i)\)</span> from <span class="math notranslate nohighlight">\(\mathcal{R}\)</span></p></li>
<li><p>For each sampled transition:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(y_i \leftarrow r_i + \gamma q(s'_i, d(s'_i; \boldsymbol{w}_{target}); \boldsymbol{\theta}_{target})\)</span></p></li>
</ol>
</li>
<li><p>Update Q-network: <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{n+1} \leftarrow \boldsymbol{\theta}_n - \alpha_q \nabla_{\boldsymbol{\theta}} \frac{1}{b}\sum_i(y_i - q(s_i,a_i;\boldsymbol{\theta}_n))^2\)</span></p></li>
<li><p>Update policy: <span class="math notranslate nohighlight">\(\boldsymbol{w}_{n+1} \leftarrow \boldsymbol{w}_n + \alpha_d \frac{1}{b}\sum_i \nabla_a q(s_i,a;\boldsymbol{\theta}_{n+1})|_{a=d(s_i;\boldsymbol{w}_n)} \nabla_{\boldsymbol{w}} d(s_i;\boldsymbol{w}_n)\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(n \bmod K = 0\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{target} \leftarrow \boldsymbol{\theta}_n\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{w}_{target} \leftarrow \boldsymbol{w}_n\)</span></p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(n \leftarrow n + 1\)</span></p></li>
</ol>
</li>
</ol>
<p><strong>return</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_n\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{w}_n\)</span></p>
</section>
</div></section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="from-deterministic-to-stochastic-policies-soft-actor-critic">
<h1><span class="section-number">21. </span>From Deterministic to Stochastic Policies: Soft Actor-Critic<a class="headerlink" href="#from-deterministic-to-stochastic-policies-soft-actor-critic" title="Link to this heading">#</a></h1>
<p>Adapting the intuition of NFQCA to the smooth Bellman optimality equations leads us to the soft actor-critic algorithm <span id="id3">[<a class="reference internal" href="bibliography.html#id30" title="Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the 35th International Conference on Machine Learning (ICML), 1861–1870. PMLR, 2018.">18</a>]</span>. To understand this connection, let’s first examine how the smooth Bellman equations emerge naturally from entropy regularization.</p>
<p>Consider the standard Bellman operator augmented with an entropy term. The smooth Bellman operator <span class="math notranslate nohighlight">\(\mathrm{L}_\beta\)</span> takes the form:</p>
<div class="math notranslate nohighlight">
\[
(\mathrm{L}_\beta v)(s) = \max_{d \in D^{MR}}\{\mathbb{E}_{a \sim d}[r(s,a) + \gamma v(s')] + \beta\mathcal{H}(d)\}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{H}(d) = -\mathbb{E}_{a \sim d}[\log d(a|s)]\)</span> represents the entropy of the policy. To find the solution to the optimization problem embedded in the operator <span class="math notranslate nohighlight">\(\mathrm{L}_\beta\)</span>, we set the functional derivative of the objective with respect to the decision rule to zero:</p>
<div class="math notranslate nohighlight">
\[
\frac{\delta}{\delta d(a|s)} \left[\int_A d(a|s)(r(s,a) + \gamma v(s'))da - \beta\int_A d(a|s)\log d(a|s)da \right] = 0
\]</div>
<p>Enforcing that <span class="math notranslate nohighlight">\(\int_A d(a|s)da = 1\)</span> leads to the following Lagrangian:</p>
<div class="math notranslate nohighlight">
\[
r(s,a) + \gamma v(s') - \beta(1 + \log d(a|s)) - \lambda(s) = 0
\]</div>
<p>Solving for <span class="math notranslate nohighlight">\(d\)</span> shows that the optimal policy is a Boltzmann distribution</p>
<div class="math notranslate nohighlight">
\[
d^*(a|s) = \frac{\exp(\frac{1}{\beta}(r(s,a) + \gamma \mathbb{E}_{s'}[v(')]))}{Z(s)}
\]</div>
<p>When we substitute this optimal policy back into the entropy-regularized objective, we obtain:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
v(s) &amp;= \mathbb{E}_{a \sim d^*}[r(s,a) + \gamma v(s')] + \beta\mathcal{H}(d^*) \\
&amp;= \beta \log \int_A \exp(\frac{1}{\beta}(r(s,a) + \gamma \mathbb{E}_{s'}[v(s')]))da
\end{align*}
\end{split}\]</div>
<p>As we saw at the beginning of this chapter, the smooth Bellman optimality operator for Q-factors is defined as:</p>
<div class="math notranslate nohighlight">
\[
(\mathrm{L}_\beta q)(s,a) = r(s,a) + \gamma \mathbb{E}_{s'}\left[\beta \log \int_A \exp(\frac{1}{\beta}q(s',a'))da'\right]
\]</div>
<p>This operator maintains the contraction property of its standard counterpart, guaranteeing a unique fixed point <span class="math notranslate nohighlight">\(q^*\)</span>. The optimal policy takes the form:</p>
<div class="math notranslate nohighlight">
\[
d^*(a|s) = \frac{\exp(\frac{1}{\beta}q^*(s,a))}{Z(s)}
\]</div>
<p>where <span class="math notranslate nohighlight">\(Z(s) = \int_A \exp(\frac{1}{\beta}q^*(s,a))da\)</span>. The optimal value function can be recovered as:</p>
<div class="math notranslate nohighlight">
\[
v^*(s) = \beta \log \int_A \exp(\frac{1}{\beta}q^*(s,a))da
\]</div>
<section id="fitted-q-iteration-for-the-smooth-bellman-equations">
<h2><span class="section-number">21.1. </span>Fitted Q-Iteration for the Smooth Bellman Equations<a class="headerlink" href="#fitted-q-iteration-for-the-smooth-bellman-equations" title="Link to this heading">#</a></h2>
<p>Following the principles of fitted value iteration, we can approximate approximate the effect of the smooth Bellman operator by computing it exactly at a number of basepoints and generalizing elsewhere using function approximation. Concretely, given a collection of states <span class="math notranslate nohighlight">\(s_i\)</span> and actions <span class="math notranslate nohighlight">\(a_i\)</span>, we would compute regression target values:</p>
<div class="math notranslate nohighlight">
\[
y_i = r(s_i,a_i) + \gamma \mathbb{E}_{s'}\left[\beta \log \int_A \exp(\frac{1}{\beta}q_\theta(s',a'))da'\right]
\]</div>
<p>and fit our Q-function approximator by minimizing:</p>
<div class="math notranslate nohighlight">
\[
\min_\theta \sum_i (q_\theta(s_i,a_i) - y_i)^2
\]</div>
<p>The expectation over next states can be handled through Monte Carlo estimation using samples from the environment: given a transition <span class="math notranslate nohighlight">\((s_i,a_i,s'_i)\)</span>, we can approximate:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}_{s'}\left[\beta \log \int_A \exp(\frac{1}{\beta}q_\theta(s',a'))da'\right] \approx \beta \log \int_A \exp(\frac{1}{\beta}q_\theta(s'_i,a'))da'
\]</div>
<p>However, we still face the challenge of computing the integral over actions. This motivates maintaining separate function approximators for both Q and V, using samples from the current policy to estimate the value function:</p>
<div class="math notranslate nohighlight">
\[
v_\psi(s) \approx \mathbb{E}_{a \sim d(\cdot|s;\phi)}\left[q_\theta(s,a) - \beta \log d(a|s;\phi)\right]
\]</div>
<p>By maintaining both approximators, we can estimate targets using sampled actions from our policy. Specifically, if we have a transition <span class="math notranslate nohighlight">\((s_i,a_i,s'_i)\)</span> and sample <span class="math notranslate nohighlight">\(a'_i \sim d(\cdot|s'_i;\phi)\)</span>, our target becomes:</p>
<div class="math notranslate nohighlight">
\[
y_i = r(s_i,a_i) + \gamma\left(q_\theta(s'_i,a'_i) - \beta \log d(a'_i|s'_i;\phi)\right)
\]</div>
<p>This is a remarkable idea! One that exists only due to the dual representation of the smooth Bellman equations as an entropy-regularized problem which transforms the intractable log-sum-exp into a form we can estimate efficiently through sampling.</p>
</section>
<section id="approximating-boltzmann-policies-by-gaussians">
<h2><span class="section-number">21.2. </span>Approximating Boltzmann Policies by Gaussians<a class="headerlink" href="#approximating-boltzmann-policies-by-gaussians" title="Link to this heading">#</a></h2>
<p>The entropy-regularized objective and the smooth Bellman equation are mathematically equivalent. However, both formulations face a practical challenge: they require evaluating an intractable integral due to the Boltzmann distribution. Soft Actor-Critic (SAC) addresses this problem by approximating the optimal policy with a simpler, more tractable Gaussian distribution. Given the optimal soft policy:</p>
<div class="math notranslate nohighlight">
\[
d^*(a|s) = \frac{\exp(\frac{1}{\beta}q^*(s,a))}{Z(s)}
\]</div>
<p>we seek to approximate it with a Gaussian policy:</p>
<div class="math notranslate nohighlight">
\[
d(a|s;\phi) = \mathcal{N}(\mu_\phi(s), \sigma_\phi(s))
\]</div>
<p>This approximation task naturally raises the question of how to measure the “closeness” between the target Boltzmann distribution and a candidate Gaussian approximation. Following common practice in deep learning, we employ the Kullback-Leibler (KL) divergence as our measure of distributional distance. To find the best approximation, we minimize the KL divergence between our policy and the optimal policy, using our current estimate <span class="math notranslate nohighlight">\(q_\theta\)</span> of <span class="math notranslate nohighlight">\(q^*\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\operatorname{minimize}_{\phi} \mathbb{E}_{s \sim \mu(s)}\left[D_{KL}\left(d(\cdot|s;\phi) \| \frac{\exp(\frac{1}{\beta}q_\theta(s,\cdot))}{Z(s)}\right)\right]
\]</div>
<p>However, an important question remains: how can we solve this optimization problem when it involves the intractable partition function <span class="math notranslate nohighlight">\(Z(s)\)</span>? To see this, recall that for two distributions p and q, the KL divergence takes the form <span class="math notranslate nohighlight">\(D_{KL}(p\|q) = \mathbb{E}_{x \sim p}[\log p(x) - \log q(x)]\)</span>. Let’s denote the target Boltzmann distribution based on our current Q-estimate as:</p>
<div class="math notranslate nohighlight">
\[
d_\theta(a|s) = \frac{\exp(\frac{1}{\beta}q_\theta(s,a))}{Z_\theta(s)}
\]</div>
<p>Then the KL minimization becomes:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
D_{KL}(d(\cdot|s;\phi)\|d_\theta) &amp;= \mathbb{E}_{a \sim d(\cdot|s;\phi)}[\log d(a|s;\phi) - \log d_\theta(a|s)] \\
&amp;= \mathbb{E}_{a \sim d(\cdot|s;\phi)}\left[\log d(a|s;\phi) - \log \left(\frac{\exp(\frac{1}{\beta}q_\theta(s,a))}{Z_\theta(s)}\right)\right] \\
&amp;= \mathbb{E}_{a \sim d(\cdot|s;\phi)}\left[\log d(a|s;\phi) - \frac{1}{\beta}q_\theta(s,a) + \log Z_\theta(s)\right]
\end{align*}
\end{split}\]</div>
<p>Since <span class="math notranslate nohighlight">\(\log Z(s)\)</span> is constant with respect to <span class="math notranslate nohighlight">\(\phi\)</span>, minimizing this KL divergence is equivalent to:</p>
<div class="math notranslate nohighlight">
\[
\operatorname{minimize}_{\phi} \mathbb{E}_{s \sim \mu(s)}\mathbb{E}_{a \sim d(\cdot|s;\phi)}[\log d(a|s;\phi) - \frac{1}{\beta}q_\theta(s,a)]
\]</div>
</section>
<section id="reparameterizating-the-objective">
<h2><span class="section-number">21.3. </span>Reparameterizating the Objective<a class="headerlink" href="#reparameterizating-the-objective" title="Link to this heading">#</a></h2>
<p>One last challenge remains: <span class="math notranslate nohighlight">\(\phi\)</span> appears in the distribution underlying the inner expectation, not just in the integrand. This setting departs from standard empirical risk minimization (ERM) in supervised learning where the distribution of the data (e.g., cats and dogs in image classification) remains fixed regardless of model parameters. Here, however, the “data” - our sampled actions - depends directly on the parameters <span class="math notranslate nohighlight">\(\phi\)</span> we’re trying to optimize.</p>
<p>This dependence prevents us from simply using sample average estimators and differentiating through them, as we typically do in supervised learning. The challenge of correctly and efficiently estimating such derivatives has been extensively studied in the simulation literature under the umbrella of “derivative estimation.” SAC adopts a particular solution known as the reparameterization trick in deep learning (or the IPA estimator in simulation literature). This approach transforms the problem by pushing <span class="math notranslate nohighlight">\(\phi\)</span> inside the expectation through a change of variables.</p>
<p>To address this, we can express our Gaussian policy through a deterministic function <span class="math notranslate nohighlight">\(f_\phi\)</span> that transforms noise samples to actions:</p>
<div class="math notranslate nohighlight">
\[
f_\phi(s,\epsilon) = \mu_\phi(s) + \sigma_\phi(s)\epsilon, \quad \epsilon \sim \mathcal{N}(0,1)
\]</div>
<p>This transformation allows us to rewrite our objective using an expectation over the fixed noise distribution:</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
&amp;\mathbb{E}_{s \sim \mu(s)}\mathbb{E}_{\epsilon \sim \mathcal{N}(0,1)}[\log d(f_\phi(s,\epsilon)|s;\phi) - \frac{1}{\beta}q_\theta(s,f_\phi(s,\epsilon))]
\end{align*}
\]</div>
<p>Now <span class="math notranslate nohighlight">\(\phi\)</span> appears only in the integrand through the function <span class="math notranslate nohighlight">\(f_\phi\)</span>, not in the sampling distribution. The objective involves two terms. First, the log-probability of our Gaussian policy has a simple closed form:</p>
<div class="math notranslate nohighlight">
\[
\log d(f_\phi(s,\epsilon)|s;\phi) = -\frac{1}{2}\log(2\pi\sigma_\phi(s)^2) - \frac{(f_\phi(s,\epsilon)-\mu_\phi(s))^2}{2\sigma_\phi(s)^2}
\]</div>
<p>Second, <span class="math notranslate nohighlight">\(\phi\)</span> enters through the composition of <span class="math notranslate nohighlight">\(q^\star\)</span> with <span class="math notranslate nohighlight">\(f_\phi\)</span>: <span class="math notranslate nohighlight">\(q^\star(s,f_\phi(s,\epsilon))\)</span>. The chain rule for this composition would involve derivatives of both functions. While this might be problematic if the Q-factors were to come from outside of our control (ie. not in the computational graph), but since SAC learns it simultaneously with the policy, then we can simply compute all required derivatives through automatic differentiation.</p>
<p>This composition of policy and value functions - where <span class="math notranslate nohighlight">\(f_\phi\)</span> enters as input to <span class="math notranslate nohighlight">\(q_\theta\)</span> - directly parallels the structure we encountered in deterministic policy methods like NFQCA and DDPG. In those methods, we optimized:</p>
<div class="math notranslate nohighlight">
\[
\max_{\phi} \mathbb{E}_{s \sim \mu(s)}[q_\theta(s, f_\phi(s))]
\]</div>
<p>where <span class="math notranslate nohighlight">\(f_\phi(s)\)</span> was a deterministic policy. SAC extends this idea to stochastic policies by having <span class="math notranslate nohighlight">\(f_\phi\)</span> transform both state and noise:</p>
<div class="math notranslate nohighlight">
\[
\max_{\phi} \mathbb{E}_{s \sim \mu(s)}\mathbb{E}_{\epsilon \sim \mathcal{N}(0,1)}[q_\theta(s,f_\phi(s,\epsilon))]
\]</div>
<p>Thus, rather than learning a single action for each state as in DDPG, we learn a function that transforms random noise into actions, explicitly parameterizing a distribution over actions while maintaining the same underlying principle of differentiating through composed policy and value functions.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="adp.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">13. </span>Approximate Dynamic Programming</p>
      </div>
    </a>
    <a class="right-next"
       href="bibliography.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">22. </span>Bibliography</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">18. Policy Parametrization Methods</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#embedded-optimization">19. Embedded Optimization</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#amortized-optimization-approach">19.1. Amortized Optimization Approach</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#deterministic-parametrized-policies">20. Deterministic Parametrized Policies</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-fitted-q-iteration-for-continuous-actions-hafner-et-al-2011">20.1. Neural Fitted Q-iteration for Continuous Actions (Hafner et al. 2011)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-deterministic-policy-gradient-ddpg">20.2. Deep Deterministic Policy Gradient (DDPG)</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#from-deterministic-to-stochastic-policies-soft-actor-critic">21. From Deterministic to Stochastic Policies: Soft Actor-Critic</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitted-q-iteration-for-the-smooth-bellman-equations">21.1. Fitted Q-Iteration for the Smooth Bellman Equations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#approximating-boltzmann-policies-by-gaussians">21.2. Approximating Boltzmann Policies by Gaussians</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reparameterizating-the-objective">21.3. Reparameterizating the Objective</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Pierre-Luc Bacon
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>