<!DOCTYPE html><html lang="en" class="" style="scroll-padding:60px"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width,initial-scale=1"/><title>Why This Book? - Reinforcement Learning Beyond the Agent Loop: Models, Control, and Data</title><meta property="og:title" content="Why This Book? - Reinforcement Learning Beyond the Agent Loop: Models, Control, and Data"/><meta name="generator" content="mystmd"/><meta name="description" content="A graduate-level introduction to reinforcement learning as a framework for modeling, optimization, and control, connecting dynamic models, data, and applications beyond standard benchmarks."/><meta property="og:description" content="A graduate-level introduction to reinforcement learning as a framework for modeling, optimization, and control, connecting dynamic models, data, and applications beyond standard benchmarks."/><meta name="keywords" content=""/><link rel="stylesheet" href="/rlbook/build/_assets/app-5WKS5EPQ.css"/><link rel="stylesheet" href="/rlbook/build/_assets/thebe-core-VKVHG5VY.css"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jupyter-matplotlib@0.11.3/css/mpl_widget.css"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.css"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous"/><link rel="icon" href="/rlbook/favicon.ico"/><link rel="stylesheet" href="/rlbook/myst-theme.css"/><script>
  const savedTheme = localStorage.getItem("myst:theme");
  const theme = window.matchMedia("(prefers-color-scheme: light)").matches ? 'light' : 'dark';
  const classes = document.documentElement.classList;
  const hasAnyTheme = classes.contains('light') || classes.contains('dark');
  if (!hasAnyTheme) classes.add(savedTheme ?? theme);
</script></head><body class="m-0 transition-colors duration-500 bg-white dark:bg-stone-900"><div class="fixed top-1 left-1 h-[0px] w-[0px] focus-within:z-40 focus-within:h-auto focus-within:w-auto bg-white overflow-hidden focus-within:p-2 focus-within:ring-1" aria-label="skip to content options"><a href="#skip-to-frontmatter" class="block px-2 py-1 text-black underline">Skip to article frontmatter</a><a href="#skip-to-article" class="block px-2 py-1 text-black underline">Skip to article content</a></div><div class="bg-white/80 backdrop-blur dark:bg-stone-900/80 shadow dark:shadow-stone-700 p-3 md:px-8 sticky w-screen top-0 z-30 h-[60px]"><nav class="flex items-center justify-between flex-nowrap max-w-[1440px] mx-auto"><div class="flex flex-row xl:min-w-[19.5rem] mr-2 sm:mr-7 justify-start items-center shrink-0"><div class="block xl:hidden"><button class="flex items-center border-stone-400 text-stone-800 hover:text-stone-900 dark:text-stone-200 hover:dark:text-stone-100"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" width="2rem" height="2rem" class="m-1"><path fill-rule="evenodd" d="M3 6.75A.75.75 0 0 1 3.75 6h16.5a.75.75 0 0 1 0 1.5H3.75A.75.75 0 0 1 3 6.75ZM3 12a.75.75 0 0 1 .75-.75h16.5a.75.75 0 0 1 0 1.5H3.75A.75.75 0 0 1 3 12Zm0 5.25a.75.75 0 0 1 .75-.75h16.5a.75.75 0 0 1 0 1.5H3.75a.75.75 0 0 1-.75-.75Z" clip-rule="evenodd"></path></svg><span class="sr-only">Open Menu</span></button></div><a class="flex items-center ml-3 dark:text-white w-fit md:ml-5 xl:ml-7" href="/rlbook/"><div class="p-1 mr-3 dark:bg-white dark:rounded"><img src="/rlbook/build/logo-439a25d1a718f5f15fe2838e2a61a91a.png" class="h-9" alt="RL &amp; Control" height="2.25rem"/></div><span class="text-md sm:text-xl tracking-tight sm:mr-5">RL &amp; Control</span></a></div><div class="flex items-center flex-grow w-auto"><div class="flex-grow hidden text-md lg:block"></div><div class="flex-grow block"></div><button type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:R74op:" data-state="closed" class="flex items-center h-10 aspect-square sm:w-64 text-left text-gray-400 border border-gray-300 dark:border-gray-600 rounded-lg bg-gray-50 dark:bg-gray-700 hover:ring-blue-500 dark:hover:ring-blue-500 hover:border-blue-500 dark:hover:border-blue-500"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="p-2.5 h-10 w-10 aspect-square"><path fill-rule="evenodd" d="M10.5 3.75a6.75 6.75 0 1 0 0 13.5 6.75 6.75 0 0 0 0-13.5ZM2.25 10.5a8.25 8.25 0 1 1 14.59 5.28l4.69 4.69a.75.75 0 1 1-1.06 1.06l-4.69-4.69A8.25 8.25 0 0 1 2.25 10.5Z" clip-rule="evenodd"></path></svg><span class="hidden sm:block grow">Search</span><div aria-hidden="true" class="items-center hidden mx-1 font-mono text-sm text-gray-400 sm:flex gap-x-1"><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none hide-mac">CTRL</kbd><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none show-mac">⌘</kbd><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none ">K</kbd><script>
;(() => {
const script = document.currentScript;
const root = script.parentElement;

const isMac = /mac/i.test(
      window.navigator.userAgentData?.platform ?? window.navigator.userAgent,
    );
root.querySelectorAll(".hide-mac").forEach(node => {node.classList.add(isMac ? "hidden" : "block")});
root.querySelectorAll(".show-mac").forEach(node => {node.classList.add(!isMac ? "hidden" : "block")});
})()</script></div></button><button class="theme rounded-full aspect-square border border-stone-700 dark:border-white hover:bg-neutral-100 border-solid overflow-hidden text-stone-700 dark:text-white hover:text-stone-500 dark:hover:text-neutral-800 w-8 h-8 mx-3" title="Toggle theme between light and dark mode" aria-label="Toggle theme between light and dark mode"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="h-full w-full p-0.5 hidden dark:block"><path fill-rule="evenodd" d="M9.528 1.718a.75.75 0 0 1 .162.819A8.97 8.97 0 0 0 9 6a9 9 0 0 0 9 9 8.97 8.97 0 0 0 3.463-.69.75.75 0 0 1 .981.98 10.503 10.503 0 0 1-9.694 6.46c-5.799 0-10.5-4.7-10.5-10.5 0-4.368 2.667-8.112 6.46-9.694a.75.75 0 0 1 .818.162Z" clip-rule="evenodd"></path></svg><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-full w-full p-0.5 dark:hidden"><path stroke-linecap="round" stroke-linejoin="round" d="M12 3v2.25m6.364.386-1.591 1.591M21 12h-2.25m-.386 6.364-1.591-1.591M12 18.75V21m-4.773-4.227-1.591 1.591M5.25 12H3m4.227-4.773L5.636 5.636M15.75 12a3.75 3.75 0 1 1-7.5 0 3.75 3.75 0 0 1 7.5 0Z"></path></svg></button><div class="block sm:hidden"><div class="relative" data-headlessui-state=""><div><button class="flex text-sm bg-transparent rounded-full focus:outline-none" id="headlessui-menu-button-:Rr4op:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open Menu</span><div class="flex items-center text-stone-200 hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" width="2rem" height="2rem" class="p-1"><path fill-rule="evenodd" d="M10.5 6a1.5 1.5 0 1 1 3 0 1.5 1.5 0 0 1-3 0Zm0 6a1.5 1.5 0 1 1 3 0 1.5 1.5 0 0 1-3 0Zm0 6a1.5 1.5 0 1 1 3 0 1.5 1.5 0 0 1-3 0Z" clip-rule="evenodd"></path></svg></div></button></div></div></div><div class="hidden sm:block"><a href="https://github.com/pierrelux/rlbook" target="_blank" rel="noopener noreferrer" class="inline-block px-4 py-2 mx-1 mt-0 leading-none border rounded text-md border-stone-700 dark:border-white text-stone-700 dark:text-white hover:text-stone-500 dark:hover:text-neutral-800 hover:bg-neutral-100">View on GitHub</a></div></div></nav></div><div class="fixed xl:article-grid grid-gap xl:w-screen xl:pointer-events-none overflow-auto max-xl:min-w-[300px] hidden z-10" style="top:60px"><div class="pointer-events-auto xl:col-margin-left flex-col overflow-hidden hidden xl:flex"><div class="flex-grow py-6 overflow-y-auto primary-scrollbar"><nav aria-label="Navigation" class="overflow-y-hidden transition-opacity ml-3 xl:ml-0 mr-3 max-w-[350px] lg:hidden"><div class="w-full px-1 dark:text-white font-medium"></div></nav><div class="my-3 border-b-2 lg:hidden"></div><nav aria-label="Table of Contents" class="flex-grow overflow-y-hidden transition-opacity ml-3 xl:ml-0 mr-3 max-w-[350px]"><div class="w-full px-1 dark:text-white"><a title="Reinforcement Learning Beyond the Agent Loop: Models, Control, and Data" class="block break-words focus:outline outline-blue-200 outline-2 rounded p-2 my-1 rounded-lg hover:bg-slate-300/30 font-bold" href="/rlbook/">Reinforcement Learning Beyond the Agent Loop: Models, Control, and Data</a><div data-state="closed" class="w-full"><div class="flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="Modeling" class="block break-words rounded py-2 grow cursor-pointer">Modeling</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:Rmp8p:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:Rmp8p:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="closed" class="w-full"><div class="flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="Numerical Trajectory Optimization" class="block break-words rounded py-2 grow cursor-pointer">Numerical Trajectory Optimization</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:Rup8p:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:Rup8p:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="closed" class="w-full"><div class="flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="From Trajectories to Policies" class="block break-words rounded py-2 grow cursor-pointer">From Trajectories to Policies</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:R16p8p:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:R16p8p:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="closed" class="w-full"><div class="flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="Approximate Dynamic Programming" class="block break-words rounded py-2 grow cursor-pointer">Approximate Dynamic Programming</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:R1ep8p:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:R1ep8p:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="closed" class="w-full"><div class="flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="Appendix" class="block break-words rounded py-2 grow cursor-pointer">Appendix</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:R1mp8p:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:R1mp8p:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="closed" class="w-full"><div class="flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="References" class="block break-words rounded py-2 grow cursor-pointer">References</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:R1up8p:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:R1up8p:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div></div></nav></div><div class="flex-none py-6 transition-all duration-700 translate-y-6 opacity-0"><a class="flex mx-auto text-gray-700 w-fit hover:text-blue-700 dark:text-gray-200 dark:hover:text-blue-400" href="https://mystmd.org/made-with-myst" target="_blank" rel="noreferrer"><svg style="width:24px;height:24px" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100" stroke="none"><g id="icon"><path fill="currentColor" d="M23.8,54.8v-3.6l4.7-0.8V17.5l-4.7-0.8V13H36l13.4,31.7h0.2l13-31.7h12.6v3.6l-4.7,0.8v32.9l4.7,0.8v3.6h-15
          v-3.6l4.9-0.8V20.8H65L51.4,53.3h-3.8l-14-32.5h-0.1l0.2,17.4v12.1l5,0.8v3.6H23.8z"></path><path fill="#F37726" d="M47,86.9c0-5.9-3.4-8.8-10.1-8.8h-8.4c-5.2,0-9.4-1.3-12.5-3.8c-3.1-2.5-5.4-6.2-6.8-11l4.8-1.6
          c1.8,5.6,6.4,8.6,13.8,8.8h9.2c6.4,0,10.8,2.5,13.1,7.5c2.3-5,6.7-7.5,13.1-7.5h8.4c7.8,0,12.7-2.9,14.6-8.7l4.8,1.6
          c-1.4,4.9-3.6,8.6-6.8,11.1c-3.1,2.5-7.3,3.7-12.4,3.8H63c-6.7,0-10,2.9-10,8.8"></path></g></svg><span class="self-center ml-2 text-sm">Made with MyST</span></a></div></div></div><main class="article-grid grid-gap"><article class="article-grid subgrid-gap col-screen article content"><div class="hidden"></div><div id="skip-to-frontmatter" aria-label="article frontmatter" class="mb-8 pt-9"><div class="flex items-center h-6 mb-5 text-sm font-light"><div class="flex-grow"></div><a href="https://github.com/pierrelux/rlbook" title="GitHub Repository: pierrelux/rlbook" target="_blank" rel="noopener noreferrer" class="text-inherit hover:text-inherit"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="1.25rem" height="1.25rem" class="inline-block mr-1 opacity-60 hover:opacity-100"><path d="M12 2.5c-5.4 0-9.8 4.4-9.8 9.7 0 4.3 2.8 8 6.7 9.2.5.1.7-.2.7-.5v-1.8c-2.4.5-3.1-.6-3.3-1.1-.1-.3-.6-1.1-1-1.4-.3-.2-.8-.6 0-.6s1.3.7 1.5 1c.9 1.5 2.3 1.1 2.8.8.1-.6.3-1.1.6-1.3-2.2-.2-4.4-1.1-4.4-4.8 0-1.1.4-1.9 1-2.6-.1-.2-.4-1.2.1-2.6 0 0 .8-.3 2.7 1 .8-.2 1.6-.3 2.4-.3.8 0 1.7.1 2.4.3 1.9-1.3 2.7-1 2.7-1 .5 1.3.2 2.3.1 2.6.6.7 1 1.5 1 2.6 0 3.7-2.3 4.6-4.4 4.8.4.3.7.9.7 1.8V21c0 .3.2.6.7.5 3.9-1.3 6.6-4.9 6.6-9.2 0-5.4-4.4-9.8-9.8-9.8z"></path></svg></a><a href="https://github.com/pierrelux/rlbook/edit/main/intro.md" title="Edit This Page" target="_blank" rel="noopener noreferrer" class="text-inherit hover:text-inherit"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.25rem" height="1.25rem" class="inline-block mr-1 opacity-60 hover:opacity-100"><path stroke-linecap="round" stroke-linejoin="round" d="m16.862 4.487 1.687-1.688a1.875 1.875 0 1 1 2.652 2.652L10.582 16.07a4.5 4.5 0 0 1-1.897 1.13L6 18l.8-2.685a4.5 4.5 0 0 1 1.13-1.897l8.932-8.931Zm0 0L19.5 7.125M18 14v4.75A2.25 2.25 0 0 1 15.75 21H5.25A2.25 2.25 0 0 1 3 18.75V8.25A2.25 2.25 0 0 1 5.25 6H10"></path></svg></a><div class="relative flex inline-block mx-1 grow-0" data-headlessui-state=""><button class="relative ml-2 -mr-1" id="headlessui-menu-button-:Rs8top:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Downloads</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.25rem" height="1.25rem"><title>Download</title><path stroke-linecap="round" stroke-linejoin="round" d="M3 16.5v2.25A2.25 2.25 0 0 0 5.25 21h13.5A2.25 2.25 0 0 0 21 18.75V16.5M16.5 12 12 16.5m0 0L7.5 12m4.5 4.5V3"></path></svg></button></div></div><h1 class="mb-0">Why This Book?</h1><header class="mt-4 not-prose"><div><span class="font-semibold text-sm inline-block"><button class="focus:shadow-[0_0_0_2px] focus:shadow-black outline-none hover:underline" aria-label="Author Details" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:R78top:" data-state="closed">Pierre-Luc Bacon</button></span></div></header></div><div class="block my-10 lg:sticky lg:z-10 lg:h-0 lg:pt-0 lg:my-0 lg:ml-10 lg:col-margin-right" style="top:60px"><nav></nav></div><div id="skip-to-article"></div><p>Reinforcement learning often captures headlines with breakthroughs: AlphaGo defeating world champions, agents mastering Atari, chatbots engaging millions in conversation. Yet compared to other branches of machine learning—especially supervised learning, which has become routine in industry and research—RL has not yet achieved the same widespread adoption in everyday decision-making and operations.</p><p>For perspective, consider supervised learning. Standardized tools like scikit-learn, TensorFlow, or PyTorch have made it straightforward for data scientists to integrate classification and regression models into production workflows. In medicine, hundreds of FDA-approved devices now incorporate supervised machine learning. Randomized controlled trials regularly evaluate diagnostic and predictive models built with supervised learning techniques.</p><p>By contrast, reinforcement learning’s adoption is still in earlier stages. Among more than a thousand FDA-cleared AI-enabled medical devices as of 2025, none explicitly reference reinforcement learning in their public summaries <span class="cite-group parenthetical"><cite class="" data-state="closed"><a href="https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-and-machine-learning-aiml-enabled-medical-devices" target="_blank" rel="noreferrer" class="hover-link">U.S. Food and Drug Administration, 2025</a></cite></span>. A recent <em>Lancet Digital Health</em> review of 86 clinical trials involving AI identified only two studies that tested RL-based decision rules <span class="cite-group parenthetical"><cite class="" data-state="closed"><span class="hover-link">Kleijnen &amp; others, 2024</span></cite></span>. Most AI solutions in healthcare remain supervised learning models trained on labeled datasets.</p><p>Outside healthcare, promising results exist, but scale remains modest. In building automation, a 2025 survey identified many field demonstrations of reinforcement learning and Model Predictive Control (MPC) for HVAC control. Yet, fewer than a third met basic methodological criteria. Among reliable studies, average cost reductions were around 13–16% <span class="cite-group parenthetical"><cite class="" data-state="closed"><a href="https://example.com/hvac-rl-review" target="_blank" rel="noreferrer" class="hover-link">Chen &amp; others, 2025</a></cite></span>. This is encouraging, yet still short of broad, industry-wide adoption.</p><p>Even in high-profile reinforcement learning deployments, adoption at scale is sometimes uncertain or difficult to confirm. Google DeepMind famously reported significant cooling-energy reductions in data centers using RL back in 2018 <span class="cite-group parenthetical"><cite class="" data-state="closed"><a href="https://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill" target="_blank" rel="noreferrer" class="hover-link">Evans &amp; DeepMind, 2018</a></cite></span>. However, more recent public confirmation of widespread autonomous use has been lacking. Meta, in a 2024 engineering blog, provided clear evidence that an RL-based airflow controller was achieving meaningful reductions in energy and water usage, and stated that broader deployment was underway, though specifics on scale were not disclosed <span class="cite-group parenthetical"><cite class="" data-state="closed"><a href="https://engineering.fb.com/2024/09/10/data-center/rl-sustainable-cooling" target="_blank" rel="noreferrer" class="hover-link">Meta AI, 2024</a></cite></span>.</p><p>Nevertheless, notable successes continue to emerge. Uber has successfully integrated reinforcement learning into its ride-matching system, with measurable improvements across hundreds of cities <span class="cite-group parenthetical"><cite class="" data-state="closed"><a href="https://www.uber.com/blog/rl-ride-matching" target="_blank" rel="noreferrer" class="hover-link">Uber AI Labs, 2025</a></cite></span>. ANYbotics has commercially deployed quadruped robots whose locomotion policies, trained entirely via RL in simulation, now reliably perform complex industrial inspections <span class="cite-group parenthetical"><cite class="" data-state="closed"><a href="https://www.anybotics.com/anymal-inspection" target="_blank" rel="noreferrer" class="hover-link">ANYbotics, 2023</a></cite></span>.</p><p>These examples illustrate genuine progress. Yet reinforcement learning has not yet reached the “plug-and-play” status enjoyed by supervised learning. This is largely due to fundamental differences in problem structure. Supervised learning tasks typically involve well-defined inputs, outputs, and objective metrics. Reinforcement learning, by contrast, requires explicit problem formulation, exploration, interactive data collection, and a nuanced understanding of the environment’s structure.</p><p>As <span class="cite-group narrative"><cite class="" data-state="closed"><a href="https://doi.org/10.1093/ectj/utaa019" target="_blank" rel="noreferrer" class="hover-link">Iskhakov <em>et al.</em> (2020)</a></cite></span> notes in econometrics, a primary challenge facing adoption of any sequential decision-making tool is:</p><blockquote><p><em>“The difficulty of learning about the objective function and environment facing real-world decision-makers.”</em></p></blockquote><p>In reinforcement learning, this difficulty is integral. We cannot sidestep defining the problem, the objective, and the constraints; these are not incidental but central. Supervised learning allows practitioners to abstract away most of these concerns into standardized data formats and evaluation metrics. Reinforcement learning practitioners do not have this luxury.</p><p>That is where this book begins.</p><p>I did not fully appreciate this difference as a PhD student. Like many others trained in machine learning, I focused on tuning algorithms, chasing benchmarks, and climbing leaderboards. Problem definition was abstracted away, often assumed or left as someone else’s responsibility.</p><p>Working in industry and consulting changed that. Real problems rarely fit neatly into a predefined framework. Sensors produce noisy data; constraints are non-negotiable; objectives may shift or conflict. I discovered firsthand that most effort goes into formulating the decision problem, long before selecting an algorithm.</p><p>John Rust captures this precisely when reflecting on dynamic programming in practice:</p><blockquote><p><em>“The range of known real-world applications of Dynamic Programming seems disappointingly small, given the immense computer power and decades of research that have produced a myriad of alternative solution methods for DP problems. I believe the biggest constraint on progress is not limited computer power, but instead the difficulty of learning the underlying structure of the decision problem.”</em> <span class="cite-group narrative"><cite class="" data-state="closed"><a href="https://doi.org/10.1016/s1574-0021(96)01016-7" target="_blank" rel="noreferrer" class="hover-link">Rust (1996)</a></cite></span></p></blockquote><p>In other words, solving a real-world decision problem starts with formulating it correctly. What are we optimizing? What can we observe and control? How does information flow? These questions are not secondary details; they define the problem itself.</p><p>The chapters that follow address these challenges explicitly. They offer strategies to bridge the gap from theoretical reinforcement learning formulations to practically useful systems. By carefully structuring decision problems, we can help reinforcement learning achieve broader impact, just as supervised learning already has.</p><h2 id="what-problem-are-we-solving" class="relative group"><span class="heading-text">What Problem Are We Solving?</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#what-problem-are-we-solving" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>The term <em>reinforcement learning</em> gets used in many different ways. In the formal sense defined by <span class="cite-group narrative"><cite class="" data-state="closed"><a href="http://incompleteideas.net/book/the-book-2nd.html" target="_blank" rel="noreferrer" class="hover-link">Sutton &amp; Barto (2018)</a></cite></span>, RL is a problem: learning to act through interaction with an environment. But in common usage, reinforcement learning can mean a family of algorithms, a research community, or even a long-term scientific agenda. For some, it is part of an effort to “solve intelligence.” For others, it is a toolbox for solving control problems with data.</p><p>This book takes the latter view.</p><p>We are not in the business of solving intelligence. Our concern is more immediate: helping systems make good decisions using the data they already produce. That means we treat reinforcement learning not as an end in itself, but as a vocabulary for reasoning about decisions under uncertainty. When optimization, feedback, and data intersect, we are in the territory of reinforcement learning, whether we use temporal-difference learning, model-based planning, or simple policy rules. What unifies these approaches is not a specific algorithm, but a shared structure: decision-making through experience.</p><p>Being clear about the problem matters. If the goal is to understand cognition, then abstraction and simulation are appropriate tools. But if the goal is to improve real-world systems, whether in energy, logistics, health, or agriculture, then the hard part is not choosing an algorithm. It is defining the task. What are we optimizing? What constraints apply? What information is available, and when? These are modeling questions, and they sit upstream of any learning method.</p><p>This book begins with the problem, not the solution. We use reinforcement learning in its broadest sense: as a perspective on how to improve decision-making from data, rather than as a collection of algorithms.</p><p>In doing so, we take inspiration from Sutton’s philosophy, while shifting its emphasis. Sutton famously advises: <em>“Approximate the solution, not the problem.”</em> In his view, the reinforcement learning agent should be shaped by experience, not by handcrafted structure or strong priors. That framing has led to influential ideas and a high degree of generality.</p><p>But it also reflects a particular philosophy. For Sutton, <em>“the problem”</em> is the world itself—complex, unknown, and handed to us as-is. We do not design it; we simply confront it. Reinforcement learning, in this view, is a path toward understanding intelligence through interaction, not engineering through modeling.</p><p>This book takes a more pragmatic stance. In practice, the problem is rarely just “given.” It must be defined: what are the goals, what decisions are available, what feedback is observable, and under what constraints? Before we can solve a problem, we have to formulate it—and that formulation shapes everything that follows.</p><h2 id="what-does-it-mean-to-model-a-decision-problem" class="relative group"><span class="heading-text">What Does It Mean to Model a Decision Problem?</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#what-does-it-mean-to-model-a-decision-problem" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>Modeling is not about feeding data into a black box. It is about deciding what matters. It involves structuring a problem: defining objectives, specifying constraints, clarifying what is observable, and determining how decisions unfold over time.</p><p>Take an HVAC system. The goal might be “maximize comfort while minimizing energy.” But what does comfort mean? A fixed temperature? Acceptable humidity? Rate of change? Occupant preferences? Is comfort linearly traded against energy, or are there thresholds? And how do you ensure safety and respect equipment limitations?</p><p>In irrigation, similar questions arise. Should irrigation be based on soil dryness, weather forecasts, plant health, or electricity prices? Should we water now or wait? How often should we revisit this choice? The answers depend on sensor availability, environmental dynamics, and risk tolerance.</p><p>Even time plays a central role. Are we planning for the next few minutes, or the next growing season? Should we model time in discrete steps or continuously? These are not afterthoughts. They shape what is learnable, controllable, and feasible.</p><p>Real-world systems come with hard constraints: physical limits, budgets, safety regulations, human expectations. Ignoring them may simplify the math, but it makes any solution irrelevant in practice. Good modeling incorporates these constraints from the beginning.</p><p>This kind of modeling is what Operations Research (OR) has long emphasized. By the 1970s, the foundational theory of dynamic programming was already in place. Today’s OR community often focuses on solving concrete decision problems, drawing on tools like mixed-integer linear programming when appropriate.</p><p>Through consulting, I came to appreciate OR’s pragmatism. My colleagues built decision-support systems connected to real-time data. In practice, they were doing reinforcement learning, just without Temporal Difference methods. They could not rely on massive data streams. Instead, they had to wrestle directly with business logic, real-world variability, and engineering constraints.</p><p>That does not mean OR has all the answers, or that reinforcement learning has been misguided. On the contrary, general-purpose solutions, even on toy problems, have advanced theory in meaningful ways. Simplified settings enable validation without full domain understanding: Did the pole balance? Did the agent reach the goal? Did it beat the Atari score? These abstractions follow good software engineering principles: separation of concerns, clear interfaces, and rapid iteration.</p><p>But abstraction is only one part of the equation. It is useful until it is not. A strong framework offers clarity at first, but eventually gets in the way, layering on complexity, edge cases, and configuration knobs. This mirrors the lifecycle of many software tools, where the initial elegance gives way to accumulated mess. I have come to treat modeling the same way: start small, surface the hard constraints early, and only add structure when it is required by the data, physics, or policy context.</p><p>We should not try to cram every control task into the discounted Markov Decision Process (MDP) format just because it is the default interface. Instead, we should keep a lean toolbox and reach for what the problem demands. This mindset, start simple, avoid premature generality, do not confuse abstraction with robustness, is well known to engineers. If that means choosing a basic model-predictive controller over a trendy reinforcement learning library, or the reverse, so be it.</p><p>Over time, this habit of zooming in when needed and zooming out when possible reshaped how I approached research. Once I started questioning the abstractions, I began to see what they were hiding.</p><p>Peeling back the layers revealed dynamics not captured by benchmarks, constraints ignored by rewards, and theory that only emerged in contact with the real world. This convinced me that some of the most meaningful discoveries still lie beneath the surface.</p><p>Reinforcement learning is a framework for learning to make decisions through experience. But experience is only useful if we have posed the right problem. Modeling determines both <em>how</em> learning proceeds and <em>what</em> we can learn at all.</p><p>At first, this might sound easy, just specify a reward and let the agent learn. That is the promise behind the “reward is enough” hypothesis. But in the real world, rewards are not handed to us. They must be constructed, inferred, or negotiated. And even when we manage that, rewards only express part of what matters. They do not tell us what information is available, what tradeoffs are acceptable, or how to handle ambiguity, delay, or disagreement.</p><p>In short, posing the right problem is itself a hard problem, and one for which we have few systematic tools.</p><p>Often, the only place we can turn is to people. Domain experts act, react, and judge, even when they cannot explain their reasoning explicitly. Their preferences show up in behavior, in corrections, in choices they make under pressure. If we cannot write down what we want, perhaps we can learn it indirectly from them.</p><hr class="py-2 my-5 translate-y-2"/><h2 id="learning-from-humans" class="relative group"><span class="heading-text">Learning From Humans</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#learning-from-humans" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>When we cannot write down the right behavior, we often try to learn it from examples. This is the idea behind imitation learning: watch the expert, then generalize. But in practice, good demonstrations are hard to collect and rarely cover the full range of relevant situations, especially the rare or risky ones.</p><p>That is why many real-world applications require more than direct imitation. If we cannot show what to do in every case, we must express what we want. This is where reward design, cost functions, and preference modeling come in. These tools attempt to capture the underlying objective by observing both what people do and what they seem to value.</p><p>Preference elicitation offers one route. Rather than specifying the optimal solution directly, we infer it from comparisons, rankings, or feedback. Under mild assumptions, the von Neumann–Morgenstern theorem tells us that such preferences correspond to a utility function. This principle forms the basis of <em>Reinforcement Learning from Human Feedback</em> (RLHF), now central to training large models.</p><p>But here, too, we face limits. Once we have inferred preferences or objectives from humans, what comes next? In many systems, the default answer is to treat this as a standard supervised learning problem: fit a black-box model to human-labeled data, then optimize the resulting predictions.</p><p>This approach can go surprisingly far. Recent work, such as Decision Transformers, has shown that supervised learning can recover policies that perform competitively, sometimes even state-of-the-art. But these successes are often built on vast datasets, carefully curated environments, and tight control over evaluation. In the real world, we rarely have that luxury.</p><p>Supervised learning assumes that if we show enough examples, the system will generalize appropriately. But generalization is fragile when data is limited, feedback is partial, or the stakes are high. Without the right structure in place, we risk building policies that extrapolate poorly, violate constraints, or break in unexpected ways.</p><p>This is where modeling matters again.</p><p>By modeling the decision process, which includes the constraints, objectives, time structure, and information flows, we introduce the <em>right inductive biases</em> into the learning system. These biases are not arbitrary. They reflect how the world works and what the agent can and cannot do. They make learning tractable even when data is scarce and help ensure that the resulting decisions behave reasonably under uncertainty.</p><p>So while supervised learning plays a role, it is not the full story. The point is to <strong>embed</strong> what we have learned into a framework where decision-making remains accountable, robust, and grounded in structure.</p><p>That is the aim of this book: to take what we can learn from humans and from data, and combine it with modeling discipline to build systems that act for the right reasons.</p><h2 id="the-path-forward" class="relative group"><span class="heading-text">The Path Forward</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#the-path-forward" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>Rust’s critique remains timely. After decades of algorithmic progress, we still struggle to help people make better decisions in the settings that matter most. Reinforcement learning has pushed the boundaries of simulation. But in practice, its reach remains limited, not because the tools are broken, but because we have struggled to formulate problems in ways that connect to the real world.</p><p>It is not that decision problems cannot be solved. It is that we often fail to pose them in solvable form.</p><p>This is not a limitation of algorithms. It is a limitation in how we frame our goals.</p><p>But that may be changing.</p><p>Rust once wrote:</p><blockquote><p><em>“Humans are learning to replicate the type of subconscious model building that goes on inside their brains and bring it to the conscious, formal level, but they are doing this modeling themselves, since it is not clear <strong>how to teach computers how to model</strong>.”</em></p></blockquote><p>That might have been true then. But today, we are beginning to see what it might look like to <em>teach</em> machines to model, or at least assist us in doing so.</p><p>One school of thought, inspired by Sutton’s long-term vision, imagines that we will not need to model at all. Instead, we build general-purpose agents trained on vast, unstructured experience. We do not hand them objectives or constraints. We do not define environments. We let them learn everything from scratch. The hope is that once such an agent is sufficiently broad and capable, it can generalize everywhere, even to tasks we have not yet imagined.</p><p>That is a bold bet. But it is still a bet. And if your goal is to solve meaningful problems now, in healthcare, infrastructure, climate, or logistics, then the challenge of formulation does not disappear. Even the most flexible agent cannot act reliably in a domain where the goals, tradeoffs, and structure remain unclear.</p><p>What is more, our current models of generality, especially large language models, are not agents in the classical sense. They produce text, but they do not act in the world. They reason in language, but they do not optimize over time. Despite growing trends to describe these systems as “agents,” they are better understood as modeling tools: systems trained on enormous corpora of human knowledge, capable of reflecting, translating, and helping us express complex ideas.</p><p>And that might be their greatest strength.</p><p>We may not hand full control to a language model anytime soon, but we <em>will</em> use these systems to help us model. They will assist in articulating objectives, surfacing hidden assumptions, identifying constraints, and mapping informal goals into structured forms. They will not replace modeling; they will augment it.</p><p>And what takes action, what makes real decisions in the world, will still rely on explicit optimization, grounded in formal structure, and designed to behave predictably. The language model may help us design that system, but it will not be the one executing it.</p><p>That is where I believe the near future lies: general-purpose models as modeling assistants, paired with optimization and control systems that retain structure, constraints, and accountability.</p><p>This book is about building that bridge: from goals to models, from data to decisions, from abstraction to action.</p><p>This is the modeling mindset. And it is what turns reinforcement learning into a practical tool for solving real problems.</p><div></div><section id="references" class="article-grid subgrid-gap col-screen"><div><header class="text-lg font-semibold text-stone-900 dark:text-white group">References<a class="no-underline text-inherit hover:text-inherit ml-2 select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#references" title="Link to References" aria-label="Link to References">¶</a></header></div><div class="pl-3 mb-8 text-xs text-stone-500 dark:text-stone-300"><ol><li class="break-words" id="cite-FDA2025">U.S. Food and Drug Administration. (2025). <i>Artificial Intelligence and Machine Learning (AI/ML)-Enabled Medical Devices</i>. <a target="_blank" rel="noreferrer" href="https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-and-machine-learning-aiml-enabled-medical-devices">https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-and-machine-learning-aiml-enabled-medical-devices</a></li><li class="break-words" id="cite-Kleijnen2024">Kleijnen, J., & others. (2024). Scoping review of prospective evaluations of AI in healthcare decision-making. <i>Lancet Digital Health</i>, <i>6</i>(3), e200–e212.</li><li class="break-words" id="cite-Chen2025">Chen, L., & others. (2025). <i>A Review of Real-World Deployments of Reinforcement Learning and MPC in HVAC Systems</i>. <a target="_blank" rel="noreferrer" href="https://example.com/hvac-rl-review">https://example.com/hvac-rl-review</a></li><li class="break-words" id="cite-Evans2018">Evans, D., & DeepMind. (2018). <i>DeepMind AI Reduces Google Data Centre Cooling Bill by 40%</i>. <a target="_blank" rel="noreferrer" href="https://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill">https://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill</a></li><li class="break-words" id="cite-Meta2024">Meta AI. (2024). <i>Reinforcement Learning for Sustainable Cooling in Data Centers</i>. <a target="_blank" rel="noreferrer" href="https://engineering.fb.com/2024/09/10/data-center/rl-sustainable-cooling">https://engineering.fb.com/2024/09/10/data-center/rl-sustainable-cooling</a></li><li class="break-words" id="cite-Uber2025">Uber AI Labs. (2025). <i>Reinforcement Learning at Scale for Ride-Matching Optimization</i>. <a target="_blank" rel="noreferrer" href="https://www.uber.com/blog/rl-ride-matching">https://www.uber.com/blog/rl-ride-matching</a></li><li class="break-words" id="cite-ANYbotics2023">ANYbotics. (2023). <i>ANYmal: RL-Powered Autonomous Inspection</i>. <a target="_blank" rel="noreferrer" href="https://www.anybotics.com/anymal-inspection">https://www.anybotics.com/anymal-inspection</a></li><li class="break-words" id="cite-Iskhakov2020">Iskhakov, F., Rust, J., & Schjerning, B. (2020). Machine learning and structural econometrics: contrasts and synergies. <i>The Econometrics Journal</i>, <i>23</i>(3), S81–S124. <a target="_blank" rel="noreferrer" href="https://doi.org/10.1093/ectj/utaa019">10.1093/ectj/utaa019</a></li><li class="break-words" id="cite-Rust1996">Rust, J. (1996). Chapter 14 Numerical dynamic programming in economics. In <i>Handbook of Computational Economics</i> (pp. 619–729). Elsevier. <a target="_blank" rel="noreferrer" href="https://doi.org/10.1016/s1574-0021(96)01016-7">10.1016/s1574-0021(96)01016-7</a></li><li class="break-words" id="cite-SuttonBarto2018">Sutton, R. S., & Barto, A. G. (2018). <i>Reinforcement Learning: An Introduction</i> (2nd ed.). MIT Press. <a target="_blank" rel="noreferrer" href="http://incompleteideas.net/book/the-book-2nd.html">http://incompleteideas.net/book/the-book-2nd.html</a></li></ol></div></section><div class="flex pt-10 mb-10 space-x-4"><a class="flex-1 block p-4 font-normal text-gray-600 no-underline border border-gray-200 rounded shadow-sm group hover:border-blue-600 dark:hover:border-blue-400 hover:text-blue-600 dark:hover:text-blue-400 dark:text-gray-100 dark:border-gray-500 hover:shadow-lg dark:shadow-neutral-700" href="/rlbook/modeling"><div class="flex h-full align-middle"><div class="flex-grow"><div class="text-xs text-gray-500 dark:text-gray-400">Modeling</div>Why Build a Model? For Whom?</div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.5rem" height="1.5rem" class="self-center transition-transform group-hover:translate-x-1 shrink-0"><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 4.5 21 12m0 0-7.5 7.5M21 12H3"></path></svg></div></a></div></article></main><script>((a,d)=>{if(!window.history.state||!window.history.state.key){let h=Math.random().toString(32).slice(2);window.history.replaceState({key:h},"")}try{let f=JSON.parse(sessionStorage.getItem(a)||"{}")[d||window.history.state.key];typeof f=="number"&&window.scrollTo(0,f)}catch(h){console.error(h),sessionStorage.removeItem(a)}})("positions", null)</script><link rel="modulepreload" href="/rlbook/build/entry.client-UNPC4GT3.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-OCTKKCIL.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-UAI5KRM7.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-2NH4LW52.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-XWET6RJ7.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-HBJK6BW3.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-HYMQ7M2K.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-OHOXABTA.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-OCWQY3HK.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-C7FW3E47.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-3CVK3PYF.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-J6FHCSRC.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-ND43KHSX.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-GUCIBHGO.js"/><link rel="modulepreload" href="/rlbook/build/root-ZJOPFBMV.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-NFM2H3KQ.js"/><link rel="modulepreload" href="/rlbook/build/routes/$-CQPS5IOR.js"/><script>window.__remixContext = {"url":"/index","state":{"loaderData":{"root":{"config":{"version":2,"myst":"1.6.3","options":{"logo":"/rlbook/build/logo-439a25d1a718f5f15fe2838e2a61a91a.png","logo_text":"RL \u0026 Control"},"nav":[],"actions":[{"title":"View on GitHub","url":"https://github.com/pierrelux/rlbook","internal":false,"static":false}],"projects":[{"bibliography":["/Users/pierre-luc.bacon/Documents/rlbook/references.bib"],"math":{"\\Proj":{"macro":"\\mathsf{P}"},"\\Residual":{"macro":"\\mathsf{N}"},"\\Contraction":{"macro":"\\mathsf{T}"},"\\Bellman":{"macro":"\\mathsf{L}"},"\\BellmanPi":{"macro":"\\mathsf{L}_\\pi"},"\\BellmanQuad":{"macro":"\\widetilde{\\mathsf{L}}"},"\\EulerResidual":{"macro":"\\mathsf{H}"}},"exports":[{"format":"pdf","filename":"book.pdf","url":"/rlbook/build/book-bceca6481db041911b084ebd03cc1269.pdf"}],"title":"Reinforcement Learning Beyond the Agent Loop: Models, Control, and Data","description":"A graduate-level introduction to reinforcement learning as a framework for modeling, optimization, and control, connecting dynamic models, data, and applications beyond standard benchmarks.","authors":[{"nameParsed":{"literal":"Pierre-Luc Bacon","given":"Pierre-Luc","family":"Bacon"},"name":"Pierre-Luc Bacon","id":"contributors-myst-generated-uid-0"}],"github":"https://github.com/pierrelux/rlbook","id":"3c112bb5-93ac-429f-8216-c0b9947c00d4","toc":[{"file":"intro.md"},{"children":[{"file":"modeling.md"},{"file":"ssm.md"},{"file":"simulation.md"}],"title":"Modeling"},{"children":[{"file":"ocp.md"},{"file":"cocp.md"}],"title":"Numerical Trajectory Optimization"},{"children":[{"file":"mpc.md"},{"file":"dp.md"}],"title":"From Trajectories to Policies"},{"children":[{"file":"regmdp.md"},{"file":"projdp.md"},{"children":[{"file":"simadp.md"},{"file":"batch_rl.md"},{"file":"online_rl.md"}],"title":"Simulation-Based Methods"},{"file":"cadp.md"}],"title":"Approximate Dynamic Programming"},{"children":[{"file":"appendix_examples.md"},{"file":"appendix_ivps.md"},{"file":"appendix_nlp.md"}],"title":"Appendix"},{"children":[{"file":"bibliography.md"}],"title":"References"}],"index":"index","pages":[{"level":1,"title":"Modeling"},{"slug":"modeling","title":"Why Build a Model? For Whom?","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"ssm","title":"Dynamics Models for Decision Making","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"simulation","title":"Programs as Models","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Numerical Trajectory Optimization"},{"slug":"ocp","title":"Discrete-Time Trajectory Optimization","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"cocp","title":"Trajectory Optimization in Continuous Time","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"From Trajectories to Policies"},{"slug":"mpc","title":"Model Predictive Control","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"dp","title":"Dynamic Programming","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Approximate Dynamic Programming"},{"slug":"regmdp","title":"Smooth Bellman Optimality Equations","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"projdp","title":"Weighted Residual Methods for Functional Equations","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":2,"title":"Simulation-Based Methods"},{"slug":"simadp","title":"Simulation-Based Approximate Dynamic Programming","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"batch-rl","title":"Batch Reinforcement Learning","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"online-rl","title":"Online Reinforcement Learning","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"cadp","title":"Policy Parametrization Methods","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Appendix"},{"slug":"appendix-examples","title":"Example COCPs","description":"","date":"","thumbnail":"/rlbook/build/heat_exchanger-acfdd83b1501b4c220f686fe21df7820.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"appendix-ivps","title":"Solving Initial Value Problems","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"appendix-nlp","title":"Nonlinear Programming","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"References"},{"slug":"bibliography","title":"Bibliography","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2}]}]},"CONTENT_CDN_PORT":"3101","MODE":"static","BASE_URL":"/rlbook"},"routes/$":{"config":{"version":2,"myst":"1.6.3","options":{"logo":"/rlbook/build/logo-439a25d1a718f5f15fe2838e2a61a91a.png","logo_text":"RL \u0026 Control"},"nav":[],"actions":[{"title":"View on GitHub","url":"https://github.com/pierrelux/rlbook","internal":false,"static":false}],"projects":[{"bibliography":["/Users/pierre-luc.bacon/Documents/rlbook/references.bib"],"math":{"\\Proj":{"macro":"\\mathsf{P}"},"\\Residual":{"macro":"\\mathsf{N}"},"\\Contraction":{"macro":"\\mathsf{T}"},"\\Bellman":{"macro":"\\mathsf{L}"},"\\BellmanPi":{"macro":"\\mathsf{L}_\\pi"},"\\BellmanQuad":{"macro":"\\widetilde{\\mathsf{L}}"},"\\EulerResidual":{"macro":"\\mathsf{H}"}},"exports":[{"format":"pdf","filename":"book.pdf","url":"/rlbook/build/book-bceca6481db041911b084ebd03cc1269.pdf"}],"title":"Reinforcement Learning Beyond the Agent Loop: Models, Control, and Data","description":"A graduate-level introduction to reinforcement learning as a framework for modeling, optimization, and control, connecting dynamic models, data, and applications beyond standard benchmarks.","authors":[{"nameParsed":{"literal":"Pierre-Luc Bacon","given":"Pierre-Luc","family":"Bacon"},"name":"Pierre-Luc Bacon","id":"contributors-myst-generated-uid-0"}],"github":"https://github.com/pierrelux/rlbook","id":"3c112bb5-93ac-429f-8216-c0b9947c00d4","toc":[{"file":"intro.md"},{"children":[{"file":"modeling.md"},{"file":"ssm.md"},{"file":"simulation.md"}],"title":"Modeling"},{"children":[{"file":"ocp.md"},{"file":"cocp.md"}],"title":"Numerical Trajectory Optimization"},{"children":[{"file":"mpc.md"},{"file":"dp.md"}],"title":"From Trajectories to Policies"},{"children":[{"file":"regmdp.md"},{"file":"projdp.md"},{"children":[{"file":"simadp.md"},{"file":"batch_rl.md"},{"file":"online_rl.md"}],"title":"Simulation-Based Methods"},{"file":"cadp.md"}],"title":"Approximate Dynamic Programming"},{"children":[{"file":"appendix_examples.md"},{"file":"appendix_ivps.md"},{"file":"appendix_nlp.md"}],"title":"Appendix"},{"children":[{"file":"bibliography.md"}],"title":"References"}],"index":"index","pages":[{"level":1,"title":"Modeling"},{"slug":"modeling","title":"Why Build a Model? For Whom?","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"ssm","title":"Dynamics Models for Decision Making","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"simulation","title":"Programs as Models","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Numerical Trajectory Optimization"},{"slug":"ocp","title":"Discrete-Time Trajectory Optimization","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"cocp","title":"Trajectory Optimization in Continuous Time","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"From Trajectories to Policies"},{"slug":"mpc","title":"Model Predictive Control","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"dp","title":"Dynamic Programming","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Approximate Dynamic Programming"},{"slug":"regmdp","title":"Smooth Bellman Optimality Equations","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"projdp","title":"Weighted Residual Methods for Functional Equations","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":2,"title":"Simulation-Based Methods"},{"slug":"simadp","title":"Simulation-Based Approximate Dynamic Programming","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"batch-rl","title":"Batch Reinforcement Learning","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"online-rl","title":"Online Reinforcement Learning","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"cadp","title":"Policy Parametrization Methods","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Appendix"},{"slug":"appendix-examples","title":"Example COCPs","description":"","date":"","thumbnail":"/rlbook/build/heat_exchanger-acfdd83b1501b4c220f686fe21df7820.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"appendix-ivps","title":"Solving Initial Value Problems","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"appendix-nlp","title":"Nonlinear Programming","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"References"},{"slug":"bibliography","title":"Bibliography","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2}]}]},"page":{"version":2,"kind":"Article","sha256":"c8b58ca07090f85169d4e48e13d3ab3ab24bc97d022e4813b480871b056de319","slug":"index","location":"/intro.md","dependencies":[],"frontmatter":{"title":"Why This Book?","content_includes_title":false,"authors":[{"nameParsed":{"literal":"Pierre-Luc Bacon","given":"Pierre-Luc","family":"Bacon"},"name":"Pierre-Luc Bacon","id":"contributors-myst-generated-uid-0"}],"github":"https://github.com/pierrelux/rlbook","math":{"\\Proj":{"macro":"\\mathsf{P}"},"\\Residual":{"macro":"\\mathsf{N}"},"\\Contraction":{"macro":"\\mathsf{T}"},"\\Bellman":{"macro":"\\mathsf{L}"},"\\BellmanPi":{"macro":"\\mathsf{L}_\\pi"},"\\BellmanQuad":{"macro":"\\widetilde{\\mathsf{L}}"},"\\EulerResidual":{"macro":"\\mathsf{H}"}},"source_url":"https://github.com/pierrelux/rlbook/blob/main/intro.md","edit_url":"https://github.com/pierrelux/rlbook/edit/main/intro.md","exports":[{"format":"md","filename":"intro.md","url":"/rlbook/build/intro-ccee16020b8b6aaf22b4156b7175f6f3.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Reinforcement learning often captures headlines with breakthroughs: AlphaGo defeating world champions, agents mastering Atari, chatbots engaging millions in conversation. Yet compared to other branches of machine learning—especially supervised learning, which has become routine in industry and research—RL has not yet achieved the same widespread adoption in everyday decision-making and operations.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"ir7UimiyfU"}],"key":"rr7HJ02LmX"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"For perspective, consider supervised learning. Standardized tools like scikit-learn, TensorFlow, or PyTorch have made it straightforward for data scientists to integrate classification and regression models into production workflows. In medicine, hundreds of FDA-approved devices now incorporate supervised machine learning. Randomized controlled trials regularly evaluate diagnostic and predictive models built with supervised learning techniques.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"JipMY353DF"}],"key":"fRMKKaQT7l"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"By contrast, reinforcement learning’s adoption is still in earlier stages. Among more than a thousand FDA-cleared AI-enabled medical devices as of 2025, none explicitly reference reinforcement learning in their public summaries ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"fjHEcUcsNY"},{"type":"citeGroup","kind":"parenthetical","children":[{"type":"cite","kind":"parenthetical","label":"FDA2025","identifier":"fda2025","children":[{"type":"text","value":"U.S. Food and Drug Administration, 2025","key":"QG724rG2HP"}],"enumerator":"1","key":"rBOIvICuzu"}],"key":"h01raqB0lk"},{"type":"text","value":". A recent ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"vOPgFqO3vt"},{"type":"emphasis","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Lancet Digital Health","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"tsjGHlim7r"}],"key":"p0BYtgQBCK"},{"type":"text","value":" review of 86 clinical trials involving AI identified only two studies that tested RL-based decision rules ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"BwbHET6lNA"},{"type":"citeGroup","kind":"parenthetical","children":[{"type":"cite","kind":"parenthetical","label":"Kleijnen2024","identifier":"kleijnen2024","children":[{"type":"text","value":"Kleijnen \u0026 others, 2024","key":"GWosTlgcSy"}],"enumerator":"2","key":"ivt8EiouZp"}],"key":"lwPQyvozLB"},{"type":"text","value":". Most AI solutions in healthcare remain supervised learning models trained on labeled datasets.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"mtV003Ai0w"}],"key":"bzy42m3Xru"},{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Outside healthcare, promising results exist, but scale remains modest. In building automation, a 2025 survey identified many field demonstrations of reinforcement learning and Model Predictive Control (MPC) for HVAC control. Yet, fewer than a third met basic methodological criteria. Among reliable studies, average cost reductions were around 13–16% ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"Yvfxf8qlJC"},{"type":"citeGroup","kind":"parenthetical","children":[{"type":"cite","kind":"parenthetical","label":"Chen2025","identifier":"chen2025","children":[{"type":"text","value":"Chen \u0026 others, 2025","key":"WZeJVKwXRC"}],"enumerator":"3","key":"NU9qffeXSz"}],"key":"OEde08rlhP"},{"type":"text","value":". This is encouraging, yet still short of broad, industry-wide adoption.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"ntxlVflGmy"}],"key":"zOCYOtcC4J"},{"type":"paragraph","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Even in high-profile reinforcement learning deployments, adoption at scale is sometimes uncertain or difficult to confirm. Google DeepMind famously reported significant cooling-energy reductions in data centers using RL back in 2018 ","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"vhmiPGwsu7"},{"type":"citeGroup","kind":"parenthetical","children":[{"type":"cite","kind":"parenthetical","label":"Evans2018","identifier":"evans2018","children":[{"type":"text","value":"Evans \u0026 DeepMind, 2018","key":"BaGQwFjK3d"}],"enumerator":"4","key":"zh40T4rRTK"}],"key":"bqdAiJ4iti"},{"type":"text","value":". However, more recent public confirmation of widespread autonomous use has been lacking. Meta, in a 2024 engineering blog, provided clear evidence that an RL-based airflow controller was achieving meaningful reductions in energy and water usage, and stated that broader deployment was underway, though specifics on scale were not disclosed ","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"ka9LU1EvOB"},{"type":"citeGroup","kind":"parenthetical","children":[{"type":"cite","kind":"parenthetical","label":"Meta2024","identifier":"meta2024","children":[{"type":"text","value":"Meta AI, 2024","key":"nP57Hg2nvH"}],"enumerator":"5","key":"KRjWWHaGpy"}],"key":"rMjvOCM8fR"},{"type":"text","value":".","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"e1QV37pkUi"}],"key":"DUITc6QD8V"},{"type":"paragraph","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"Nevertheless, notable successes continue to emerge. Uber has successfully integrated reinforcement learning into its ride-matching system, with measurable improvements across hundreds of cities ","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"zEHqXowzOm"},{"type":"citeGroup","kind":"parenthetical","children":[{"type":"cite","kind":"parenthetical","label":"Uber2025","identifier":"uber2025","children":[{"type":"text","value":"Uber AI Labs, 2025","key":"Fe6GqjXJLR"}],"enumerator":"6","key":"RilFQyGfh4"}],"key":"VK2uiE4sHN"},{"type":"text","value":". ANYbotics has commercially deployed quadruped robots whose locomotion policies, trained entirely via RL in simulation, now reliably perform complex industrial inspections ","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"leEMGIF2qt"},{"type":"citeGroup","kind":"parenthetical","children":[{"type":"cite","kind":"parenthetical","label":"ANYbotics2023","identifier":"anybotics2023","children":[{"type":"text","value":"ANYbotics, 2023","key":"tfXXw9HQms"}],"enumerator":"7","key":"HeCLg2Q3D2"}],"key":"YXIru4lm0H"},{"type":"text","value":".","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"QIDkfxk2HZ"}],"key":"EZfruEzyQ4"},{"type":"paragraph","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"text","value":"These examples illustrate genuine progress. Yet reinforcement learning has not yet reached the “plug-and-play” status enjoyed by supervised learning. This is largely due to fundamental differences in problem structure. Supervised learning tasks typically involve well-defined inputs, outputs, and objective metrics. Reinforcement learning, by contrast, requires explicit problem formulation, exploration, interactive data collection, and a nuanced understanding of the environment’s structure.","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"D1OPY0swjH"}],"key":"EPyL2MhyBo"},{"type":"paragraph","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"As ","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"Mu9Q49xSSZ"},{"type":"citeGroup","kind":"narrative","children":[{"type":"cite","kind":"narrative","label":"Iskhakov2020","identifier":"iskhakov2020","children":[{"type":"text","value":"Iskhakov ","key":"D4DDUDvLam"},{"type":"emphasis","children":[{"type":"text","value":"et al.","key":"eXpUtumMm8"}],"key":"cIUu7Jsq7w"},{"type":"text","value":" (2020)","key":"iGv1JLlbBn"}],"enumerator":"8","key":"jidzKQPj4r"}],"key":"xH0fNxiHGd"},{"type":"text","value":" notes in econometrics, a primary challenge facing adoption of any sequential decision-making tool is:","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"MCwujWcsaK"}],"key":"nUByl993zo"},{"type":"blockquote","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"emphasis","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"“The difficulty of learning about the objective function and environment facing real-world decision-makers.”","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"BAKej8Y4XR"}],"key":"Pf8O782TaA"}],"key":"Xi4qvDl3Eb"}],"key":"jgn7FJRgUm"},{"type":"paragraph","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"In reinforcement learning, this difficulty is integral. We cannot sidestep defining the problem, the objective, and the constraints; these are not incidental but central. Supervised learning allows practitioners to abstract away most of these concerns into standardized data formats and evaluation metrics. Reinforcement learning practitioners do not have this luxury.","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"Nc5wHcLoYS"}],"key":"wfAjIrTjpB"},{"type":"paragraph","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"That is where this book begins.","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"d5ovpnVuJA"}],"key":"v7Ry9JiZxg"},{"type":"paragraph","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"text","value":"I did not fully appreciate this difference as a PhD student. Like many others trained in machine learning, I focused on tuning algorithms, chasing benchmarks, and climbing leaderboards. Problem definition was abstracted away, often assumed or left as someone else’s responsibility.","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"seU6yXeF2E"}],"key":"URt75hupZP"},{"type":"paragraph","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"text","value":"Working in industry and consulting changed that. Real problems rarely fit neatly into a predefined framework. Sensors produce noisy data; constraints are non-negotiable; objectives may shift or conflict. I discovered firsthand that most effort goes into formulating the decision problem, long before selecting an algorithm.","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"sWoQAfp8U1"}],"key":"Mn9kxPeTcG"},{"type":"paragraph","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"text","value":"John Rust captures this precisely when reflecting on dynamic programming in practice:","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"ayB8A28LSj"}],"key":"pEVoVj3wJH"},{"type":"blockquote","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"emphasis","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"text","value":"“The range of known real-world applications of Dynamic Programming seems disappointingly small, given the immense computer power and decades of research that have produced a myriad of alternative solution methods for DP problems. I believe the biggest constraint on progress is not limited computer power, but instead the difficulty of learning the underlying structure of the decision problem.”","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"LOqR3UZ0Lj"}],"key":"thWdNlWogn"},{"type":"text","value":" ","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"Y4gfiumjSj"},{"type":"citeGroup","kind":"narrative","children":[{"type":"cite","kind":"narrative","label":"Rust1996","identifier":"rust1996","children":[{"type":"text","value":"Rust (1996)","key":"sGHYh7tY1L"}],"enumerator":"9","key":"Y9xqL3ZrNo"}],"key":"PeZOSdkmrp"}],"key":"VBIj3DFq3F"}],"key":"VOr7AawdlI"},{"type":"paragraph","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"children":[{"type":"text","value":"In other words, solving a real-world decision problem starts with formulating it correctly. What are we optimizing? What can we observe and control? How does information flow? These questions are not secondary details; they define the problem itself.","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"key":"g19EMscPge"}],"key":"WcM1cCkuyY"},{"type":"paragraph","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"children":[{"type":"text","value":"The chapters that follow address these challenges explicitly. They offer strategies to bridge the gap from theoretical reinforcement learning formulations to practically useful systems. By carefully structuring decision problems, we can help reinforcement learning achieve broader impact, just as supervised learning already has.","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"key":"D7AhrhA5Oe"}],"key":"hCJSoIBck1"},{"type":"heading","depth":2,"position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"text","value":"What Problem Are We Solving?","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"DQiI207Rq8"}],"identifier":"what-problem-are-we-solving","label":"What Problem Are We Solving?","html_id":"what-problem-are-we-solving","implicit":true,"key":"gTlDE2BdLs"},{"type":"paragraph","position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"children":[{"type":"text","value":"The term ","position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"key":"HIxjLf9smL"},{"type":"emphasis","position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"children":[{"type":"text","value":"reinforcement learning","position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"key":"FFcuScPrwT"}],"key":"QrgXMrCDdV"},{"type":"text","value":" gets used in many different ways. In the formal sense defined by ","position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"key":"g0UchpnsrR"},{"type":"citeGroup","kind":"narrative","children":[{"type":"cite","kind":"narrative","label":"SuttonBarto2018","identifier":"suttonbarto2018","children":[{"type":"text","value":"Sutton \u0026 Barto (2018)","key":"ks4rc288tt"}],"enumerator":"10","key":"clouhHnPDe"}],"key":"BspwYHLQ2S"},{"type":"text","value":", RL is a problem: learning to act through interaction with an environment. But in common usage, reinforcement learning can mean a family of algorithms, a research community, or even a long-term scientific agenda. For some, it is part of an effort to “solve intelligence.” For others, it is a toolbox for solving control problems with data.","position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"key":"nhcdCePbdY"}],"key":"iJ99yMtiup"},{"type":"paragraph","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"children":[{"type":"text","value":"This book takes the latter view.","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"zHW33nyQGT"}],"key":"akuE3rRole"},{"type":"paragraph","position":{"start":{"line":43,"column":1},"end":{"line":43,"column":1}},"children":[{"type":"text","value":"We are not in the business of solving intelligence. Our concern is more immediate: helping systems make good decisions using the data they already produce. That means we treat reinforcement learning not as an end in itself, but as a vocabulary for reasoning about decisions under uncertainty. When optimization, feedback, and data intersect, we are in the territory of reinforcement learning, whether we use temporal-difference learning, model-based planning, or simple policy rules. What unifies these approaches is not a specific algorithm, but a shared structure: decision-making through experience.","position":{"start":{"line":43,"column":1},"end":{"line":43,"column":1}},"key":"Xqps9ASt5o"}],"key":"fKQ4d1VeAI"},{"type":"paragraph","position":{"start":{"line":45,"column":1},"end":{"line":45,"column":1}},"children":[{"type":"text","value":"Being clear about the problem matters. If the goal is to understand cognition, then abstraction and simulation are appropriate tools. But if the goal is to improve real-world systems, whether in energy, logistics, health, or agriculture, then the hard part is not choosing an algorithm. It is defining the task. What are we optimizing? What constraints apply? What information is available, and when? These are modeling questions, and they sit upstream of any learning method.","position":{"start":{"line":45,"column":1},"end":{"line":45,"column":1}},"key":"hx7dsrGVfR"}],"key":"LD2AXbUA5N"},{"type":"paragraph","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"children":[{"type":"text","value":"This book begins with the problem, not the solution. We use reinforcement learning in its broadest sense: as a perspective on how to improve decision-making from data, rather than as a collection of algorithms.","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"srDOZdjmsL"}],"key":"YNGy9ZrF48"},{"type":"paragraph","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"children":[{"type":"text","value":"In doing so, we take inspiration from Sutton’s philosophy, while shifting its emphasis. Sutton famously advises: ","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"key":"xaLkJQvEoa"},{"type":"emphasis","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"children":[{"type":"text","value":"“Approximate the solution, not the problem.”","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"key":"jAsuKlJLL9"}],"key":"SObG8PH1Um"},{"type":"text","value":" In his view, the reinforcement learning agent should be shaped by experience, not by handcrafted structure or strong priors. That framing has led to influential ideas and a high degree of generality.","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"key":"W0ajcs4Bgq"}],"key":"IZs32PPdkx"},{"type":"paragraph","position":{"start":{"line":51,"column":1},"end":{"line":51,"column":1}},"children":[{"type":"text","value":"But it also reflects a particular philosophy. For Sutton, ","position":{"start":{"line":51,"column":1},"end":{"line":51,"column":1}},"key":"CTglVF481w"},{"type":"emphasis","position":{"start":{"line":51,"column":1},"end":{"line":51,"column":1}},"children":[{"type":"text","value":"“the problem”","position":{"start":{"line":51,"column":1},"end":{"line":51,"column":1}},"key":"cPPfv8oDMM"}],"key":"ogU6a4NYG4"},{"type":"text","value":" is the world itself—complex, unknown, and handed to us as-is. We do not design it; we simply confront it. Reinforcement learning, in this view, is a path toward understanding intelligence through interaction, not engineering through modeling.","position":{"start":{"line":51,"column":1},"end":{"line":51,"column":1}},"key":"fGUekXwoXc"}],"key":"EVR4dx99Tu"},{"type":"paragraph","position":{"start":{"line":53,"column":1},"end":{"line":53,"column":1}},"children":[{"type":"text","value":"This book takes a more pragmatic stance. In practice, the problem is rarely just “given.” It must be defined: what are the goals, what decisions are available, what feedback is observable, and under what constraints? Before we can solve a problem, we have to formulate it—and that formulation shapes everything that follows.","position":{"start":{"line":53,"column":1},"end":{"line":53,"column":1}},"key":"SZgUPwNCrz"}],"key":"Pl8FJXvcz7"},{"type":"heading","depth":2,"position":{"start":{"line":55,"column":1},"end":{"line":55,"column":1}},"children":[{"type":"text","value":"What Does It Mean to Model a Decision Problem?","position":{"start":{"line":55,"column":1},"end":{"line":55,"column":1}},"key":"AwTC5rHxwy"}],"identifier":"what-does-it-mean-to-model-a-decision-problem","label":"What Does It Mean to Model a Decision Problem?","html_id":"what-does-it-mean-to-model-a-decision-problem","implicit":true,"key":"g9olYpEtdc"},{"type":"paragraph","position":{"start":{"line":57,"column":1},"end":{"line":57,"column":1}},"children":[{"type":"text","value":"Modeling is not about feeding data into a black box. It is about deciding what matters. It involves structuring a problem: defining objectives, specifying constraints, clarifying what is observable, and determining how decisions unfold over time.","position":{"start":{"line":57,"column":1},"end":{"line":57,"column":1}},"key":"SoqLUPbYgR"}],"key":"ioprovfGzK"},{"type":"paragraph","position":{"start":{"line":59,"column":1},"end":{"line":59,"column":1}},"children":[{"type":"text","value":"Take an HVAC system. The goal might be “maximize comfort while minimizing energy.” But what does comfort mean? A fixed temperature? Acceptable humidity? Rate of change? Occupant preferences? Is comfort linearly traded against energy, or are there thresholds? And how do you ensure safety and respect equipment limitations?","position":{"start":{"line":59,"column":1},"end":{"line":59,"column":1}},"key":"PSjaYvBSnp"}],"key":"b9Wi9r8e6r"},{"type":"paragraph","position":{"start":{"line":61,"column":1},"end":{"line":61,"column":1}},"children":[{"type":"text","value":"In irrigation, similar questions arise. Should irrigation be based on soil dryness, weather forecasts, plant health, or electricity prices? Should we water now or wait? How often should we revisit this choice? The answers depend on sensor availability, environmental dynamics, and risk tolerance.","position":{"start":{"line":61,"column":1},"end":{"line":61,"column":1}},"key":"q2ehGjCmdN"}],"key":"BPZjeIyEaw"},{"type":"paragraph","position":{"start":{"line":63,"column":1},"end":{"line":63,"column":1}},"children":[{"type":"text","value":"Even time plays a central role. Are we planning for the next few minutes, or the next growing season? Should we model time in discrete steps or continuously? These are not afterthoughts. They shape what is learnable, controllable, and feasible.","position":{"start":{"line":63,"column":1},"end":{"line":63,"column":1}},"key":"q9uZXd5eJc"}],"key":"vTE2OrlOTA"},{"type":"paragraph","position":{"start":{"line":65,"column":1},"end":{"line":65,"column":1}},"children":[{"type":"text","value":"Real-world systems come with hard constraints: physical limits, budgets, safety regulations, human expectations. Ignoring them may simplify the math, but it makes any solution irrelevant in practice. Good modeling incorporates these constraints from the beginning.","position":{"start":{"line":65,"column":1},"end":{"line":65,"column":1}},"key":"jdYgRkhLuA"}],"key":"g10uXUeRuV"},{"type":"paragraph","position":{"start":{"line":67,"column":1},"end":{"line":67,"column":1}},"children":[{"type":"text","value":"This kind of modeling is what Operations Research (OR) has long emphasized. By the 1970s, the foundational theory of dynamic programming was already in place. Today’s OR community often focuses on solving concrete decision problems, drawing on tools like mixed-integer linear programming when appropriate.","position":{"start":{"line":67,"column":1},"end":{"line":67,"column":1}},"key":"l0r5KQIU3l"}],"key":"AENemBIMIo"},{"type":"paragraph","position":{"start":{"line":69,"column":1},"end":{"line":69,"column":1}},"children":[{"type":"text","value":"Through consulting, I came to appreciate OR’s pragmatism. My colleagues built decision-support systems connected to real-time data. In practice, they were doing reinforcement learning, just without Temporal Difference methods. They could not rely on massive data streams. Instead, they had to wrestle directly with business logic, real-world variability, and engineering constraints.","position":{"start":{"line":69,"column":1},"end":{"line":69,"column":1}},"key":"CGrv5PLa49"}],"key":"l73fipWwYd"},{"type":"paragraph","position":{"start":{"line":71,"column":1},"end":{"line":71,"column":1}},"children":[{"type":"text","value":"That does not mean OR has all the answers, or that reinforcement learning has been misguided. On the contrary, general-purpose solutions, even on toy problems, have advanced theory in meaningful ways. Simplified settings enable validation without full domain understanding: Did the pole balance? Did the agent reach the goal? Did it beat the Atari score? These abstractions follow good software engineering principles: separation of concerns, clear interfaces, and rapid iteration.","position":{"start":{"line":71,"column":1},"end":{"line":71,"column":1}},"key":"fi5PNgmEqk"}],"key":"VsYlajpS0g"},{"type":"paragraph","position":{"start":{"line":73,"column":1},"end":{"line":73,"column":1}},"children":[{"type":"text","value":"But abstraction is only one part of the equation. It is useful until it is not. A strong framework offers clarity at first, but eventually gets in the way, layering on complexity, edge cases, and configuration knobs. This mirrors the lifecycle of many software tools, where the initial elegance gives way to accumulated mess. I have come to treat modeling the same way: start small, surface the hard constraints early, and only add structure when it is required by the data, physics, or policy context.","position":{"start":{"line":73,"column":1},"end":{"line":73,"column":1}},"key":"iQo3qeAl82"}],"key":"CaQUGG7Sgo"},{"type":"paragraph","position":{"start":{"line":75,"column":1},"end":{"line":75,"column":1}},"children":[{"type":"text","value":"We should not try to cram every control task into the discounted Markov Decision Process (MDP) format just because it is the default interface. Instead, we should keep a lean toolbox and reach for what the problem demands. This mindset, start simple, avoid premature generality, do not confuse abstraction with robustness, is well known to engineers. If that means choosing a basic model-predictive controller over a trendy reinforcement learning library, or the reverse, so be it.","position":{"start":{"line":75,"column":1},"end":{"line":75,"column":1}},"key":"d5zEYqYUUk"}],"key":"jIg2aCG6vE"},{"type":"paragraph","position":{"start":{"line":77,"column":1},"end":{"line":77,"column":1}},"children":[{"type":"text","value":"Over time, this habit of zooming in when needed and zooming out when possible reshaped how I approached research. Once I started questioning the abstractions, I began to see what they were hiding.","position":{"start":{"line":77,"column":1},"end":{"line":77,"column":1}},"key":"i2Gwc0DkKU"}],"key":"bMwGyKxnEC"},{"type":"paragraph","position":{"start":{"line":79,"column":1},"end":{"line":79,"column":1}},"children":[{"type":"text","value":"Peeling back the layers revealed dynamics not captured by benchmarks, constraints ignored by rewards, and theory that only emerged in contact with the real world. This convinced me that some of the most meaningful discoveries still lie beneath the surface.","position":{"start":{"line":79,"column":1},"end":{"line":79,"column":1}},"key":"tF8rdGdemD"}],"key":"juNMbxHkuk"},{"type":"paragraph","position":{"start":{"line":81,"column":1},"end":{"line":81,"column":1}},"children":[{"type":"text","value":"Reinforcement learning is a framework for learning to make decisions through experience. But experience is only useful if we have posed the right problem. Modeling determines both ","position":{"start":{"line":81,"column":1},"end":{"line":81,"column":1}},"key":"F45lpDYEVg"},{"type":"emphasis","position":{"start":{"line":81,"column":1},"end":{"line":81,"column":1}},"children":[{"type":"text","value":"how","position":{"start":{"line":81,"column":1},"end":{"line":81,"column":1}},"key":"o4YS8Ir68c"}],"key":"koIWLI0XeL"},{"type":"text","value":" learning proceeds and ","position":{"start":{"line":81,"column":1},"end":{"line":81,"column":1}},"key":"iWsTMPX3cm"},{"type":"emphasis","position":{"start":{"line":81,"column":1},"end":{"line":81,"column":1}},"children":[{"type":"text","value":"what","position":{"start":{"line":81,"column":1},"end":{"line":81,"column":1}},"key":"Yq1dtbDEHB"}],"key":"YQd5L5X2IF"},{"type":"text","value":" we can learn at all.","position":{"start":{"line":81,"column":1},"end":{"line":81,"column":1}},"key":"IoIxhhw53G"}],"key":"Zj47i32lHg"},{"type":"paragraph","position":{"start":{"line":83,"column":1},"end":{"line":83,"column":1}},"children":[{"type":"text","value":"At first, this might sound easy, just specify a reward and let the agent learn. That is the promise behind the “reward is enough” hypothesis. But in the real world, rewards are not handed to us. They must be constructed, inferred, or negotiated. And even when we manage that, rewards only express part of what matters. They do not tell us what information is available, what tradeoffs are acceptable, or how to handle ambiguity, delay, or disagreement.","position":{"start":{"line":83,"column":1},"end":{"line":83,"column":1}},"key":"ZMNoK1mQEQ"}],"key":"fZhsfRw0bB"},{"type":"paragraph","position":{"start":{"line":85,"column":1},"end":{"line":85,"column":1}},"children":[{"type":"text","value":"In short, posing the right problem is itself a hard problem, and one for which we have few systematic tools.","position":{"start":{"line":85,"column":1},"end":{"line":85,"column":1}},"key":"JAUJgHDcGt"}],"key":"ruZHZsY1zi"},{"type":"paragraph","position":{"start":{"line":87,"column":1},"end":{"line":87,"column":1}},"children":[{"type":"text","value":"Often, the only place we can turn is to people. Domain experts act, react, and judge, even when they cannot explain their reasoning explicitly. Their preferences show up in behavior, in corrections, in choices they make under pressure. If we cannot write down what we want, perhaps we can learn it indirectly from them.","position":{"start":{"line":87,"column":1},"end":{"line":87,"column":1}},"key":"KlQmmz6v28"}],"key":"C9GP9gPAzL"},{"type":"thematicBreak","position":{"start":{"line":89,"column":1},"end":{"line":89,"column":1}},"key":"SXhRA8mPJq"},{"type":"heading","depth":2,"position":{"start":{"line":91,"column":1},"end":{"line":91,"column":1}},"children":[{"type":"text","value":"Learning From Humans","position":{"start":{"line":91,"column":1},"end":{"line":91,"column":1}},"key":"Oj5UsExNYI"}],"identifier":"learning-from-humans","label":"Learning From Humans","html_id":"learning-from-humans","implicit":true,"key":"NwdbZEYJxx"},{"type":"paragraph","position":{"start":{"line":93,"column":1},"end":{"line":93,"column":1}},"children":[{"type":"text","value":"When we cannot write down the right behavior, we often try to learn it from examples. This is the idea behind imitation learning: watch the expert, then generalize. But in practice, good demonstrations are hard to collect and rarely cover the full range of relevant situations, especially the rare or risky ones.","position":{"start":{"line":93,"column":1},"end":{"line":93,"column":1}},"key":"Mh7PYaZFJ3"}],"key":"u1P0Qwd0Yn"},{"type":"paragraph","position":{"start":{"line":95,"column":1},"end":{"line":95,"column":1}},"children":[{"type":"text","value":"That is why many real-world applications require more than direct imitation. If we cannot show what to do in every case, we must express what we want. This is where reward design, cost functions, and preference modeling come in. These tools attempt to capture the underlying objective by observing both what people do and what they seem to value.","position":{"start":{"line":95,"column":1},"end":{"line":95,"column":1}},"key":"kOtdKlgJ17"}],"key":"mwR6IgRnhe"},{"type":"paragraph","position":{"start":{"line":97,"column":1},"end":{"line":97,"column":1}},"children":[{"type":"text","value":"Preference elicitation offers one route. Rather than specifying the optimal solution directly, we infer it from comparisons, rankings, or feedback. Under mild assumptions, the von Neumann–Morgenstern theorem tells us that such preferences correspond to a utility function. This principle forms the basis of ","position":{"start":{"line":97,"column":1},"end":{"line":97,"column":1}},"key":"ULZU4VtLCr"},{"type":"emphasis","position":{"start":{"line":97,"column":1},"end":{"line":97,"column":1}},"children":[{"type":"text","value":"Reinforcement Learning from Human Feedback","position":{"start":{"line":97,"column":1},"end":{"line":97,"column":1}},"key":"fc4NL3nNr8"}],"key":"sOKasoKajF"},{"type":"text","value":" (RLHF), now central to training large models.","position":{"start":{"line":97,"column":1},"end":{"line":97,"column":1}},"key":"rl5z0JhCku"}],"key":"k3qwsZX4gH"},{"type":"paragraph","position":{"start":{"line":99,"column":1},"end":{"line":99,"column":1}},"children":[{"type":"text","value":"But here, too, we face limits. Once we have inferred preferences or objectives from humans, what comes next? In many systems, the default answer is to treat this as a standard supervised learning problem: fit a black-box model to human-labeled data, then optimize the resulting predictions.","position":{"start":{"line":99,"column":1},"end":{"line":99,"column":1}},"key":"nDhdwjw40a"}],"key":"myKtIk5RX7"},{"type":"paragraph","position":{"start":{"line":101,"column":1},"end":{"line":101,"column":1}},"children":[{"type":"text","value":"This approach can go surprisingly far. Recent work, such as Decision Transformers, has shown that supervised learning can recover policies that perform competitively, sometimes even state-of-the-art. But these successes are often built on vast datasets, carefully curated environments, and tight control over evaluation. In the real world, we rarely have that luxury.","position":{"start":{"line":101,"column":1},"end":{"line":101,"column":1}},"key":"N3YRFTqyPz"}],"key":"wWzjLHk6oJ"},{"type":"paragraph","position":{"start":{"line":103,"column":1},"end":{"line":103,"column":1}},"children":[{"type":"text","value":"Supervised learning assumes that if we show enough examples, the system will generalize appropriately. But generalization is fragile when data is limited, feedback is partial, or the stakes are high. Without the right structure in place, we risk building policies that extrapolate poorly, violate constraints, or break in unexpected ways.","position":{"start":{"line":103,"column":1},"end":{"line":103,"column":1}},"key":"HrKEcbj2ct"}],"key":"surI6N5fop"},{"type":"paragraph","position":{"start":{"line":105,"column":1},"end":{"line":105,"column":1}},"children":[{"type":"text","value":"This is where modeling matters again.","position":{"start":{"line":105,"column":1},"end":{"line":105,"column":1}},"key":"PjkPEXudUD"}],"key":"lTOCY3yJmq"},{"type":"paragraph","position":{"start":{"line":107,"column":1},"end":{"line":107,"column":1}},"children":[{"type":"text","value":"By modeling the decision process, which includes the constraints, objectives, time structure, and information flows, we introduce the ","position":{"start":{"line":107,"column":1},"end":{"line":107,"column":1}},"key":"ylDVAy8grx"},{"type":"emphasis","position":{"start":{"line":107,"column":1},"end":{"line":107,"column":1}},"children":[{"type":"text","value":"right inductive biases","position":{"start":{"line":107,"column":1},"end":{"line":107,"column":1}},"key":"g6i7QZMpxY"}],"key":"US22t2gSVe"},{"type":"text","value":" into the learning system. These biases are not arbitrary. They reflect how the world works and what the agent can and cannot do. They make learning tractable even when data is scarce and help ensure that the resulting decisions behave reasonably under uncertainty.","position":{"start":{"line":107,"column":1},"end":{"line":107,"column":1}},"key":"ePKvtDbNAN"}],"key":"RsRjx0nHuY"},{"type":"paragraph","position":{"start":{"line":109,"column":1},"end":{"line":109,"column":1}},"children":[{"type":"text","value":"So while supervised learning plays a role, it is not the full story. The point is to ","position":{"start":{"line":109,"column":1},"end":{"line":109,"column":1}},"key":"ZnBb9KR1p2"},{"type":"strong","position":{"start":{"line":109,"column":1},"end":{"line":109,"column":1}},"children":[{"type":"text","value":"embed","position":{"start":{"line":109,"column":1},"end":{"line":109,"column":1}},"key":"nmjUoe68PC"}],"key":"JEyiXWeQzP"},{"type":"text","value":" what we have learned into a framework where decision-making remains accountable, robust, and grounded in structure.","position":{"start":{"line":109,"column":1},"end":{"line":109,"column":1}},"key":"mOoXwzvXz0"}],"key":"C6Y49U1alj"},{"type":"paragraph","position":{"start":{"line":111,"column":1},"end":{"line":111,"column":1}},"children":[{"type":"text","value":"That is the aim of this book: to take what we can learn from humans and from data, and combine it with modeling discipline to build systems that act for the right reasons.","position":{"start":{"line":111,"column":1},"end":{"line":111,"column":1}},"key":"f8cgaDWdhv"}],"key":"kXaOvlCXF3"},{"type":"heading","depth":2,"position":{"start":{"line":114,"column":1},"end":{"line":114,"column":1}},"children":[{"type":"text","value":"The Path Forward","position":{"start":{"line":114,"column":1},"end":{"line":114,"column":1}},"key":"cox0QAe8Xf"}],"identifier":"the-path-forward","label":"The Path Forward","html_id":"the-path-forward","implicit":true,"key":"iAtcDKSs2Y"},{"type":"paragraph","position":{"start":{"line":116,"column":1},"end":{"line":116,"column":1}},"children":[{"type":"text","value":"Rust’s critique remains timely. After decades of algorithmic progress, we still struggle to help people make better decisions in the settings that matter most. Reinforcement learning has pushed the boundaries of simulation. But in practice, its reach remains limited, not because the tools are broken, but because we have struggled to formulate problems in ways that connect to the real world.","position":{"start":{"line":116,"column":1},"end":{"line":116,"column":1}},"key":"puQzpEZRXL"}],"key":"GB0p8WGuXl"},{"type":"paragraph","position":{"start":{"line":118,"column":1},"end":{"line":118,"column":1}},"children":[{"type":"text","value":"It is not that decision problems cannot be solved. It is that we often fail to pose them in solvable form.","position":{"start":{"line":118,"column":1},"end":{"line":118,"column":1}},"key":"G1XZJMZBTE"}],"key":"j5QZApUh8w"},{"type":"paragraph","position":{"start":{"line":120,"column":1},"end":{"line":120,"column":1}},"children":[{"type":"text","value":"This is not a limitation of algorithms. It is a limitation in how we frame our goals.","position":{"start":{"line":120,"column":1},"end":{"line":120,"column":1}},"key":"TttlW5zpkY"}],"key":"tqBAqVks8X"},{"type":"paragraph","position":{"start":{"line":122,"column":1},"end":{"line":122,"column":1}},"children":[{"type":"text","value":"But that may be changing.","position":{"start":{"line":122,"column":1},"end":{"line":122,"column":1}},"key":"h1befeKV1t"}],"key":"Wf1yjvkfs3"},{"type":"paragraph","position":{"start":{"line":124,"column":1},"end":{"line":124,"column":1}},"children":[{"type":"text","value":"Rust once wrote:","position":{"start":{"line":124,"column":1},"end":{"line":124,"column":1}},"key":"n0haK3JinI"}],"key":"M570NJsOQX"},{"type":"blockquote","position":{"start":{"line":126,"column":1},"end":{"line":126,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":126,"column":1},"end":{"line":126,"column":1}},"children":[{"type":"emphasis","position":{"start":{"line":126,"column":1},"end":{"line":126,"column":1}},"children":[{"type":"text","value":"“Humans are learning to replicate the type of subconscious model building that goes on inside their brains and bring it to the conscious, formal level, but they are doing this modeling themselves, since it is not clear ","position":{"start":{"line":126,"column":1},"end":{"line":126,"column":1}},"key":"QDcZT42pqZ"},{"type":"strong","position":{"start":{"line":126,"column":1},"end":{"line":126,"column":1}},"children":[{"type":"text","value":"how to teach computers how to model","position":{"start":{"line":126,"column":1},"end":{"line":126,"column":1}},"key":"RZPtHBvbl1"}],"key":"qiHl7HDdJ3"},{"type":"text","value":".”","position":{"start":{"line":126,"column":1},"end":{"line":126,"column":1}},"key":"bh4oyizp3v"}],"key":"SmVjzBb8cf"}],"key":"OaDQQ3evbs"}],"key":"C7tb4az0uL"},{"type":"paragraph","position":{"start":{"line":128,"column":1},"end":{"line":128,"column":1}},"children":[{"type":"text","value":"That might have been true then. But today, we are beginning to see what it might look like to ","position":{"start":{"line":128,"column":1},"end":{"line":128,"column":1}},"key":"rG0ahbL3pV"},{"type":"emphasis","position":{"start":{"line":128,"column":1},"end":{"line":128,"column":1}},"children":[{"type":"text","value":"teach","position":{"start":{"line":128,"column":1},"end":{"line":128,"column":1}},"key":"k4x1nC3kBI"}],"key":"XsMQSzmTlO"},{"type":"text","value":" machines to model, or at least assist us in doing so.","position":{"start":{"line":128,"column":1},"end":{"line":128,"column":1}},"key":"CD4AyMqB3F"}],"key":"Q8sUwKTr3Q"},{"type":"paragraph","position":{"start":{"line":130,"column":1},"end":{"line":130,"column":1}},"children":[{"type":"text","value":"One school of thought, inspired by Sutton’s long-term vision, imagines that we will not need to model at all. Instead, we build general-purpose agents trained on vast, unstructured experience. We do not hand them objectives or constraints. We do not define environments. We let them learn everything from scratch. The hope is that once such an agent is sufficiently broad and capable, it can generalize everywhere, even to tasks we have not yet imagined.","position":{"start":{"line":130,"column":1},"end":{"line":130,"column":1}},"key":"HQxWVF5OzT"}],"key":"jbFSaOFjKh"},{"type":"paragraph","position":{"start":{"line":132,"column":1},"end":{"line":132,"column":1}},"children":[{"type":"text","value":"That is a bold bet. But it is still a bet. And if your goal is to solve meaningful problems now, in healthcare, infrastructure, climate, or logistics, then the challenge of formulation does not disappear. Even the most flexible agent cannot act reliably in a domain where the goals, tradeoffs, and structure remain unclear.","position":{"start":{"line":132,"column":1},"end":{"line":132,"column":1}},"key":"BFzjsapZyx"}],"key":"a2pBtdLEmY"},{"type":"paragraph","position":{"start":{"line":134,"column":1},"end":{"line":134,"column":1}},"children":[{"type":"text","value":"What is more, our current models of generality, especially large language models, are not agents in the classical sense. They produce text, but they do not act in the world. They reason in language, but they do not optimize over time. Despite growing trends to describe these systems as “agents,” they are better understood as modeling tools: systems trained on enormous corpora of human knowledge, capable of reflecting, translating, and helping us express complex ideas.","position":{"start":{"line":134,"column":1},"end":{"line":134,"column":1}},"key":"o0DEh8Hwtq"}],"key":"YwODrHioAu"},{"type":"paragraph","position":{"start":{"line":136,"column":1},"end":{"line":136,"column":1}},"children":[{"type":"text","value":"And that might be their greatest strength.","position":{"start":{"line":136,"column":1},"end":{"line":136,"column":1}},"key":"jRresqOLao"}],"key":"KGf1M5pq90"},{"type":"paragraph","position":{"start":{"line":138,"column":1},"end":{"line":138,"column":1}},"children":[{"type":"text","value":"We may not hand full control to a language model anytime soon, but we ","position":{"start":{"line":138,"column":1},"end":{"line":138,"column":1}},"key":"cwKSozc6QL"},{"type":"emphasis","position":{"start":{"line":138,"column":1},"end":{"line":138,"column":1}},"children":[{"type":"text","value":"will","position":{"start":{"line":138,"column":1},"end":{"line":138,"column":1}},"key":"VvGeNrqRfs"}],"key":"VMIyQC8pb6"},{"type":"text","value":" use these systems to help us model. They will assist in articulating objectives, surfacing hidden assumptions, identifying constraints, and mapping informal goals into structured forms. They will not replace modeling; they will augment it.","position":{"start":{"line":138,"column":1},"end":{"line":138,"column":1}},"key":"KfRpldWxPh"}],"key":"pEJ1OZ5xId"},{"type":"paragraph","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"children":[{"type":"text","value":"And what takes action, what makes real decisions in the world, will still rely on explicit optimization, grounded in formal structure, and designed to behave predictably. The language model may help us design that system, but it will not be the one executing it.","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"aemseobS3a"}],"key":"decHF67Fiz"},{"type":"paragraph","position":{"start":{"line":142,"column":1},"end":{"line":142,"column":1}},"children":[{"type":"text","value":"That is where I believe the near future lies: general-purpose models as modeling assistants, paired with optimization and control systems that retain structure, constraints, and accountability.","position":{"start":{"line":142,"column":1},"end":{"line":142,"column":1}},"key":"wSHeVLH3pn"}],"key":"pdKJFXrc8v"},{"type":"paragraph","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"children":[{"type":"text","value":"This book is about building that bridge: from goals to models, from data to decisions, from abstraction to action.","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"M9dHreNHFY"}],"key":"LqbGB5z8Go"},{"type":"paragraph","position":{"start":{"line":146,"column":1},"end":{"line":146,"column":1}},"children":[{"type":"text","value":"This is the modeling mindset. And it is what turns reinforcement learning into a practical tool for solving real problems.","position":{"start":{"line":146,"column":1},"end":{"line":146,"column":1}},"key":"Sz2AiJGrtj"}],"key":"lECa0kpva9"}],"key":"Cd0bxhIeLG"}],"key":"SefgndpPFw"},"references":{"cite":{"order":["FDA2025","Kleijnen2024","Chen2025","Evans2018","Meta2024","Uber2025","ANYbotics2023","Iskhakov2020","Rust1996","SuttonBarto2018"],"data":{"FDA2025":{"label":"FDA2025","enumerator":"1","html":"U.S. Food and Drug Administration. (2025). \u003ci\u003eArtificial Intelligence and Machine Learning (AI/ML)-Enabled Medical Devices\u003c/i\u003e. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-and-machine-learning-aiml-enabled-medical-devices\"\u003ehttps://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-and-machine-learning-aiml-enabled-medical-devices\u003c/a\u003e","url":"https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-and-machine-learning-aiml-enabled-medical-devices"},"Kleijnen2024":{"label":"Kleijnen2024","enumerator":"2","html":"Kleijnen, J., \u0026 others. (2024). Scoping review of prospective evaluations of AI in healthcare decision-making. \u003ci\u003eLancet Digital Health\u003c/i\u003e, \u003ci\u003e6\u003c/i\u003e(3), e200–e212."},"Chen2025":{"label":"Chen2025","enumerator":"3","html":"Chen, L., \u0026 others. (2025). \u003ci\u003eA Review of Real-World Deployments of Reinforcement Learning and MPC in HVAC Systems\u003c/i\u003e. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://example.com/hvac-rl-review\"\u003ehttps://example.com/hvac-rl-review\u003c/a\u003e","url":"https://example.com/hvac-rl-review"},"Evans2018":{"label":"Evans2018","enumerator":"4","html":"Evans, D., \u0026 DeepMind. (2018). \u003ci\u003eDeepMind AI Reduces Google Data Centre Cooling Bill by 40%\u003c/i\u003e. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill\"\u003ehttps://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill\u003c/a\u003e","url":"https://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill"},"Meta2024":{"label":"Meta2024","enumerator":"5","html":"Meta AI. (2024). \u003ci\u003eReinforcement Learning for Sustainable Cooling in Data Centers\u003c/i\u003e. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://engineering.fb.com/2024/09/10/data-center/rl-sustainable-cooling\"\u003ehttps://engineering.fb.com/2024/09/10/data-center/rl-sustainable-cooling\u003c/a\u003e","url":"https://engineering.fb.com/2024/09/10/data-center/rl-sustainable-cooling"},"Uber2025":{"label":"Uber2025","enumerator":"6","html":"Uber AI Labs. (2025). \u003ci\u003eReinforcement Learning at Scale for Ride-Matching Optimization\u003c/i\u003e. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://www.uber.com/blog/rl-ride-matching\"\u003ehttps://www.uber.com/blog/rl-ride-matching\u003c/a\u003e","url":"https://www.uber.com/blog/rl-ride-matching"},"ANYbotics2023":{"label":"ANYbotics2023","enumerator":"7","html":"ANYbotics. (2023). \u003ci\u003eANYmal: RL-Powered Autonomous Inspection\u003c/i\u003e. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://www.anybotics.com/anymal-inspection\"\u003ehttps://www.anybotics.com/anymal-inspection\u003c/a\u003e","url":"https://www.anybotics.com/anymal-inspection"},"Iskhakov2020":{"label":"Iskhakov2020","enumerator":"8","doi":"10.1093/ectj/utaa019","html":"Iskhakov, F., Rust, J., \u0026 Schjerning, B. (2020). Machine learning and structural econometrics: contrasts and synergies. \u003ci\u003eThe Econometrics Journal\u003c/i\u003e, \u003ci\u003e23\u003c/i\u003e(3), S81–S124. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.1093/ectj/utaa019\"\u003e10.1093/ectj/utaa019\u003c/a\u003e","url":"https://doi.org/10.1093/ectj/utaa019"},"Rust1996":{"label":"Rust1996","enumerator":"9","doi":"10.1016/s1574-0021(96)01016-7","html":"Rust, J. (1996). Chapter 14 Numerical dynamic programming in economics. In \u003ci\u003eHandbook of Computational Economics\u003c/i\u003e (pp. 619–729). Elsevier. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.1016/s1574-0021(96)01016-7\"\u003e10.1016/s1574-0021(96)01016-7\u003c/a\u003e","url":"https://doi.org/10.1016/s1574-0021(96)01016-7"},"SuttonBarto2018":{"label":"SuttonBarto2018","enumerator":"10","html":"Sutton, R. S., \u0026 Barto, A. G. (2018). \u003ci\u003eReinforcement Learning: An Introduction\u003c/i\u003e (2nd ed.). MIT Press. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"http://incompleteideas.net/book/the-book-2nd.html\"\u003ehttp://incompleteideas.net/book/the-book-2nd.html\u003c/a\u003e","url":"http://incompleteideas.net/book/the-book-2nd.html"}}}},"footer":{"navigation":{"next":{"title":"Why Build a Model? For Whom?","url":"/modeling","group":"Modeling"}}},"domain":"http://localhost:3003"},"project":{"bibliography":["/Users/pierre-luc.bacon/Documents/rlbook/references.bib"],"math":{"\\Proj":{"macro":"\\mathsf{P}"},"\\Residual":{"macro":"\\mathsf{N}"},"\\Contraction":{"macro":"\\mathsf{T}"},"\\Bellman":{"macro":"\\mathsf{L}"},"\\BellmanPi":{"macro":"\\mathsf{L}_\\pi"},"\\BellmanQuad":{"macro":"\\widetilde{\\mathsf{L}}"},"\\EulerResidual":{"macro":"\\mathsf{H}"}},"exports":[{"format":"pdf","filename":"book.pdf","url":"/rlbook/build/book-bceca6481db041911b084ebd03cc1269.pdf"}],"title":"Reinforcement Learning Beyond the Agent Loop: Models, Control, and Data","description":"A graduate-level introduction to reinforcement learning as a framework for modeling, optimization, and control, connecting dynamic models, data, and applications beyond standard benchmarks.","authors":[{"nameParsed":{"literal":"Pierre-Luc Bacon","given":"Pierre-Luc","family":"Bacon"},"name":"Pierre-Luc Bacon","id":"contributors-myst-generated-uid-0"}],"github":"https://github.com/pierrelux/rlbook","id":"3c112bb5-93ac-429f-8216-c0b9947c00d4","toc":[{"file":"intro.md"},{"children":[{"file":"modeling.md"},{"file":"ssm.md"},{"file":"simulation.md"}],"title":"Modeling"},{"children":[{"file":"ocp.md"},{"file":"cocp.md"}],"title":"Numerical Trajectory Optimization"},{"children":[{"file":"mpc.md"},{"file":"dp.md"}],"title":"From Trajectories to Policies"},{"children":[{"file":"regmdp.md"},{"file":"projdp.md"},{"children":[{"file":"simadp.md"},{"file":"batch_rl.md"},{"file":"online_rl.md"}],"title":"Simulation-Based Methods"},{"file":"cadp.md"}],"title":"Approximate Dynamic Programming"},{"children":[{"file":"appendix_examples.md"},{"file":"appendix_ivps.md"},{"file":"appendix_nlp.md"}],"title":"Appendix"},{"children":[{"file":"bibliography.md"}],"title":"References"}],"index":"index","pages":[{"level":1,"title":"Modeling"},{"slug":"modeling","title":"Why Build a Model? For Whom?","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"ssm","title":"Dynamics Models for Decision Making","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"simulation","title":"Programs as Models","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Numerical Trajectory Optimization"},{"slug":"ocp","title":"Discrete-Time Trajectory Optimization","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"cocp","title":"Trajectory Optimization in Continuous Time","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"From Trajectories to Policies"},{"slug":"mpc","title":"Model Predictive Control","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"dp","title":"Dynamic Programming","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Approximate Dynamic Programming"},{"slug":"regmdp","title":"Smooth Bellman Optimality Equations","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"projdp","title":"Weighted Residual Methods for Functional Equations","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":2,"title":"Simulation-Based Methods"},{"slug":"simadp","title":"Simulation-Based Approximate Dynamic Programming","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"batch-rl","title":"Batch Reinforcement Learning","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"online-rl","title":"Online Reinforcement Learning","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"cadp","title":"Policy Parametrization Methods","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Appendix"},{"slug":"appendix-examples","title":"Example COCPs","description":"","date":"","thumbnail":"/rlbook/build/heat_exchanger-acfdd83b1501b4c220f686fe21df7820.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"appendix-ivps","title":"Solving Initial Value Problems","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"appendix-nlp","title":"Nonlinear Programming","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"References"},{"slug":"bibliography","title":"Bibliography","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2}]}}},"actionData":null,"errors":null},"future":{"unstable_dev":false,"unstable_postcss":false,"unstable_tailwind":false,"v2_errorBoundary":true,"v2_headers":true,"v2_meta":true,"v2_normalizeFormMethod":true,"v2_routeConvention":true}};</script><script type="module" async="">import "/rlbook/build/manifest-ED236704.js";
import * as route0 from "/rlbook/build/root-ZJOPFBMV.js";
import * as route1 from "/rlbook/build/routes/$-CQPS5IOR.js";
window.__remixRouteModules = {"root":route0,"routes/$":route1};

import("/rlbook/build/entry.client-UNPC4GT3.js");</script></body></html>