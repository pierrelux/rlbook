<!DOCTYPE html><html lang="en" class="" style="scroll-padding:60px"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width,initial-scale=1"/><title>Why This Book? - Building Up RL: From Dynamics and Control to Learning</title><meta property="og:title" content="Why This Book? - Building Up RL: From Dynamics and Control to Learning"/><meta name="generator" content="mystmd"/><meta name="description" content="A graduate-level introduction to reinforcement learning as a framework for modeling, optimization, and control, connecting dynamic models, data, and applications beyond standard benchmarks."/><meta property="og:description" content="A graduate-level introduction to reinforcement learning as a framework for modeling, optimization, and control, connecting dynamic models, data, and applications beyond standard benchmarks."/><meta name="keywords" content=""/><link rel="stylesheet" href="/rlbook/build/_assets/app-IZWEOBHI.css"/><link rel="stylesheet" href="/rlbook/build/_assets/thebe-core-VKVHG5VY.css"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jupyter-matplotlib@0.11.3/css/mpl_widget.css"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.css"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous"/><link rel="icon" href="/rlbook/favicon.ico"/><link rel="stylesheet" href="/rlbook/myst-theme.css"/><script>
  const savedTheme = localStorage.getItem("myst:theme");
  const theme = window.matchMedia("(prefers-color-scheme: light)").matches ? 'light' : 'dark';
  const classes = document.documentElement.classList;
  const hasAnyTheme = classes.contains('light') || classes.contains('dark');
  if (!hasAnyTheme) classes.add(savedTheme ?? theme);
</script></head><body class="m-0 transition-colors duration-500 bg-white dark:bg-stone-900"><div class="fixed top-1 left-1 h-[0px] w-[0px] focus-within:z-40 focus-within:h-auto focus-within:w-auto bg-white overflow-hidden focus-within:p-2 focus-within:ring-1" aria-label="skip to content options"><a href="#skip-to-frontmatter" class="block px-2 py-1 text-black underline">Skip to article frontmatter</a><a href="#skip-to-article" class="block px-2 py-1 text-black underline">Skip to article content</a></div><div class="bg-white/80 backdrop-blur dark:bg-stone-900/80 shadow dark:shadow-stone-700 p-3 md:px-8 sticky w-screen top-0 z-30 h-[60px]"><nav class="flex items-center justify-between flex-nowrap max-w-[1440px] mx-auto"><div class="flex flex-row xl:min-w-[19.5rem] mr-2 sm:mr-7 justify-start items-center shrink-0"><div class="block xl:hidden"><button class="flex items-center border-stone-400 text-stone-800 hover:text-stone-900 dark:text-stone-200 hover:dark:text-stone-100"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" width="2rem" height="2rem" class="m-1"><path fill-rule="evenodd" d="M3 6.75A.75.75 0 0 1 3.75 6h16.5a.75.75 0 0 1 0 1.5H3.75A.75.75 0 0 1 3 6.75ZM3 12a.75.75 0 0 1 .75-.75h16.5a.75.75 0 0 1 0 1.5H3.75A.75.75 0 0 1 3 12Zm0 5.25a.75.75 0 0 1 .75-.75h16.5a.75.75 0 0 1 0 1.5H3.75a.75.75 0 0 1-.75-.75Z" clip-rule="evenodd"></path></svg><span class="sr-only">Open Menu</span></button></div><a class="flex items-center ml-3 dark:text-white w-fit md:ml-5 xl:ml-7" href="/rlbook/"><div class="p-1 mr-3 dark:bg-white dark:rounded"><img src="/rlbook/build/logo-439a25d1a718f5f15fe2838e2a61a91a.png" class="h-9" alt="RL &amp; Control" height="2.25rem"/></div><span class="text-md sm:text-xl tracking-tight sm:mr-5">RL &amp; Control</span></a></div><div class="flex items-center flex-grow w-auto"><div class="flex-grow hidden text-md lg:block"></div><div class="flex-grow block"></div><button type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:R74op:" data-state="closed" class="flex items-center h-10 aspect-square sm:w-64 text-left text-gray-400 border border-gray-300 dark:border-gray-600 rounded-lg bg-gray-50 dark:bg-gray-700 hover:ring-blue-500 dark:hover:ring-blue-500 hover:border-blue-500 dark:hover:border-blue-500"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="p-2.5 h-10 w-10 aspect-square"><path fill-rule="evenodd" d="M10.5 3.75a6.75 6.75 0 1 0 0 13.5 6.75 6.75 0 0 0 0-13.5ZM2.25 10.5a8.25 8.25 0 1 1 14.59 5.28l4.69 4.69a.75.75 0 1 1-1.06 1.06l-4.69-4.69A8.25 8.25 0 0 1 2.25 10.5Z" clip-rule="evenodd"></path></svg><span class="hidden sm:block grow">Search</span><div aria-hidden="true" class="items-center hidden mx-1 font-mono text-sm text-gray-400 sm:flex gap-x-1"><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none hide-mac">CTRL</kbd><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none show-mac">⌘</kbd><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none ">K</kbd><script>
;(() => {
const script = document.currentScript;
const root = script.parentElement;

const isMac = /mac/i.test(
      window.navigator.userAgentData?.platform ?? window.navigator.userAgent,
    );
root.querySelectorAll(".hide-mac").forEach(node => {node.classList.add(isMac ? "hidden" : "block")});
root.querySelectorAll(".show-mac").forEach(node => {node.classList.add(!isMac ? "hidden" : "block")});
})()</script></div></button><button class="theme rounded-full aspect-square border border-stone-700 dark:border-white hover:bg-neutral-100 border-solid overflow-hidden text-stone-700 dark:text-white hover:text-stone-500 dark:hover:text-neutral-800 w-8 h-8 mx-3" title="Toggle theme between light and dark mode" aria-label="Toggle theme between light and dark mode"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="h-full w-full p-0.5 hidden dark:block"><path fill-rule="evenodd" d="M9.528 1.718a.75.75 0 0 1 .162.819A8.97 8.97 0 0 0 9 6a9 9 0 0 0 9 9 8.97 8.97 0 0 0 3.463-.69.75.75 0 0 1 .981.98 10.503 10.503 0 0 1-9.694 6.46c-5.799 0-10.5-4.7-10.5-10.5 0-4.368 2.667-8.112 6.46-9.694a.75.75 0 0 1 .818.162Z" clip-rule="evenodd"></path></svg><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-full w-full p-0.5 dark:hidden"><path stroke-linecap="round" stroke-linejoin="round" d="M12 3v2.25m6.364.386-1.591 1.591M21 12h-2.25m-.386 6.364-1.591-1.591M12 18.75V21m-4.773-4.227-1.591 1.591M5.25 12H3m4.227-4.773L5.636 5.636M15.75 12a3.75 3.75 0 1 1-7.5 0 3.75 3.75 0 0 1 7.5 0Z"></path></svg></button><div class="block sm:hidden"><div class="relative" data-headlessui-state=""><div><button class="flex text-sm bg-transparent rounded-full focus:outline-none" id="headlessui-menu-button-:Rr4op:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open Menu</span><div class="flex items-center text-stone-200 hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" width="2rem" height="2rem" class="p-1"><path fill-rule="evenodd" d="M10.5 6a1.5 1.5 0 1 1 3 0 1.5 1.5 0 0 1-3 0Zm0 6a1.5 1.5 0 1 1 3 0 1.5 1.5 0 0 1-3 0Zm0 6a1.5 1.5 0 1 1 3 0 1.5 1.5 0 0 1-3 0Z" clip-rule="evenodd"></path></svg></div></button></div></div></div><div class="hidden sm:block"><a href="https://github.com/pierrelux/rlbook" target="_blank" rel="noopener noreferrer" class="inline-block px-4 py-2 mx-1 mt-0 leading-none border rounded text-md border-stone-700 dark:border-white text-stone-700 dark:text-white hover:text-stone-500 dark:hover:text-neutral-800 hover:bg-neutral-100">View on GitHub</a></div></div></nav></div><div class="fixed xl:article-grid grid-gap xl:w-screen xl:pointer-events-none overflow-auto max-xl:min-w-[300px] hidden z-10" style="top:60px"><div class="pointer-events-auto xl:col-margin-left flex-col overflow-hidden hidden xl:flex"><div class="flex-grow py-6 overflow-y-auto primary-scrollbar"><nav aria-label="Navigation" class="overflow-y-hidden transition-opacity ml-3 xl:ml-0 mr-3 max-w-[350px] lg:hidden"><div class="w-full px-1 dark:text-white font-medium"></div></nav><div class="my-3 border-b-2 lg:hidden"></div><nav aria-label="Table of Contents" class="flex-grow overflow-y-hidden transition-opacity ml-3 xl:ml-0 mr-3 max-w-[350px]"><div class="w-full px-1 dark:text-white"><a title="Building Up RL: From Dynamics and Control to Learning" class="block break-words focus:outline outline-blue-200 outline-2 rounded p-2 my-1 rounded-lg hover:bg-slate-300/30 font-bold" href="/rlbook/">Building Up RL: From Dynamics and Control to Learning</a><div data-state="closed" class="w-full"><div class="flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="Modeling" class="block break-words rounded py-2 grow cursor-pointer">Modeling</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:Rmp8p:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:Rmp8p:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="closed" class="w-full"><div class="flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="Numerical Trajectory Optimization" class="block break-words rounded py-2 grow cursor-pointer">Numerical Trajectory Optimization</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:Rup8p:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:Rup8p:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="closed" class="w-full"><div class="flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="From Trajectories to Policies" class="block break-words rounded py-2 grow cursor-pointer">From Trajectories to Policies</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:R16p8p:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:R16p8p:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="closed" class="w-full"><div class="flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="Approximate Dynamic Programming" class="block break-words rounded py-2 grow cursor-pointer">Approximate Dynamic Programming</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:R1ep8p:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:R1ep8p:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="closed" class="w-full"><div class="flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><div title="Appendix" class="block break-words rounded py-2 grow cursor-pointer">Appendix</div><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:R1mp8p:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:R1mp8p:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div></div></nav></div><div class="flex-none py-6 transition-all duration-700 translate-y-6 opacity-0"><a class="flex mx-auto text-gray-700 w-fit hover:text-blue-700 dark:text-gray-200 dark:hover:text-blue-400" href="https://mystmd.org/made-with-myst" target="_blank" rel="noreferrer"><svg style="width:24px;height:24px" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100" stroke="none"><g id="icon"><path fill="currentColor" d="M23.8,54.8v-3.6l4.7-0.8V17.5l-4.7-0.8V13H36l13.4,31.7h0.2l13-31.7h12.6v3.6l-4.7,0.8v32.9l4.7,0.8v3.6h-15
          v-3.6l4.9-0.8V20.8H65L51.4,53.3h-3.8l-14-32.5h-0.1l0.2,17.4v12.1l5,0.8v3.6H23.8z"></path><path fill="#F37726" d="M47,86.9c0-5.9-3.4-8.8-10.1-8.8h-8.4c-5.2,0-9.4-1.3-12.5-3.8c-3.1-2.5-5.4-6.2-6.8-11l4.8-1.6
          c1.8,5.6,6.4,8.6,13.8,8.8h9.2c6.4,0,10.8,2.5,13.1,7.5c2.3-5,6.7-7.5,13.1-7.5h8.4c7.8,0,12.7-2.9,14.6-8.7l4.8,1.6
          c-1.4,4.9-3.6,8.6-6.8,11.1c-3.1,2.5-7.3,3.7-12.4,3.8H63c-6.7,0-10,2.9-10,8.8"></path></g></svg><span class="self-center ml-2 text-sm">Made with MyST</span></a></div></div></div><main class="article-grid grid-gap"><article class="article-grid subgrid-gap col-screen article content"><div class="hidden"></div><div id="skip-to-frontmatter" aria-label="article frontmatter" class="mb-8 pt-9"><div class="flex items-center mb-5 h-6 text-sm font-light"><div class="flex-grow"></div><a href="https://github.com/pierrelux/rlbook" title="GitHub Repository: pierrelux/rlbook" target="_blank" rel="noopener noreferrer" class="text-inherit hover:text-inherit"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="1.25rem" height="1.25rem" class="inline-block mr-1 opacity-60 hover:opacity-100"><path d="M12 2.5c-5.4 0-9.8 4.4-9.8 9.7 0 4.3 2.8 8 6.7 9.2.5.1.7-.2.7-.5v-1.8c-2.4.5-3.1-.6-3.3-1.1-.1-.3-.6-1.1-1-1.4-.3-.2-.8-.6 0-.6s1.3.7 1.5 1c.9 1.5 2.3 1.1 2.8.8.1-.6.3-1.1.6-1.3-2.2-.2-4.4-1.1-4.4-4.8 0-1.1.4-1.9 1-2.6-.1-.2-.4-1.2.1-2.6 0 0 .8-.3 2.7 1 .8-.2 1.6-.3 2.4-.3.8 0 1.7.1 2.4.3 1.9-1.3 2.7-1 2.7-1 .5 1.3.2 2.3.1 2.6.6.7 1 1.5 1 2.6 0 3.7-2.3 4.6-4.4 4.8.4.3.7.9.7 1.8V21c0 .3.2.6.7.5 3.9-1.3 6.6-4.9 6.6-9.2 0-5.4-4.4-9.8-9.8-9.8z"></path></svg></a><a href="https://github.com/pierrelux/rlbook/edit/main/intro.md" title="Edit This Page" target="_blank" rel="noopener noreferrer" class="text-inherit hover:text-inherit"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.25rem" height="1.25rem" class="inline-block mr-1 opacity-60 hover:opacity-100"><path stroke-linecap="round" stroke-linejoin="round" d="m16.862 4.487 1.687-1.688a1.875 1.875 0 1 1 2.652 2.652L10.582 16.07a4.5 4.5 0 0 1-1.897 1.13L6 18l.8-2.685a4.5 4.5 0 0 1 1.13-1.897l8.932-8.931Zm0 0L19.5 7.125M18 14v4.75A2.25 2.25 0 0 1 15.75 21H5.25A2.25 2.25 0 0 1 3 18.75V8.25A2.25 2.25 0 0 1 5.25 6H10"></path></svg></a><div class="relative flex inline-block mx-1 grow-0" data-headlessui-state=""><button class="relative ml-2 -mr-1" id="headlessui-menu-button-:Rs8top:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Downloads</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.25rem" height="1.25rem"><title>Download</title><path stroke-linecap="round" stroke-linejoin="round" d="M3 16.5v2.25A2.25 2.25 0 0 0 5.25 21h13.5A2.25 2.25 0 0 0 21 18.75V16.5M16.5 12 12 16.5m0 0L7.5 12m4.5 4.5V3"></path></svg></button></div></div><h1 class="mb-0">Why This Book?</h1><header class="mt-4 not-prose"><div><span class="font-semibold text-sm inline-block"><button class="focus:shadow-[0_0_0_2px] focus:shadow-black outline-none hover:underline" aria-label="Author Details" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:R78top:" data-state="closed">Pierre-Luc Bacon</button></span></div></header></div><div class="block my-10 lg:sticky lg:z-10 lg:h-0 lg:pt-0 lg:my-0 lg:ml-10 lg:col-margin-right" style="top:60px"><nav></nav></div><div id="skip-to-article"></div><p>Reinforcement learning offers a powerful framework for decision-making: systems that learn to act through interaction with their environment. From AlphaGo defeating world champions to Uber’s ride-matching system optimizing across hundreds of cities <span class="cite-group parenthetical"><cite class="" data-state="closed"><a href="https://www.uber.com/blog/rl-ride-matching" target="_blank" rel="noreferrer" class="hover-link">Uber AI Labs, 2025</a></cite></span>, from chatbots engaging millions to ANYbotics’ quadruped robots performing complex industrial inspections with policies trained entirely in simulation <span class="cite-group parenthetical"><cite class="" data-state="closed"><a href="https://www.anybotics.com/anymal-inspection" target="_blank" rel="noreferrer" class="hover-link">ANYbotics, 2023</a></cite></span>, RL has demonstrated remarkable capabilities.</p><p>Yet compared to supervised learning, which has become routine in industry, reinforcement learning has not achieved the same widespread adoption. Supervised learning benefits from standardized tools and well-defined interfaces: inputs, outputs, and objective metrics. Reinforcement learning, by contrast, requires explicit problem formulation: defining objectives, constraints, and how decisions unfold over time. This additional structure is also what makes RL applicable to a broader class of problems.</p><p>As <span class="cite-group narrative"><cite class="" data-state="closed"><a href="https://doi.org/10.1093/ectj/utaa019" target="_blank" rel="noreferrer" class="hover-link">Iskhakov <em>et al.</em> (2020)</a></cite></span> notes, a primary challenge is <em>“the difficulty of learning about the objective function and environment facing real-world decision-makers.”</em> We cannot sidestep defining the problem, the objective, and the constraints.</p><p>Working in industry and consulting taught me what my PhD did not: real problems rarely fit neatly into predefined frameworks. Sensors produce noisy data; constraints are non-negotiable; objectives may shift or conflict. Most effort goes into formulating the decision problem, long before selecting an algorithm.</p><p>The chapters that follow address this challenge explicitly. They offer strategies to bridge the gap from theoretical RL formulations to practically useful systems. By carefully structuring decision problems, we can help reinforcement learning achieve broader impact.</p><h2 id="the-decision-problem" class="relative group"><span class="heading-text">The Decision Problem</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#the-decision-problem" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>The term <em>reinforcement learning</em> gets used in many ways. In the formal sense defined by <span class="cite-group narrative"><cite class="" data-state="closed"><a href="http://incompleteideas.net/book/the-book-2nd.html" target="_blank" rel="noreferrer" class="hover-link">Sutton &amp; Barto (2018)</a></cite></span>, RL is a problem: learning to act through interaction with an environment. But in common usage, it can mean a family of algorithms, a research community, or a long-term scientific agenda.</p><p>This book takes a practical view: reinforcement learning as a vocabulary for reasoning about decisions under uncertainty. When optimization, feedback, and data intersect, we are in the territory of reinforcement learning, whether we use temporal-difference learning, model-based planning, or simple policy rules. What unifies these approaches is not a specific algorithm, but a shared structure: decision-making through experience.</p><p>In mainstream RL research, the problem is often treated as given. Sutton famously advises: <em>“Approximate the solution, not the problem.”</em> The agent should be shaped by experience, not by handcrafted structure. For Sutton, <em>the problem</em> is the world itself: complex, unknown, and handed to us as-is. We do not design it; we confront it.</p><p>This book takes a different stance. We consider a variety of decision problems, each with its own structure: finite or infinite horizon, discrete or continuous time, deterministic or stochastic dynamics. The problem is not handed to us; it must be defined. What are the goals? What decisions are available? What feedback is observable, and under what constraints?</p><p>This is what Operations Research has long emphasized. Through consulting, I saw it firsthand: production systems running overnight, MIP solvers optimizing decisions with XGBoost predictors as inputs, all live and working. In practice, these systems are doing reinforcement learning, just without calling it that. They define objectives, encode constraints, and optimize over time. The vocabulary differs, but the structure is the same.</p><h2 id="what-this-book-offers" class="relative group"><span class="heading-text">What This Book Offers</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#what-this-book-offers" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>Reinforcement learning did not develop in isolation. Its foundations draw from control theory, dynamic programming, operations research, and economics. Many of the same ideas appear under different names in different communities, often with complementary perspectives.</p><p>This book aims to give you a broader view of that landscape. Where do RL algorithms come from? What mathematical structures underlie them? How do they connect to classical methods in optimization and control? Understanding these connections helps you see when a method applies, when it does not, and what alternatives exist.</p><p>The goal is not to survey every technique superficially. It is to go deep on the mathematical foundations that are shared across methods: dynamic programming, function approximation, optimization, and the interplay between them. These structures recur throughout sequential decision-making, whether in reinforcement learning, control theory, or operations research. Master them once, and you can recognize them in different guises.</p><div></div><section id="references" class="article-grid subgrid-gap col-screen"><div><header class="text-lg font-semibold text-stone-900 dark:text-white group">References<a class="no-underline text-inherit hover:text-inherit ml-2 select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#references" title="Link to References" aria-label="Link to References">¶</a></header></div><div class="pl-3 mb-8 text-xs text-stone-500 dark:text-stone-300"><ol><li class="break-words" id="cite-Uber2025">Uber AI Labs. (2025). <i>Reinforcement Learning at Scale for Ride-Matching Optimization</i>. <a target="_blank" rel="noreferrer" href="https://www.uber.com/blog/rl-ride-matching">https://www.uber.com/blog/rl-ride-matching</a></li><li class="break-words" id="cite-ANYbotics2023">ANYbotics. (2023). <i>ANYmal: RL-Powered Autonomous Inspection</i>. <a target="_blank" rel="noreferrer" href="https://www.anybotics.com/anymal-inspection">https://www.anybotics.com/anymal-inspection</a></li><li class="break-words" id="cite-Iskhakov2020">Iskhakov, F., Rust, J., & Schjerning, B. (2020). Machine learning and structural econometrics: contrasts and synergies. <i>The Econometrics Journal</i>, <i>23</i>(3), S81–S124. <a target="_blank" rel="noreferrer" href="https://doi.org/10.1093/ectj/utaa019">10.1093/ectj/utaa019</a></li><li class="break-words" id="cite-SuttonBarto2018">Sutton, R. S., & Barto, A. G. (2018). <i>Reinforcement Learning: An Introduction</i> (2nd ed.). MIT Press. <a target="_blank" rel="noreferrer" href="http://incompleteideas.net/book/the-book-2nd.html">http://incompleteideas.net/book/the-book-2nd.html</a></li></ol></div></section><div class="flex pt-10 mb-10 space-x-4"><a class="flex-1 block p-4 font-normal text-gray-600 no-underline border border-gray-200 rounded shadow-sm group hover:border-blue-600 dark:hover:border-blue-400 hover:text-blue-600 dark:hover:text-blue-400 dark:text-gray-100 dark:border-gray-500 hover:shadow-lg dark:shadow-neutral-700" href="/rlbook/dynamics"><div class="flex h-full align-middle"><div class="flex-grow"><div class="text-xs text-gray-500 dark:text-gray-400">Modeling</div>Dynamics and State-Space Models</div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.5rem" height="1.5rem" class="self-center transition-transform group-hover:translate-x-1 shrink-0"><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 4.5 21 12m0 0-7.5 7.5M21 12H3"></path></svg></div></a></div></article></main><script>((a,d)=>{if(!window.history.state||!window.history.state.key){let h=Math.random().toString(32).slice(2);window.history.replaceState({key:h},"")}try{let f=JSON.parse(sessionStorage.getItem(a)||"{}")[d||window.history.state.key];typeof f=="number"&&window.scrollTo(0,f)}catch(h){console.error(h),sessionStorage.removeItem(a)}})("positions", null)</script><link rel="modulepreload" href="/rlbook/build/entry.client-UNPC4GT3.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-OCTKKCIL.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-UAI5KRM7.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-2NH4LW52.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-F7G67JTZ.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-HBJK6BW3.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-HYMQ7M2K.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-OHOXABTA.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-OCWQY3HK.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-CPTH56EW.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-3CVK3PYF.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-J6FHCSRC.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-S4SWV34C.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-GUCIBHGO.js"/><link rel="modulepreload" href="/rlbook/build/root-7TUVC4ZT.js"/><link rel="modulepreload" href="/rlbook/build/_shared/chunk-INOWNUZ6.js"/><link rel="modulepreload" href="/rlbook/build/routes/$-P6PGXPYX.js"/><script>window.__remixContext = {"url":"/index","state":{"loaderData":{"root":{"config":{"version":2,"myst":"1.6.3","options":{"logo":"/rlbook/build/logo-439a25d1a718f5f15fe2838e2a61a91a.png","logo_text":"RL \u0026 Control"},"nav":[],"actions":[{"title":"View on GitHub","url":"https://github.com/pierrelux/rlbook","internal":false,"static":false}],"projects":[{"bibliography":["/Users/pierre-luc.bacon/Documents/rlbook/references.bib"],"math":{"\\Proj":{"macro":"\\mathsf{P}"},"\\Residual":{"macro":"\\mathsf{N}"},"\\Contraction":{"macro":"\\mathsf{T}"},"\\Bellman":{"macro":"\\mathsf{L}"},"\\BellmanPi":{"macro":"\\mathsf{L}_\\pi"},"\\BellmanQuad":{"macro":"\\widetilde{\\mathsf{L}}"},"\\EulerResidual":{"macro":"\\mathsf{H}"},"\\Policy":{"macro":"\\pi"}},"exports":[{"format":"pdf","filename":"book.pdf","url":"/rlbook/build/book-bceca6481db041911b084ebd03cc1269.pdf"}],"title":"Building Up RL: From Dynamics and Control to Learning","description":"A graduate-level introduction to reinforcement learning as a framework for modeling, optimization, and control, connecting dynamic models, data, and applications beyond standard benchmarks.","authors":[{"nameParsed":{"literal":"Pierre-Luc Bacon","given":"Pierre-Luc","family":"Bacon"},"name":"Pierre-Luc Bacon","id":"contributors-myst-generated-uid-0"}],"github":"https://github.com/pierrelux/rlbook","id":"3c112bb5-93ac-429f-8216-c0b9947c00d4","toc":[{"file":"intro.md"},{"children":[{"file":"dynamics.md"}],"title":"Modeling"},{"children":[{"file":"trajectories.md"},{"file":"collocation.md"}],"title":"Numerical Trajectory Optimization"},{"children":[{"file":"mpc.md"},{"file":"dp.md"}],"title":"From Trajectories to Policies"},{"children":[{"file":"smoothing.md"},{"file":"projection.md"},{"children":[{"file":"montecarlo.md"},{"file":"fqi.md"},{"file":"amortization.md"},{"file":"pg.md"}],"title":"Simulation-Based Methods"}],"title":"Approximate Dynamic Programming"},{"children":[{"file":"appendix_examples.md"},{"file":"appendix_ivps.md"},{"file":"appendix_nlp.md"}],"title":"Appendix"}],"index":"index","pages":[{"level":1,"title":"Modeling"},{"slug":"dynamics","title":"Dynamics and State-Space Models","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Numerical Trajectory Optimization"},{"slug":"trajectories","title":"Discrete-Time Trajectory Optimization","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"collocation","title":"Trajectory Optimization in Continuous Time","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"From Trajectories to Policies"},{"slug":"mpc","title":"Model Predictive Control","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"dp","title":"Dynamic Programming","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Approximate Dynamic Programming"},{"slug":"smoothing","title":"Smooth Bellman Optimality Equations","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"projection","title":"Weighted Residual Methods for Functional Equations","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":2,"title":"Simulation-Based Methods"},{"slug":"montecarlo","title":"Monte Carlo Integration in Approximate Dynamic Programming","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"fqi","title":"Fitted Q-Iteration Methods","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"amortization","title":"Fitted Q-Iteration for Continuous Action Spaces","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"pg","title":"Policy Gradient Methods","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"level":1,"title":"Appendix"},{"slug":"appendix-examples","title":"Example COCPs","description":"","date":"","thumbnail":"/rlbook/build/heat_exchanger-36d42c7ce90c38ee24e241c608f46188.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"appendix-ivps","title":"Solving Initial Value Problems","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"appendix-nlp","title":"Nonlinear Programming","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2}]}]},"CONTENT_CDN_PORT":"3101","MODE":"static","BASE_URL":"/rlbook"},"routes/$":{"config":{"version":2,"myst":"1.6.3","options":{"logo":"/rlbook/build/logo-439a25d1a718f5f15fe2838e2a61a91a.png","logo_text":"RL \u0026 Control"},"nav":[],"actions":[{"title":"View on GitHub","url":"https://github.com/pierrelux/rlbook","internal":false,"static":false}],"projects":[{"bibliography":["/Users/pierre-luc.bacon/Documents/rlbook/references.bib"],"math":{"\\Proj":{"macro":"\\mathsf{P}"},"\\Residual":{"macro":"\\mathsf{N}"},"\\Contraction":{"macro":"\\mathsf{T}"},"\\Bellman":{"macro":"\\mathsf{L}"},"\\BellmanPi":{"macro":"\\mathsf{L}_\\pi"},"\\BellmanQuad":{"macro":"\\widetilde{\\mathsf{L}}"},"\\EulerResidual":{"macro":"\\mathsf{H}"},"\\Policy":{"macro":"\\pi"}},"exports":[{"format":"pdf","filename":"book.pdf","url":"/rlbook/build/book-bceca6481db041911b084ebd03cc1269.pdf"}],"title":"Building Up RL: From Dynamics and Control to Learning","description":"A graduate-level introduction to reinforcement learning as a framework for modeling, optimization, and control, connecting dynamic models, data, and applications beyond standard benchmarks.","authors":[{"nameParsed":{"literal":"Pierre-Luc Bacon","given":"Pierre-Luc","family":"Bacon"},"name":"Pierre-Luc Bacon","id":"contributors-myst-generated-uid-0"}],"github":"https://github.com/pierrelux/rlbook","id":"3c112bb5-93ac-429f-8216-c0b9947c00d4","toc":[{"file":"intro.md"},{"children":[{"file":"dynamics.md"}],"title":"Modeling"},{"children":[{"file":"trajectories.md"},{"file":"collocation.md"}],"title":"Numerical Trajectory Optimization"},{"children":[{"file":"mpc.md"},{"file":"dp.md"}],"title":"From Trajectories to Policies"},{"children":[{"file":"smoothing.md"},{"file":"projection.md"},{"children":[{"file":"montecarlo.md"},{"file":"fqi.md"},{"file":"amortization.md"},{"file":"pg.md"}],"title":"Simulation-Based Methods"}],"title":"Approximate Dynamic Programming"},{"children":[{"file":"appendix_examples.md"},{"file":"appendix_ivps.md"},{"file":"appendix_nlp.md"}],"title":"Appendix"}],"index":"index","pages":[{"level":1,"title":"Modeling"},{"slug":"dynamics","title":"Dynamics and State-Space Models","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Numerical Trajectory Optimization"},{"slug":"trajectories","title":"Discrete-Time Trajectory Optimization","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"collocation","title":"Trajectory Optimization in Continuous Time","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"From Trajectories to Policies"},{"slug":"mpc","title":"Model Predictive Control","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"dp","title":"Dynamic Programming","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Approximate Dynamic Programming"},{"slug":"smoothing","title":"Smooth Bellman Optimality Equations","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"projection","title":"Weighted Residual Methods for Functional Equations","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":2,"title":"Simulation-Based Methods"},{"slug":"montecarlo","title":"Monte Carlo Integration in Approximate Dynamic Programming","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"fqi","title":"Fitted Q-Iteration Methods","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"amortization","title":"Fitted Q-Iteration for Continuous Action Spaces","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"pg","title":"Policy Gradient Methods","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"level":1,"title":"Appendix"},{"slug":"appendix-examples","title":"Example COCPs","description":"","date":"","thumbnail":"/rlbook/build/heat_exchanger-36d42c7ce90c38ee24e241c608f46188.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"appendix-ivps","title":"Solving Initial Value Problems","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"appendix-nlp","title":"Nonlinear Programming","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2}]}]},"page":{"version":2,"kind":"Article","sha256":"ed36abbbb6f050c2d456d9b0cf8c0fdedd681cb20274a98121fd39d52c619ee2","slug":"index","location":"/intro.md","dependencies":[],"frontmatter":{"title":"Why This Book?","content_includes_title":false,"authors":[{"nameParsed":{"literal":"Pierre-Luc Bacon","given":"Pierre-Luc","family":"Bacon"},"name":"Pierre-Luc Bacon","id":"contributors-myst-generated-uid-0"}],"github":"https://github.com/pierrelux/rlbook","math":{"\\Proj":{"macro":"\\mathsf{P}"},"\\Residual":{"macro":"\\mathsf{N}"},"\\Contraction":{"macro":"\\mathsf{T}"},"\\Bellman":{"macro":"\\mathsf{L}"},"\\BellmanPi":{"macro":"\\mathsf{L}_\\pi"},"\\BellmanQuad":{"macro":"\\widetilde{\\mathsf{L}}"},"\\EulerResidual":{"macro":"\\mathsf{H}"},"\\Policy":{"macro":"\\pi"}},"source_url":"https://github.com/pierrelux/rlbook/blob/main/intro.md","edit_url":"https://github.com/pierrelux/rlbook/edit/main/intro.md","exports":[{"format":"md","filename":"intro.md","url":"/rlbook/build/intro-827fa783c5783b2cccf205c3d38fdce1.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Reinforcement learning offers a powerful framework for decision-making: systems that learn to act through interaction with their environment. From AlphaGo defeating world champions to Uber’s ride-matching system optimizing across hundreds of cities ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"RDLbQvPBZn"},{"type":"citeGroup","kind":"parenthetical","children":[{"type":"cite","kind":"parenthetical","label":"Uber2025","identifier":"uber2025","children":[{"type":"text","value":"Uber AI Labs, 2025","key":"ie1orUEWMp"}],"enumerator":"1","key":"Ao5IRcDJEE"}],"key":"uFV8rnF6rG"},{"type":"text","value":", from chatbots engaging millions to ANYbotics’ quadruped robots performing complex industrial inspections with policies trained entirely in simulation ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"RDB6NvCqo7"},{"type":"citeGroup","kind":"parenthetical","children":[{"type":"cite","kind":"parenthetical","label":"ANYbotics2023","identifier":"anybotics2023","children":[{"type":"text","value":"ANYbotics, 2023","key":"jejKbBYyCa"}],"enumerator":"2","key":"geP5gZJZpr"}],"key":"IFanUWmNWG"},{"type":"text","value":", RL has demonstrated remarkable capabilities.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"R88Yla4MdK"}],"key":"m75qkwUwEL"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Yet compared to supervised learning, which has become routine in industry, reinforcement learning has not achieved the same widespread adoption. Supervised learning benefits from standardized tools and well-defined interfaces: inputs, outputs, and objective metrics. Reinforcement learning, by contrast, requires explicit problem formulation: defining objectives, constraints, and how decisions unfold over time. This additional structure is also what makes RL applicable to a broader class of problems.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"Hq5poc30Wz"}],"key":"JfFUvgKpvQ"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"As ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"AR0pWKnob4"},{"type":"citeGroup","kind":"narrative","children":[{"type":"cite","kind":"narrative","label":"Iskhakov2020","identifier":"iskhakov2020","children":[{"type":"text","value":"Iskhakov ","key":"zJ3QytuOCl"},{"type":"emphasis","children":[{"type":"text","value":"et al.","key":"L7lUu5N5Pw"}],"key":"TIdkv32k1t"},{"type":"text","value":" (2020)","key":"UGHyIkDa5V"}],"enumerator":"3","key":"NyB6ZWhsoW"}],"key":"X5Mi06sQb2"},{"type":"text","value":" notes, a primary challenge is ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"iCJLj4l4qU"},{"type":"emphasis","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"“the difficulty of learning about the objective function and environment facing real-world decision-makers.”","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"PAm715c086"}],"key":"ToY54AgG0N"},{"type":"text","value":" We cannot sidestep defining the problem, the objective, and the constraints.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"dcSeAI9yV3"}],"key":"hZrOiJil5b"},{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Working in industry and consulting taught me what my PhD did not: real problems rarely fit neatly into predefined frameworks. Sensors produce noisy data; constraints are non-negotiable; objectives may shift or conflict. Most effort goes into formulating the decision problem, long before selecting an algorithm.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"KkzUxqdKze"}],"key":"n8KF8pqfjM"},{"type":"paragraph","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"The chapters that follow address this challenge explicitly. They offer strategies to bridge the gap from theoretical RL formulations to practically useful systems. By carefully structuring decision problems, we can help reinforcement learning achieve broader impact.","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"JSyLPsWmd3"}],"key":"ODVUsatExa"},{"type":"heading","depth":2,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"The Decision Problem","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"pjzPpUAT1g"}],"identifier":"the-decision-problem","label":"The Decision Problem","html_id":"the-decision-problem","implicit":true,"key":"VbKcACYJkU"},{"type":"paragraph","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"text","value":"The term ","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"TVbSRzYvRM"},{"type":"emphasis","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"text","value":"reinforcement learning","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"aCohcLdFUz"}],"key":"oOBHmmrBTt"},{"type":"text","value":" gets used in many ways. In the formal sense defined by ","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"Q03HlIGAPU"},{"type":"citeGroup","kind":"narrative","children":[{"type":"cite","kind":"narrative","label":"SuttonBarto2018","identifier":"suttonbarto2018","children":[{"type":"text","value":"Sutton \u0026 Barto (2018)","key":"C2FYxyxdHc"}],"enumerator":"4","key":"Glmp9jE3MB"}],"key":"kXTsHaqXbA"},{"type":"text","value":", RL is a problem: learning to act through interaction with an environment. But in common usage, it can mean a family of algorithms, a research community, or a long-term scientific agenda.","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"KZaW5o1r1o"}],"key":"oDS8Xc2xwK"},{"type":"paragraph","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"This book takes a practical view: reinforcement learning as a vocabulary for reasoning about decisions under uncertainty. When optimization, feedback, and data intersect, we are in the territory of reinforcement learning, whether we use temporal-difference learning, model-based planning, or simple policy rules. What unifies these approaches is not a specific algorithm, but a shared structure: decision-making through experience.","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"bqY1wyrLgw"}],"key":"TOy8XzSvO0"},{"type":"paragraph","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"In mainstream RL research, the problem is often treated as given. Sutton famously advises: ","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"I78WSm88wP"},{"type":"emphasis","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"“Approximate the solution, not the problem.”","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"LBxO5O8OAj"}],"key":"v2EYvKFHcF"},{"type":"text","value":" The agent should be shaped by experience, not by handcrafted structure. For Sutton, ","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"jpY3TFpLhd"},{"type":"emphasis","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"the problem","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"wgElxtHkwg"}],"key":"Ce8pjqNaeF"},{"type":"text","value":" is the world itself: complex, unknown, and handed to us as-is. We do not design it; we confront it.","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"uz6MHlzsCm"}],"key":"gEtyIQ4vZp"},{"type":"paragraph","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"This book takes a different stance. We consider a variety of decision problems, each with its own structure: finite or infinite horizon, discrete or continuous time, deterministic or stochastic dynamics. The problem is not handed to us; it must be defined. What are the goals? What decisions are available? What feedback is observable, and under what constraints?","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"PDaI5QpyCV"}],"key":"RFtoNL7ocI"},{"type":"paragraph","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"This is what Operations Research has long emphasized. Through consulting, I saw it firsthand: production systems running overnight, MIP solvers optimizing decisions with XGBoost predictors as inputs, all live and working. In practice, these systems are doing reinforcement learning, just without calling it that. They define objectives, encode constraints, and optimize over time. The vocabulary differs, but the structure is the same.","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"wgsKpeMYrr"}],"key":"h59nrtaovQ"},{"type":"heading","depth":2,"position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"text","value":"What This Book Offers","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"yqtybOtJcw"}],"identifier":"what-this-book-offers","label":"What This Book Offers","html_id":"what-this-book-offers","implicit":true,"key":"uTeE39DnRq"},{"type":"paragraph","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"text","value":"Reinforcement learning did not develop in isolation. Its foundations draw from control theory, dynamic programming, operations research, and economics. Many of the same ideas appear under different names in different communities, often with complementary perspectives.","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"qGOsZ1yRh5"}],"key":"LhIAYqBhrP"},{"type":"paragraph","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"text","value":"This book aims to give you a broader view of that landscape. Where do RL algorithms come from? What mathematical structures underlie them? How do they connect to classical methods in optimization and control? Understanding these connections helps you see when a method applies, when it does not, and what alternatives exist.","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"qjojFttJwQ"}],"key":"HfePOwMCpT"},{"type":"paragraph","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"text","value":"The goal is not to survey every technique superficially. It is to go deep on the mathematical foundations that are shared across methods: dynamic programming, function approximation, optimization, and the interplay between them. These structures recur throughout sequential decision-making, whether in reinforcement learning, control theory, or operations research. Master them once, and you can recognize them in different guises.","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"MER4gx807Y"}],"key":"iS9mucs8RR"}],"key":"FITtjRwPV3"}],"key":"VojDmFQgD1"},"references":{"cite":{"order":["Uber2025","ANYbotics2023","Iskhakov2020","SuttonBarto2018"],"data":{"Uber2025":{"label":"Uber2025","enumerator":"1","html":"Uber AI Labs. (2025). \u003ci\u003eReinforcement Learning at Scale for Ride-Matching Optimization\u003c/i\u003e. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://www.uber.com/blog/rl-ride-matching\"\u003ehttps://www.uber.com/blog/rl-ride-matching\u003c/a\u003e","url":"https://www.uber.com/blog/rl-ride-matching"},"ANYbotics2023":{"label":"ANYbotics2023","enumerator":"2","html":"ANYbotics. (2023). \u003ci\u003eANYmal: RL-Powered Autonomous Inspection\u003c/i\u003e. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://www.anybotics.com/anymal-inspection\"\u003ehttps://www.anybotics.com/anymal-inspection\u003c/a\u003e","url":"https://www.anybotics.com/anymal-inspection"},"Iskhakov2020":{"label":"Iskhakov2020","enumerator":"3","doi":"10.1093/ectj/utaa019","html":"Iskhakov, F., Rust, J., \u0026 Schjerning, B. (2020). Machine learning and structural econometrics: contrasts and synergies. \u003ci\u003eThe Econometrics Journal\u003c/i\u003e, \u003ci\u003e23\u003c/i\u003e(3), S81–S124. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.1093/ectj/utaa019\"\u003e10.1093/ectj/utaa019\u003c/a\u003e","url":"https://doi.org/10.1093/ectj/utaa019"},"SuttonBarto2018":{"label":"SuttonBarto2018","enumerator":"4","html":"Sutton, R. S., \u0026 Barto, A. G. (2018). \u003ci\u003eReinforcement Learning: An Introduction\u003c/i\u003e (2nd ed.). MIT Press. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"http://incompleteideas.net/book/the-book-2nd.html\"\u003ehttp://incompleteideas.net/book/the-book-2nd.html\u003c/a\u003e","url":"http://incompleteideas.net/book/the-book-2nd.html"}}}},"footer":{"navigation":{"next":{"title":"Dynamics and State-Space Models","url":"/dynamics","group":"Modeling"}}},"domain":"http://localhost:3003"},"project":{"bibliography":["/Users/pierre-luc.bacon/Documents/rlbook/references.bib"],"math":{"\\Proj":{"macro":"\\mathsf{P}"},"\\Residual":{"macro":"\\mathsf{N}"},"\\Contraction":{"macro":"\\mathsf{T}"},"\\Bellman":{"macro":"\\mathsf{L}"},"\\BellmanPi":{"macro":"\\mathsf{L}_\\pi"},"\\BellmanQuad":{"macro":"\\widetilde{\\mathsf{L}}"},"\\EulerResidual":{"macro":"\\mathsf{H}"},"\\Policy":{"macro":"\\pi"}},"exports":[{"format":"pdf","filename":"book.pdf","url":"/rlbook/build/book-bceca6481db041911b084ebd03cc1269.pdf"}],"title":"Building Up RL: From Dynamics and Control to Learning","description":"A graduate-level introduction to reinforcement learning as a framework for modeling, optimization, and control, connecting dynamic models, data, and applications beyond standard benchmarks.","authors":[{"nameParsed":{"literal":"Pierre-Luc Bacon","given":"Pierre-Luc","family":"Bacon"},"name":"Pierre-Luc Bacon","id":"contributors-myst-generated-uid-0"}],"github":"https://github.com/pierrelux/rlbook","id":"3c112bb5-93ac-429f-8216-c0b9947c00d4","toc":[{"file":"intro.md"},{"children":[{"file":"dynamics.md"}],"title":"Modeling"},{"children":[{"file":"trajectories.md"},{"file":"collocation.md"}],"title":"Numerical Trajectory Optimization"},{"children":[{"file":"mpc.md"},{"file":"dp.md"}],"title":"From Trajectories to Policies"},{"children":[{"file":"smoothing.md"},{"file":"projection.md"},{"children":[{"file":"montecarlo.md"},{"file":"fqi.md"},{"file":"amortization.md"},{"file":"pg.md"}],"title":"Simulation-Based Methods"}],"title":"Approximate Dynamic Programming"},{"children":[{"file":"appendix_examples.md"},{"file":"appendix_ivps.md"},{"file":"appendix_nlp.md"}],"title":"Appendix"}],"index":"index","pages":[{"level":1,"title":"Modeling"},{"slug":"dynamics","title":"Dynamics and State-Space Models","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Numerical Trajectory Optimization"},{"slug":"trajectories","title":"Discrete-Time Trajectory Optimization","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"collocation","title":"Trajectory Optimization in Continuous Time","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"From Trajectories to Policies"},{"slug":"mpc","title":"Model Predictive Control","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"dp","title":"Dynamic Programming","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":1,"title":"Approximate Dynamic Programming"},{"slug":"smoothing","title":"Smooth Bellman Optimality Equations","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"projection","title":"Weighted Residual Methods for Functional Equations","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"level":2,"title":"Simulation-Based Methods"},{"slug":"montecarlo","title":"Monte Carlo Integration in Approximate Dynamic Programming","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"fqi","title":"Fitted Q-Iteration Methods","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"amortization","title":"Fitted Q-Iteration for Continuous Action Spaces","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"slug":"pg","title":"Policy Gradient Methods","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":3},{"level":1,"title":"Appendix"},{"slug":"appendix-examples","title":"Example COCPs","description":"","date":"","thumbnail":"/rlbook/build/heat_exchanger-36d42c7ce90c38ee24e241c608f46188.svg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"appendix-ivps","title":"Solving Initial Value Problems","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"appendix-nlp","title":"Nonlinear Programming","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2}]}}},"actionData":null,"errors":null},"future":{"unstable_dev":false,"unstable_postcss":false,"unstable_tailwind":false,"v2_errorBoundary":true,"v2_headers":true,"v2_meta":true,"v2_normalizeFormMethod":true,"v2_routeConvention":true}};</script><script type="module" async="">import "/rlbook/build/manifest-3481E987.js";
import * as route0 from "/rlbook/build/root-7TUVC4ZT.js";
import * as route1 from "/rlbook/build/routes/$-P6PGXPYX.js";
window.__remixRouteModules = {"root":route0,"routes/$":route1};

import("/rlbook/build/entry.client-UNPC4GT3.js");</script></body></html>