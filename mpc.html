
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Model Predictive Control &#8212; Practical Reinforcement Learning: From Algorithms to Applications</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=b4b7a797" />
    <link rel="stylesheet" type="text/css" href="_static/graphviz.css?v=fd3f3429" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"bm": ["{\\boldsymbol #1}", 1]}, "processEscapes": true}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'mpc';</script>
    <script src="_static/iframe-modal.js?v=f72a1242"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Dynamic Programming" href="dp.html" />
    <link rel="prev" title="Trajectory Optimization in Continuous Time" href="cocp.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Practical Reinforcement Learning: From Algorithms to Applications</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Why This Book?
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Modeling</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="modeling.html">Why Build a Model? For Whom?</a></li>

<li class="toctree-l1"><a class="reference internal" href="ssm.html">Dynamics Models for Decision Making</a></li>




<li class="toctree-l1"><a class="reference internal" href="simulation.html">Programs as Models</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Numerical Trajectory Optimization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ocp.html">Discrete-Time Trajectory Optimization</a></li>


<li class="toctree-l1"><a class="reference internal" href="cocp.html">Trajectory Optimization in Continuous Time</a></li>




</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">From Trajectories to Policies</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Model Predictive Control</a></li>



<li class="toctree-l1"><a class="reference internal" href="dp.html">Dynamic Programming</a></li>



</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Learning from Data</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="adp.html">Approximate Dynamic Programming</a></li>





<li class="toctree-l1"><a class="reference internal" href="cadp.html">Policy Parametrization Methods</a></li>







</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="appendix_examples.html">Example COCPs</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix_ivps.html">Solving Initial Value Problems</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix_nlp.html">Nonlinear Programming</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/pierrelux/rlbook" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pierrelux/rlbook/edit/main/mpc.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pierrelux/rlbook/issues/new?title=Issue%20on%20page%20%2Fmpc.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/mpc.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Model Predictive Control</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Model Predictive Control</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#closing-the-loop-by-replanning">Closing the Loop by Replanning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-receding-horizon-principle">The Receding Horizon Principle</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#horizon-selection-and-problem-formulation">Horizon Selection and Problem Formulation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#infinite-horizon-regulation">Infinite-Horizon Regulation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#finite-duration-tasks">Finite-Duration Tasks</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#periodic-tasks">Periodic Tasks</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-mpc-algorithm">The MPC Algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-dynamic-programming">Connection to Dynamic Programming</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#successive-linearization-and-quadratic-approximations">Successive Linearization and Quadratic Approximations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#theoretical-guarantees">Theoretical Guarantees</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-terminal-ingredients-in-practice">Computing Terminal Ingredients in Practice</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-implications">Performance Implications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#suboptimality-bounds">Suboptimality Bounds</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#the-landscape-of-mpc-variants">The Landscape of MPC Variants</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tracking-mpc">Tracking MPC</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regulatory-mpc">Regulatory MPC</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#economic-mpc">Economic MPC</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#robust-mpc">Robust MPC</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-mpc">Stochastic MPC</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hybrid-and-mixed-integer-mpc">Hybrid and Mixed-Integer MPC</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-and-decentralized-mpc">Distributed and Decentralized MPC</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#adaptive-and-learning-based-mpc">Adaptive and Learning-Based MPC</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#robustness-in-real-time-mpc">Robustness in Real-Time MPC</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-wind-farm-yield-optimization">Example: Wind Farm Yield Optimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#softening-constraints-through-slack-variables">Softening Constraints Through Slack Variables</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feasibility-restoration">Feasibility Restoration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reference-governors">Reference Governors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backup-controllers">Backup Controllers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cascade-architectures">Cascade Architectures</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maintaining-solution-continuity">Maintaining Solution Continuity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-chemical-reactor-control-under-failure">Example: Chemical Reactor Control Under Failure</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#computational-efficiency-via-parametric-programming">Computational Efficiency via Parametric Programming</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#general-framework-parametric-optimization">General Framework: Parametric Optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-quick-reminder-the-implicit-function-theorem">A quick reminder: the implicit function theorem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictor-corrector-mpc">Predictor-Corrector MPC</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#application-to-mpc">Application to MPC</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#two-approaches-to-efficient-mpc">Two Approaches to Efficient MPC</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explicit-mpc">Explicit MPC</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sensitivity-based-mpc">Sensitivity-Based MPC</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#amortized-optimization-and-neural-approximation-of-controllers">Amortized Optimization and Neural Approximation of Controllers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#imitation-learning-framework">Imitation Learning Framework</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-propofol-infusion-control">Example: Propofol Infusion Control</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="model-predictive-control">
<h1>Model Predictive Control<a class="headerlink" href="#model-predictive-control" title="Link to this heading">#</a></h1>
<p>The trajectory optimization methods presented so far compute a complete control trajectory from an initial state to a final time or state. Once computed, this trajectory is executed without modification, making these methods fundamentally open-loop. The control function, <span class="math notranslate nohighlight">\(\mathbf{u}[k]\)</span> in discrete time or <span class="math notranslate nohighlight">\(\mathbf{u}(t)\)</span> in continuous time, depends only on the clock, reading off precomputed values from memory or interpolating between them. This approach assumes perfect models and no disturbances. Under these idealized conditions, repeating the same control sequence from the same initial state would always produce identical results.</p>
<p>Real systems face modeling errors, external disturbances, and measurement noise that accumulate over time. A precomputed trajectory becomes increasingly irrelevant as these perturbations push the actual system state away from the predicted path. The solution is to incorporate feedback, making control decisions that respond to the current state rather than blindly following a predetermined schedule. While dynamic programming provides the theoretical framework for deriving feedback policies through value functions and Bellman equations, there exists a more direct approach that leverages the trajectory optimization methods already developed.</p>
<section id="closing-the-loop-by-replanning">
<h2>Closing the Loop by Replanning<a class="headerlink" href="#closing-the-loop-by-replanning" title="Link to this heading">#</a></h2>
<p>Model Predictive Control creates a feedback controller by repeatedly solving trajectory optimization problems. Rather than computing a single trajectory for the entire task duration, MPC solves a finite-horizon problem at each time step, starting from the current measured state. The controller then applies only the first control action from this solution before repeating the entire process. This strategy transforms any trajectory optimization method into a feedback controller.</p>
<section id="the-receding-horizon-principle">
<h3>The Receding Horizon Principle<a class="headerlink" href="#the-receding-horizon-principle" title="Link to this heading">#</a></h3>
<p>The defining characteristic of MPC is its receding horizon strategy. At each time step, the controller solves an optimization problem looking a fixed duration into the future, but this prediction window constantly moves forward in time. The horizon “recedes” because it always starts from the current time and extends forward by the same amount.</p>
<p>Consider the discrete-time optimal control problem in Bolza form:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\text{minimize} \quad &amp; c_T(\mathbf{x}_N) + \sum_{k=0}^{N-1} c(\mathbf{x}_k, \mathbf{u}_k) \\
\text{subject to} \quad &amp; \mathbf{x}_{k+1} = \mathbf{f}(\mathbf{x}_k, \mathbf{u}_k) \\
&amp; \mathbf{g}(\mathbf{x}_k, \mathbf{u}_k) \leq \mathbf{0} \\
&amp; \mathbf{u}_{\text{min}} \leq \mathbf{u}_k \leq \mathbf{u}_{\text{max}} \\
\text{given} \quad &amp; \mathbf{x}_0 = \mathbf{x}_{\text{current}}
\end{aligned}
\end{split}\]</div>
<p>At time step <span class="math notranslate nohighlight">\(t\)</span>, this problem optimizes over the interval <span class="math notranslate nohighlight">\([t, t+N]\)</span>. At the next time step <span class="math notranslate nohighlight">\(t+1\)</span>, the horizon shifts to <span class="math notranslate nohighlight">\([t+1, t+N+1]\)</span>. The key insight is that only the first control <span class="math notranslate nohighlight">\(\mathbf{u}_0^*\)</span> from each optimization is applied. The remaining controls <span class="math notranslate nohighlight">\(\mathbf{u}_1^*, \ldots, \mathbf{u}_{N-1}^*\)</span> are discarded, though they may initialize the next optimization through warm-starting.</p>
<p>This receding horizon principle enables feedback without computing an explicit policy. By constantly updating predictions based on current measurements, MPC naturally corrects for disturbances and model errors. The apparent waste of computing but not using most of the trajectory is actually the mechanism that provides robustness.</p>
</section>
<section id="horizon-selection-and-problem-formulation">
<h3>Horizon Selection and Problem Formulation<a class="headerlink" href="#horizon-selection-and-problem-formulation" title="Link to this heading">#</a></h3>
<p>The relationship between the prediction horizon and the control objective fundamentally changes the problem structure. We must distinguish between three cases, each requiring different mathematical formulations.</p>
<section id="infinite-horizon-regulation">
<h4>Infinite-Horizon Regulation<a class="headerlink" href="#infinite-horizon-regulation" title="Link to this heading">#</a></h4>
<p>For stabilization problems where the system must operate indefinitely around an equilibrium, the true objective is:</p>
<div class="math notranslate nohighlight">
\[
J_\infty = \sum_{k=0}^{\infty} c(\mathbf{x}_k, \mathbf{u}_k)
\]</div>
<p>Since this cannot be solved directly, MPC approximates it with:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\text{minimize} \quad &amp; V_f(\mathbf{x}_N) + \sum_{k=0}^{N-1} c(\mathbf{x}_k, \mathbf{u}_k) \\
\text{subject to} \quad &amp; \mathbf{x}_{k+1} = \mathbf{f}(\mathbf{x}_k, \mathbf{u}_k) \\
&amp; \mathbf{x}_N \in \mathcal{X}_f \\
&amp; \text{other constraints}
\end{aligned}
\end{split}\]</div>
<p>The terminal cost <span class="math notranslate nohighlight">\(V_f(\mathbf{x}_N)\)</span> approximates <span class="math notranslate nohighlight">\(\sum_{k=N}^{\infty} c(\mathbf{x}_k, \mathbf{u}_k)\)</span>, the cost-to-go beyond the horizon. The terminal constraint <span class="math notranslate nohighlight">\(\mathbf{x}_N \in \mathcal{X}_f\)</span> ensures the state reaches a region where a known stabilizing controller exists. Without these terminal ingredients, the finite-horizon approximation may produce unstable behavior, as the controller ignores consequences beyond the horizon.</p>
</section>
<section id="finite-duration-tasks">
<h4>Finite-Duration Tasks<a class="headerlink" href="#finite-duration-tasks" title="Link to this heading">#</a></h4>
<p>For tasks ending at time <span class="math notranslate nohighlight">\(t_f\)</span>, the true objective spans from current time <span class="math notranslate nohighlight">\(t\)</span> to <span class="math notranslate nohighlight">\(t_f\)</span>:</p>
<div class="math notranslate nohighlight">
\[
J_{[t, t_f]} = c_f(\mathbf{x}(t_f)) + \sum_{k=t}^{t_f-1} c(\mathbf{x}_k, \mathbf{u}_k)
\]</div>
<p>The MPC formulation must adapt as time progresses:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\text{minimize} \quad &amp; c_{T,k}(\mathbf{x}_{N_k}) + \sum_{j=0}^{N_k-1} c(\mathbf{x}_j, \mathbf{u}_j) \\
\text{where} \quad &amp; N_k = \min(N, t_f - t_k) \\
&amp; c_{T,k} = \begin{cases}
c_f &amp; \text{if } t_k + N_k = t_f \\
c_T &amp; \text{otherwise}
\end{cases}
\end{aligned}
\end{split}\]</div>
<p>As the task approaches completion, the horizon shrinks and the terminal cost switches from the approximation <span class="math notranslate nohighlight">\(c_T\)</span> to the true final cost <span class="math notranslate nohighlight">\(c_f\)</span>. This prevents the controller from optimizing beyond task completion, which would produce meaningless or aggressive control actions.</p>
</section>
<section id="periodic-tasks">
<h4>Periodic Tasks<a class="headerlink" href="#periodic-tasks" title="Link to this heading">#</a></h4>
<p>For tasks with period <span class="math notranslate nohighlight">\(T_p\)</span>, such as daily building operations, the formulation accounts for transitions across period boundaries:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\text{minimize} \quad &amp; \sum_{k=0}^{N-1} c_k(\mathbf{x}_k, \mathbf{u}_k, \phi_k) \\
\text{where} \quad &amp; \phi_k = (t + k) \mod T_p \\
&amp; c_k(\cdot, \cdot, \phi) = \begin{cases}
c_{\text{day}}(\cdot, \cdot) &amp; \text{if } \phi \in [6\text{am}, 6\text{pm}] \\
c_{\text{night}}(\cdot, \cdot) &amp; \text{otherwise}
\end{cases}
\end{aligned}
\end{split}\]</div>
<p>The cost function changes based on the phase <span class="math notranslate nohighlight">\(\phi\)</span> within the period. Constraints may similarly depend on the phase, reflecting different operational requirements at different times.</p>
</section>
</section>
<section id="the-mpc-algorithm">
<h3>The MPC Algorithm<a class="headerlink" href="#the-mpc-algorithm" title="Link to this heading">#</a></h3>
<p>The complete MPC procedure implements the receding horizon principle through repeated optimization:</p>
<div class="proof algorithm admonition" id="alg-mpc-complete">
<p class="admonition-title"><span class="caption-number">Algorithm 2 </span> (Model Predictive Control with Horizon Management)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong></p>
<ul class="simple">
<li><p>Nominal prediction horizon <span class="math notranslate nohighlight">\(N\)</span></p></li>
<li><p>Sampling period <span class="math notranslate nohighlight">\(\Delta t\)</span></p></li>
<li><p>Task type: {infinite, finite with duration <span class="math notranslate nohighlight">\(t_f\)</span>, periodic with period <span class="math notranslate nohighlight">\(T_p\)</span>}</p></li>
<li><p>Cost functions and dynamics</p></li>
<li><p>Constraints</p></li>
</ul>
<p><strong>Procedure:</strong></p>
<ol class="arabic simple">
<li><p>Initialize time <span class="math notranslate nohighlight">\(t \leftarrow 0\)</span></p></li>
<li><p>Measure initial state <span class="math notranslate nohighlight">\(\mathbf{x}_{\text{current}} \leftarrow \mathbf{x}(t)\)</span></p></li>
<li><p><strong>While</strong> task continues:</p>
<ol class="arabic simple" start="4">
<li><p><strong>Determine effective horizon and costs:</strong></p>
<ul class="simple">
<li><p>If infinite task:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(N_{\text{eff}} \leftarrow N\)</span></p></li>
<li><p>Use terminal cost <span class="math notranslate nohighlight">\(V_f\)</span> and constraint <span class="math notranslate nohighlight">\(\mathcal{X}_f\)</span></p></li>
</ul>
</li>
<li><p>If finite task:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(N_{\text{eff}} \leftarrow \min(N, \lfloor(t_f - t)/\Delta t\rfloor)\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(t + N_{\text{eff}}\Delta t = t_f\)</span>: use final cost <span class="math notranslate nohighlight">\(c_f\)</span></p></li>
<li><p>Otherwise: use approximation <span class="math notranslate nohighlight">\(c_T\)</span></p></li>
</ul>
</li>
<li><p>If periodic task:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(N_{\text{eff}} \leftarrow N\)</span></p></li>
<li><p>Adjust costs/constraints based on phase</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Solve optimization:</strong>
Minimize over <span class="math notranslate nohighlight">\(\mathbf{u}_{0:N_{\text{eff}}-1}\)</span> subject to dynamics, constraints, and <span class="math notranslate nohighlight">\(\mathbf{x}_0 = \mathbf{x}_{\text{current}}\)</span></p></li>
<li><p><strong>Apply receding horizon control:</strong></p>
<ul class="simple">
<li><p>Extract <span class="math notranslate nohighlight">\(\mathbf{u}^*_0\)</span> from solution</p></li>
<li><p>Apply to system for duration <span class="math notranslate nohighlight">\(\Delta t\)</span></p></li>
<li><p>Measure new state</p></li>
<li><p>Advance time: <span class="math notranslate nohighlight">\(t \leftarrow t + \Delta t\)</span></p></li>
</ul>
</li>
</ol>
</li>
<li><p><strong>End While</strong></p></li>
</ol>
</section>
</div><!-- 
### Computational Considerations

The receding horizon principle requires solving optimization problems in real-time, placing stringent demands on the solver. Each problem must be solved within the sampling period $\Delta t$. If the solver requires more time, the system operates without new control updates, potentially degrading performance or stability.

Fortunately, successive MPC problems differ only in their initial conditions and possibly their horizons. This similarity enables warm-starting strategies where the previous solution initializes the current optimization. The standard approach shifts the previous trajectory forward by one time step and appends a nominal control at the end. This initialization typically lies close to the new optimum, dramatically reducing iteration counts.

The computational burden also depends on the horizon length $N$. Longer horizons provide better approximations to infinite-horizon problems and enable more sophisticated maneuvers, but increase problem size. The choice of $N$ balances solution quality against computational resources. For linear systems with quadratic costs, horizons of 10-50 steps often suffice. Nonlinear systems may require longer horizons to capture essential dynamics, though move-blocking and other parameterization techniques can reduce the effective number of decision variables. -->
</section>
<section id="connection-to-dynamic-programming">
<h3>Connection to Dynamic Programming<a class="headerlink" href="#connection-to-dynamic-programming" title="Link to this heading">#</a></h3>
<p>The receding horizon principle connects MPC to the dynamic programming framework covered in the next chapter. Each MPC optimization implicitly computes the optimal cost-to-go <span class="math notranslate nohighlight">\(V_N(\mathbf{x})\)</span> from the current state over the horizon. This finite-horizon value function approximates the true infinite-horizon value function that dynamic programming seeks globally.</p>
<p>MPC essentially performs one step of the Bellman equation:</p>
<div class="math notranslate nohighlight">
\[
u^*_{\text{MPC}} = \arg\min_u \left[ c(\mathbf{x}, u) + V_{N-1}(f(\mathbf{x}, u)) \right]
\]</div>
<p>where <span class="math notranslate nohighlight">\(V_{N-1}\)</span> is computed locally through trajectory optimization rather than stored globally. The terminal cost <span class="math notranslate nohighlight">\(c_T\)</span> in MPC serves exactly the role of a value function approximation for states at the horizon boundary. This perspective suggests hybrid approaches where approximate value functions from dynamic programming provide terminal costs for MPC, combining global optimality properties with local constraint handling capabilities.</p>
<p>This idea is what we would refer to as <strong>bootstrapping</strong> when working with temporal difference learning methods in reinforcement learning. In temporal difference methods like Q-learning or SARSA, bootstrapping occurs when we use our current estimate of the value function to update itself—essentially “pulling ourselves up by our bootstraps.” Similarly, MPC bootstraps by using its finite-horizon value function approximation (computed through optimization) to make decisions, even though this approximation may not be perfect. The terminal cost <span class="math notranslate nohighlight">\(c_T\)</span> acts as a bootstrap target, providing a value estimate for states beyond the horizon that guides the optimization process.</p>
</section>
<section id="successive-linearization-and-quadratic-approximations">
<h3>Successive Linearization and Quadratic Approximations<a class="headerlink" href="#successive-linearization-and-quadratic-approximations" title="Link to this heading">#</a></h3>
<p>For many regulation and tracking problems, the nonlinear dynamics and costs we encounter can be approximated locally by linear and quadratic functions. The basic idea is to linearize the system around the current operating point and approximate the cost with a quadratic form. This reduces each MPC subproblem to a <strong>quadratic program (QP)</strong>, which can be solved reliably and very quickly using standard solvers.</p>
<p>Suppose the true dynamics are nonlinear,</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}_{k+1} = f(\mathbf{x}_k,\mathbf{u}_k).
\]</div>
<p>Around a nominal trajectory <span class="math notranslate nohighlight">\((\bar{\mathbf{x}}_k,\bar{\mathbf{u}}_k)\)</span>, we take a first-order expansion:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}_{k+1} \approx f(\bar{\mathbf{x}}_k,\bar{\mathbf{u}}_k) 
+ A_k(\mathbf{x}_k - \bar{\mathbf{x}}_k) 
+ B_k(\mathbf{u}_k - \bar{\mathbf{u}}_k),
\]</div>
<p>with Jacobians</p>
<div class="math notranslate nohighlight">
\[
A_k = \frac{\partial f}{\partial \mathbf{x}}(\bar{\mathbf{x}}_k,\bar{\mathbf{u}}_k), 
\qquad
B_k = \frac{\partial f}{\partial \mathbf{u}}(\bar{\mathbf{x}}_k,\bar{\mathbf{u}}_k).
\]</div>
<p>Similarly, if the stage cost is nonlinear,</p>
<div class="math notranslate nohighlight">
\[
c(\mathbf{x}_k,\mathbf{u}_k),
\]</div>
<p>we approximate it quadratically near the nominal point:</p>
<div class="math notranslate nohighlight">
\[
c(\mathbf{x}_k,\mathbf{u}_k) \;\approx\; 
\|\mathbf{x}_k - \mathbf{x}_k^{\text{ref}}\|_{Q_k}^2 
+ \|\mathbf{u}_k - \mathbf{u}_k^{\text{ref}}\|_{R_k}^2,
\]</div>
<p>with positive semidefinite weighting matrices <span class="math notranslate nohighlight">\(Q_k\)</span> and <span class="math notranslate nohighlight">\(R_k\)</span>.</p>
<p>The resulting MPC subproblem has the form</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\min_{\mathbf{x}_{0:N},\mathbf{u}_{0:N-1}} \quad &amp;
\|\mathbf{x}_N - \mathbf{x}_N^{\text{ref}}\|_{P}^2
+ \sum_{k=0}^{N-1} 
\left(
\|\mathbf{x}_k - \mathbf{x}_k^{\text{ref}}\|_{Q_k}^2
+ \|\mathbf{u}_k - \mathbf{u}_k^{\text{ref}}\|_{R_k}^2
\right) \\
\text{s.t.} \quad &amp;
\mathbf{x}_{k+1} = A_k \mathbf{x}_k + B_k \mathbf{u}_k + d_k, \\
&amp; \mathbf{u}_{\min} \leq \mathbf{u}_k \leq \mathbf{u}_{\max}, \\
&amp; \mathbf{x}_{\min} \leq \mathbf{x}_k \leq \mathbf{x}_{\max}, \\
&amp; \mathbf{x}_0 = \mathbf{x}_{\text{current}} ,
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(d_k = f(\bar{\mathbf{x}}_k,\bar{\mathbf{u}}_k) - A_k \bar{\mathbf{x}}_k - B_k \bar{\mathbf{u}}_k\)</span> captures the local affine offset.</p>
<p>Because the dynamics are now linear and the cost quadratic, this optimization problem is a convex quadratic program. Quadratic programs are attractive in practice: they can be solved at kilohertz rates with mature numerical methods, making them the backbone of many real-time MPC implementations.</p>
<p>At each MPC step, the controller updates its linearization around the new operating point, constructs the local QP, and solves it. The process repeats, with the linear model and quadratic cost refreshed at every reoptimization. Despite the approximation, this yields a closed-loop controller that inherits the fast computation of QPs while retaining the ability to track trajectories of the underlying nonlinear system.</p>
</section>
</section>
<section id="theoretical-guarantees">
<h2>Theoretical Guarantees<a class="headerlink" href="#theoretical-guarantees" title="Link to this heading">#</a></h2>
<p>The finite-horizon approximation in MPC brings a new challenge: the controller cannot see consequences beyond the horizon. Without proper design, this myopia can destabilize even simple systems. The solution is to carefully encode information about the infinite-horizon problem into the finite-horizon optimization through its terminal conditions.</p>
<p>The standard MPC formulation augments the finite-horizon problem with three interconnected components:</p>
<ol class="arabic simple">
<li><p><strong>Terminal cost</strong> <span class="math notranslate nohighlight">\(V_f(\mathbf{x})\)</span>: Approximates the cost-to-go beyond the horizon</p></li>
<li><p><strong>Terminal constraint set</strong> <span class="math notranslate nohighlight">\(\mathcal{X}_f\)</span>: A region where we know how to stabilize the system</p></li>
<li><p><strong>Terminal controller</strong> <span class="math notranslate nohighlight">\(\kappa_f(\mathbf{x})\)</span>: A local stabilizing control law valid in <span class="math notranslate nohighlight">\(\mathcal{X}_f\)</span></p></li>
</ol>
<p>These components must satisfy specific compatibility conditions to provide guarantees:</p>
<div class="proof theorem admonition" id="thm-mpc-stability">
<p class="admonition-title"><span class="caption-number">Theorem 1 </span> (Recursive Feasibility and Asymptotic Stability)</p>
<section class="theorem-content" id="proof-content">
<p>Consider the MPC problem with terminal cost <span class="math notranslate nohighlight">\(V_f\)</span>, terminal set <span class="math notranslate nohighlight">\(\mathcal{X}_f\)</span>, and local controller <span class="math notranslate nohighlight">\(\kappa_f\)</span>. If:</p>
<ol class="arabic">
<li><p><strong>Control invariance</strong>: For all <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathcal{X}_f\)</span>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{f}(\mathbf{x}, \kappa_f(\mathbf{x})) \in \mathcal{X}_f\)</span> (the set is invariant)</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{g}(\mathbf{x}, \kappa_f(\mathbf{x})) \leq \mathbf{0}\)</span> (constraints remain satisfied)</p></li>
</ul>
</li>
<li><p><strong>Lyapunov decrease</strong>: For all <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathcal{X}_f\)</span>:</p>
<div class="math notranslate nohighlight">
\[V_f(\mathbf{f}(\mathbf{x}, \kappa_f(\mathbf{x}))) - V_f(\mathbf{x}) \leq -\ell(\mathbf{x}, \kappa_f(\mathbf{x}))\]</div>
<p>where <span class="math notranslate nohighlight">\(\ell\)</span> is the stage cost</p>
</li>
</ol>
<p>Then the MPC controller achieves:</p>
<ul class="simple">
<li><p><strong>Recursive feasibility</strong>: If the problem is feasible at time <span class="math notranslate nohighlight">\(k\)</span>, it remains feasible at time <span class="math notranslate nohighlight">\(k+1\)</span></p></li>
<li><p><strong>Asymptotic stability</strong>: For regulation problems, the closed-loop system is asymptotically stable</p></li>
<li><p><strong>Monotonic cost decrease</strong>: <span class="math notranslate nohighlight">\(V_N(\mathbf{x}_k)\)</span> decreases along trajectories</p></li>
</ul>
</section>
</div><p>To understand why the terminal ingredients guarantee recursive feasibility and asymptotic stability, it’s helpful to think operationally: what does the controller actually <em>do</em> from one step to the next?</p>
<p>Suppose at time <span class="math notranslate nohighlight">\(k\)</span> the MPC optimizer finds an optimal sequence of controls <span class="math notranslate nohighlight">\((\mathbf{u}_0^*, \ldots, \mathbf{u}_{N-1}^*)\)</span> and states <span class="math notranslate nohighlight">\((\mathbf{x}_0^*, \ldots, \mathbf{x}_N^*)\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{x}_N^* \in \mathcal{X}_f\)</span>. The first control <span class="math notranslate nohighlight">\(\mathbf{u}_0^*\)</span> is applied to the system, and the rest of the plan is discarded.</p>
<p>At time <span class="math notranslate nohighlight">\(k+1\)</span>, we need a new plan starting from the new state <span class="math notranslate nohighlight">\(\mathbf{x}_{\text{new}} = \mathbf{x}_1^*\)</span>. A natural fallback is to <strong>shift</strong> the previous plan forward by one step and <strong>append the terminal controller</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\kappa}_f\)</span> at the end:</p>
<ul class="simple">
<li><p>Controls: <span class="math notranslate nohighlight">\((\mathbf{u}_1^*, \ldots, \mathbf{u}_{N-1}^*, \boldsymbol{\kappa}_f(\mathbf{x}_N^*))\)</span></p></li>
<li><p>States: recomputed using the dynamics, now starting from <span class="math notranslate nohighlight">\(\mathbf{x}_1^*\)</span></p></li>
</ul>
<p>This shifted plan may no longer be optimal, but the <strong>Lyapunov decrease condition</strong> ensures it’s still good enough: it leads to a lower total cost, and all constraints are satisfied (because of the invariance of <span class="math notranslate nohighlight">\(\mathcal{X}_f\)</span>).</p>
<p>The condition</p>
<div class="math notranslate nohighlight">
\[
V_f(\mathbf{f}(\mathbf{x}, \kappa_f(\mathbf{x}))) - V_f(\mathbf{x}) \leq -\ell(\mathbf{x}, \kappa_f(\mathbf{x}))
\quad \forall\, \mathbf{x} \in \mathcal{X}_f
\]</div>
<p>means the terminal cost <span class="math notranslate nohighlight">\(V_f\)</span> must <strong>decrease faster than the stage cost accumulates</strong> when following <span class="math notranslate nohighlight">\(\boldsymbol{\kappa}_f\)</span> inside <span class="math notranslate nohighlight">\(\mathcal{X}_f\)</span>. In other words, the controller is making progress not just in terms of state evolution, but also in terms of predicted cost-to-go.</p>
<p>This gives us a kind of “one-step contractiveness”: if you’re inside the terminal set and apply <span class="math notranslate nohighlight">\(\kappa_f\)</span>, the value drops. When used at the tail of the horizon, this ensures that even a <strong>non-optimal shifted trajectory</strong> leads to lower overall cost, which makes <span class="math notranslate nohighlight">\(V_N\)</span> behave like a Lyapunov function.</p>
<section id="computing-terminal-ingredients-in-practice">
<h3>Computing Terminal Ingredients in Practice<a class="headerlink" href="#computing-terminal-ingredients-in-practice" title="Link to this heading">#</a></h3>
<p>For linear systems with quadratic costs, the terminal ingredients follow naturally from LQR theory:</p>
<p><strong>Step 1: Solve the infinite-horizon LQR</strong></p>
<div class="math notranslate nohighlight">
\[\mathbf{P} = \mathbf{Q} + \mathbf{A}^T \mathbf{P} \mathbf{A} - \mathbf{A}^T \mathbf{P} \mathbf{B}(\mathbf{R} + \mathbf{B}^T \mathbf{P} \mathbf{B})^{-1} \mathbf{B}^T \mathbf{P} \mathbf{A}\]</div>
<div class="math notranslate nohighlight">
\[\mathbf{K} = -(\mathbf{R} + \mathbf{B}^T \mathbf{P} \mathbf{B})^{-1} \mathbf{B}^T \mathbf{P} \mathbf{A}\]</div>
<p><strong>Step 2: Set terminal cost and controller</strong></p>
<div class="math notranslate nohighlight">
\[V_f(\mathbf{x}) = \mathbf{x}^T \mathbf{P} \mathbf{x}, \quad \kappa_f(\mathbf{x}) = \mathbf{K}\mathbf{x}\]</div>
<p><strong>Step 3: Construct the terminal set</strong> <span class="math notranslate nohighlight">\(\mathcal{X}_f\)</span></p>
<p>Three common approaches, in order of increasing conservatism but decreasing computation:</p>
<ol class="arabic">
<li><p><strong>Maximal control-invariant set</strong>: Compute the largest set where <span class="math notranslate nohighlight">\(\mathbf{u} = \mathbf{K}\mathbf{x}\)</span> keeps the state feasible indefinitely. This involves fixed-point iterations on polytopes—powerful but computationally intensive.</p></li>
<li><p><strong>Ellipsoidal approximation</strong>: Find the largest <span class="math notranslate nohighlight">\(\alpha\)</span> such that:</p>
<div class="math notranslate nohighlight">
\[\mathcal{X}_f = \{\mathbf{x} : \mathbf{x}^T \mathbf{P} \mathbf{x} \leq \alpha\}\]</div>
<p>satisfies all constraints under <span class="math notranslate nohighlight">\(\mathbf{u} = \mathbf{K}\mathbf{x}\)</span>. This requires checking constraint satisfaction at the ellipsoid boundary.</p>
</li>
<li><p><strong>Small safe set</strong>: Start with a tiny <span class="math notranslate nohighlight">\(\alpha\)</span> where constraints are clearly satisfied and grow it until hitting a constraint boundary. Conservative but always works.</p></li>
</ol>
<p>For nonlinear systems, linearize around the equilibrium to compute <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{K}\)</span>, then verify the decrease condition holds locally. The terminal set becomes a neighborhood where the linear approximation remains valid.</p>
</section>
<section id="performance-implications">
<h3>Performance Implications<a class="headerlink" href="#performance-implications" title="Link to this heading">#</a></h3>
<p>The terminal ingredients create a tradeoff between conservatism and computational burden:</p>
<ul class="simple">
<li><p><strong>Larger</strong> <span class="math notranslate nohighlight">\(\mathcal{X}_f\)</span>: Greater region of attraction, less restrictive on trajectories, but harder to compute</p></li>
<li><p><strong>Smaller</strong> <span class="math notranslate nohighlight">\(\mathcal{X}_f\)</span>: May require longer horizons to reach from typical initial conditions</p></li>
<li><p><strong>Better</strong> <span class="math notranslate nohighlight">\(V_f\)</span>: Tighter approximation of infinite-horizon cost, enabling shorter horizons</p></li>
</ul>
<p>As the horizon <span class="math notranslate nohighlight">\(N\)</span> increases, the importance of terminal ingredients diminishes. With <span class="math notranslate nohighlight">\(N \to \infty\)</span>, any stabilizing terminal controller suffices. In practice:</p>
<ul class="simple">
<li><p><strong>Short horizons (N = 10-20)</strong>: Terminal ingredients crucial for stability</p></li>
<li><p><strong>Medium horizons (N = 20-50)</strong>: Help performance but less critical</p></li>
<li><p><strong>Long horizons (N &gt; 50)</strong>: Often omitted entirely, relying on the horizon for stability</p></li>
</ul>
</section>
<section id="suboptimality-bounds">
<h3>Suboptimality Bounds<a class="headerlink" href="#suboptimality-bounds" title="Link to this heading">#</a></h3>
<p>The finite-horizon MPC value <span class="math notranslate nohighlight">\(V_N(\mathbf{x})\)</span> upper-bounds but approximates the true infinite-horizon value <span class="math notranslate nohighlight">\(V_\infty(\mathbf{x})\)</span>. With proper terminal ingredients:</p>
<div class="math notranslate nohighlight">
\[V_\infty(\mathbf{x}) \leq V_N(\mathbf{x}) \leq V_\infty(\mathbf{x}) + \varepsilon_N\]</div>
<p>where <span class="math notranslate nohighlight">\(\varepsilon_N \to 0\)</span> as <span class="math notranslate nohighlight">\(N \to \infty\)</span>. For linear-quadratic problems with the LQR terminal cost, the convergence is exponential in <span class="math notranslate nohighlight">\(N\)</span>. This means surprisingly short horizons (N = 10-30) often achieve near-optimal performance.</p>
<!-- 
### When Terminal Constraints Cause Infeasibility

The terminal constraint $\mathbf{x}_N \in \mathcal{X}_f$ can make the optimization infeasible, especially for:
- Large disturbances pushing the state far from equilibrium
- Short horizons that cannot reach $\mathcal{X}_f$ in time
- Conservative terminal sets that are unnecessarily small

Common remedies:

1. **Soft terminal constraints**: Replace hard constraint with penalty
   $$\text{minimize} \quad V_f(\mathbf{x}_N) + \rho \cdot d(\mathbf{x}_N, \mathcal{X}_f) + \ldots$$
   where $d(\cdot, \mathcal{X}_f)$ measures distance to the set

2. **Adaptive horizons**: Extend horizon when far from $\mathcal{X}_f$

3. **Backup strategy**: If infeasible, switch to unconstrained MPC or a fallback controller, then re-enable terminal constraints once feasible

The choice depends on whether theoretical guarantees or practical performance takes priority. Many industrial implementations omit terminal constraints entirely, relying on well-tuned horizons and costs to ensure stability.
 -->
<!-- 
# The Landscape of MPC Variants

Once the basic idea of receding horizon control is clear, it is helpful to see how the same backbone accommodates many variations. In every case, we transcribe the continuous problem to a nonlinear program of the form

$$
\begin{aligned}
\text{minimize}\quad & c_T(\mathbf{x}_N)+\sum_{k=0}^{N-1} w_k\,c(\mathbf{x}_k,\mathbf{u}_k) \\
\text{subject to}\quad & \mathbf{x}_{k+1}=\mathbf{F}_k(\mathbf{x}_k,\mathbf{u}_k) \\
& \mathbf{g}(\mathbf{x}_k,\mathbf{u}_k)\leq \mathbf{0}, \\
& \mathbf{x}_{\min}\leq \mathbf{x}_k\leq \mathbf{x}_{\max},\quad \mathbf{u}_{\min}\leq \mathbf{u}_k\leq \mathbf{u}_{\max}, \\
& \mathbf{x}_0=\mathbf{x}_{\text{current}} ,
\end{aligned}
$$

with \$\mathbf{F}\_k\$ the chosen discretization of the dynamics and \$w\_k\$ the quadrature weights. From this skeleton, several families of MPC emerge.

In **tracking MPC**, the stage and terminal costs are quadratic penalties on deviation from a reference trajectory,

$$
c(\mathbf{x}_k,\mathbf{u}_k)=\|\mathbf{x}_k-\mathbf{x}_k^{\text{ref}}\|_{\mathbf{Q}}^2+\|\mathbf{u}_k-\mathbf{u}_k^{\text{ref}}\|_{\mathbf{R}}^2 , \qquad c_T(\mathbf{x}_N)=\|\mathbf{x}_N-\mathbf{x}_N^{\text{ref}}\|_{\mathbf{P}}^2 .
$$

This is the industrial workhorse, ensuring that the system follows a desired profile within limits.

In **regulatory MPC**, the reference is fixed at an equilibrium \$(\mathbf{x}^e,\mathbf{u}^e)\$, and the quadratic penalty encourages return to this point. Terminal constraints are often added so that stability can be guaranteed.

In **economic MPC**, the quadratic structure disappears altogether. Instead, the cost encodes economic performance,

$$
c(\mathbf{x}_k,\mathbf{u}_k)=c_{\text{econ}}(\mathbf{x}_k,\mathbf{u}_k),
$$

for instance energy cost, profit, or resource efficiency. The optimization then steers the system not toward a setpoint but toward economically optimal regimes.

When uncertainty is represented by bounded sets, one arrives at **robust MPC**, which seeks controls that satisfy the constraints for all admissible disturbances. The resulting NLP has a min–max structure. A tractable alternative is tube MPC, where the nominal optimization is carried out with tightened constraints to guarantee feasibility of the true system under a disturbance feedback law.

If uncertainty is stochastic, the formulation turns into **stochastic MPC**, where the objective is the expected cost and the constraints are imposed with high probability,

$$
\mathbb{P}\!\left[\mathbf{g}(\mathbf{x}_k,\mathbf{u}_k)\leq \mathbf{0}\right]\geq 1-\varepsilon .
$$

Scenario-based versions replace expectations by a sampled, deterministic problem.

Some systems require discrete choices, such as switching devices on or off. **Hybrid MPC** introduces integer variables into the transcription, producing a mixed-integer NLP that can handle such logic.

For large networks of subsystems, **distributed MPC** coordinates several local predictive controllers that optimize their own subsystems while communicating through coupling constraints.

Finally, in settings where models are uncertain or slowly drifting, one finds **adaptive or learning-based MPC**, which uses parameter estimation or machine learning to update the model \$\mathbf{F}\_k(\cdot;\theta\_t)\$ and possibly the cost function. The optimization step remains the same, but the model evolves as more data are collected.

These formulations illustrate that MPC is less a single algorithm than a recipe. The scaffolding is always the same: finite horizon prediction, state and input constraints, and receding horizon application of the control. What changes from one variant to another is the cost function, the way uncertainty is treated, the presence of discrete decisions, or the architecture across multiple agents. -->
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="the-landscape-of-mpc-variants">
<h1>The Landscape of MPC Variants<a class="headerlink" href="#the-landscape-of-mpc-variants" title="Link to this heading">#</a></h1>
<p>Once the basic idea of receding-horizon control is clear, it is helpful to see how the same backbone accommodates many variations. In every case, we transcribe the continuous-time optimal control problem into a nonlinear program of the form</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \text{minimize} \quad &amp; c(\mathbf{x}_N) + \sum_{k=0}^{N-1} w_k\,c(\mathbf{x}_k, \mathbf{u}_k) \\
    \text{subject to} \quad &amp; \mathbf{x}_{k+1} = \mathbf{F}_k(\mathbf{x}_k, \mathbf{u}_k) \\
                            &amp; \mathbf{g}(\mathbf{x}_k, \mathbf{u}_k) \leq \mathbf{0} \\
                            &amp; \mathbf{x}_{\min} \leq \mathbf{x}_k \leq \mathbf{x}_{\max} \\
                            &amp; \mathbf{u}_{\min} \leq \mathbf{u}_k \leq \mathbf{u}_{\max} \\
    \text{given} \quad &amp; \mathbf{x}_0 = \hat{\mathbf{x}}(t) \enspace .
\end{aligned}
\end{split}\]</div>
<p>The components in this NLP come from discretizing the continuous-time problem with a fixed horizon <span class="math notranslate nohighlight">\([t, t+T]\)</span> and step size <span class="math notranslate nohighlight">\(\Delta t\)</span>. The stage weights <span class="math notranslate nohighlight">\(w_k\)</span> and discrete dynamics <span class="math notranslate nohighlight">\(\mathbf{F}_k\)</span> are determined by the choice of quadrature and integration scheme. With this blueprint in place, the rest is a matter of interpretation: how we define the cost, how we handle uncertainty, how we treat constraints, and what structure we exploit.</p>
<section id="tracking-mpc">
<h2>Tracking MPC<a class="headerlink" href="#tracking-mpc" title="Link to this heading">#</a></h2>
<p>The most common setup is reference tracking. Here, we are given time-varying target trajectories <span class="math notranslate nohighlight">\((\mathbf{x}_k^{\text{ref}}, \mathbf{u}_k^{\text{ref}})\)</span>, and the controller’s job is to keep the system close to these. The cost is typically quadratic:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    c(\mathbf{x}_k, \mathbf{u}_k) &amp;= \| \mathbf{x}_k - \mathbf{x}_k^{\text{ref}} \|_{\mathbf{Q}}^2 + \| \mathbf{u}_k - \mathbf{u}_k^{\text{ref}} \|_{\mathbf{R}}^2 \\
    c(\mathbf{x}_N) &amp;= \| \mathbf{x}_N - \mathbf{x}_N^{\text{ref}} \|_{\mathbf{P}}^2 \enspace .
\end{aligned}
\end{split}\]</div>
<p>When dynamics are linear and constraints are polyhedral, this yields a convex quadratic program at each time step.</p>
</section>
<section id="regulatory-mpc">
<h2>Regulatory MPC<a class="headerlink" href="#regulatory-mpc" title="Link to this heading">#</a></h2>
<p>In regulation tasks, we aim to bring the system back to an equilibrium point <span class="math notranslate nohighlight">\((\mathbf{x}^e, \mathbf{u}^e)\)</span>, typically in the presence of disturbances. This is simply tracking MPC with constant references:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    c(\mathbf{x}_k, \mathbf{u}_k) &amp;= \| \mathbf{x}_k - \mathbf{x}^e \|_{\mathbf{Q}}^2 + \| \mathbf{u}_k - \mathbf{u}^e \|_{\mathbf{R}}^2 \\
    c(\mathbf{x}_N) &amp;= \| \mathbf{x}_N - \mathbf{x}^e \|_{\mathbf{P}}^2 \enspace .
\end{aligned}
\end{split}\]</div>
<p>To guarantee stability, it is common to include a terminal constraint <span class="math notranslate nohighlight">\(\mathbf{x}_N \in \mathcal{X}_f\)</span>, where <span class="math notranslate nohighlight">\(\mathcal{X}_f\)</span> is a control-invariant set under a known feedback law.</p>
</section>
<section id="economic-mpc">
<h2>Economic MPC<a class="headerlink" href="#economic-mpc" title="Link to this heading">#</a></h2>
<p>Not all systems operate around a reference. Sometimes the goal is to optimize a true economic objective (eg. energy cost, revenue, efficiency) directly. This gives rise to <strong>economic MPC</strong>, where the cost functions reflect real operational performance:</p>
<div class="math notranslate nohighlight">
\[
c(\mathbf{x}_k, \mathbf{u}_k) = c_{\text{op}}(\mathbf{x}_k, \mathbf{u}_k), \qquad
c(\mathbf{x}_N) = c_{\text{op},T}(\mathbf{x}_N) \enspace .
\]</div>
<p>There is no reference trajectory here. The optimal behavior emerges from the cost itself. In this setting, standard stability arguments no longer apply automatically, and one must be careful to add terminal penalties or constraints that ensure the closed-loop system remains well-behaved.</p>
</section>
<section id="robust-mpc">
<h2>Robust MPC<a class="headerlink" href="#robust-mpc" title="Link to this heading">#</a></h2>
<p>Some systems are exposed to external disturbances or small errors in the model. In those cases, we want the controller to make decisions that will still work no matter what happens, as long as the disturbances stay within some known bounds. This is the idea behind <strong>robust MPC</strong>.</p>
<p>Instead of planning a single trajectory, the controller plans a “nominal” path (what would happen in the absence of any disturbance) and then adds a feedback correction to react to whatever disturbances actually occur. This looks like:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{u}_k = \bar{\mathbf{u}}_k + \mathbf{K} (\mathbf{x}_k - \bar{\mathbf{x}}_k) \enspace ,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{\mathbf{u}}_k\)</span> is the planned input and <span class="math notranslate nohighlight">\(\mathbf{K}\)</span> is a feedback gain that pulls the system back toward the nominal path if it deviates.</p>
<p>Because we know the worst-case size of the disturbance, we can estimate how far the real state might drift from the plan, and “shrink” the constraints accordingly. The result is that the nominal plan is kept safely away from constraint boundaries, so even if the system gets pushed around, it stays inside limits. This is often called <strong>tube MPC</strong> because the true trajectory stays inside a tube around the nominal one.</p>
<p>The main benefit is that we can handle uncertainty without solving a complicated worst-case optimization at every time step. All the uncertainty is accounted for in the design of the feedback <span class="math notranslate nohighlight">\(\mathbf{K}\)</span> and the tightened constraints.</p>
</section>
<section id="stochastic-mpc">
<h2>Stochastic MPC<a class="headerlink" href="#stochastic-mpc" title="Link to this heading">#</a></h2>
<p>If disturbances are random rather than adversarial, a natural goal is to optimize expected cost while enforcing constraints probabilistically. This gives rise to <strong>stochastic MPC</strong>, in which:</p>
<ul>
<li><p>The cost becomes an expectation:</p>
<div class="math notranslate nohighlight">
\[
  \mathbb{E} \left[ c(\mathbf{x}_N) + \sum_{k=0}^{N-1} w_k\, c(\mathbf{x}_k, \mathbf{u}_k) \right]
  \]</div>
</li>
<li><p>Constraints are allowed to be violated with small probability:</p>
<div class="math notranslate nohighlight">
\[
  \mathbb{P}[\mathbf{g}(\mathbf{x}_k, \mathbf{u}_k) \leq \mathbf{0}] \geq 1 - \varepsilon
  \]</div>
</li>
</ul>
<p>In practice, expectations are approximated using a finite set of disturbance scenarios drawn ahead of time. For each scenario, the system dynamics are simulated forward using the same control inputs <span class="math notranslate nohighlight">\(\mathbf{u}_k\)</span>, which are shared across all scenarios to respect non-anticipativity. The result is a single deterministic optimization problem with multiple parallel copies of the dynamics, one per sampled future. This retains the standard MPC structure, with only moderate growth in problem size.</p>
<p>Despite appearances, this is not dynamic programming. There is no value function or tree of all possible paths. There is only a finite set of futures chosen a priori, and optimized over directly. This scenario-based approach is common in energy systems such as hydro scheduling, where inflows are uncertain but sample trajectories can be generated from forecasts.</p>
<p>Risk constraints are typically enforced across all scenarios or encoded using risk measures like CVaR. For example, one might penalize violations that occur in the worst <span class="math notranslate nohighlight">\((1 - \alpha)\%\)</span> of samples, while still optimizing expected performance overall.</p>
</section>
<section id="hybrid-and-mixed-integer-mpc">
<h2>Hybrid and Mixed-Integer MPC<a class="headerlink" href="#hybrid-and-mixed-integer-mpc" title="Link to this heading">#</a></h2>
<p>When systems involve discrete switches — such as on/off valves, mode selection, or combinatorial logic — the MPC problem must include integer or binary variables. These show up in constraints like</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\delta}_k \in \{0,1\}^m, \qquad \mathbf{u}_k \in \mathcal{U}(\boldsymbol{\delta}_k)
\]</div>
<p>along with mode-dependent dynamics and costs. The resulting formulation is a <strong>mixed-integer nonlinear program</strong> (MINLP). The receding-horizon idea is the same, but each solve is more expensive due to the combinatorial nature of the decision space.</p>
</section>
<section id="distributed-and-decentralized-mpc">
<h2>Distributed and Decentralized MPC<a class="headerlink" href="#distributed-and-decentralized-mpc" title="Link to this heading">#</a></h2>
<p>Large-scale systems often consist of interacting subsystems. Distributed MPC decomposes the global NLP into smaller ones that run in parallel, with coordination constraints enforcing consistency across shared variables:</p>
<div class="math notranslate nohighlight">
\[
\sum_{i} \mathbf{H}^i \mathbf{z}^i_k = \mathbf{0} \qquad \text{(coupling constraint)}
\]</div>
<p>Each subsystem solves a local problem over its own state and input variables, then exchanges information with neighbors. Coordination can be done via primal–dual methods, ADMM, or consensus schemes, but each local block looks like a standard MPC problem.</p>
</section>
<section id="adaptive-and-learning-based-mpc">
<h2>Adaptive and Learning-Based MPC<a class="headerlink" href="#adaptive-and-learning-based-mpc" title="Link to this heading">#</a></h2>
<p>In practice, we may not know the true model <span class="math notranslate nohighlight">\(\mathbf{F}_k\)</span> or cost function <span class="math notranslate nohighlight">\(c\)</span> precisely. In <strong>adaptive MPC</strong>, these are updated online from data:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}_{k+1} = \mathbf{F}_k(\mathbf{x}_k, \mathbf{u}_k; \boldsymbol{\theta}_t), \qquad
c(\mathbf{x}_k, \mathbf{u}_k) = c(\mathbf{x}_k, \mathbf{u}_k; \boldsymbol{\phi}_t)
\]</div>
<p>The parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_t\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_t\)</span> are learned in real time. When combined with policy distillation, value approximation, or trajectory imitation, this leads to overlaps with reinforcement learning where the MPC solutions act as supervision for a reactive policy.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="robustness-in-real-time-mpc">
<h1>Robustness in Real-Time MPC<a class="headerlink" href="#robustness-in-real-time-mpc" title="Link to this heading">#</a></h1>
<p>The trajectory optimization methods we have studied assume perfect models and deterministic dynamics. In practice, however, MPC controllers must operate in environments where models are approximate, disturbances are unpredictable, and computational resources are limited. The mathematical elegance of optimal control must always yield to the engineering reality of robust operation as <strong>perfect optimality is less important than reliable operation</strong>. This philosophy permeates industrial MPC applications. A controller that achieves 95% performance 100% of the time is superior to one that achieves 100% performance 95% of the time and fails catastrophically the remaining 5%. Airlines accept suboptimal fuel consumption over missed approaches, power grids tolerate efficiency losses to prevent blackouts, and chemical plants sacrifice yield for safety. By designing for failure, we want to to create MPC systems that degrade gracefully rather than fail catastrophically, maintaining safety and stability even when the impossible is asked of them.</p>
<section id="example-wind-farm-yield-optimization">
<h2>Example: Wind Farm Yield Optimization<a class="headerlink" href="#example-wind-farm-yield-optimization" title="Link to this heading">#</a></h2>
<p>Consider a wind farm where MPC controllers coordinate individual turbines to maximize overall power production while minimizing wake interference. Each turbine can adjust both its thrust coefficient (through blade pitch) and yaw angle to redirect its wake away from downstream turbines. At time <span class="math notranslate nohighlight">\(t_k\)</span>, the MPC controller solves the optimization problem:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\min_{\mathbf{u}_{0:N-1}} \quad &amp; \sum_{i=0}^{N-1} \|\mathbf{x}_i - \mathbf{x}_i^{\text{ref}}\|_Q^2 + \|\mathbf{u}_i\|_R^2 \\
\text{s.t.} \quad &amp; \mathbf{x}_{i+1} = \mathbf{f}(\mathbf{x}_i, \mathbf{u}_i) \\
&amp; \mathbf{x}_i \in \mathcal{X}_{\text{safe}} \\
&amp; \|\mathbf{u}_i\|_\infty \leq u_{\max} \\
&amp; \mathbf{x}_0 = \mathbf{x}_{\text{current}}
\end{aligned}
\end{split}\]</div>
<p>Now suppose an unexpected wind direction change occurs, shifting the incoming wind vector by 30 degrees. The current state <span class="math notranslate nohighlight">\(\mathbf{x}_{\text{current}}\)</span> reflects wake patterns that no longer align with the new wind direction, and the optimizer discovers that no feasible trajectory exists that can redirect all wakes appropriately within the physical limits of yaw rate and thrust adjustment. The solver reports infeasibility.</p>
<p>This scenario reveals the fundamental challenge of real-time MPC: <strong>constraint incompatibility</strong>. When disturbances push the system into states from which recovery appears impossible, or when reference trajectories demand physically impossible maneuvers, the intersection of all constraint sets becomes empty. Model mismatch compounds this problem as prediction errors accumulate over the horizon.</p>
<p>Even when feasible solutions exist, <strong>computational constraints</strong> can prevent their discovery. A control loop running at 100 Hz allows only 10 milliseconds per iteration. If the solver requires 15 milliseconds to converge, we face an impossible choice: delay the control action and risk destabilizing the system, or apply an unconverged iterate that may violate critical constraints.</p>
<p>A third failure mode involves <strong>numerical instabilities</strong>—ill-conditioned matrices, rank deficiency, or division by zero in the linear algebra routines. These failures are particularly problematic because they occur sporadically, triggered by specific state configurations that create near-singular conditions in the optimization problem.</p>
</section>
<section id="softening-constraints-through-slack-variables">
<h2>Softening Constraints Through Slack Variables<a class="headerlink" href="#softening-constraints-through-slack-variables" title="Link to this heading">#</a></h2>
<p>The first approach to handling infeasibility recognizes that not all constraints carry equal importance. A chemical reactor’s temperature must never exceed the runaway threshold—this is a hard constraint that cannot be violated. However, maintaining temperature within an optimal efficiency band is merely desirable—this can be treated as a soft constraint that we prefer to satisfy but can relax when necessary.</p>
<p>This hierarchy motivates reformulating the optimization problem using <strong>slack variables</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\min_{\mathbf{u}, \boldsymbol{\epsilon}} \quad &amp; \sum_{i=0}^{N-1} \|\mathbf{x}_i - \mathbf{x}_i^{\text{ref}}\|_Q^2 + \|\mathbf{u}_i\|_R^2 + \boldsymbol{\rho}^T \boldsymbol{\epsilon}_i \\
\text{s.t.} \quad &amp; \mathbf{x}_{i+1} = \mathbf{f}(\mathbf{x}_i, \mathbf{u}_i) \\
&amp; \mathbf{g}_{\text{hard}}(\mathbf{x}_i, \mathbf{u}_i) \leq \mathbf{0} \\
&amp; \mathbf{g}_{\text{soft}}(\mathbf{x}_i, \mathbf{u}_i) \leq \boldsymbol{\epsilon}_i \\
&amp; \boldsymbol{\epsilon}_i \geq \mathbf{0}
\end{aligned}
\end{split}\]</div>
<p>The penalty weights <span class="math notranslate nohighlight">\(\boldsymbol{\rho}\)</span> encode our priorities. Safety constraints might use <span class="math notranslate nohighlight">\(\rho_j = 10^6\)</span>, while comfort constraints use <span class="math notranslate nohighlight">\(\rho_j = 1\)</span>. The key insight is that this reformulated problem is always feasible as long as the hard constraints alone admit a solution—we can always make the slack variables <span class="math notranslate nohighlight">\(\boldsymbol{\epsilon}\)</span> sufficiently large to satisfy the soft constraints.</p>
<p>Rather than treating constraints as binary hard/soft categories, we can establish a <strong>constraint hierarchy</strong> that enables graceful degradation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\text{Safety:} \quad &amp; T_{\text{reactor}} \leq T_{\text{runaway}} - 10 \quad &amp; \rho = \infty \text{ (hard)} \\
\text{Equipment:} \quad &amp; 0 \leq u_{\text{valve}} \leq 100 \quad &amp; \rho = 10^4 \\
\text{Efficiency:} \quad &amp; T_{\text{optimal}} - 5 \leq T \leq T_{\text{optimal}} + 5 \quad &amp; \rho = 10^2 \\
\text{Comfort:} \quad &amp; |T - T_{\text{setpoint}}| \leq 1 \quad &amp; \rho = 1
\end{aligned}
\end{split}\]</div>
<p>As conditions deteriorate, the controller abandons objectives in reverse priority order, maintaining safety even when optimality becomes impossible.</p>
</section>
<section id="feasibility-restoration">
<h2>Feasibility Restoration<a class="headerlink" href="#feasibility-restoration" title="Link to this heading">#</a></h2>
<p>When even soft constraints prove insufficient—perhaps due to catastrophic solver failure or corrupted problem structure—we need <strong>feasibility restoration</strong> that finds any feasible point regardless of optimality:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\min_{\mathbf{u}, \mathbf{s}} \quad &amp; \|\mathbf{s}\|_1 \\
\text{s.t.} \quad &amp; \mathbf{x}_{i+1} = \mathbf{f}(\mathbf{x}_i, \mathbf{u}_i) + \mathbf{s}_i \\
&amp; \mathbf{x}_{\min} - \mathbf{s}_{x,i} \leq \mathbf{x}_i \leq \mathbf{x}_{\max} + \mathbf{s}_{x,i} \\
&amp; \mathbf{u}_{\min} \leq \mathbf{u}_i \leq \mathbf{u}_{\max} \\
&amp; \mathbf{s} \geq \mathbf{0}
\end{aligned}
\end{split}\]</div>
<p>This formulation temporarily relaxes even the dynamics constraints, finding the “least infeasible” solution. It answers the question: if we must violate something, what is the minimal violation required? Once feasibility is restored, we can warm-start the original problem from this point.</p>
</section>
<section id="reference-governors">
<h2>Reference Governors<a class="headerlink" href="#reference-governors" title="Link to this heading">#</a></h2>
<p>Rather than reacting to infeasibility after it occurs, we can prevent it by filtering references through a <strong>reference governor</strong>. Consider an aircraft following waypoints. Instead of passing waypoints directly to the MPC, the governor asks: what is the closest approachable reference from our current state?</p>
<div class="math notranslate nohighlight">
\[
\mathbf{r}_{\text{filtered}} = \arg\max_{\kappa \in [0,1]} \kappa \quad \text{s.t. MPC}(\mathbf{x}_{\text{current}}, \kappa \mathbf{r}_{\text{desired}} + (1-\kappa)\mathbf{x}_{\text{current}}) \text{ is feasible}
\]</div>
<p>The governor performs a line search between the current state (always feasible since staying put requires no action) and the desired reference (potentially infeasible). This guarantees the MPC always receives feasible problems while making maximum progress toward the goal.</p>
<p>For computational efficiency, we can pre-compute the <strong>maximal output admissible set</strong>:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{O}_\infty = \{\mathbf{r} : \exists \text{ feasible trajectory from } \mathbf{x} \text{ to } \mathbf{r} \text{ respecting all constraints}\}
\]</div>
<p>Online, the governor simply projects the desired reference onto <span class="math notranslate nohighlight">\(\mathcal{O}_\infty\)</span>.</p>
</section>
<section id="backup-controllers">
<h2>Backup Controllers<a class="headerlink" href="#backup-controllers" title="Link to this heading">#</a></h2>
<p>When MPC fails entirely—due to solver crashes, timeouts, or numerical failures—we need backup controllers that require minimal computation while guaranteeing stability and keeping the system away from dangerous regions.</p>
<p>The standard approach uses a pre-computed <strong>local LQR controller</strong> around the equilibrium:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{K}_{\text{LQR}}, \mathbf{P} = \text{LQR}(\mathbf{A}, \mathbf{B}, \mathbf{Q}, \mathbf{R})
\]</div>
<p>where <span class="math notranslate nohighlight">\((\mathbf{A}, \mathbf{B})\)</span> are the linearized dynamics at equilibrium. When MPC fails:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{u}_{\text{backup}} = \begin{cases}
\mathbf{K}_{\text{LQR}}(\mathbf{x} - \mathbf{x}_{\text{eq}}) &amp; \text{if } \mathbf{x} \in \mathcal{X}_{\text{LQR}} \\
\mathbf{u}_{\text{safe}} &amp; \text{otherwise}
\end{cases}
\end{split}\]</div>
<p>The region <span class="math notranslate nohighlight">\(\mathcal{X}_{\text{LQR}} = \{\mathbf{x} : (\mathbf{x} - \mathbf{x}_{\text{eq}})^T \mathbf{P} (\mathbf{x} - \mathbf{x}_{\text{eq}}) \leq \alpha\}\)</span> represents the largest invariant set where LQR is guaranteed to work.</p>
</section>
<section id="cascade-architectures">
<h2>Cascade Architectures<a class="headerlink" href="#cascade-architectures" title="Link to this heading">#</a></h2>
<p>Production MPC systems rarely rely on a single solver. Instead, they implement a <strong>cascade of increasingly conservative controllers</strong> that trade optimality for reliability:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_control</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">time_budget</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multi-level cascade for robust real-time control</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">time_remaining</span> <span class="o">=</span> <span class="n">time_budget</span>
    
    <span class="c1"># Level 1: Full nonlinear MPC</span>
    <span class="k">if</span> <span class="n">time_remaining</span> <span class="o">&gt;</span> <span class="mf">5e-3</span><span class="p">:</span>  <span class="c1"># 5ms minimum</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">u</span><span class="p">,</span> <span class="n">solve_time</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">solve_nmpc</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">time_remaining</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">converged</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">u</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">pass</span>
        <span class="n">time_remaining</span> <span class="o">-=</span> <span class="n">solve_time</span>
    
    <span class="c1"># Level 2: Simplified linear MPC</span>
    <span class="k">if</span> <span class="n">time_remaining</span> <span class="o">&gt;</span> <span class="mf">1e-3</span><span class="p">:</span>  <span class="c1"># 1ms minimum</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Linearize around current state</span>
            <span class="n">A</span><span class="p">,</span> <span class="n">B</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linearize_dynamics</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">u</span><span class="p">,</span> <span class="n">solve_time</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">solve_lmpc</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">time_remaining</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">u</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">pass</span>
        <span class="n">time_remaining</span> <span class="o">-=</span> <span class="n">solve_time</span>
    
    <span class="c1"># Level 3: Explicit MPC lookup</span>
    <span class="k">if</span> <span class="n">time_remaining</span> <span class="o">&gt;</span> <span class="mf">1e-4</span><span class="p">:</span>  <span class="c1"># 0.1ms minimum</span>
        <span class="n">region</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">find_critical_region</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">region</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">explicit_control_law</span><span class="p">[</span><span class="n">region</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
    
    <span class="c1"># Level 4: LQR backup</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_lqr_region</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">K_lqr</span> <span class="o">@</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_eq</span><span class="p">)</span>
    
    <span class="c1"># Level 5: Emergency safe mode</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">emergency_stop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>Each level trades optimality for reliability: Level 1 provides optimal but computationally expensive control, Level 2 offers suboptimal but faster solutions, Level 3 provides pre-computed instant evaluation, Level 4 ensures stabilizing control without tracking, and Level 5 implements safe shutdown.</p>
</section>
<section id="maintaining-solution-continuity">
<h2>Maintaining Solution Continuity<a class="headerlink" href="#maintaining-solution-continuity" title="Link to this heading">#</a></h2>
<p>Even when using backup controllers, we can maintain solution continuity through <strong>persistent warm-starting</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{z}_{\text{warm}}^{(k+1)} = \begin{cases}
\text{shift}(\mathbf{z}^{(k)}) &amp; \text{if MPC succeeded at time } k \\
\text{lift}(\mathbf{u}_{\text{backup}}^{(k)}) &amp; \text{if backup controller used} \\
\text{propagate}(\mathbf{z}_{\text{warm}}^{(k)}) &amp; \text{if maintaining virtual solution}
\end{cases}
\end{aligned}
\end{split}\]</div>
<p>The key insight is that even when MPC fails, we maintain a “virtual” trajectory by propagating the previous solution forward. This keeps the warm-start relevant for when MPC recovers.</p>
</section>
<section id="example-chemical-reactor-control-under-failure">
<h2>Example: Chemical Reactor Control Under Failure<a class="headerlink" href="#example-chemical-reactor-control-under-failure" title="Link to this heading">#</a></h2>
<p>Consider a continuous stirred tank reactor (CSTR) where an exothermic reaction must be controlled:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\dot{C}_A &amp;= \frac{q}{V}(C_{A,in} - C_A) - k_0 e^{-E/RT} C_A \\
\dot{T} &amp;= \frac{q}{V}(T_{in} - T) + \frac{\Delta H}{\rho c_p} k_0 e^{-E/RT} C_A - \frac{UA}{\rho c_p V}(T - T_c)
\end{aligned}
\end{split}\]</div>
<p>The MPC must maintain temperature below the runaway threshold <span class="math notranslate nohighlight">\(T_{\text{runaway}}\)</span> while maximizing conversion. Under normal operation, it solves:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\min \quad &amp; -C_A(t_f) + \int_0^{t_f} \|T - T_{\text{optimal}}\|^2 dt \\
\text{s.t.} \quad &amp; T \leq T_{\text{runaway}} - \Delta T_{\text{safety}} \\
&amp; q_{\min} \leq q \leq q_{\max}
\end{aligned}
\end{split}\]</div>
<p>When the cooling system partially fails, <span class="math notranslate nohighlight">\(T_c\)</span> suddenly increases. The MPC cannot maintain <span class="math notranslate nohighlight">\(T_{\text{optimal}}\)</span> within safety limits. The cascade activates: soft constraints allow <span class="math notranslate nohighlight">\(T\)</span> to exceed <span class="math notranslate nohighlight">\(T_{\text{optimal}}\)</span> with penalty, the reference governor reduces the production target <span class="math notranslate nohighlight">\(C_{A,\text{target}}\)</span>, and if still infeasible, the backup controller switches to maximum cooling <span class="math notranslate nohighlight">\(q = q_{\max}\)</span>. If temperature approaches runaway, emergency shutdown stops the feed with <span class="math notranslate nohighlight">\(q = 0\)</span>.</p>
<p>This cascade ensures the reactor never reaches dangerous conditions even under multiple simultaneous failures.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="computational-efficiency-via-parametric-programming">
<h1>Computational Efficiency via Parametric Programming<a class="headerlink" href="#computational-efficiency-via-parametric-programming" title="Link to this heading">#</a></h1>
<p>Real-time model predictive control places strict limits on computation. In applications such as adaptive optics, the controller must run at kilohertz rates. A sampling frequency of 1000 Hz allows only one millisecond per step to compute and apply a control input. This makes efficiency a first-class concern.</p>
<p>The structure of MPC lends itself naturally to optimization reuse. Each time step requires solving a problem with the same dynamics and constraints. Only the initial state, forecasts, or reference signals change. Instead of treating each instance as a new problem, we can frame MPC as a <em>parametric optimization problem</em> and focus on how the solution evolves with the parameter.</p>
<section id="general-framework-parametric-optimization">
<h2>General Framework: Parametric Optimization<a class="headerlink" href="#general-framework-parametric-optimization" title="Link to this heading">#</a></h2>
<p>We begin with a general optimization problem indexed by a parameter <span class="math notranslate nohighlight">\(\boldsymbol{\theta} \in \Theta \subset \mathbb{R}^p\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\min_{\mathbf{x} \in \mathbb{R}^n} \quad &amp; f(\mathbf{x}; \boldsymbol{\theta}) \\
\text{s.t.} \quad &amp; \mathbf{g}(\mathbf{x}; \boldsymbol{\theta}) \le \mathbf{0}, \\
&amp; \mathbf{h}(\mathbf{x}; \boldsymbol{\theta}) = \mathbf{0}.
\end{aligned}
\end{split}\]</div>
<p>For each value of <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>, we obtain a concrete optimization problem. The goal is not just to solve it once, but to understand how the optimizer <span class="math notranslate nohighlight">\(\mathbf{x}^\star(\boldsymbol{\theta})\)</span> and value function</p>
<div class="math notranslate nohighlight">
\[
v(\boldsymbol{\theta}) := \inf\{\, f(\mathbf{x}; \boldsymbol{\theta}) : \mathbf{x} \text{ feasible at } \boldsymbol{\theta}\,\}
\]</div>
<p>depend on <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>.</p>
<p>When the problem is smooth and regular, the Karush–Kuhn–Tucker (KKT) conditions characterize optimality:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\nabla_{\mathbf{x}} f(\mathbf{x}; \boldsymbol{\theta})
+ \nabla_{\mathbf{x}} \mathbf{g}(\mathbf{x}; \boldsymbol{\theta})^\top \boldsymbol{\lambda}
+ \nabla_{\mathbf{x}} \mathbf{h}(\mathbf{x}; \boldsymbol{\theta})^\top \boldsymbol{\nu} &amp;= 0, \\
\mathbf{g}(\mathbf{x}; \boldsymbol{\theta}) \le 0, \quad
\boldsymbol{\lambda} \ge 0, \quad
\lambda_i g_i(\mathbf{x}; \boldsymbol{\theta}) &amp;= 0, \\
\mathbf{h}(\mathbf{x}; \boldsymbol{\theta}) &amp;= 0.
\end{aligned}
\end{split}\]</div>
<p>If the active set remains fixed over changes in <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>, the implicit function theorem ensures that the mappings</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\theta} \mapsto \mathbf{x}^\star(\boldsymbol{\theta}), \quad
\boldsymbol{\theta} \mapsto \boldsymbol{\lambda}^\star(\boldsymbol{\theta}), \quad
\boldsymbol{\theta} \mapsto \boldsymbol{\nu}^\star(\boldsymbol{\theta})
\]</div>
<p>are differentiable.</p>
<p>In linear and quadratic programming, this structure becomes even more tractable. Consider a linear program with affine dependence on <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\min_{\mathbf{x}} \ \mathbf{c}(\boldsymbol{\theta})^\top \mathbf{x}
\quad \text{s.t.} \quad \mathbf{A}(\boldsymbol{\theta})\mathbf{x} \le \mathbf{b}(\boldsymbol{\theta}).
\]</div>
<p>Each active set determines a basis and thus a region in <span class="math notranslate nohighlight">\(\Theta\)</span> where the solution is affine in <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>. The feasible parameter space is partitioned into polyhedral regions, each with its own affine law.</p>
<p>Similarly, in strictly convex quadratic programs</p>
<div class="math notranslate nohighlight">
\[
\min_{\mathbf{x}} \ \tfrac{1}{2} \mathbf{x}^\top \mathbf{H} \mathbf{x} + \mathbf{q}(\boldsymbol{\theta})^\top \mathbf{x}
\quad \text{s.t.} \quad \mathbf{A}\mathbf{x} \le \mathbf{b}(\boldsymbol{\theta}), \qquad \mathbf{H} \succ 0,
\]</div>
<p>each active set again leads to an affine optimizer, with piecewise-affine global structure and a piecewise-quadratic value function.</p>
<p>Parametric programming focuses on the structure of the map <span class="math notranslate nohighlight">\(\boldsymbol{\theta} \mapsto \mathbf{x}^\star(\boldsymbol{\theta})\)</span>, and the regions over which this map takes a simple form.</p>
<section id="a-quick-reminder-the-implicit-function-theorem">
<h3>A quick reminder: the implicit function theorem<a class="headerlink" href="#a-quick-reminder-the-implicit-function-theorem" title="Link to this heading">#</a></h3>
<p>We often meet equations of the form</p>
<div class="math notranslate nohighlight">
\[
F(y,\boldsymbol{\theta})=0,
\]</div>
<p>where <span class="math notranslate nohighlight">\(y\in\mathbb{R}^m\)</span> are unknowns and <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\in\mathbb{R}^p\)</span> are parameters. The <strong>implicit function theorem</strong> says that, if <span class="math notranslate nohighlight">\(F\)</span> is smooth and the Jacobian with respect to <span class="math notranslate nohighlight">\(y\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial F}{\partial y}(y^\star,\boldsymbol{\theta}^\star),
\]</div>
<p>is invertible at a solution <span class="math notranslate nohighlight">\((y^\star,\boldsymbol{\theta}^\star)\)</span>, then in a neighborhood of <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^\star\)</span> there exists a unique smooth mapping <span class="math notranslate nohighlight">\(y(\boldsymbol{\theta})\)</span> with <span class="math notranslate nohighlight">\(F(y(\boldsymbol{\theta}),\boldsymbol{\theta})=0\)</span> and <span class="math notranslate nohighlight">\(y(\boldsymbol{\theta}^\star)=y^\star\)</span>. Moreover, its derivative is</p>
<div class="math notranslate nohighlight">
\[
\frac{d y}{d\boldsymbol{\theta}}(\boldsymbol{\theta}^\star)
\;=\;
-\Big(\tfrac{\partial F}{\partial y}(y^\star,\boldsymbol{\theta}^\star)\Big)^{-1}
\;\tfrac{\partial F}{\partial \boldsymbol{\theta}}(y^\star,\boldsymbol{\theta}^\star).
\]</div>
<p>In words: if the square Jacobian in <span class="math notranslate nohighlight">\(y\)</span> is nonsingular, the solution varies smoothly with the parameter, and we can differentiate it by solving one linear system.</p>
<p>Return to <span class="math notranslate nohighlight">\((P_{\theta})\)</span> and its KKT system. Collect the primal and dual variables into</p>
<div class="math notranslate nohighlight">
\[
y \;:=\; (\mathbf{x},\,\boldsymbol{\lambda},\,\boldsymbol{\nu}),
\]</div>
<p>and write the KKT equations as a single residual</p>
<div class="math notranslate nohighlight">
\[\begin{split}
F(y,\boldsymbol{\theta}) \;=\; 
\begin{bmatrix}
\nabla_{\mathbf{x}} f(\mathbf{x};\boldsymbol{\theta})
+ \nabla_{\mathbf{x}} \mathbf{g}(\mathbf{x};\boldsymbol{\theta})^\top \boldsymbol{\lambda}
+ \nabla_{\mathbf{x}} \mathbf{h}(\mathbf{x};\boldsymbol{\theta})^\top \boldsymbol{\nu} \\
\mathbf{h}(\mathbf{x};\boldsymbol{\theta}) \\
\mathbf{g}_\mathcal{A}(\mathbf{x};\boldsymbol{\theta})
\end{bmatrix}
\;=\; \mathbf{0}.
\end{split}\]</div>
<p>Here <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> denotes the set of inequality constraints active at the solution (the complementarity part is encoded by keeping <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> fixed; see below).</p>
<p>To invoke IFT, we need the Jacobian <span class="math notranslate nohighlight">\(\partial F/\partial y\)</span> to be invertible at <span class="math notranslate nohighlight">\((y^\star,\boldsymbol{\theta}^\star)\)</span>. Standard regularity conditions that ensure this are:</p>
<ul class="simple">
<li><p><strong>LICQ (Linear Independence Constraint Qualification)</strong> at <span class="math notranslate nohighlight">\((\mathbf{x}^\star,\boldsymbol{\theta}^\star)\)</span>: the gradients of all active constraints are linearly independent.</p></li>
<li><p><strong>Second-order sufficiency</strong> on the critical cone (the Lagrangian Hessian is positive definite on feasible directions).</p></li>
<li><p><strong>Strict complementarity</strong> (optional but convenient): each active inequality has strictly positive multiplier.</p></li>
</ul>
<p>Under these, the <strong>KKT matrix</strong>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
K \;=\;
\frac{\partial F}{\partial y}(y^\star,\boldsymbol{\theta}^\star)
\;=\;
\begin{bmatrix}
\nabla^2_{\mathbf{x}\mathbf{x}} \mathcal{L}(\mathbf{x}^\star,\boldsymbol{\lambda}^\star,\boldsymbol{\nu}^\star;\boldsymbol{\theta}^\star)
&amp; \nabla_{\mathbf{x}} \mathbf{g}_\mathcal{A}(\mathbf{x}^\star;\boldsymbol{\theta}^\star)^\top
&amp; \nabla_{\mathbf{x}} \mathbf{h}(\mathbf{x}^\star;\boldsymbol{\theta}^\star)^\top \\
\nabla_{\mathbf{x}} \mathbf{g}_\mathcal{A}(\mathbf{x}^\star;\boldsymbol{\theta}^\star) &amp; 0 &amp; 0 \\
\nabla_{\mathbf{x}} \mathbf{h}(\mathbf{x}^\star;\boldsymbol{\theta}^\star) &amp; 0 &amp; 0
\end{bmatrix},
\end{split}\]</div>
<p>is nonsingular. Here <span class="math notranslate nohighlight">\(\mathcal{L}=f+\boldsymbol{\lambda}^\top \mathbf{g}+\boldsymbol{\nu}^\top \mathbf{h}\)</span>.</p>
<p>The right-hand side sensitivity to parameters is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
G \;=\; \frac{\partial F}{\partial \boldsymbol{\theta}}(y^\star,\boldsymbol{\theta}^\star)
\;=\;
\begin{bmatrix}
\nabla_{\boldsymbol{\theta}}\nabla_{\mathbf{x}} f
+ \sum_{i\in\mathcal{A}} \lambda_i^\star \nabla_{\boldsymbol{\theta}}\nabla_{\mathbf{x}} g_i
+ \sum_j \nu_j^\star \nabla_{\boldsymbol{\theta}}\nabla_{\mathbf{x}} h_j \\
\nabla_{\boldsymbol{\theta}} \mathbf{h} \\
\nabla_{\boldsymbol{\theta}} \mathbf{g}_\mathcal{A}
\end{bmatrix}_{(\mathbf{x}^\star,\boldsymbol{\theta}^\star)} .
\end{split}\]</div>
<p>IFT then gives <strong>local differentiability of the optimizer and multipliers</strong>:</p>
<div class="math notranslate nohighlight">
\[
\frac{d y^\star}{d\boldsymbol{\theta}}(\boldsymbol{\theta}^\star)
\;=\; -\,K^{-1} G.
\]</div>
<p>The formula above is valid <strong>as long as the active set <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> does not change</strong>. If a constraint switches between active/inactive, the mapping remains piecewise smooth, but the derivative may jump. In MPC, this is exactly why warm-starts are very effective most of the time and occasionally require a refactorization when the active set flips.</p>
<p>In parametric MPC, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> gathers the current state, references, and forecasts. The IFT tells us that, under regularity and a stable active set, the optimal trajectory and first input vary smoothly with <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>. The linear map <span class="math notranslate nohighlight">\(-K^{-1}G\)</span> is exactly the object used in sensitivity-based warm starts and real-time iterations: small changes in <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> can be propagated through a single KKT solve to update the primal–dual guess before taking one or two Newton/SQP steps.</p>
</section>
<section id="predictor-corrector-mpc">
<h3>Predictor-Corrector MPC<a class="headerlink" href="#predictor-corrector-mpc" title="Link to this heading">#</a></h3>
<p>We start with a smooth root-finding problem</p>
<div class="math notranslate nohighlight">
\[
F(z)=0,\qquad F:\mathbb{R}^m\to\mathbb{R}^m.
\]</div>
<p><strong>Newton’s method</strong> iterates</p>
<div class="math notranslate nohighlight">
\[
z^{(t+1)} \;=\; z^{(t)} - \big[\nabla F(z^{(t)})\big]^{-1} F\big(z^{(t)}\big),
\]</div>
<p>or equivalently solves the linearized system</p>
<div class="math notranslate nohighlight">
\[
\nabla F(z^{(t)})\,\Delta z^{(t)} = -F\big(z^{(t)}\big),\qquad z^{(t+1)}=z^{(t)}+\Delta z^{(t)}.
\]</div>
<p>Convergence is local and fast when the Jacobian is nonsingular and the initial guess is close.</p>
<p>Now suppose the root depends on a parameter:</p>
<div class="math notranslate nohighlight">
\[
F\big(z,\theta\big)=0,\qquad \theta\in\mathbb{R}.
\]</div>
<p>We want the solution path <span class="math notranslate nohighlight">\(\theta\mapsto z^\star(\theta)\)</span>. <strong>Numerical continuation</strong> advances <span class="math notranslate nohighlight">\(\theta\)</span> in small steps and uses the previous solution as a warm start for the next Newton solve. This is the simplest and most effective way to “track” solutions of parametric systems.</p>
<p>At a known solution <span class="math notranslate nohighlight">\((z^\star,\theta^\star)\)</span>, differentiate <span class="math notranslate nohighlight">\(F(z^\star(\theta),\theta)=0\)</span> with respect to <span class="math notranslate nohighlight">\(\theta\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\nabla_z F(z^\star,\theta^\star)\,\frac{dz^\star}{d\theta}(\theta^\star) \;+\; \nabla_\theta F(z^\star,\theta^\star) \;=\; 0.
\]</div>
<p>If <span class="math notranslate nohighlight">\(\nabla_z F\)</span> is invertible (IFT conditions), the <strong>tangent</strong> is</p>
<div class="math notranslate nohighlight">
\[
\frac{dz^\star}{d\theta}(\theta^\star) \;=\; -\big[\nabla_z F(z^\star,\theta^\star)\big]^{-1}\,\nabla_\theta F(z^\star,\theta^\star).
\]</div>
<p>This is exactly the <strong>implicit differentiation</strong> formula. Continuation uses it as a <strong>predictor</strong>:</p>
<div class="math notranslate nohighlight">
\[
z_{\text{pred}} \;=\; z^\star(\theta^\star) \;+\; \Delta\theta\;\frac{dz^\star}{d\theta}(\theta^\star).
\]</div>
<p>Then a few <strong>corrector</strong> steps apply Newton to <span class="math notranslate nohighlight">\(F(\,\cdot\,,\theta^\star+\Delta\theta)=0\)</span> starting from <span class="math notranslate nohighlight">\(z_{\text{pred}}\)</span>. If Newton converges quickly, the step <span class="math notranslate nohighlight">\(\Delta\theta\)</span> was appropriate; otherwise reduce <span class="math notranslate nohighlight">\(\Delta\theta\)</span> and retry.</p>
<p>For parametric KKT systems, set <span class="math notranslate nohighlight">\(y=(x,\lambda,\nu)\)</span> and <span class="math notranslate nohighlight">\(F(y,\theta)=0\)</span> the KKT residual with <span class="math notranslate nohighlight">\(\theta\)</span> collecting state, references, forecasts. The <strong>KKT matrix</strong> <span class="math notranslate nohighlight">\(K=\partial F/\partial y\)</span> and <strong>parameter sensitivity</strong> <span class="math notranslate nohighlight">\(G=\partial F/\partial \theta\)</span> give the tangent</p>
<div class="math notranslate nohighlight">
\[
\frac{dy^\star}{d\theta} \;=\; -\,K^{-1}G.
\]</div>
<p>Continuation then becomes:</p>
<ol class="arabic simple">
<li><p><strong>Predictor</strong>: <span class="math notranslate nohighlight">\(y_{\text{pred}} = y^\star + (\Delta\theta)\,(-K^{-1}G)\)</span>.</p></li>
<li><p><strong>Corrector</strong>: a few Newton/SQP steps on the KKT equations at the new <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
</ol>
<p>In MPC, this yields efficient <strong>warm starts</strong> across time: as the parameter <span class="math notranslate nohighlight">\(\theta_t\)</span> (current state, references) changes slightly, we predict the new primal–dual point and correct with 1–2 iterations—often enough to hit tolerance in real time.</p>
</section>
</section>
<section id="application-to-mpc">
<h2>Application to MPC<a class="headerlink" href="#application-to-mpc" title="Link to this heading">#</a></h2>
<p>We now specialize this idea to the structure of finite-horizon MPC. Fix a prediction horizon <span class="math notranslate nohighlight">\(N\)</span>. At each time step, we solve a problem with fixed structure and varying data. Define</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\theta} := (\mathbf{x}_0,\, \mathbf{r},\, \mathbf{w}),
\]</div>
<p>which includes the current state <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>, reference signals <span class="math notranslate nohighlight">\(\mathbf{r}\)</span>, and exogenous forecasts <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>.</p>
<p>The finite-horizon problem becomes</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\min_{z} \quad &amp; J(z;\boldsymbol{\theta}) \\
\text{s.t.} \quad &amp; c(z;\boldsymbol{\theta}) = 0 \\
&amp; d(z;\boldsymbol{\theta}) \leq 0,
\end{aligned}
\end{split}\]</div>
<p>with decision variable <span class="math notranslate nohighlight">\(z = (\mathbf{x}_{0:N}, \mathbf{u}_{0:N-1})\)</span>. The equality constraints enforce dynamics and terminal conditions. The inequalities encode input and state bounds.</p>
<p>Solving <span class="math notranslate nohighlight">\((P_\theta)\)</span> produces an optimal trajectory <span class="math notranslate nohighlight">\(z^\star(\boldsymbol{\theta})\)</span>. The control law is the first input:</p>
<div class="math notranslate nohighlight">
\[
\pi(\boldsymbol{\theta}) := \mathbf{u}_0^\star(\boldsymbol{\theta}).
\]</div>
<p>This mapping from parameter to input defines the MPC policy. Parametric programming helps us understand and exploit its structure to speed up evaluation.</p>
</section>
<section id="two-approaches-to-efficient-mpc">
<h2>Two Approaches to Efficient MPC<a class="headerlink" href="#two-approaches-to-efficient-mpc" title="Link to this heading">#</a></h2>
<p>Parametric structure can be used in two main ways: either to construct an explicit control law offline, or to warm-start the optimizer online using sensitivity information.</p>
<section id="explicit-mpc">
<h3>Explicit MPC<a class="headerlink" href="#explicit-mpc" title="Link to this heading">#</a></h3>
<p>When the problem is a linear or quadratic program with affine dependence on <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>, we can work out the solution symbolically. The parameter space is partitioned into regions <span class="math notranslate nohighlight">\(\mathcal{R}_1, \dots, \mathcal{R}_M\)</span>, each associated with a fixed active set. On each region:</p>
<div class="math notranslate nohighlight">
\[
z^\star(\boldsymbol{\theta}) = A_r\,\boldsymbol{\theta} + b_r,
\qquad
\pi(\boldsymbol{\theta}) = K_r\,\boldsymbol{\theta} + k_r.
\]</div>
<p>At runtime, we identify which region contains <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>, then apply the corresponding affine formula. This approach avoids optimization entirely during deployment.</p>
<p>It requires storing the region definitions and control laws. The number of regions grows with horizon length and constraint count, which limits this approach to systems with low state dimension and short horizons.</p>
</section>
<section id="sensitivity-based-mpc">
<h3>Sensitivity-Based MPC<a class="headerlink" href="#sensitivity-based-mpc" title="Link to this heading">#</a></h3>
<p>When symbolic enumeration is intractable, we can still track how the solution varies locally. Suppose we have a solution <span class="math notranslate nohighlight">\(\bar{y} = (\bar{z}, \bar{\lambda}, \bar{\nu})\)</span> at parameter <span class="math notranslate nohighlight">\(\bar{\boldsymbol{\theta}}\)</span>. The KKT system reads:</p>
<div class="math notranslate nohighlight">
\[
F(y; \boldsymbol{\theta}) = 0.
\]</div>
<p>Differentiating with respect to <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial F}{\partial y}(\bar{y}; \bar{\boldsymbol{\theta}})\, \mathrm{d}y
= - \frac{\partial F}{\partial \boldsymbol{\theta}}(\bar{y}; \bar{\boldsymbol{\theta}})\, \mathrm{d}\boldsymbol{\theta}.
\]</div>
<p>Let <span class="math notranslate nohighlight">\(K\)</span> be the KKT matrix and <span class="math notranslate nohighlight">\(G\)</span> the sensitivity of the residual. Then the sensitivity operator <span class="math notranslate nohighlight">\(T\)</span> satisfies</p>
<div class="math notranslate nohighlight">
\[
K T = -G \quad \Rightarrow \quad \mathrm{d}y = T\, \mathrm{d}\boldsymbol{\theta}.
\]</div>
<p>If <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> changes slightly, we update the primal-dual pair:</p>
<div class="math notranslate nohighlight">
\[
y^{(0)} \leftarrow \bar{y} + T\,\Delta\boldsymbol{\theta},
\]</div>
<p>and use it as the starting point for Newton or SQP.</p>
<p>This is the basis of real-time iteration schemes. When the active set is stable, the warm start is accurate to first order. When it changes, we refactorize and repeat, still with far less effort than solving from scratch.</p>
</section>
</section>
<section id="amortized-optimization-and-neural-approximation-of-controllers">
<h2>Amortized Optimization and Neural Approximation of Controllers<a class="headerlink" href="#amortized-optimization-and-neural-approximation-of-controllers" title="Link to this heading">#</a></h2>
<p>The idea of reusing structure across similar optimization problems is not exclusive to parametric programming. In machine learning, a related concept known as <strong>amortized optimization</strong> aims to reduce the cost of repeated inference by replacing explicit optimization with a function that has been <em>learned</em> to approximate the solution map. This approach shifts the computational burden from online solving to offline training.</p>
<p>The goal is to construct a function <span class="math notranslate nohighlight">\(\hat{\pi}_{\phi}(\boldsymbol{\theta})\)</span>, typically parameterized by a neural network, that maps the input <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> to an approximate solution <span class="math notranslate nohighlight">\(\hat{z}^\star(\boldsymbol{\theta})\)</span> or control action <span class="math notranslate nohighlight">\(\hat{\mathbf{u}}_0^\star(\boldsymbol{\theta})\)</span>. Once trained, this map can be evaluated quickly at runtime, with no need to solve an optimization problem explicitly.</p>
<p>Amortized optimization has emerged in several contexts:</p>
<ul class="simple">
<li><p>In <strong>probabilistic inference</strong>, where variational autoencoders (VAEs) amortize the computation of posterior distributions across a dataset.</p></li>
<li><p>In <strong>meta-learning</strong>, where the objective is to learn a model that generalizes across tasks by internalizing how to adapt.</p></li>
<li><p>In <strong>hyperparameter optimization</strong>, where learning a surrogate model can guide the search over configuration space efficiently.</p></li>
</ul>
<p>This perspective has also begun to influence control. Recent work investigates how to <strong>amortize nonlinear MPC (NMPC)</strong> policies into neural networks. The training data come from solving many instances of the underlying optimal control problem offline. The resulting neural policy <span class="math notranslate nohighlight">\(\hat{\pi}_\phi\)</span> acts as a differentiable, low-latency controller that can generalize to new situations within the training distribution.</p>
<p>Compared to explicit MPC, which partitions the parameter space and stores exact solutions region by region, amortized control smooths over the domain by learning an approximate policy globally. It is less precise, but scalable to high-dimensional problems where enumeration of regions is impossible.</p>
<p>Neural network amortization is advantageous due to the expressivity of these models. However, the challenge is ensuring <strong>constraint satisfaction and safety</strong>, which are hard to guarantee with unconstrained neural approximators. Hybrid approaches attempt to address this by combining a neural warm-start policy with a final projection step, or by embedding the network within a constrained optimization layer. Other strategies include learning structured architectures that respect known physics or control symmetries.</p>
</section>
<section id="imitation-learning-framework">
<h2>Imitation Learning Framework<a class="headerlink" href="#imitation-learning-framework" title="Link to this heading">#</a></h2>
<p>Consider a fixed horizon <span class="math notranslate nohighlight">\(N\)</span> and parameter vector <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> encoding the current state, references, and forecasts. The oracle MPC controller solves</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
z^\star(\boldsymbol{\theta}) \in \arg\min_{z=(x_{0:N},u_{0:N-1})}
&amp;\; J(z;\boldsymbol{\theta})\\
\text{s.t. }&amp; x_{k+1}=f(x_k,u_k;\boldsymbol{\theta}),\quad k=0..N-1,\\
&amp; g(x_k,u_k;\boldsymbol{\theta})\le 0,\; h(x_N;\boldsymbol{\theta})=0.
\end{aligned}
\end{split}\]</div>
<p>The applied action is <span class="math notranslate nohighlight">\(\pi^\star(\boldsymbol{\theta}) := u_0^\star(\boldsymbol{\theta})\)</span>. Our goal is to learn a fast surrogate mapping <span class="math notranslate nohighlight">\(\hat{\pi}_\phi:\boldsymbol{\theta}\mapsto \hat u_0 \approx \pi^\star(\boldsymbol{\theta})\)</span> that can be evaluated in microseconds, optionally followed by a safety projection layer.</p>
<p><strong>Supervised learning from oracle solutions.</strong>
One first samples parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(i)}\)</span> from the operational domain and solves the corresponding NMPC problems offline. The resulting dataset</p>
<div class="math notranslate nohighlight">
\[
\mathcal{D} = \{ (\boldsymbol{\theta}^{(i)},\, u_0^\star(\boldsymbol{\theta}^{(i)})) \}_{i=1}^M
\]</div>
<p>is then used to train a neural network <span class="math notranslate nohighlight">\(\hat{\pi}_\phi\)</span> by minimizing</p>
<div class="math notranslate nohighlight">
\[
\min_\phi \; \frac{1}{M}\sum_{i=1}^M \big\|\hat{\pi}_\phi(\boldsymbol{\theta}^{(i)}) - u_0^\star(\boldsymbol{\theta}^{(i)})\big\|^2 .
\]</div>
<p>Once trained, the network acts as a surrogate for the optimizer, providing instantaneous evaluations that approximate the MPC law.</p>
</section>
<section id="example-propofol-infusion-control">
<h2>Example: Propofol Infusion Control<a class="headerlink" href="#example-propofol-infusion-control" title="Link to this heading">#</a></h2>
<p>This problem explores the control of propofol infusion in total intravenous anesthesia (TIVA). Our presentation follows the problem formulation developped by <span id="id1">Sawaguchi <em>et al.</em> [<a class="reference internal" href="bibliography.html#id19" title="Y. Sawaguchi, E. Furutani, G. Shirakami, M. Araki, and K. Fukuda. A model-predictive hypnosis control system under total intravenous anesthesia. IEEE Transactions on Biomedical Engineering, 55(3):874–887, March 2008. URL: http://dx.doi.org/10.1109/tbme.2008.915670, doi:10.1109/tbme.2008.915670.">38</a>]</span>. The primary objective is to maintain the desired level of unconsciousness while minimizing adverse reactions and ensuring quick recovery after surgery.</p>
<p>The level of unconsciousness is measured by the Bispectral Index (BIS), which is obtained using an electroencephalography (EEG) device. The BIS ranges from <span class="math notranslate nohighlight">\(0\)</span> (complete suppression of brain activity) to <span class="math notranslate nohighlight">\(100\)</span> (fully awake), with the target range for general anesthesia typically between <span class="math notranslate nohighlight">\(40\)</span> and <span class="math notranslate nohighlight">\(60\)</span>.</p>
<p>The goal is to design a control system that regulates the infusion rate of propofol to maintain the BIS within the target range. This can be formulated as an optimal control problem:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\min_{u(t)} &amp; \int_{0}^{T} \left( BIS(t) - BIS_{\text{target}} \right)^2 + \gamma u(t)^2 \, dt \\
\text{subject to:} \\
\dot{x}_1 &amp;= -(k_{10} + k_{12} + k_{13})x_1 + k_{21}x_2 + k_{31}x_3 + \frac{u(t)}{V_1} \\
\dot{x}_2 &amp;= k_{12}x_1 - k_{21}x_2 \\
\dot{x}_3 &amp;= k_{13}x_1 - k_{31}x_3 \\
\dot{x}_e &amp;= k_{e0}(x_1 - x_e) \\
BIS(t) &amp;= E_0 - E_{\text{max}}\frac{x_e^\gamma}{x_e^\gamma + EC_{50}^\gamma}
\end{align*}
\end{split}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(u(t)\)</span> is the propofol infusion rate (mg/kg/h)</p></li>
<li><p><span class="math notranslate nohighlight">\(x_1\)</span>, <span class="math notranslate nohighlight">\(x_2\)</span>, and <span class="math notranslate nohighlight">\(x_3\)</span> are the drug concentrations in different body compartments</p></li>
<li><p><span class="math notranslate nohighlight">\(x_e\)</span> is the effect-site concentration</p></li>
<li><p><span class="math notranslate nohighlight">\(k_{ij}\)</span> are rate constants for drug transfer between compartments</p></li>
<li><p><span class="math notranslate nohighlight">\(BIS(t)\)</span> is the Bispectral Index</p></li>
<li><p><span class="math notranslate nohighlight">\(\gamma\)</span> is a regularization parameter penalizing excessive drug use</p></li>
<li><p><span class="math notranslate nohighlight">\(E_0\)</span>, <span class="math notranslate nohighlight">\(E_{\text{max}}\)</span>, <span class="math notranslate nohighlight">\(EC_{50}\)</span>, and <span class="math notranslate nohighlight">\(\gamma\)</span> are parameters of the pharmacodynamic model</p></li>
</ul>
<p>The specific dynamics model used in this problem is so-called “Pharmacokinetic-Pharmacodynamic Model” and consists of three main components:</p>
<ol class="arabic simple">
<li><p><strong>Pharmacokinetic Model</strong>, which describes how the drug distributes through the body over time. It’s based on a three-compartment model:</p>
<ul class="simple">
<li><p>Central compartment (blood and well-perfused organs)</p></li>
<li><p>Shallow peripheral compartment (muscle and other tissues)</p></li>
<li><p>Deep peripheral compartment (fat)</p></li>
</ul>
</li>
<li><p><strong>Effect Site Model</strong>, which represents the delay between drug concentration in the blood and its effect on the brain.</p></li>
<li><p><strong>Pharmacodynamic Model</strong> that relates the effect-site concentration to the observed BIS.</p></li>
</ol>
<p>The propofol infusion control problem presents several interesting challenges from a research perspective.
First, there is a delay in how fast the drug can reach a different compartments in addition to the BIS measurements which can lag. This could lead to instability if not properly addressed in the control design.</p>
<p>Furthermore, every patient is different from another. Hence, we cannot simply learn a single controller offline and hope that it will generalize to an entire patient population. We will account for this variability through Model Predictive Control (MPC) and dynamically adapt to the model mismatch through replanning. How a patient will react to a given dose of drug also varies and must be carefully controlled to avoid overdoses. This adds an additional layer of complexity since we have to incorporate safety constraints. Finally, the patient might suddenly change state, for example due to surgical stimuli, and the controller must be able to adapt quickly to compensate for the disturbance to the system.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">class</span> <span class="nc">Patient</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">age</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">age</span> <span class="o">=</span> <span class="n">age</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_pk_params</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_pd_params</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">set_pk_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v1</span> <span class="o">=</span> <span class="mf">4.27</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">/</span> <span class="mi">70</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.71</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">age</span> <span class="o">/</span> <span class="mi">30</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.39</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v2</span> <span class="o">=</span> <span class="mf">18.9</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">/</span> <span class="mi">70</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.64</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">age</span> <span class="o">/</span> <span class="mi">30</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.62</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v3</span> <span class="o">=</span> <span class="mi">238</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">/</span> <span class="mi">70</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.95</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cl1</span> <span class="o">=</span> <span class="mf">1.89</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">/</span> <span class="mi">70</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.75</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">age</span> <span class="o">/</span> <span class="mi">30</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.25</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cl2</span> <span class="o">=</span> <span class="mf">1.29</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">/</span> <span class="mi">70</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.62</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cl3</span> <span class="o">=</span> <span class="mf">0.836</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">/</span> <span class="mi">70</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.77</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k10</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cl1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">v1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k12</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cl2</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">v1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k13</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cl3</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">v1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k21</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cl2</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">v2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k31</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cl3</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">v3</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ke0</span> <span class="o">=</span> <span class="mf">0.456</span>

    <span class="k">def</span> <span class="nf">set_pd_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">E0</span> <span class="o">=</span> <span class="mi">100</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Emax</span> <span class="o">=</span> <span class="mi">100</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">EC50</span> <span class="o">=</span> <span class="mf">3.4</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="mi">3</span>

<span class="k">def</span> <span class="nf">pk_model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">patient</span><span class="p">):</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">,</span> <span class="n">xe</span> <span class="o">=</span> <span class="n">x</span>
    <span class="n">dx1</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">patient</span><span class="o">.</span><span class="n">k10</span> <span class="o">+</span> <span class="n">patient</span><span class="o">.</span><span class="n">k12</span> <span class="o">+</span> <span class="n">patient</span><span class="o">.</span><span class="n">k13</span><span class="p">)</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">+</span> <span class="n">patient</span><span class="o">.</span><span class="n">k21</span> <span class="o">*</span> <span class="n">x2</span> <span class="o">+</span> <span class="n">patient</span><span class="o">.</span><span class="n">k31</span> <span class="o">*</span> <span class="n">x3</span> <span class="o">+</span> <span class="n">u</span> <span class="o">/</span> <span class="n">patient</span><span class="o">.</span><span class="n">v1</span>
    <span class="n">dx2</span> <span class="o">=</span> <span class="n">patient</span><span class="o">.</span><span class="n">k12</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">-</span> <span class="n">patient</span><span class="o">.</span><span class="n">k21</span> <span class="o">*</span> <span class="n">x2</span>
    <span class="n">dx3</span> <span class="o">=</span> <span class="n">patient</span><span class="o">.</span><span class="n">k13</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">-</span> <span class="n">patient</span><span class="o">.</span><span class="n">k31</span> <span class="o">*</span> <span class="n">x3</span>
    <span class="n">dxe</span> <span class="o">=</span> <span class="n">patient</span><span class="o">.</span><span class="n">ke0</span> <span class="o">*</span> <span class="p">(</span><span class="n">x1</span> <span class="o">-</span> <span class="n">xe</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">dx1</span><span class="p">,</span> <span class="n">dx2</span><span class="p">,</span> <span class="n">dx3</span><span class="p">,</span> <span class="n">dxe</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">pd_model</span><span class="p">(</span><span class="n">ce</span><span class="p">,</span> <span class="n">patient</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">patient</span><span class="o">.</span><span class="n">E0</span> <span class="o">-</span> <span class="n">patient</span><span class="o">.</span><span class="n">Emax</span> <span class="o">*</span> <span class="p">(</span><span class="n">ce</span> <span class="o">**</span> <span class="n">patient</span><span class="o">.</span><span class="n">gamma</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">ce</span> <span class="o">**</span> <span class="n">patient</span><span class="o">.</span><span class="n">gamma</span> <span class="o">+</span> <span class="n">patient</span><span class="o">.</span><span class="n">EC50</span> <span class="o">**</span> <span class="n">patient</span><span class="o">.</span><span class="n">gamma</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">simulate_step</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">patient</span><span class="p">,</span> <span class="n">dt</span><span class="p">):</span>
    <span class="n">x_next</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">dt</span> <span class="o">*</span> <span class="n">pk_model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">patient</span><span class="p">)</span>
    <span class="n">bis</span> <span class="o">=</span> <span class="n">pd_model</span><span class="p">(</span><span class="n">x_next</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">patient</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_next</span><span class="p">,</span> <span class="n">bis</span>

<span class="k">def</span> <span class="nf">objective</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">patient</span><span class="p">,</span> <span class="n">dt</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">target_bis</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x0</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">total_cost</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">bis</span> <span class="o">=</span> <span class="n">simulate_step</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">u</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">patient</span><span class="p">,</span> <span class="n">dt</span><span class="p">)</span>
        <span class="n">total_cost</span> <span class="o">+=</span> <span class="p">(</span><span class="n">bis</span> <span class="o">-</span> <span class="n">target_bis</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">u</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">return</span> <span class="n">total_cost</span>

<span class="k">def</span> <span class="nf">mpc_step</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">patient</span><span class="p">,</span> <span class="n">dt</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">target_bis</span><span class="p">):</span>
    <span class="n">u0</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>  <span class="c1"># Initial guess</span>
    <span class="n">bounds</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">)]</span> <span class="o">*</span> <span class="n">N</span>  <span class="c1"># Infusion rate between 0 and 20 mg/kg/h</span>
    
    <span class="n">result</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">objective</span><span class="p">,</span> <span class="n">u0</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">patient</span><span class="p">,</span> <span class="n">dt</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">target_bis</span><span class="p">),</span>
                      <span class="n">method</span><span class="o">=</span><span class="s1">&#39;SLSQP&#39;</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">result</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># Return only the first control input</span>

<span class="k">def</span> <span class="nf">run_mpc_simulation</span><span class="p">(</span><span class="n">patient</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">dt</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">target_bis</span><span class="p">):</span>
    <span class="n">steps</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">T</span> <span class="o">/</span> <span class="n">dt</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">steps</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">bis</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">steps</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">steps</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
        <span class="c1"># Add noise to the current state to simulate real-world uncertainty</span>
        <span class="n">x_noisy</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
        
        <span class="c1"># Use noisy state for MPC planning</span>
        <span class="n">u</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">mpc_step</span><span class="p">(</span><span class="n">x_noisy</span><span class="p">,</span> <span class="n">patient</span><span class="p">,</span> <span class="n">dt</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">target_bis</span><span class="p">)</span>
        
        <span class="c1"># Evolve the true state using the deterministic model</span>
        <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">bis</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">simulate_step</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">u</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">patient</span><span class="p">,</span> <span class="n">dt</span><span class="p">)</span>
    
    <span class="n">bis</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd_model</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">patient</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">bis</span><span class="p">,</span> <span class="n">u</span>

<span class="c1"># Set up the problem</span>
<span class="n">patient</span> <span class="o">=</span> <span class="n">Patient</span><span class="p">(</span><span class="n">age</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="mi">70</span><span class="p">)</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">120</span>  <span class="c1"># Total time in minutes</span>
<span class="n">dt</span> <span class="o">=</span> <span class="mf">0.5</span>  <span class="c1"># Time step in minutes</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c1"># Prediction horizon</span>
<span class="n">target_bis</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># Target BIS value</span>

<span class="c1"># Run MPC simulation</span>
<span class="n">x</span><span class="p">,</span> <span class="n">bis</span><span class="p">,</span> <span class="n">u</span> <span class="o">=</span> <span class="n">run_mpc_simulation</span><span class="p">(</span><span class="n">patient</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">dt</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">target_bis</span><span class="p">)</span>

<span class="c1"># Plot results</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">T</span><span class="o">+</span><span class="n">dt</span><span class="p">,</span> <span class="n">dt</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">,</span> <span class="n">ax3</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">bis</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;BIS&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">target_bis</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>

<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">u</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Infusion Rate (mg/kg/h)&#39;</span><span class="p">)</span>

<span class="n">ax3</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Effect-site Concentration (µg/mL)&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Time (min)&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Initial BIS: </span><span class="si">{</span><span class="n">bis</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Final BIS: </span><span class="si">{</span><span class="n">bis</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean infusion rate: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">u</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> mg/kg/h&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Final effect-site concentration: </span><span class="si">{</span><span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> µg/mL&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="_images/50134a776d25022facf5fa2471e9bbae9d94c74cfb4ee2b6b2f7c9898c044498.png" src="_images/50134a776d25022facf5fa2471e9bbae9d94c74cfb4ee2b6b2f7c9898c044498.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Initial BIS: 100.00
Final BIS: 50.04
Mean infusion rate: 8.85 mg/kg/h
Final effect-site concentration: 3.40 µg/mL
</pre></div>
</div>
</div>
</div>
<!-- ### Deployment Patterns

There are several ways to use an amortized controller once it has been trained. The simplest option is **direct amortization**, where the control input is taken to be $u = \hat{\pi}_\phi(\boldsymbol{\theta})$. In this case, the neural network provides the control action directly, with no optimization performed during deployment.

A second option is **amortization with projection**, where the network output $\tilde u = \hat{\pi}_\phi(\boldsymbol{\theta})$ is passed through a small optimization step, such as a quadratic program or barrier-function filter, in order to enforce constraints. This adds a negligible computational overhead but restores guarantees of feasibility and safety.

We could for example integrate a convex approximation of the MPC subproblem directly as a differentiable layer inside the network. The network proposes a candidate action $\tilde u$, which is then corrected through a small quadratic program:

$$
u = \arg\min_v \tfrac12\|v-\tilde u\|^2 \quad \text{s.t. } g(x,v)\le 0.
$$

Gradients are propagated through this correction using implicit differentiation, allowing the network to be trained end-to-end while retaining constraint satisfaction. This hybrid keeps the fast evaluation of a learned map while preserving the structure of MPC.

A third option is **amortized warm-starting**, where the neural network provides an initialization for one or two Newton or SQP iterations of the underlying NMPC problem. In this setting, the learned map delivers an excellent starting point, so the optimizer converges quickly and the cost of re-solving at each time step is greatly reduced. -->
<!-- ## Demo: Batch Bioreactor MPC with do-mpc

We illustrate nonlinear MPC on a fed-batch bioreactor. The process has four states: biomass concentration \(X_s\), substrate \(S_s\), product \(P_s\), and liquid volume \(V_s\). The manipulated feed flow \(u_{\text{inp}}\) augments volume and changes concentrations. The dynamics are

$$
\begin{aligned}
\dot X_s &= \mu(S_s)X_s - \tfrac{u_{\text{inp}}}{V_s} X_s, \\
\dot S_s &= -\tfrac{\mu(S_s)X_s}{Y_x} - \tfrac{v X_s}{Y_p} + \tfrac{u_{\text{inp}}}{V_s}(S_{\text{in}} - S_s), \\
\dot P_s &= v X_s - \tfrac{u_{\text{inp}}}{V_s} P_s, \\
\dot V_s &= u_{\text{inp}},
\end{aligned}
$$

with inhibited Monod kinetics

$$
\mu(S_s) = \frac{\mu_m S_s}{K_m + S_s + S_s^2/K_i}.
$$

We impose bounds on states and input, e.g. \(0 \le X_s \le 3.7\), \(0 \le P_s \le 3.0\), and \(0 \le u_{\text{inp}} \le 0.2\). Two parameters are uncertain: yield \(Y_x\) and inlet concentration \(S_{\text{in}}\). We treat them via a small scenario set with non-anticipativity (a single input sequence is shared across scenarios).

At each MPC step, we solve a finite-horizon problem that encourages product formation while regularizing effort:

$$
\min_{x_{0:N},u_{0:N-1}} \; -\,P_s(N) + \sum_{k=0}^{N-1} \big( -\,P_s(k) + \rho\, u_k^2 \big)
$$

subject to the discretized dynamics and box constraints for all uncertainty scenarios, sharing the inputs across scenarios. The continuous-time ODEs are discretized by orthogonal collocation on finite elements, producing an NLP [orthogonal collocation](https://www.do-mpc.com/en/latest/theory_orthogonal_collocation.html). The resulting NMPC is re-solved in a receding-horizon loop [MPC basics](https://www.do-mpc.com/en/latest/theory_mpc.html).

The cell below runs the closed-loop simulation and plots the states and input. The script is adapted from the do-mpc Batch Bioreactor example.

```{code-cell} ipython3
:tags: [hide-input]
:load: _static/do_mpc_batch_bioreactor.py
```

### Interactive Animation

The following cell creates an interactive animation of the batch bioreactor control process, showing the MPC predictions and the evolution of the system states in real-time. The visualization includes a tank representation with liquid level and biomass particles, along with time-series plots of all states and control inputs.

```{code-cell} ipython3
:tags: [hide-input]
:load: _static/do_mpc_batch_bioreactor_animated.py
``` --></section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="cocp.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Trajectory Optimization in Continuous Time</p>
      </div>
    </a>
    <a class="right-next"
       href="dp.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Dynamic Programming</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Model Predictive Control</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#closing-the-loop-by-replanning">Closing the Loop by Replanning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-receding-horizon-principle">The Receding Horizon Principle</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#horizon-selection-and-problem-formulation">Horizon Selection and Problem Formulation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#infinite-horizon-regulation">Infinite-Horizon Regulation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#finite-duration-tasks">Finite-Duration Tasks</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#periodic-tasks">Periodic Tasks</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-mpc-algorithm">The MPC Algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-dynamic-programming">Connection to Dynamic Programming</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#successive-linearization-and-quadratic-approximations">Successive Linearization and Quadratic Approximations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#theoretical-guarantees">Theoretical Guarantees</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-terminal-ingredients-in-practice">Computing Terminal Ingredients in Practice</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-implications">Performance Implications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#suboptimality-bounds">Suboptimality Bounds</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#the-landscape-of-mpc-variants">The Landscape of MPC Variants</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tracking-mpc">Tracking MPC</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regulatory-mpc">Regulatory MPC</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#economic-mpc">Economic MPC</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#robust-mpc">Robust MPC</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-mpc">Stochastic MPC</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hybrid-and-mixed-integer-mpc">Hybrid and Mixed-Integer MPC</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-and-decentralized-mpc">Distributed and Decentralized MPC</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#adaptive-and-learning-based-mpc">Adaptive and Learning-Based MPC</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#robustness-in-real-time-mpc">Robustness in Real-Time MPC</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-wind-farm-yield-optimization">Example: Wind Farm Yield Optimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#softening-constraints-through-slack-variables">Softening Constraints Through Slack Variables</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feasibility-restoration">Feasibility Restoration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reference-governors">Reference Governors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backup-controllers">Backup Controllers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cascade-architectures">Cascade Architectures</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maintaining-solution-continuity">Maintaining Solution Continuity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-chemical-reactor-control-under-failure">Example: Chemical Reactor Control Under Failure</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#computational-efficiency-via-parametric-programming">Computational Efficiency via Parametric Programming</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#general-framework-parametric-optimization">General Framework: Parametric Optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-quick-reminder-the-implicit-function-theorem">A quick reminder: the implicit function theorem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictor-corrector-mpc">Predictor-Corrector MPC</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#application-to-mpc">Application to MPC</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#two-approaches-to-efficient-mpc">Two Approaches to Efficient MPC</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explicit-mpc">Explicit MPC</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sensitivity-based-mpc">Sensitivity-Based MPC</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#amortized-optimization-and-neural-approximation-of-controllers">Amortized Optimization and Neural Approximation of Controllers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#imitation-learning-framework">Imitation Learning Framework</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-propofol-infusion-control">Example: Propofol Infusion Control</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Pierre-Luc Bacon
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>